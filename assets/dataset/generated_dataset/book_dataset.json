{
    "cap-introducao": [
        {
            "pergunta": "O que é Processamento de Linguagem Natural (PLN) e qual é o objetivo principal dessa área de pesquisa?",
            "resposta": "O Processamento de Linguagem Natural (PLN) é um campo de pesquisa que tem como objetivo investigar e propor métodos e sistemas de processamento computacional da linguagem humana. O objetivo principal é desenvolver sistemas capazes de processar e entender a linguagem humana de forma eficaz."
        },
        {
            "pergunta": "Qual é a relação entre o PLN e a Inteligência Artificial (IA)?",
            "resposta": "O PLN está ligado à área de Inteligência Artificial (IA) e também está intrinsecamente relacionada à Linguística Computacional. Isso significa que o PLN utiliza técnicas e conceitos da IA para desenvolver sistemas capazes de processar e entender a linguagem humana."
        },
        {
            "pergunta": "Quais são as principais subáreas do PLN?",
            "resposta": "O PLN se divide em duas grandes subáreas: Interpretação (ou Compreensão) de Linguagem Natural – NLU e Geração de Linguagem Natural – NLG. A NLU se refere ao processamento que visa à análise e à interpretação da língua, enquanto a NLG se refere à geração de linguagem natural."
        },
        {
            "pergunta": "O que é a Interpretação (ou Compreensão) de Linguagem Natural – NLU?",
            "resposta": "A NLU é a subárea do PLN que se refere ao processamento que visa à análise e à interpretação da língua. Isso inclui a segmentação e classificação dos componentes linguísticos, bem como a tentativa de apreender significados construídos pelo ser humano."
        },
        {
            "pergunta": "O que é a Geração de Linguagem Natural – NLG?",
            "resposta": "A NLG é a subárea do PLN que se refere à geração de linguagem natural. Isso inclui a geração de respostas ao usuário dos chatbots, bem como a decisão do que responder e como apresentar essa resposta ao usuário."
        },
        {
            "pergunta": "Quais são os principais conceitos utilizados no PLN?",
            "resposta": "Os principais conceitos utilizados no PLN incluem aplicações, recursos e ferramentas. As aplicações fornecem um resultado ao usuário tendo uma entrada (input) ou saída (output) em linguagem natural. As ferramentas auxiliam na construção de uma aplicação, enquanto os recursos fornecem informações linguísticas necessárias para que as aplicações consigam processar a língua da maneira adequada."
        },
        {
            "pergunta": "Qual é o objetivo do livro?",
            "resposta": "O objetivo do livro é aumentar gradativamente a complexidade do tratamento da língua no PLN, com foco no português brasileiro. Isso inclui apresentar o objeto de pesquisa, os principais paradigmas do PLN e destacar os principais pontos apresentados no capítulo."
        },
        {
            "pergunta": "Qual é a importância da linguagem para o ser humano?",
            "resposta": "A linguagem é fundamental para o ser humano, pois permite que ele se expresse, comunique e represente sua realidade de forma única. Ela é um dos principais fatores que distinguem o ser humano dos outros seres vivos. A capacidade de criar significados, expressar-se e ser compreendida é um dos grandes avanços no desenvolvimento de uma criança."
        },
        {
            "pergunta": "Como a linguagem escrita é definida no contexto do Processamento de Linguagem Natural (PLN)?",
            "resposta": "No contexto do PLN, a linguagem escrita é definida como texto, que se refere a sequências de caracteres representados de forma grafológica, os quais constroem significados para nós humanos. Isso distingue a linguagem escrita da linguagem oral, que é chamada de fala."
        },
        {
            "pergunta": "Quais são as diferentes dimensões da linguagem humana?",
            "resposta": "A linguagem humana se organiza em diferentes dimensões, incluindo a fonética, fonologia, morfologia, sintaxe, semântica, pragmática e discurso. Cada uma dessas dimensões estuda um aspecto específico da linguagem, desde os sons e sua organização até o significado de palavras e frases e como elas são utilizadas na interação."
        },
        {
            "pergunta": "Como as subáreas da linguística se relacionam entre si?",
            "resposta": "As subáreas da linguística se relacionam entre si de forma concêntrica, com cada uma delas estudando um aspecto específico da linguagem. A fonética e a fonologia estudam os sons e sua organização, a morfologia estuda como os morfemas se organizam para formar palavras, a sintaxe estuda como as palavras se organizam em estruturas para formar sintagmas e orações, e assim por diante. Cada subárea se apoia nas outras para entender a linguagem como um todo."
        },
        {
            "pergunta": "Quais são os desafios do PLN em relação às subáreas da linguística?",
            "resposta": "Os desafios do PLN em relação às subáreas da linguística incluem a necessidade de considerar conhecimentos de várias subáreas ao mesmo tempo para processar a linguagem de forma eficaz. Por exemplo, no processamento morfossintático, informações morfológicas e sintáticas são consideradas para determinar a categoria gramatical de uma palavra. Além disso, as especificidades de cada língua, como as regras de combinação de caracteres e a estrutura sintática, também precisam ser consideradas."
        },
        {
            "pergunta": "Por que é importante considerar as especificidades de cada língua no PLN?",
            "resposta": "É importante considerar as especificidades de cada língua no PLN porque elas determinam como os caracteres podem ser combinados para compor uma palavra, como as palavras se organizam em estruturas para formar sintagmas e orações, e assim por diante. Se essas especificidades não forem consideradas, o processamento da linguagem pode ser ineficaz ou até mesmo errado."
        },
        {
            "pergunta": "O que é o paradigma simbólico no Processamento de Linguagem Natural (PLN) e como ele era utilizado até a década de 1980?",
            "resposta": "O paradigma simbólico é uma abordagem no PLN que se baseia em expressar explicitamente todo o conhecimento sobre a língua em formalismos como léxicos, regras e linguagens lógicas. Isso significa que as regras da língua são escritas de forma explícita e compreensível para os humanos. Por exemplo, é possível escrever regras que determinem a concordância entre o gênero gramatical de um substantivo e o gênero de um adjetivo que o acompanha. Essa abordagem foi amplamente utilizada até a década de 1980."
        },
        {
            "pergunta": "Como o paradigma estatístico surgiu e como ele difere do paradigma simbólico?",
            "resposta": "O paradigma estatístico surgiu no início dos anos 1990, quando as máquinas ganharam mais capacidade de memória e processamento. Essa abordagem se baseia em grandes conjuntos de textos (corpus) para aprender regras e padrões da língua. Diferentemente do paradigma simbólico, o paradigma estatístico não utiliza regras explícitas, mas sim modelos probabilísticos aprendidos a partir da frequência de ocorrência de fenômenos linguísticos. Isso significa que a língua é representada por probabilidades calculadas a partir de exemplos, em vez de regras explícitas."
        },
        {
            "pergunta": "O que são Redes Neurais Profundas (Deep Learning) e como elas são utilizadas no PLN?",
            "resposta": "Redes Neurais Profundas (Deep Learning) são uma classe de algoritmos de aprendizado de máquina que se baseiam em várias camadas de unidades de processamento para reconhecer padrões recorrentes em dados. No PLN, as Redes Neurais Profundas são utilizadas para aprender modelos de linguagem a partir de grandes conjuntos de textos. Diferentemente de outras técnicas de aprendizado de máquina, as Redes Neurais Profundas não especificam como o aprendizado deve ocorrer, mas sim permitem que o modelo aprenda a partir da complexidade das arquiteturas compostas por diversas camadas de processamento."
        },
        {
            "pergunta": "Quais são as principais diferenças entre o paradigma neural e o paradigma estatístico no PLN?",
            "resposta": "As principais diferenças entre o paradigma neural e o paradigma estatístico no PLN são: (1) a forma como o aprendizado é realizado: no paradigma estatístico, os algoritmos especificam como o aprendizado deve ocorrer, enquanto no paradigma neural, a complexidade das arquiteturas compostas por diversas camadas de processamento não permite saber exatamente como o aprendizado ocorre; (2) a representação do conhecimento: no paradigma estatístico, o conhecimento é representado por modelos probabilísticos, enquanto no paradigma neural, o conhecimento é representado por valores numéricos; (3) a explicabilidade: no paradigma estatístico, é possível entender como o modelo foi aprendido, enquanto no paradigma neural, o código é opaco e o efeito não é previsível."
        },
        {
            "pergunta": "Quais são as principais aplicações do PLN na atualidade?",
            "resposta": "As principais aplicações do PLN na atualidade incluem: (1) mineração de textos; (2) recuperação de informação; (3) ciência de dados; (4) tradução automática; (5) classificação de textos; (6) resumo de textos; (7) geração de textos. Além disso, o PLN tem intersecção com diversos campos de pesquisa e de aplicação no mercado de trabalho, como inteligência artificial, processamento de linguagem natural, ciência de dados e mineração de textos."
        }
    ],
    "cap-fala": [
        {
            "pergunta": "Qual é a importância da fala na interação humana?",
            "resposta": "A fala é o principal instrumento para a troca de informações e de coesão social. É através da fala que expressamos nossas emoções, a nossa atitude em relação a fatos e eventos, bem como negociamos ideias e ações. A capacidade linguística nos diferencia de outras espécies, mas é a fala, e o que ela nos proporciona, que nos identifica como humanos."
        },
        {
            "pergunta": "Quais são as habilidades exclusivas da linguagem humana?",
            "resposta": "A chamada “dupla articulação” presente na linguagem humana é uma habilidade exclusiva da nossa espécie. Ela se caracteriza por ser a articulação entre unidades significativas (morfemas) e fonemas, que são elementos finitos que se combinam de forma variada, criando infinitas possibilidades de morfemas."
        },
        {
            "pergunta": "Como evoluiu o processamento de língua falada em português?",
            "resposta": "O processamento de língua falada em português era bastante limitado devido à falta de recursos computacionais e técnicas apropriadas. No entanto, com o avanço da tecnologia e o aumento do poder computacional, novas técnicas e abordagens foram desenvolvidas, resultando em avanços significativos nessa área. A partir da década de 1990, técnicas baseadas em estatística começaram a ganhar popularidade. Esses modelos estatísticos utilizam algoritmos de aprendizado de máquina, como as redes neurais artificiais, para melhorar o desempenho do processamento de língua falada em português."
        },
        {
            "pergunta": "Quais são os desafios no desenvolvimento de sistemas de processamento de língua falada?",
            "resposta": "Os desafios incluem robustez, flexibilidade, facilidade de integração e eficiência de engenharia. Além disso, é preciso que haja datasets e corpora de fala de alta qualidade para que se alcancem bons resultados no processamento computacional da fala."
        },
        {
            "pergunta": "Qual é o papel dos modelos de linguagem neural no processamento de língua falada?",
            "resposta": "Os modelos de linguagem neural, como os modelos de transformação de sequência a sequência (Seq2Seq) e as redes neurais convolucionais (CNNs) e recorrentes (RNNs), têm oferecido resultados impressionantes em várias tarefas de processamento de língua falada, como reconhecimento automático de fala, tradução automática de fala e resumo automático de áudio."
        },
        {
            "pergunta": "Como é feita a digitalização da fala?",
            "resposta": "O processo de digitalização da fala envolve a conversão do sinal analógico das ondas sonoras em um formato digital que pode ser armazenado e manipulado por um computador. Isso é normalmente feito usando-se um conversor analógico-digital (CAD), que amostra, isto é, faz uma amostragem da onda sonora em intervalos regulares e converte cada amostra em um número binário."
        },
        {
            "pergunta": "Quais são os componentes necessários para um sistema computacional de língua falada?",
            "resposta": "Um sistema computacional para a língua falada necessita de capacidades tanto de reconhecimento quanto de síntese de fala. Além disso, um componente de compreensão e diálogo é necessário para a interação com o usuário; o conhecimento de domínio é necessário para guiar a interpretação da fala pelo sistema e permitir que ele determine a ação apropriada."
        },
        {
            "pergunta": "Qual é o papel da língua falada na comunicação humana?",
            "resposta": "A língua falada é utilizada para diversas funções que se estabelecem entre falantes e ouvintes, permitindo a troca de informações e a construção de significados. Ela envolve a produção e a percepção de sons, que são processados pelo cérebro para criar uma representação simbólica da linguagem. A língua falada é fundamental para a comunicação humana, pois permite que as pessoas se expressem e sejam compreendidas por outras."
        },
        {
            "pergunta": "Como se inicia o processo de produção da fala?",
            "resposta": "O processo de produção da fala começa com uma intenção (volição) de comunicação no cérebro do falante. Essa intenção ativa movimentos musculares para a produção de sons, que são então processados pelo sistema de linguagem para criar uma sequência de palavras. A produção da fala envolve a coordenação de vários componentes, incluindo a articulação, a fonética e a fonologia."
        },
        {
            "pergunta": "Qual é o papel do ouvinte no processo de comunicação?",
            "resposta": "O ouvinte recebe os sinais sonoros da fala e os processa para transformá-los em sinais neurológicos que o cérebro pode compreender. O ouvinte também desempenha um papel ativo na comunicação, pois ele deve interpretar os sons e criar uma representação simbólica da linguagem. A compreensão da fala envolve a análise de frequência, a transdução neural e a extração de recursos."
        },
        {
            "pergunta": "Como se dá a relação entre a produção e a compreensão da fala?",
            "resposta": "A produção e a compreensão da fala são processos interconectados. A produção da fala envolve a criação de uma sequência de sons que são então processados pelo ouvinte para criar uma representação simbólica da linguagem. A compreensão da fala, por sua vez, envolve a análise dos sons e a criação de uma representação simbólica da linguagem. A relação entre a produção e a compreensão da fala é fundamental para a comunicação humana."
        },
        {
            "pergunta": "Qual é o papel da sintaxe, semântica e estrutura informacional na língua falada?",
            "resposta": "A sintaxe, semântica e estrutura informacional são componentes fundamentais da língua falada. A sintaxe refere-se à estrutura das frases e à relação entre as palavras. A semântica refere-se ao significado das palavras e à relação entre elas. A estrutura informacional refere-se à organização da informação na língua falada. Esses componentes trabalham juntos para criar uma representação simbólica da linguagem que pode ser compreendida pelo ouvinte."
        }
    ],
    "cap-recursos-fala": [
        {
            "pergunta": "Qual era a quantidade de dados de fala públicos ou abertos para pesquisas acadêmicas em português brasileiro até a metade de 2020?",
            "resposta": "Até a metade de 2020, o português brasileiro possuía apenas algumas dezenas de horas de dados de fala públicos ou abertos para pesquisas acadêmicas. Para o treinamento de modelos de reconhecimento de fala, havia aproximadamente 60 horas, divididas em quatro pequenos conjuntos de dados de fala lida. Para o treinamento de modelos de síntese de fala, havia um conjunto de dados de um único locutor com 10 horas e 28 minutos de fala."
        },
        {
            "pergunta": "Quais são os principais conjuntos de dados de fala lida em português brasileiro utilizados para treinamento de modelos de reconhecimento de fala?",
            "resposta": "Os principais conjuntos de dados de fala lida em português brasileiro utilizados para treinamento de modelos de reconhecimento de fala são: o Common Voice Corpus versão 5.1, o dataset Sid, o VoxForge e o LapsBM 1.4. Além disso, o projeto TaRSila criou o corpus CORAA ASR versão 1.1, composto por quatro corpora disponíveis na literatura, que foram validados para a tarefa de ASR, e uma coleção de TeD Talks, totalizando aproximadamente 290 horas."
        },
        {
            "pergunta": "O que é o projeto TaRSila e qual é o seu objetivo?",
            "resposta": "O projeto TaRSila é um projeto do Center for Artificial Intelligence da Universidade de São Paulo, financiado pela IBM e FAPESP, que visa aumentar os conjuntos de dados de fala em português brasileiro tanto para treinamento de sistemas como também para pesquisas linguísticas nas seguintes tarefas do processamento de fala: reconhecimento de fala, síntese de fala, reconhecimento de emoções, diarização, restauração de fala e verificação de locutor."
        },
        {
            "pergunta": "Quais são os recursos de fala criados no projeto TaRSila?",
            "resposta": "Os recursos de fala criados no projeto TaRSila incluem: o TTS-Portuguese Corpus, corpus para treinamento de modelos de síntese de fala, criado e disponibilizado no início de 2020 com a fala de um único locutor; o corpus CORAA NURC-SP, que contém 334 horas de fala espontânea e fala preparada de falantes de São Paulo; o corpus CORAA ASR versão 1.1, composto por quatro corpora disponíveis na literatura, que foram validados para a tarefa de ASR, e uma coleção de TeD Talks; o CORAA SER versão 1.0, composto por aproximadamente 50 minutos de segmentos de áudio rotulados em três classes: neutro, não neutro feminino e não neutro masculino; e o corpus do Museu da Pessoa (MuPe), com 300 horas de áudios de histórias de vida e transcrições com pontuação."
        },
        {
            "pergunta": "Qual é o papel do alinhamento de áudio-transcrição no treinamento de modelos de reconhecimento de fala?",
            "resposta": "O alinhamento de áudio-transcrição é fundamental para o treinamento de modelos de reconhecimento de fala, pois permite que o modelo aprenda a associar os sons da fala com as transcrições correspondentes. Esse alinhamento é feito manualmente ou por meio de técnicas de processamento de fala, e é essencial para a avaliação do desempenho de um reconhecedor de fala."
        },
        {
            "pergunta": "Qual foi o grande avanço que permitiu a popularização e aprimoramento de assistentes virtuais?",
            "resposta": "O grande avanço foi a aplicação do deep learning (Goodfellow et al., 2016) na área de síntese de fala, que permitiu a popularização e aprimoramento de assistentes virtuais, como Apple Siri, Amazon Alexa e Google Home."
        },
        {
            "pergunta": "Quais são os principais componentes de um sistema de síntese de fala tradicional?",
            "resposta": "Os principais componentes de um sistema de síntese de fala tradicional incluem um analisador de texto, um analisador fonético, um estimador de duração, um modelo acústico e um vocoder."
        },
        {
            "pergunta": "Qual é o principal desafio dos modelos baseados em deep learning para síntese de fala?",
            "resposta": "O principal desafio dos modelos baseados em deep learning para síntese de fala é a necessidade de uma grande quantidade de dados para treinamento, o que pode ser um desafio para idiomas com poucos recursos disponíveis."
        },
        {
            "pergunta": "Quais são os principais corpora utilizados para treinar modelos de síntese de fala em inglês?",
            "resposta": "Os principais corpora utilizados para treinar modelos de síntese de fala em inglês incluem o VCTK, LJ Speech, LibriTTS e LibriTTS-R."
        },
        {
            "pergunta": "Qual é o corpus TTS-Portuguese Corpus e como foi criado?",
            "resposta": "O corpus TTS-Portuguese Corpus é um corpus de síntese de fala em português brasileiro, criado em 2019, que consiste em 71358 palavras faladas, 13311 palavras únicas, resultando em 3632 arquivos de áudio e totalizando 10 horas e 28 minutos de fala. O corpus foi criado utilizando textos de domínio público, incluindo artigos da Wikipédia e sentenças foneticamente balanceadas."
        },
        {
            "pergunta": "Quais são os principais desafios dos corpora CETUC e Multilingual LibriSpeech para síntese de fala?",
            "resposta": "Os principais desafios dos corpora CETUC e Multilingual LibriSpeech para síntese de fala são a baixa taxa de amostragem (16 kHz) e a falta de pontuação nos textos, o que dificulta a aplicação desses corpora para síntese de fala."
        },
        {
            "pergunta": "Qual é o corpus CML-TTS e como foi criado?",
            "resposta": "O corpus CML-TTS é um corpus de síntese de fala baseado no corpus Multilingual LibriSpeech (MLS), adaptado para treinamento de modelos de síntese de fala. O CML-TTS é composto por audiolivros em sete idiomas, incluindo o português, e foi criado recriando o corpus MLS mantendo a pontuação e os áudios com uma taxa de amostragem de 24 kHz."
        },
        {
            "pergunta": "Quais são os principais recursos utilizados em aplicações que consideram fronteiras prosódicas para o inglês?",
            "resposta": "Os principais recursos utilizados em aplicações que consideram fronteiras prosódicas para o inglês são o Santa Barbara Corpus of Spoken American English e o Boston University Radio Speech Corpus. O primeiro contém aproximadamente 20 horas de fala espontânea de gêneros variados, transcritas e segmentadas manualmente em unidades entoacionais final e não final. Já o segundo contém 10 horas de notícias de rádio, das quais 3,5 horas estão prosodicamente anotadas de acordo com o sistema ToBI."
        },
        {
            "pergunta": "Quais são os principais recursos utilizados em aplicações que consideram fronteiras prosódicas para o português brasileiro?",
            "resposta": "Os principais recursos utilizados em aplicações que consideram fronteiras prosódicas para o português brasileiro são o C-ORAL–Brasil e o CORAA NURC-SP. O C-ORAL–Brasil é um corpus de fala espontânea que contém excertos de fala monológica masculina, provenientes dos corpora anotados C-ORAL–Brasil I e II. Já o CORAA NURC-SP é um corpus de fala que contém aproximadamente 334 horas de fala transcrita, das quais pelo menos 40 horas serão prosodicamente anotadas."
        },
        {
            "pergunta": "Qual é o conceito de unidade prosódica utilizado no Corpus Mínimo do NURC-SP?",
            "resposta": "O conceito de unidade prosódica utilizado no Corpus Mínimo do NURC-SP está fundamentado nos princípios do método de segmentação prosódica do C-ORAL–BRASIL. No fluxo da fala, podemos reconhecer fronteiras de unidades com valores terminais ou não terminais. Quebras prosódicas terminais (TB, terminal break) marcam sequências terminadas, ou seja, comunicam a conclusão do enunciado, formando a menor unidade pragmaticamente autônoma da fala, enquanto quebras prosódicas não terminais (NTB, non-terminal break) sinalizam uma unidade prosódica não autônoma e cuja informação não está concluída dentro de um mesmo enunciado."
        },
        {
            "pergunta": "Quais são as principais pistas para uma quebra prosódica no português brasileiro?",
            "resposta": "As principais pistas para uma quebra prosódica no português brasileiro são a inserção de pausas e mudanças relacionadas à frequência fundamental e à duração. A identificação das quebras prosódicas é baseada principalmente na relevância perceptiva (auditiva) das pistas prosódicas, mas também na inspeção visual da síntese do sinal acústico fornecida pelo Praat."
        },
        {
            "pergunta": "Qual é a relevância de um corpus de português brasileiro processado e anotado prosodicamente?",
            "resposta": "A relevância de um corpus de português brasileiro processado e anotado prosodicamente está no fato de que a delimitação de fronteiras prosódicas melhora o desempenho de sistemas de processamento de línguas naturais e é input para a predição de pontuações automáticas. Além disso, é possível usar tal corpus como um conjunto de referência para o treinamento de sistemas automáticos de reconhecimento de fala espontânea, detecção de sotaques e parsing, e assim alavancar o desenvolvimento de métodos de processamento de fala do português brasileiro e viabilizar novos estudos linguísticos."
        },
        {
            "pergunta": "Quais são os principais corpora para a criação de sistemas de reconhecimento de fala voltados para a língua portuguesa?",
            "resposta": "Os principais corpora para a criação de sistemas de reconhecimento de fala voltados para a língua portuguesa incluem CORAA ASR, Common Voice, MultiLingual LibriSpeech, MultiLingual TeDx Corpus, Spotify, Multilingual Spoken Corpus, LapsBM, Sidney, VoxForge, CoVoST e Vox Populi. Esses corpora fornecem uma variedade de recursos para treinar e testar sistemas de reconhecimento de fala, incluindo áudios, transcrições e traduções."
        },
        {
            "pergunta": "O que é o CORAA ASR e como ele se diferencia de outros corpora?",
            "resposta": "O CORAA ASR é um corpus para reconhecimento automático de fala que contém também fala espontânea, um tópico pouco pesquisado em projetos similares. Ele se diferencia de outros corpora por ser uma junção de cinco projetos independentes e por conter fala espontânea, o que é mais difícil de ser reconhecida do que a fala preparada."
        },
        {
            "pergunta": "Qual é o objetivo do projeto Common Voice e como ele funciona?",
            "resposta": "O objetivo do projeto Common Voice é criar uma grande base colaborativa de áudios e transcrições para várias línguas, incluindo o português. O projeto funciona permitindo que os usuários contribuam para o crescimento da base e acessem áudios de outras pessoas. Os usuários podem doar áudios em suas próprias vozes e revisar doações de outros usuários."
        },
        {
            "pergunta": "O que é o MultiLingual LibriSpeech e como ele pode ser usado em reconhecimento de fala?",
            "resposta": "O MultiLingual LibriSpeech é um corpus que foi pensado para aplicações em síntese e reconhecimento de fala. Ele pode ser usado em reconhecimento de fala combinado com outros recursos, visto que possui relativamente poucos falantes. Além disso, o corpus pode ser usado para treinar modelos de reconhecimento de fala em cenários com pouco ruído."
        },
        {
            "pergunta": "Qual é o objetivo do projeto MultiLingual TeDx Corpus e como ele se diferencia de outros corpora?",
            "resposta": "O objetivo do projeto MultiLingual TeDx Corpus é permitir pesquisas nas áreas de reconhecimento automático da fala e tradução da fala para texto. Ele se diferencia de outros corpora por ser composto por palestras sobre os mais variados assuntos e por ter traduções das transcrições para as línguas inglesa e espanhola."
        },
        {
            "pergunta": "O que é o corpus Spotify e como ele pode ser usado em reconhecimento de fala?",
            "resposta": "O corpus Spotify é um corpus que foi lançado primeiramente para a língua inglesa e posteriormente incorporou o português. Ele oferece diversos áudios para a língua portuguesa provenientes principalmente de podcasts disponíveis na plataforma. As transcrições foram geradas automaticamente e estão sujeitas a erros de transcrição."
        },
        {
            "pergunta": "Qual é o objetivo do projeto Multilingual Spoken Corpus e como ele se diferencia de outros corpora?",
            "resposta": "O objetivo do projeto Multilingual Spoken Corpus é criar uma base de palavras faladas em 50 idiomas. Ele se diferencia de outros corpora por ser composto de palavras soltas, em vez de enunciados completos, e por ser destinado ao treinamento de sistemas de reconhecimento em domínios específicos."
        },
        {
            "pergunta": "Quais são as principais características dos corpora LapsBM, Sidney e VoxForge?",
            "resposta": "Os corpora LapsBM, Sidney e VoxForge são corpora menores que totalizam aproximadamente 4, 1 e 1 horas de áudio, respectivamente. Eles são levantados por Quintanilha et al. e estão disponíveis para download na página do pesquisador."
        },
        {
            "pergunta": "Qual é o objetivo do projeto CoVoST e como ele se diferencia de outros corpora?",
            "resposta": "O objetivo do projeto CoVoST é criar um recorte da base Common Voice, mas com foco em tradução de fala para texto. Ele se diferencia de outros corpora por ter traduções das transcrições para o inglês e por ser destinado ao treinamento de modelos de tradução de fala para texto."
        },
        {
            "pergunta": "Qual é o objetivo do projeto Vox Populi e como ele se diferencia de outros corpora?",
            "resposta": "O objetivo do projeto Vox Populi é criar uma iniciativa para o treinamento semi-supervisionado e não-supervisionado de modelos de aprendizado de máquina. Ele se diferencia de outros corpora por ter transcrições para algumas línguas, mas não para o português, e por ter 17.500 horas de áudio disponíveis para o idioma."
        },
        {
            "pergunta": "O que é o reconhecimento de emoções a partir da fala e qual é o seu objetivo?",
            "resposta": "O reconhecimento de emoções a partir da fala é uma área de estudo que visa compreender as emoções expressas vocalmente pelos indivíduos. O objetivo é desenvolver técnicas para identificar e reconhecer as emoções contidas na fala, o que pode ter aplicações práticas em áreas como análise de atendimento ao cliente, apoio na avaliação do estado emocional de indivíduos durante terapias e desenvolvimento de assistentes virtuais mais empáticos."
        },
        {
            "pergunta": "Quais são as teorias clássicas e modelos mais recentes utilizados no reconhecimento de emoções a partir da fala?",
            "resposta": "Uma das teorias clássicas é a Teoria das Emoções Básicas de Ekman, que descreve a existência de seis emoções primárias: alegria, tristeza, raiva, medo, surpresa e aversão. Outros modelos mais recentes incluem o Modelo Circumplexo de Russel, que representa as emoções em um espaço bidimensional com eixos de valência e intensidade. Além disso, modelos de aprendizado de máquina como redes neurais convolucionais, redes neurais recorrentes e Transformers têm sido amplamente aplicados para o reconhecimento de emoções."
        },
        {
            "pergunta": "Quais são os desafios principais no reconhecimento de emoções a partir da fala?",
            "resposta": "Os desafios principais incluem a representação computacional da fala, a disponibilidade de corpora anotados e a escolha de métodos de aprendizado de máquina adequados para a tarefa de reconhecimento de emoções. Além disso, a fala espontânea apresenta desafios adicionais em comparação com a fala preparada, como a presença de hesitações, pausas, repetições, ruídos e interrupções."
        },
        {
            "pergunta": "O que é o corpus CORAA-SER e qual é o seu papel no reconhecimento de emoções a partir da fala?",
            "resposta": "O corpus CORAA-SER é um conjunto de dados de fala espontânea em português, anotado com presença ou ausência de emoção. Ele foi desenvolvido no âmbito do projeto TaRSila e é um passo relevante para superar os desafios no reconhecimento de emoções a partir da fala. O corpus CORAA-SER permite a exploração de diferentes técnicas de representação e métodos de aprendizado de máquina para identificar padrões emocionais na fala espontânea em português."
        },
        {
            "pergunta": "Quais são as direções futuras e oportunidades de pesquisa no reconhecimento de emoções a partir da fala?",
            "resposta": "As direções futuras incluem a investigação de métodos de transfer learning para reconhecimento de emoções, baseado em conhecimento prévio de modelos pré-treinados para fala. Além disso, a exploração de características prosódicas, como duração, intensidade, pitch e entonação, pode fornecer maior interpretabilidade no reconhecimento de emoções a partir da fala. A criação de corpora anotados e a desenvolvimento de técnicas de aprendizado de máquina mais eficientes também são áreas de pesquisa promissoras."
        },
        {
            "pergunta": "O que é a saída de sistemas ASR convencionais e por que ela requer capitalização e pontuação?",
            "resposta": "A saída de sistemas ASR convencionais é uma sequência de palavras sem capitalização e pontuação. Isso ocorre porque esses sistemas são projetados para transcrever a fala em texto, mas não incluem informações sobre a pontuação e capitalização. A falta dessas informações torna necessário o uso de técnicas de capitalização e pontuação para tornar o texto mais legível e compreensível."
        },
        {
            "pergunta": "Quais são alguns exemplos de ASR convencionais comerciais?",
            "resposta": "Alguns exemplos de ASR convencionais comerciais incluem o Google Cloud Speech-to-Text, Microsoft Azure Speech Services, IBM Watson Speech to Text e SpeechMatics. Esses sistemas são amplamente utilizados em aplicações de reconhecimento de fala, como transcrição de áudio e vídeo, e são projetados para fornecer uma transcrição precisa da fala."
        },
        {
            "pergunta": "O que é a restauração da pontuação original e a predição da pontuação?",
            "resposta": "A restauração da pontuação original é a tarefa de adicionar pontuação e capitalização a um texto que foi originalmente escrito para ser lido em voz alta, como um discurso. A predição da pontuação, por outro lado, é a tarefa de adicionar pontuação e capitalização a um texto que foi gerado a partir de uma conversa espontânea ou fala. A predição da pontuação é mais desafiadora, pois não há uma estrutura pré-definida para a fala espontânea."
        },
        {
            "pergunta": "Qual é o dataset de teste utilizado para avaliar a tarefa de predição de pontuação no contexto de reconhecedores automáticos de fala?",
            "resposta": "O dataset de teste utilizado é o Corpus CORAA MuPe, que é um conjunto de dados balanceado por sexo, com histórias de vida de homens e mulheres. Esse dataset foi criado especificamente para avaliar a tarefa de predição de pontuação no contexto de reconhecedores automáticos de fala, e é utilizado como um benchmark para avaliar o desempenho de diferentes sistemas de reconhecimento de fala."
        },
        {
            "pergunta": "Qual é o sistema de reconhecimento de fala utilizado para avaliar a tarefa de predição de pontuação no contexto de reconhecedores automáticos de fala?",
            "resposta": "O sistema de reconhecimento de fala utilizado é o Whisper da OpenAi, que é um sistema de reconhecimento de fala avançado que consegue transcrever automaticamente a fala e fazer a predição da pontuação e capitalização. O Whisper é um exemplo de um sistema de reconhecimento de fala que vai além da transcrição simples e pode fornecer uma saída mais completa e compreensível."
        }
    ],
    "cap-mwe": [
        {
            "pergunta": "O que são expressões multipalavras e como elas se caracterizam?",
            "resposta": "Expressões multipalavras são combinações de palavras que representam significados complexos que ultrapassam os limites dos sentidos das palavras individuais. Elas apresentam idiossincrasias e especificidades que não permitem determinadas operações sintáticas e semânticas comuns a outras combinações de palavras. Além disso, o significado do todo pode não se dar pela soma das partes."
        },
        {
            "pergunta": "Qual é o objetivo do capítulo em relação às expressões multipalavras?",
            "resposta": "O objetivo do capítulo é definir quais são os conceitos fundamentais para quem vai navegar pelas águas turbulentas do tratamento computacional de expressões multipalavras. Isso inclui discutir quais são os elementos que compõem uma expressão, abordar o conceito de expressão multipalavras propriamente dito e apresentar uma definição de MWEs que seja adotada no capítulo."
        },
        {
            "pergunta": "Quais são as principais tarefas de PLN que envolvem MWEs?",
            "resposta": "As principais tarefas de PLN que envolvem MWEs se dividem basicamente em dois grandes grupos: a descoberta e a identificação de MWEs. A descoberta de MWEs envolve encontrar novas expressões multipalavras em um texto, enquanto a identificação de MWEs envolve reconhecer expressões multipalavras já conhecidas em um texto."
        },
        {
            "pergunta": "Quais são as principais características das MWEs?",
            "resposta": "As principais características das MWEs são a ambiguidade, a variabilidade e a arbitrariedade. A ambiguidade se refere ao fato de que as MWEs podem ter mais de um significado. A variabilidade se refere ao fato de que as MWEs podem ser usadas de diferentes maneiras em diferentes contextos. A arbitrariedade se refere ao fato de que as MWEs podem não seguir as regras normais da linguagem."
        },
        {
            "pergunta": "Qual é o cenário atual em relação às MWEs no português brasileiro?",
            "resposta": "O cenário atual em relação às MWEs no português brasileiro é que há poucos recursos e pesquisas disponíveis em comparação com outras línguas. Isso torna desafiador o desenvolvimento de sistemas de processamento computacional de MWEs para o português brasileiro. No entanto, há uma crescente demanda por recursos e pesquisas nessa área, e é importante que os pesquisadores e entusiastas das MWEs continuem a trabalhar para desenvolver soluções eficazes para o tratamento computacional de MWEs."
        },
        {
            "pergunta": "Qual é a principal confusão terminológica na área de processamento de linguagem natural (PLN) relacionada às expressões multipalavras (MWEs)?",
            "resposta": "A principal confusão terminológica na área de PLN relacionada às MWEs é a nomenclatura e definição das tarefas computacionais relativas às MWEs. Isso tem sido chamado de identificação, extração, aquisição, indução de dicionários, aprendizado, entre outros. No entanto, o survey de Constant et al. (2017) propôs uma terminologia que define as tarefas ligadas às MWEs em duas subtarefas: descoberta e identificação."
        },
        {
            "pergunta": "Quais são as duas subtarefas do processamento de MWEs propostas pelo survey de Constant et al. (2017)?",
            "resposta": "As duas subtarefas do processamento de MWEs propostas pelo survey de Constant et al. (2017) são a descoberta e a identificação. A descoberta tem por objetivo encontrar MWEs novas no texto e armazená-las para uso futuro em um léxico, enquanto a identificação é o processo de anotar essas expressões automaticamente em um texto, associando-as a MWEs conhecidas."
        },
        {
            "pergunta": "Qual é a diferença entre a saída da descoberta e a saída da identificação de MWEs?",
            "resposta": "A saída da descoberta é uma lista de unidades lexicais candidatas, enquanto a da identificação é um texto anotado. A lista de candidatas a MWE geralmente requer uma revisão manual por especialistas antes de ser adicionada a um léxico, enquanto a identificação gera anotações que podem ajudar a chegar ao significado correto do texto em tarefas subsequentes de PLN."
        },
        {
            "pergunta": "O que é a predição de composicionalidade e qual é sua importância no processamento de MWEs?",
            "resposta": "A predição de composicionalidade é a capacidade de determinar o grau em que o significado de uma expressão pode ser inferido a partir dos significados das palavras que a compõem. Isso é importante no processamento de MWEs porque permite decidir quais entradas devem aparecer em um léxico (pouco composicionais, cujo significado não pode ser inferido a partir das palavras) e quais não precisam (muito composicionais)."
        },
        {
            "pergunta": "Quais são as aplicações de PLN que podem se beneficiar da descoberta ou identificação prévia de MWEs?",
            "resposta": "As aplicações de PLN que podem se beneficiar da descoberta ou identificação prévia de MWEs incluem a tradução automática, a análise sintática, a recuperação de informação, a desambiguação de sentido de palavras, a etiquetagem de supersenses, a análise de sentimentos, a predição de níveis de complexidade textual, a identificação de metáforas e a detecção de discurso de ódio."
        },
        {
            "pergunta": "Qual é a principal diferença entre a descoberta de MWEs e a identificação de MWEs?",
            "resposta": "A principal diferença entre a descoberta de MWEs e a identificação de MWEs é que a descoberta de MWEs é uma tarefa que visa identificar novas expressões multipalavras que não estão presentes em léxicos, enquanto a identificação de MWEs é uma tarefa que visa identificar expressões multipalavras que já estão presentes em léxicos. Além disso, a descoberta de MWEs é uma tarefa mais difícil de avaliar, pois as MWEs candidatas muitas vezes estão ausentes dos léxicos, exigindo avaliação por especialistas."
        },
        {
            "pergunta": "Quais são as duas estratégias principais utilizadas para avaliar a descoberta de MWEs?",
            "resposta": "As duas estratégias principais utilizadas para avaliar a descoberta de MWEs são a comparação com listas de MWEs existentes e a anotação manual de uma amostra das candidatas para a avaliação da precisão do método. A primeira estratégia assume que as candidatas que estão incluídas no léxico são MWEs, enquanto a segunda estratégia envolve a anotação manual de uma amostra das candidatas para avaliar a precisão do método."
        },
        {
            "pergunta": "Quais são as medidas de avaliação utilizadas para a tarefa de identificação de MWEs?",
            "resposta": "As medidas de avaliação utilizadas para a tarefa de identificação de MWEs incluem a medida estrita, a medida baseada em links, a medida baseada em MWEs e a medida baseada em tokens. A medida baseada em MWEs é mais estrita e considera que toda a expressão precisa ser predita corretamente, do início ao fim, enquanto a medida baseada em tokens é menos rigorosa e considera parcialmente correto predizer parte de uma expressão. Além disso, as medidas de avaliação também incluem a precisão, a revocação e o F-score."
        },
        {
            "pergunta": "O que é a shared task DiMSUM e qual foi seu papel na avaliação da tarefa de identificação de MWEs?",
            "resposta": "A shared task DiMSUM foi uma competição que propôs duas medidas de avaliação para a tarefa de identificação de MWEs: a medida estrita e a medida baseada em links. Embora essas medidas sejam interessantes, foi a shared task PARSEME que definiu as métricas que são hoje a referência para essa tarefa."
        },
        {
            "pergunta": "Quais são as medidas especializadas introduzidas pela edição 1.1 da shared task PARSEME?",
            "resposta": "As medidas especializadas introduzidas pela edição 1.1 da shared task PARSEME avaliam os sistemas somente em um subconjunto de MWEs que representam um fenômeno (linguístico) específico. Isso significa que as medidas especializadas correspondem às medidas P, R e F baseadas em MWEs, mas calculadas apenas em um subconjunto das MWEs que apresentam determinadas características."
        },
        {
            "pergunta": "Qual é o interesse da comunidade de PLN nos últimos anos?",
            "resposta": "A comunidade de PLN tem demonstrado um interesse substancial no tratamento de expressões multipalavras, incluindo a anotação de corpora, disponibilização de recursos e desenvolvimento de algoritmos e sistemas para melhor compreender esse fenômeno tanto do ponto de vista linguístico quanto do tratamento computacional."
        },
        {
            "pergunta": "Quais são os avanços alcançados no tratamento de expressões multipalavras?",
            "resposta": "Os avanços incluem a evolução no sentido de uma melhor compreensão sobre o fenômeno das expressões multipalavras, tanto do ponto de vista linguístico quanto do tratamento computacional, bem como seu impacto para diversas tarefas de PLN. Além disso, o PARSEME trouxe contribuições significativas em relação a corpora multilíngues."
        },
        {
            "pergunta": "Qual é o papel do PARSEME no tratamento de expressões multipalavras?",
            "resposta": "O PARSEME é um projeto que trouxe contribuições significativas em relação a corpora multilíngues, e é possível que, no futuro, o escopo das anotações desses corpora vá além das MWEs verbais. Isso significa que o PARSEME está ajudando a avançar no tratamento de expressões multipalavras, especialmente em contextos multilíngues."
        },
        {
            "pergunta": "Quais são as áreas que ainda precisam de pesquisas e desenvolvimento no tratamento de expressões multipalavras?",
            "resposta": "Há ainda muito espaço para pesquisas diversas, projetos novos e desenvolvimento amplo de trabalhos teóricos exploratórios e de tarefas aplicadas. Isso inclui a exploração de novas abordagens para o tratamento de expressões multipalavras, a criação de novos recursos e a aplicação de técnicas de PLN em diferentes contextos."
        },
        {
            "pergunta": "Qual é o futuro do tratamento de expressões multipalavras?",
            "resposta": "O futuro do tratamento de expressões multipalavras é promissor, com muitas oportunidades para pesquisas e desenvolvimento. É provável que o escopo das anotações de corpora vá além das MWEs verbais e que novas abordagens e técnicas sejam desenvolvidas para melhorar o tratamento de expressões multipalavras em diferentes contextos."
        }
    ],
    "cap-ordem-funcao": [
        {
            "pergunta": "O que é a sintaxe e qual é sua importância no estudo da linguagem?",
            "resposta": "A sintaxe é uma subárea dos estudos linguísticos que estuda como as palavras se organizam nas estruturas que constroem as distintas funções gramaticais no escopo da frase ou sentença. É um estrato central do sistema linguístico, pois organiza funções no estrato imediatamente inferior – a morfologia – e fornece estruturas de funções que serão importantes nos estratos superiores – semântica e pragmática."
        },
        {
            "pergunta": "Qual é a diferença entre a sintaxe de constituência e a sintaxe de dependência?",
            "resposta": "A sintaxe de constituência é um tipo de análise sintática que se concentra na estrutura interna das frases, enquanto a sintaxe de dependência se concentra nas relações entre as palavras e os sintagmas. Ambas as abordagens são importantes no Processamento de Linguagem Natural (PLN) e são utilizadas para analisar a estrutura das frases e identificar as relações entre as palavras."
        },
        {
            "pergunta": "O que é a escala de ordens e como ela se relaciona com a sintaxe?",
            "resposta": "A escala de ordens é uma forma de organização do sistema linguístico que abrange as unidades utilizadas para a descrição da língua, seja ela falada ou escrita. A escala de ordens inclui as unidades de análise, como morfemas, palavras, sintagmas e orações, e se relaciona com a sintaxe pois é através da sintaxe que as palavras se organizam em estruturas que constroem as distintas funções gramaticais no escopo da frase ou sentença."
        },
        {
            "pergunta": "Qual é a importância do parsing sintático no Processamento de Linguagem Natural (PLN)?",
            "resposta": "O parsing sintático é um processo fundamental no PLN que envolve a análise da estrutura de orações para identificar as relações entre as palavras e os sintagmas. Isso é importante para uma variedade de tarefas, como a extração de informação, a tradução automática e a geração de texto. O parsing sintático toma como base a classe de palavra das distintas palavras que compõem os sintagmas e é realizado por meio de softwares denominados parsers."
        },
        {
            "pergunta": "O que é um treebank e qual é sua importância no Processamento de Linguagem Natural (PLN)?",
            "resposta": "Um treebank é um conjunto de sentenças com anotação morfossintática e com representação em diagramas de árvore. Os treebanks são importantes no PLN pois fornecem um recurso valioso para o treinamento e a avaliação de parsers e outros sistemas de PLN. Eles permitem que os pesquisadores e desenvolvedores de PLN avaliem a precisão e a eficiência de seus sistemas e identifiquem áreas para melhoria."
        },
        {
            "pergunta": "Qual é o estilo de escrita apresentado no Exemplo 6.1 e como ele foi utilizado historicamente?",
            "resposta": "O estilo de escrita apresentado no Exemplo 6.1 é conhecido como scriptio continua, que foi utilizado na antiguidade, no latim clássico. Nesse estilo, as palavras são escritas sem espaçamento entre elas e sem pontuação. Embora esse estilo seja incomum hoje em dia, ele foi um precursor para o desenvolvimento da escrita moderna."
        },
        {
            "pergunta": "Como a separação visual entre palavras e sentenças evoluiu historicamente?",
            "resposta": "A separação visual entre palavras e sentenças foi um desenvolvimento histórico que levou à forma como representamos hoje a língua na sua forma escrita. Com o tempo, as pessoas começaram a usar espaços entre palavras e pontuação para separar as sentenças e tornar a leitura mais fácil e clara."
        },
        {
            "pergunta": "Por que o texto no Exemplo 6.1 pode causar estranhamento, mas ainda assim é possível entender seu significado?",
            "resposta": "O texto no Exemplo 6.1 pode causar estranhamento devido à falta de espaçamento entre palavras e pontuação, o que é incomum para os leitores modernos. No entanto, apesar disso, é possível entender o significado do texto porque o cérebro humano é capaz de reconhecer padrões e estruturas linguísticas, mesmo quando a forma de apresentação é não convencional."
        },
        {
            "pergunta": "Quais são os agrupamentos intermediários entre as palavras individuais e a sentença como um todo que contribuem para o significado do texto?",
            "resposta": "Os agrupamentos intermediários entre as palavras individuais e a sentença como um todo são conhecidos como unidades de significado ou blocos de informação. Esses agrupamentos são formados por palavras que se combinam para criar um significado mais amplo e contribuem para o significado do texto como um todo. Exemplos de tais agrupamentos incluem 'a menina' e 'uma foto'."
        },
        {
            "pergunta": "Por que a ordem das palavras dentro desses agrupamentos é importante e como ela afeta o significado do texto?",
            "resposta": "A ordem das palavras dentro dos agrupamentos é importante porque ela afeta o significado do texto. A ordem das palavras é condicionada pela estrutura da língua e, quando essa ordem é trocada, pode resultar em uma forma que consideramos com problemas na sua formulação e que nos causa estranhamento. Por exemplo, a ordem 'a menina' é mais natural do que 'menina a'."
        },
        {
            "pergunta": "O que é sintaxe e como ela se relaciona com a organização das palavras em uma oração?",
            "resposta": "A sintaxe é o estudo de como as palavras se agrupam e se organizam em uma ordem determinada. Ela é responsável por reconhecer as regras que ditam quais agrupamentos de palavras serão aceitos e quais serão considerados problemáticos. A sintaxe ajuda a construir significados ao agrupar palavras em unidades maiores, até formar uma oração completa."
        },
        {
            "pergunta": "O que são sintagmas e como eles se relacionam com a estrutura da oração?",
            "resposta": "Sintagmas são agrupamentos de palavras que funcionam como uma unidade dentro da oração. Eles podem ser nominais, verbais, adverbiais, adjetivais ou preposicionais, e cada um tem suas próprias regras para a organização das palavras que o compõem. Os sintagmas são parte de uma unidade maior, que pode ser outra sintagma ou a oração, e exercem uma função dentro dela."
        },
        {
            "pergunta": "O que são constituintes e como eles se relacionam com a estrutura da oração?",
            "resposta": "Constituintes são as palavras individuais ou seus agrupamentos progressivos em unidades maiores. Eles são os componentes que integram todas as estruturas da sentença, incluindo os sintagmas, e podem ser compostos por outros constituintes. Os constituintes exercem uma função dentro da oração e podem ser identificados com papéis temáticos, como Agente, Paciente ou Objeto de uma ação."
        },
        {
            "pergunta": "O que é a escala de ordens e como ela se relaciona com a estrutura da oração?",
            "resposta": "A escala de ordens é uma representação da estrutura da oração, que mostra como as palavras se agrupam em unidades maiores. Ela ajuda a identificar os constituintes e os sintagmas dentro da oração e a entender como eles se relacionam entre si. A escala de ordens é importante para a análise sintática de constituência, que é um dos dois tipos de análise sintática mais utilizados em Processamento de Linguagem Natural (PLN)."
        },
        {
            "pergunta": "O que é a sintaxe de constituência e como ela se relaciona com a análise da estrutura da oração?",
            "resposta": "A sintaxe de constituência é um tipo de análise sintática que se concentra na estrutura da oração em termos de constituintes e sintagmas. Ela ajuda a entender como as palavras se agrupam em unidades maiores e como essas unidades se relacionam entre si. A sintaxe de constituência é um dos dois tipos de análise sintática mais utilizados em PLN, sendo o outro tipo a sintaxe de dependência."
        },
        {
            "pergunta": "O que é a sintaxe de constituência e como ela é representada?",
            "resposta": "A sintaxe de constituência é uma abordagem que analisa a estrutura hierárquica das sentenças, agrupando unidades menores em unidades maiores. Ela pode ser representada por meio de colchetes ou estruturas arbóreas, que mostram a relação entre os constituintes e sua hierarquia. A notação com colchetes é mais fácil de processar computacionalmente, enquanto a notação com estrutura arbórea é mais fácil de visualizar."
        },
        {
            "pergunta": "Quais são os principais constituintes da sentença e como eles são anotados?",
            "resposta": "Os principais constituintes da sentença são o sujeito (NP), o predicado (VP) e o objeto (NP). Eles são anotados com etiquetas específicas, como S para sentença, NP para sintagma nominal, VP para sintagma verbal, DET para determinantes, V para verbo, N para substantivo, ADJ para adjetivo, ADV para advérbio e P para preposição."
        },
        {
            "pergunta": "Como a notação com estrutura arbórea é lida e interpretada?",
            "resposta": "A notação com estrutura arbórea é lida de cima para baixo, começando com o nó ROOT, que representa a raiz da árvore. O nó S1 representa a sentença toda como um único constituinte, e os nós subsequentes representam os constituintes menores, como o sujeito e o predicado. As arestas representam as relações entre os nós, e as palavras individuais são as folhas da árvore."
        },
        {
            "pergunta": "Qual é a importância de identificar os constituintes e sua hierarquia na oração?",
            "resposta": "Identificar os constituintes e sua hierarquia na oração é fundamental para compreender as funções sintáticas, que são a base para as funções semânticas da oração. Isso permite entender como as palavras se relacionam entre si e como elas contribuem para o significado da sentença como um todo."
        },
        {
            "pergunta": "Quais são as vantagens e desvantagens da notação com estrutura arbórea em comparação com a notação com colchetes?",
            "resposta": "A notação com estrutura arbórea é mais fácil de visualizar e entender, pois mostra claramente as relações hierárquicas entre os constituintes. No entanto, a notação com colchetes é mais fácil de processar computacionalmente, o que é importante para aplicações de processamento de linguagem natural."
        },
        {
            "pergunta": "Qual é o nome do segundo tipo de análise sintática e quem é o autor que estabeleceu suas bases?",
            "resposta": "O segundo tipo de análise sintática é chamado de sintaxe de dependência e tem suas bases na gramática de dependência de Tesnière (1959). A gramática de dependência de Tesnière é uma abordagem que descreve as relações de dependência entre palavras em uma sentença, em vez de definir a estrutura da sentença por meio de sintagmas contidos em outros sintagmas."
        },
        {
            "pergunta": "Como a análise de dependência descreve as relações entre palavras em uma sentença?",
            "resposta": "A análise de dependência descreve as relações de dependência entre palavras em uma sentença, onde uma palavra é vista como subordinada a outra ou regida por ela, de acordo com relações sintáticas tais como sujeito-verbo; sujeito-objeto; verbo-objeto; coordenação; subordinação etc. Cada palavra é um nó de uma relação com uma outra palavra, e essas relações são estabelecidas de forma unidirecional entre uma palavra regente (head) e uma palavra regida ou dependente."
        },
        {
            "pergunta": "Qual é a importância da unidirecionalidade da relação entre palavras na sintaxe de dependência?",
            "resposta": "A unidirecionalidade da relação é importante para se estabelecer a hierarquia entre as palavras, pois determina quem é o regente (de onde a relação parte) e quem é o regido (aonde a relação chega). Isso permite estabelecer a estrutura da sentença de forma clara e precisa."
        },
        {
            "pergunta": "Como se conectam dois sintagmas na análise de dependência?",
            "resposta": "Para conectar dois sintagmas, é necessário conectar a palavra que representa o núcleo de um sintagma à palavra que representa o núcleo de outro sintagma. Além disso, também é necessário estabelecer as microrrelações dentro de um mesmo sintagma. Isso é diferente da análise de constituintes, que conecta sintagmas de forma mais geral."
        },
        {
            "pergunta": "Quais são os dois tipos de relações que existem na sintaxe de dependência?",
            "resposta": "Na sintaxe de dependência, há basicamente dois tipos de relações: (i) macrorrelações e (ii) microrrelações. As macrorrelações estabelecem relações entre os núcleos de diferentes sintagmas e geralmente conectam palavras de classe aberta. Já as microrrelações conectam elementos mais próximos, podendo ser adjacentes ou estar em uma vizinhança próxima, e geralmente conectam uma palavra de classe aberta a uma palavra de classe fechada."
        },
        {
            "pergunta": "Quais são exemplos de macrorrelações e microrrelações na sintaxe de dependência?",
            "resposta": "Um exemplo de macrorrelação é a relação que liga um verbo (núcleo do sintagma verbal) ao seu sujeito (núcleo do sintagma nominal). Já um exemplo de microrrelação é a relação entre um substantivo (palavra de classe aberta) e seu artigo (palavra de classe fechada)."
        }
    ],
    "cap-recursos-sintaxe": [
        {
            "pergunta": "O que é a tarefa de parsing e qual é o seu objetivo?",
            "resposta": "A tarefa de parsing consiste em fazer uma predição da estrutura sintática de uma sentença sem anotação. O objetivo do processamento sintático é identificar as unidades (como palavras, sintagmas e orações) na sentença e estabelecer as relações gramaticais entre elas a fim de extrair algum tipo de informação."
        },
        {
            "pergunta": "Quais são os tipos de parsing e parsers?",
            "resposta": "Existem dois tipos de parsing: parsing de constituência e parsing de dependência. Além disso, o parsing pode ser classificado como completo (deep) ou parcial (shallow). O parsing completo é também conhecido como parsing exaustivo, enquanto o parsing parcial é conhecido como chunking."
        },
        {
            "pergunta": "O que é chunking e como ele é implementado?",
            "resposta": "Chunking é uma técnica de parsing parcial que envolve a segmentação de uma sentença em pedaços ou chunks. O chunking é geralmente implementado por meio de tokenização de uma sentença em palavras, identificação da classe de palavra (PoS) e segmentação em chunks. O conceito de chunk foi proposto por Abney (1992) como uma unidade formada por uma única palavra ou por um conjunto de palavras."
        },
        {
            "pergunta": "Qual é o objetivo do parsing parcial e como ele é utilizado?",
            "resposta": "O objetivo do parsing parcial é gerar uma representação rasa da estrutura da sentença que possibilite um processamento mais rápido de grandes volumes de texto. O parsing parcial é geralmente utilizado em tarefas de Extração de Informação, como veremos no Capítulo 22."
        },
        {
            "pergunta": "Como o chunking é utilizado para identificar entidades em uma sentença?",
            "resposta": "O chunking pode ser utilizado para identificar entidades em uma sentença ao reconhecer unidades ou chunks nucleados por substantivos. Esses chunks são candidatos a entidades, e podem ser utilizados como entrada para modelos de Extração de Informação."
        }
    ],
    "cap-semantica-simbolica": [
        {
            "pergunta": "Quando surgiu o interesse por bases de conhecimento computáveis ou processáveis por máquina na área da Inteligência Artificial (IA)?",
            "resposta": "O interesse por bases de conhecimento computáveis ou processáveis por máquina surgiu na década de 60, com as primeiras redes semânticas e representações baseadas em frames, propostas por Minsky e Fillmore, respectivamente. Essas representações de conhecimento de mundo pareciam prover a solução para problemas de semântica de linguagem natural, o que atraiu rapidamente a comunidade de PLN."
        },
        {
            "pergunta": "Quais são os dois tipos de bases de conhecimento que serão examinados neste capítulo?",
            "resposta": "Os dois tipos de bases de conhecimento que serão examinados neste capítulo são: (1) recursos léxico-semânticos e (2) bases de conhecimento de senso comum. Essas bases de conhecimento são fundamentais para a análise semântica de textos e são utilizadas em tarefas de PLN, como desambiguação de sentido de palavras, perguntas e respostas, e análise semântica."
        },
        {
            "pergunta": "Quais são as bases de conhecimento mais representativas para o PLN que serão descritas neste capítulo?",
            "resposta": "As bases de conhecimento mais representativas para o PLN que serão descritas neste capítulo são: Wordnet, FrameNet e a ConceptNet, e suas bases congêneres para o português. Essas bases foram escolhidas devido à sua abrangência de entradas e representatividade para tarefas de PLN."
        },
        {
            "pergunta": "Qual é o recurso léxico-semântico mais utilizado em PLN para dar suporte a tarefas como desambiguação de sentido de palavras, perguntas e respostas, e análise semântica?",
            "resposta": "A WordNet de Princeton é, consensualmente, o recurso léxico-semântico mais utilizado em PLN para dar suporte a tarefas como desambiguação de sentido de palavras, perguntas e respostas, e análise semântica. A WordNet é uma base de conhecimento que contém informações sobre a semântica de palavras e é amplamente utilizada em tarefas de PLN."
        },
        {
            "pergunta": "Qual é a base de conhecimento de senso comum com mais entradas tanto para o inglês quanto para o português?",
            "resposta": "A ConceptNet é a base de conhecimento de senso comum com mais entradas tanto para o inglês quanto para o português. A ConceptNet é uma base de conhecimento que contém informações sobre o senso comum e é amplamente utilizada em tarefas de PLN, como análise semântica e geração de texto."
        }
    ],
    "cap-semantica-distribucional": [
        {
            "pergunta": "Quais são as origens da semântica vetorial e como ela se desenvolveu?",
            "resposta": "A semântica vetorial, também conhecida por métodos distribucionais, teve início na década de 1950, impulsionada pela convicção de que o significado de uma palavra pode ser definido a partir da sua distribuição nos contextos linguísticos em que ela ocorre. Linguistas como Joos, Harris e Firth compartilharam essa ideia, e a proposta de Osgood et al. de usar um ponto no espaço multidimensional para representar a conotação de uma palavra foi um marco importante. Desde então, a semântica vetorial se desenvolveu como uma representação vetorial que retrata o significado de uma palavra a partir da distribuição das palavras que formam o seu contexto."
        },
        {
            "pergunta": "Como a representação vetorial semântica é usada em Processamento de Linguagem Natural (PLN)?",
            "resposta": "A representação vetorial semântica é um padrão de representação muito usual em PLN, que pode retratar vários aspectos do significado das palavras, como a similaridade, a orientação de sentimento ou polaridade, e a associação entre palavras. A ideia é representar cada palavra como um ponto em um espaço vetorial multidimensional, construído a partir da distribuição de suas palavras vizinhas. Isso permite que os modelos de PLN capturem nuances do significado das palavras e façam predições mais precisas."
        },
        {
            "pergunta": "O que são vetores semânticos e como eles são representados?",
            "resposta": "Vetores semânticos são uma forma de representar o significado de uma palavra como um ponto em um espaço vetorial multidimensional. Eles são construídos a partir da distribuição de suas palavras vizinhas e podem ser representados por meio de uma matriz de coocorrência (ou distribuição de coocorrência), que retrata a frequência de coocorrência das palavras. As representações matriciais mais comuns são a matriz termo-documento e a matriz termo-contexto."
        },
        {
            "pergunta": "Quais são as principais características dos espaços vetoriais em Álgebra Linear?",
            "resposta": "Os espaços vetoriais são objetos de estudo da Álgebra Linear e são bem caracterizados pela sua dimensão, que representa o número de direções independentes no espaço. Um espaço vetorial é formado por uma coleção de objetos chamados vetores. Em um Modelo Semântico Distribucional, é possível representar palavras, sentenças e até documentos completos como vetores em um espaço multidimensional."
        },
        {
            "pergunta": "Como as matrizes de coocorrência são usadas para representar vetores semânticos?",
            "resposta": "As matrizes de coocorrência são usadas para representar vetores semânticos, pois elas retratam a frequência de coocorrência das palavras. As representações matriciais mais comuns são a matriz termo-documento, onde cada dimensão (vetor) da matriz representa um documento, e a matriz termo-contexto, onde cada dimensão representa uma palavra. Isso permite que os modelos de PLN capturem a relação entre as palavras e o contexto em que elas são usadas."
        },
        {
            "pergunta": "O que são as matrizes termo-documento e termo-contexto?",
            "resposta": "As matrizes termo-documento e termo-contexto são estruturas de dados que associam a frequência de ocorrência de cada termo ao documento ou contexto em que ocorrem. Essas matrizes são utilizadas em Processamento de Linguagem Natural (PLN) para representar a relação entre os termos e os documentos ou contextos em que eles aparecem."
        },
        {
            "pergunta": "Por que a frequência simples de um termo é pouco discriminativa?",
            "resposta": "A frequência simples de um termo é pouco discriminativa porque algumas palavras são bastante comuns e não caracterizam nenhum documento ou contexto em particular. Isso significa que a frequência de ocorrência de um termo não é suficiente para distinguir um documento ou contexto de outro."
        },
        {
            "pergunta": "O que são as medidas TF-IDF e PMI?",
            "resposta": "As medidas TF-IDF (Term Frequency-Inverse Document Frequency) e PMI (Pointwise Mutual Information) são abordagens mais avançadas para avaliar a importância de um termo em um documento ou contexto. A TF-IDF leva em conta a frequência de ocorrência de um termo em um documento e a raridade desse termo em toda a coleção de documentos. Já a PMI avalia a probabilidade de co-ocorrência de dois termos em um documento ou contexto."
        },
        {
            "pergunta": "Por que as matrizes que utilizam as medidas TF-IDF e PMI são chamadas de vetores esparsos?",
            "resposta": "As matrizes que utilizam as medidas TF-IDF e PMI são chamadas de vetores esparsos porque muitos termos nunca ocorrem em alguns documentos de uma coleção ou nunca aparecem em certos contextos. Isso resulta em vetores com muitas dimensões e esparsos, ou seja, com muitos valores nulos (zeros)."
        },
        {
            "pergunta": "O que é o LSA (Latent Semantic Analysis) e qual é o seu objetivo?",
            "resposta": "O LSA (Latent Semantic Analysis) é um modelo muito adotado em PLN que tem como objetivo reduzir a dimensionalidade de um espaço multidimensional criado com o uso do TF-IDF ou PMI. O LSA utiliza técnicas de redução de dimensionalidade para identificar os padrões latentes nos dados e reduzir a complexidade do espaço de representação."
        },
        {
            "pergunta": "O que são embeddings e como eles diferem dos vetores esparsos?",
            "resposta": "Os embeddings são uma representação de palavras por meio de vetores densos e menores, com dimensões variando entre 50-1000. Eles diferem dos vetores esparsos porque são densos, ou seja, seus valores são números reais positivos ou negativos, ao invés de contagens esparsas. Além disso, os embeddings capturam melhor as relações semânticas e contextuais entre as palavras do que os vetores esparsos."
        },
        {
            "pergunta": "Como os embeddings são aprendidos?",
            "resposta": "Os embeddings são aprendidos a partir de corpora por meio de algoritmos de aprendizado de máquina supervisionado ou não supervisionado. Por exemplo, usando redes neurais artificiais como é o caso do modelo Word2Vec, ou usando representação estatística da matriz de coocorrência de termos, como é o caso do modelo Glove."
        },
        {
            "pergunta": "Quais são as principais diferenças entre os embeddings estáticos e dinâmicos?",
            "resposta": "Os embeddings estáticos permanecem fixos uma vez aprendidos e não podem ser ajustados ou modificados para uma tarefa específica. Já os embeddings dinâmicos podem ser ajustados em tarefas específicas, se adaptando às nuances específicas da tarefa e ao contexto atual."
        },
        {
            "pergunta": "Quais são os modelos de embeddings estáticos mais comuns?",
            "resposta": "Os modelos de embeddings estáticos mais comuns incluem Word2Vec, Fasttext e GloVe. Esses modelos são amplamente utilizados em tarefas de processamento de linguagem natural (PLN)."
        },
        {
            "pergunta": "Como os embeddings podem ser definidos para diferentes unidades de representação?",
            "resposta": "Os embeddings podem ser definidos para diferentes unidades de representação, incluindo palavras, caracteres, subpalavras, sentenças e até mesmo textos com várias sentenças. Isso permite que os embeddings sejam utilizados em uma variedade de tarefas de PLN, desde a análise de sentimentos até a tradução automática."
        },
        {
            "pergunta": "Quando os embeddings começaram a ser amplamente utilizados?",
            "resposta": "Os embeddings começaram a ser amplamente utilizados a partir de 2013, com o desenvolvimento e a disponibilização do modelo Word2Vec. Desde então, os embeddings se tornaram uma ferramenta fundamental em muitas tarefas de PLN."
        }
    ],
    "cap-modelos-discursivos": [
        {
            "pergunta": "O que é discurso segundo o Dicionário Houaiss?",
            "resposta": "De acordo com o Dicionário Houaiss, discurso pode referir-se à 'língua em ação, tal como é realizada pelo falante; a um segmento contínuo de fala maior do que uma sentença (Análise de discurso); a um enunciado oral ou escrito que supõe, numa situação de comunicação, um locutor e um interlocutor'; e ainda à 'reprodução que alguém faz das palavras atribuídas a outra pessoa'."
        },
        {
            "pergunta": "Qual é a relação entre texto e discurso na Linguística?",
            "resposta": "Na Linguística, texto e discurso tendem a ser entendidos como elementos que se complementam. Segundo Lyons (1977), o texto se dá por meio do discurso, em que aquele seria qualquer passagem que apresenta a conexão do discurso, falado ou escrito, em um diálogo ou um monólogo."
        },
        {
            "pergunta": "Como o discurso é definido no Processamento de Linguagem Natural (PLN)?",
            "resposta": "No PLN, o discurso é definido como 'qualquer segmento conexo de texto ou fala, compreendendo uma ou mais frases ou segmento de frases' (Sidner, 1978). Essa definição é considerada genérica, mas conduz as pesquisas da área a tomarem texto e discurso como sinônimos."
        },
        {
            "pergunta": "Qual é a importância da coesão e coerência no discurso?",
            "resposta": "A coesão e coerência são fundamentais no discurso, pois permitem que os elementos linguísticos sejam interligados e que o sentido seja construído. A coesão se refere ao modo como os elementos linguísticos se encontram interligados, enquanto a coerência se refere ao modo como os elementos subjacentes à superfície textual vêm a construir, na mente dos interlocutores, uma configuração veiculadora de sentidos."
        },
        {
            "pergunta": "Quais são os desafios do Processamento de Linguagem Natural (PLN) em relação ao discurso?",
            "resposta": "Os desafios do PLN em relação ao discurso incluem a teorização, anotação e o processamento de dados discursivos, que são considerados grandes desafios devido à complexidade das relações estabelecidas entre os elementos no interior de um texto para a construção de sentido."
        },
        {
            "pergunta": "Quais são os modelos discursivos utilizados em pesquisas de PLN?",
            "resposta": "Os modelos discursivos utilizados em pesquisas de PLN incluem a Rhetorical Structure Theory (RST), a Cross-document Structure Theory (CST), a Teoria GSDT, a Teoria SDRT, a Teoria de Centering e a Teoria das Veias. Esses modelos são utilizados para analisar e representar as relações discursivas em textos e discursos."
        },
        {
            "pergunta": "Qual é o objetivo da Seção 11.2 do capítulo?",
            "resposta": "O objetivo da Seção 11.2 é apresentar fundamentações teóricas gerais sobre modelos de relações discursivas, exemplificando suas preocupações e potenciais aplicações por meio das teorias GSDT, SDRT, Teoria de Centering e Teoria das Veias."
        },
        {
            "pergunta": "Quais são as principais aplicações dos modelos discursivos em PLN?",
            "resposta": "As principais aplicações dos modelos discursivos em PLN incluem a análise de textos e discursos, a identificação de relações discursivas, a construção de modelos de linguagem e a realização de tarefas de processamento de linguagem natural, como a extração de informações e a resumo automático."
        },
        {
            "pergunta": "Qual é o objetivo principal da GSDT (Grosz and Sidner Discourse Theory)?",
            "resposta": "A GSDT visa modelar o aspecto intencional do discurso, identificando as intenções do autor e estruturando o conteúdo de forma a satisfazê-las. Ela organiza o discurso usando relações de contribuição e satisfação entre as intenções, definindo quatro relações principais: Dominance, Satisfaction-Precedence, Supports e Generates."
        },
        {
            "pergunta": "Como a Teoria da Representação do Discurso Segmentado (SDRT) representa o discurso?",
            "resposta": "A SDRT representa o discurso como um hipergrafo, no qual as arestas são as relações discursivas e os nós representam as Unidades de Discurso Elementar (EDUs) que contém apenas um elemento. O grafo pode ter ainda Unidades de Discurso Complexas (CDUs) que são nós com mais de um elemento simples."
        },
        {
            "pergunta": "Qual é o principal objetivo da Teoria de Centering?",
            "resposta": "O principal objetivo da Teoria de Centering é prever qual entidade discursiva tem maior importância em determinados segmentos, definindo um conjunto de regras e restrições que ditam as escolhas feitas pelos participantes do discurso."
        },
        {
            "pergunta": "Como a Teoria das Veias (Veins Theory) expande as regras de coerência local da Teoria de Centering?",
            "resposta": "A Teoria das Veias expande as regras de coerência local da Teoria de Centering para abranger a composicionalidade das unidades do discurso, definindo um conjunto de unidades do discurso que podem conter o antecedente de uma anáfora, chamado de 'veia'."
        },
        {
            "pergunta": "Qual é o objetivo principal dos modelos discursivos?",
            "resposta": "O objetivo principal dos modelos discursivos é apresentar as relações entre os elementos de um texto para depreender a sua produção e os processos de interação e intenções pertencentes a uma situação comunicativa específica."
        },
        {
            "pergunta": "Por que a GSDT é importante para a compreensão do discurso?",
            "resposta": "A GSDT é importante para a compreensão do discurso porque ela ajuda a identificar as intenções do autor e a estrutura do conteúdo, o que é fundamental para entender a mensagem pretendida."
        },
        {
            "pergunta": "Como a SDRT pode ser aplicada ao tratamento de diálogos?",
            "resposta": "A SDRT pode ser aplicada ao tratamento de diálogos porque ela permite representar contra-argumentação, um fenômeno pouco tratado em outros modelos discursivos. Além disso, ela pode ser usada para modelar diálogos e representar relações retóricas entre as unidades de discurso."
        },
        {
            "pergunta": "Qual é a diferença entre as relações de coordenação e subordinação na SDRT?",
            "resposta": "As relações de coordenação conectam segmentos do discurso no mesmo nível hierárquico, enquanto as relações de subordinação ligam um segmento do discurso a outro segmento que está um nível hierárquico abaixo."
        },
        {
            "pergunta": "Como a Teoria de Centering pode ser usada para tratar anáforas?",
            "resposta": "A Teoria de Centering pode ser usada para tratar anáforas porque ela ajuda a estabelecer a coerência nos segmentos discursivos adjacentes ao direcionar a atenção para a escolha de uma expressão referencial."
        },
        {
            "pergunta": "Qual é o conceito de 'veia' na Teoria das Veias?",
            "resposta": "O conceito de 'veia' na Teoria das Veias refere-se a um conjunto de unidades do discurso que podem conter o antecedente de uma anáfora. A veia é definida como um conjunto de unidades do discurso que podem conter o antecedente de uma anáfora, e é fundamental para manter a coerência do discurso."
        },
        {
            "pergunta": "Quais são os principais avanços das aplicações de PLN (Processamento de Linguagem Natural) em nível discursivo?",
            "resposta": "Os principais avanços das aplicações de PLN em nível discursivo incluem a criação de corpus anotados, ferramentas de anotação automática de dados, sumarização de textos, tradução automática e avaliação de redações. Esses avanços são possíveis graças à descrição dos fenômenos linguísticos em nível discursivo, que permite uma melhor compreensão da estrutura e do significado dos textos."
        },
        {
            "pergunta": "Quais são os corpora padrão ouro com relações discursivas para o português brasileiro?",
            "resposta": "Existem pelo menos dois corpora padrão ouro com relações discursivas para o português brasileiro: Summ-it e CSTNews. O corpus Summ-it reúne anotações de vários níveis linguísticos, incluindo relações retóricas da RST, correferência e entidades nomeadas. Já o corpus CSTNews contém 50 grupos de textos jornalísticos de assuntos variados, coletados manualmente das fontes de notícias Folha de São Paulo, Estadão, O Globo, Jornal do Brasil e Gazeta do Povo."
        },
        {
            "pergunta": "Como é feita a anotação no nível discursivo de textos?",
            "resposta": "A anotação no nível discursivo de textos pode ser feita de forma manual ou automática. Para alguns modelos discursivos existem analisadores automáticos, conhecidos como parsers discursivos, que visam a identificação retórica do texto, gerando uma estrutura hierárquica em que as intenções do autor são explicitadas e relacionadas entre si. Além disso, existem ferramentas semiautomáticas, como a CSTTool, que facilitam o processo de anotação de corpus com CST."
        },
        {
            "pergunta": "Quais são as principais ferramentas utilizadas para a anotação automática de textos?",
            "resposta": "Algumas das principais ferramentas utilizadas para a anotação automática de textos incluem o parser DiZer, que é treinado com textos acadêmicos e jornalísticos e gera uma estrutura arbórea com as relações discursivas. Outra ferramenta é a CSTTool, que é semiautomática e possibilita os processos de segmentação dos textos-fonte em nível sentencial e a identificação, em pares, das sentenças lexicalmente relacionadas por meio de medidas de similaridade."
        },
        {
            "pergunta": "Quais são as principais aplicações das ferramentas de anotação automática de textos?",
            "resposta": "As principais aplicações das ferramentas de anotação automática de textos incluem a sumarização de textos, a tradução automática e a avaliação de redações. Além disso, essas ferramentas também podem ser utilizadas para a análise de textos em diferentes níveis linguísticos, como a identificação de relações retóricas, correferência e entidades nomeadas."
        }
    ],
    "cap-resolucao-correferencia": [
        {
            "pergunta": "O que é o processo de construção de sentidos na língua em uso?",
            "resposta": "O processo de construção de sentidos na língua em uso é um fenômeno complexo que envolve a negociação do universo de discurso entre interlocutores. Nesse processo, os interlocutores escolhem referir-se a indivíduos ou entidades específicas, estabelecendo sua identidade e garantindo sua existência no discurso. Esse processo é fundamental para a construção de sentidos e para a constituição de um texto coerente."
        },
        {
            "pergunta": "Qual é o papel dos referentes no texto?",
            "resposta": "Os referentes desempenham um papel fundamental no texto, pois garantem a unidade temática e a coerência do discurso. Eles são concretizados no texto por expressões referenciais e atravessam o texto por inteiro, estabelecendo relações de sentido que formam a base para retomadas e coesão textual."
        },
        {
            "pergunta": "O que são cadeias de referência e como elas funcionam?",
            "resposta": "As cadeias de referência, também conhecidas como cadeias anafóricas ou coesivas, são redes referenciais construídas pelos objetos de discurso que constituem as marcas da textualidade. Elas funcionam ligando referentes à(s) sua(s) expressão(ões) referencial(is), formando a base para retomadas e coesão textual. As cadeias coesivas são essenciais para a tessitura textual e garantem que o texto seja coerente em sua extensão."
        },
        {
            "pergunta": "Quais são os mecanismos semânticos e léxico-gramaticais que garantem a coesão textual?",
            "resposta": "Os mecanismos semânticos e léxico-gramaticais que garantem a coesão textual incluem reiteração (ou retomada), associação (ou ligações de sentidos entre as palavras presentes) e conexão entre as orações (por conectores). Esses mecanismos estão em consonância e garantem que o texto seja coerente em sua extensão."
        },
        {
            "pergunta": "Como os referentes são detectados em um texto?",
            "resposta": "Os referentes são detectados em um texto por relações léxico-semânticas, como por pronome, sintagma nominal, ou outras expressões referenciais. Por exemplo, se em um texto temos uma entidade como 'Maria', nome próprio, é de se esperar que os seus referentes sejam detectados por relações léxico-semânticas do texto, como por pronome ('ela') ou por sintagma nominal ('a professora', 'a ativista', 'a mulher' etc.)."
        },
        {
            "pergunta": "O que é a Resolução de Correferência e por que é considerada uma tarefa útil e desafiadora na área de Processamento da Linguagem Natural (PLN)?",
            "resposta": "A Resolução de Correferência é uma tarefa que envolve a identificação das diferentes formas que uma mesma menção pode assumir em um discurso. É considerada útil porque permite entender melhor o significado de um texto e é desafiadora porque depende de diversos níveis de processamento, como análise sintática, morfológica e extração de sintagmas nominais."
        },
        {
            "pergunta": "Quais são as principais iniciativas para a língua portuguesa que abordam o problema da Resolução de Correferência?",
            "resposta": "As principais iniciativas para a língua portuguesa que abordam o problema da Resolução de Correferência incluem a resolução de anáfora (Basso, 2009; Bick, 2010; Ferradeira, 1993; Rocha, 2000; Vieira et al., 2005) e o estudo da correferência nominal (Fonseca, 2014; Fonseca et al., 2014; Fonseca et al., 2016a; Freitas et al., 2009)."
        },
        {
            "pergunta": "O que é a resolução de anáfora e como ela se relaciona com a Resolução de Correferência?",
            "resposta": "A resolução de anáfora é o processo de identificar as relações entre palavras ou expressões que se referem a uma mesma entidade no texto. A resolução de anáfora é uma parte importante da Resolução de Correferência, pois ajuda a identificar as diferentes formas que uma mesma menção pode assumir em um discurso."
        },
        {
            "pergunta": "O que é a correferência nominal e como ela se relaciona com a Resolução de Correferência?",
            "resposta": "A correferência nominal é o processo de identificar as relações entre nomes e expressões que se referem a uma mesma entidade no texto. A correferência nominal é uma parte importante da Resolução de Correferência, pois ajuda a identificar as diferentes formas que uma mesma menção pode assumir em um discurso."
        },
        {
            "pergunta": "O que são cadeias de correferência e como elas são formadas?",
            "resposta": "Cadeias de correferência são grupos de termos e expressões que remetem a uma mesma referência. Elas são formadas agrupando termos e expressões que se referem a uma mesma entidade no texto. Por exemplo, na sentença apresentada no Exemplo 12.1, [o único país de a União Europeia a não permitir patenteamento de genes] e [A França] formam uma cadeia de correferência, pois se referem à mesma entidade."
        },
        {
            "pergunta": "Quais idiomas são mencionados no texto como tendo abordagens para resolver correferência?",
            "resposta": "Os idiomas mencionados no texto são inglês, chinês, árabe, espanhol, galego e português. É importante notar que esses idiomas são apenas alguns exemplos de línguas que têm abordagens para resolver correferência, e que existem muitas outras línguas que também têm abordagens para esse problema."
        },
        {
            "pergunta": "Quais são as principais abordagens para resolver correferência em línguas?",
            "resposta": "As principais abordagens para resolver correferência em línguas são baseadas em aprendizado de máquina e regras linguísticas. As abordagens baseadas em aprendizado de máquina são mais comuns e podem ser divididas em diferentes propostas, como Mention-Pair, Entity-Mention, Mention-Ranking e Antecedent-Trees. Já as abordagens baseadas em regras linguísticas são menos comuns, mas ainda são importantes para resolver correferência em certas línguas."
        },
        {
            "pergunta": "Quais são as principais diferenças entre as abordagens baseadas em aprendizado de máquina e regras linguísticas para resolver correferência?",
            "resposta": "As principais diferenças entre as abordagens baseadas em aprendizado de máquina e regras linguísticas para resolver correferência são a forma como elas abordam o problema e a complexidade das regras utilizadas. As abordagens baseadas em aprendizado de máquina utilizam algoritmos para aprender padrões nos dados e fazer previsões, enquanto as abordagens baseadas em regras linguísticas utilizam regras explícitas para resolver correferência. Além disso, as abordagens baseadas em aprendizado de máquina podem ser mais flexíveis e adaptáveis a diferentes línguas e contextos, enquanto as abordagens baseadas em regras linguísticas podem ser mais precisas e eficientes em certos casos."
        },
        {
            "pergunta": "Quais são as principais propostas de abordagens baseadas em aprendizado de máquina para resolver correferência?",
            "resposta": "As principais propostas de abordagens baseadas em aprendizado de máquina para resolver correferência são Mention-Pair, Entity-Mention, Mention-Ranking e Antecedent-Trees. A abordagem Mention-Pair envolve a identificação de pares de menções que se referem ao mesmo ente. A abordagem Entity-Mention envolve a identificação de entidades e suas menções no texto. A abordagem Mention-Ranking envolve a ordenação das menções de acordo com sua probabilidade de se referir ao mesmo ente. A abordagem Antecedent-Trees envolve a construção de árvores de antecedentes para resolver correferência."
        },
        {
            "pergunta": "Quais são as principais limitações das abordagens baseadas em regras linguísticas para resolver correferência?",
            "resposta": "As principais limitações das abordagens baseadas em regras linguísticas para resolver correferência são a complexidade das regras e a dificuldade de adaptá-las a diferentes línguas e contextos. Além disso, as abordagens baseadas em regras linguísticas podem ser menos flexíveis e menos capazes de lidar com casos excepcionais ou ambíguos. No entanto, as abordagens baseadas em regras linguísticas podem ser mais precisas e eficientes em certos casos, especialmente quando as regras são bem definidas e consistentes."
        },
        {
            "pergunta": "O que é a tarefa de resolução de correferência e por que é considerada complexa?",
            "resposta": "A tarefa de resolução de correferência é complexa porque envolve diferentes níveis de processamento, como a detecção de menções, agrupamentos realizados e agrupamentos não realizados. Essa complexidade se deve ao fato de que a correferência envolve a identificação de entidades e a relação entre elas em um texto, o que pode ser influenciado por vários fatores, como o contexto, a semântica e a sintaxe da linguagem."
        },
        {
            "pergunta": "Quais são as métricas propostas para avaliar modelos de correferência e o que elas avaliam?",
            "resposta": "Existem cinco métricas propostas para avaliar modelos de correferência: MUC, B-CUBED, Ceaf_e, Ceaf_m e BLANC. Cada uma dessas métricas visa avaliar uma característica específica de cada modelo, como a precisão, a revocação e a F1-score. Por exemplo, a métrica MUC avalia a precisão e a revocação da detecção de menções, enquanto a métrica B-CUBED avalia a precisão e a revocação da agrupação de menções."
        },
        {
            "pergunta": "O que é a competição CoNLL e qual é o seu objetivo?",
            "resposta": "A competição CoNLL é uma conferência anual que visa motivar o desenvolvimento de sistemas de processamento de linguagem natural. Nos anos de 2011 e 2012, a competição foi voltada à tarefa de Resolução de Correferência, com o objetivo de avaliar os modelos participantes por meio de uma pontuação única."
        },
        {
            "pergunta": "O que é a métrica CoNLL e como ela é calculada?",
            "resposta": "A métrica CoNLL é uma medida que consiste na média da medida-F de três outras métricas da literatura. Essa métrica foi proposta pela conferência CoNLL para avaliar os modelos participantes por meio de uma pontuação única. A medida-F é uma métrica que combina a precisão e a revocação em uma única medida, e é calculada como a média harmônica da precisão e da revocação."
        },
        {
            "pergunta": "Por que é importante avaliar modelos de correferência usando diferentes métricas?",
            "resposta": "É importante avaliar modelos de correferência usando diferentes métricas porque cada métrica pode capturar aspectos diferentes do desempenho do modelo. Além disso, a escolha da métrica depende do objetivo específico da aplicação e do tipo de dados que estão sendo processados. Ao usar diferentes métricas, é possível obter uma visão mais completa do desempenho do modelo e identificar áreas para melhoria."
        },
        {
            "pergunta": "Quais são os benefícios da tarefa de Resolução de Correferência?",
            "resposta": "Os benefícios da tarefa de Resolução de Correferência podem ser significativos, principalmente se considerarmos abordagens que utilizam apoio semântico. Isso pode levar a ganhos em outras tarefas de PLN, como Agrupamento de Aspectos para Análise de Sentimentos, Reconhecimento de Entidades Nomeadas e Extração de Relação entre Entidades Nomeadas."
        },
        {
            "pergunta": "Quais são as ferramentas disponíveis para a língua portuguesa para a tarefa de Resolução de Correferência?",
            "resposta": "A ferramenta de prateleira chamada CORP (Fonseca et al., 2016b) é a única ferramenta disponível para a língua portuguesa até o momento."
        },
        {
            "pergunta": "Como a Resolução de Correferência pode ajudar no Reconhecimento de Entidades Nomeadas?",
            "resposta": "A Resolução de Correferência pode ajudar no Reconhecimento de Entidades Nomeadas identificando a referência correta de um sintagma nominal ambíguo. Por exemplo, no sintagma nominal 'o agrônomo Miguel Guerra, de a UFSC, Guerra, Guerra, o agrônomo', a Resolução de Correferência pode identificar que a menção 'Guerra' corresponde ao agrônomo Miguel Guerra e, portanto, inferir uma mesma categoria de entidade nomeada (Pessoa)."
        },
        {
            "pergunta": "Como a Resolução de Correferência pode ajudar na Extração de Relação entre Entidades Nomeadas?",
            "resposta": "A Resolução de Correferência pode ajudar na Extração de Relação entre Entidades Nomeadas identificando a relação correta entre entidades nomeadas. Por exemplo, no sintagma nominal 'o agrônomo Miguel Guerra, de a UFSC', a Resolução de Correferência pode identificar que 'Guerra' faz referência a 'Miguel Guerra' e, portanto, inferir uma relação direta entre 'Guerra' e 'UFSC'."
        },
        {
            "pergunta": "Quais são as outras tarefas de PLN que podem se beneficiar da Resolução de Correferência?",
            "resposta": "Além do Agrupamento de Aspectos para Análise de Sentimentos, Reconhecimento de Entidades Nomeadas e Extração de Relação entre Entidades Nomeadas, outras tarefas de PLN que podem se beneficiar da Resolução de Correferência incluem a Análise de Sentimentos, a Classificação de Textos e a Sumarização de Textos."
        }
    ],
    "cap-dataset-corpus": [
        {
            "pergunta": "O que é um dataset e como ele é relacionado ao Processamento de Linguagem Natural (PLN)?",
            "resposta": "Um dataset é um conjunto de dados organizados de uma certa maneira para produzir informação. No PLN, os dados são elementos linguísticos, como palavras, frases ou textos, que são organizados de acordo com classes pré-estabelecidas para explorar o conteúdo linguístico. A criação de bons datasets é fundamental para o PLN, pois eles permitem que os modelos de linguagem aprendam e sejam treinados para realizar tarefas específicas."
        },
        {
            "pergunta": "Quais são os tipos de dados que podem ser utilizados no PLN e como eles são organizados?",
            "resposta": "Os dados utilizados no PLN podem ser linguísticos, como palavras, frases ou textos, e podem ser organizados de acordo com classes pré-estabelecidas, como classe gramatical, classe semântica, posição no texto ou frequência. Além disso, os dados podem ser organizados de maneira explícita, com atributos como classe de palavra ou polaridade de opinião, ou de maneira implícita, com base na posição e frequência das palavras no texto."
        },
        {
            "pergunta": "O que é anotação de dados e como ela é utilizada no PLN?",
            "resposta": "A anotação de dados é o processo de atribuir classificações ou rótulos linguísticos aos elementos linguísticos, como palavras ou frases, para codificar alguma dimensão do entendimento sobre a linguagem. No PLN, a anotação de dados é utilizada para criar conjuntos de dados anotados, que são fundamentais para o treinamento de modelos de linguagem e para a realização de tarefas específicas, como a análise de sentimento ou a identificação de entidades nomeadas."
        },
        {
            "pergunta": "Qual é a diferença entre um dataset anotado e um dataset não anotado?",
            "resposta": "Um dataset anotado é um conjunto de dados que possui classificações ou rótulos linguísticos atribuídos aos elementos linguísticos, enquanto um dataset não anotado é um conjunto de dados que não possui essas classificações ou rótulos. No PLN, os datasets anotados são mais valiosos porque permitem que os modelos de linguagem aprendam e sejam treinados para realizar tarefas específicas, enquanto os datasets não anotados são mais utilizados para tarefas como a previsão de palavras ou a análise de frequência."
        },
        {
            "pergunta": "Por que a criação de bons datasets é fundamental para o PLN?",
            "resposta": "A criação de bons datasets é fundamental para o PLN porque os modelos de linguagem precisam de dados de alta qualidade para aprender e se tornar eficazes. Além disso, os datasets permitem que os pesquisadores e desenvolvedores de PLN avaliem e comparem os resultados de diferentes modelos e técnicas, o que é essencial para o avanço da área. Sem bons datasets, os modelos de linguagem não podem ser treinados e avaliados adequadamente, o que limita o seu desempenho e a sua utilidade."
        },
        {
            "pergunta": "Qual é a importância dos datasets linguísticos para o Processamento de Linguagem Natural (PLN)?",
            "resposta": "Os datasets linguísticos são fundamentais para o PLN porque fornecem exemplos para que os modelos de aprendizado de máquina possam aprender de maneira eficiente. Além disso, eles facilitam o processo de avaliação e comparação entre diferentes modelos e sistemas, permitindo que os pesquisadores avaliem o desempenho dos modelos e identifiquem áreas para melhoria."
        },
        {
            "pergunta": "Como os datasets linguísticos são utilizados no PLN?",
            "resposta": "Os datasets linguísticos são utilizados no PLN para treinar e avaliar modelos de aprendizado de máquina. Eles são utilizados para fornecer exemplos de linguagem natural que os modelos possam aprender a partir deles. Além disso, os datasets são utilizados para avaliar o desempenho dos modelos e comparar os resultados entre diferentes modelos e sistemas."
        },
        {
            "pergunta": "Quais são os três motivos principais pelos quais os datasets linguísticos são fundamentais no PLN?",
            "resposta": "Os três motivos principais pelos quais os datasets linguísticos são fundamentais no PLN são: 1) fornecer exemplos para que os modelos de aprendizado de máquina possam aprender de maneira eficiente; 2) facilitar o processo de avaliação e comparação entre diferentes modelos e sistemas; e 3) permitir que os pesquisadores avancem no PLN, medindo o desempenho em tarefas específicas."
        },
        {
            "pergunta": "O que é uma avaliação conjunta (shared task) no PLN?",
            "resposta": "Uma avaliação conjunta (shared task) no PLN é uma estrutura experimental comum que fornece uma estrutura para que os pesquisadores avaliem e comparem os resultados de diferentes modelos e sistemas. Ela é utilizada para incentivar a pesquisa e o desenvolvimento de uma área específica do PLN, fornecendo uma estrutura experimental comum e medidas de avaliação."
        },
        {
            "pergunta": "Como os datasets linguísticos podem ser utilizados para mitigar a presença de viés indesejado em modelos de linguagem?",
            "resposta": "Os datasets linguísticos podem ser utilizados para mitigar a presença de viés indesejado em modelos de linguagem criando datasets que sejam representativos de diferentes grupos e perspectivas. Além disso, os datasets podem ser utilizados para avaliar a presença de viés em modelos de linguagem e identificar áreas para melhoria. Por exemplo, o dataset WinoBias foi criado para avaliar a presença de viés de gênero em modelos de linguagem."
        },
        {
            "pergunta": "Qual é a responsabilidade dos criadores de datasets linguísticos no PLN?",
            "resposta": "Os criadores de datasets linguísticos têm a responsabilidade de garantir que os dados sejam representativos e precisos, e que sejam utilizados de maneira ética e responsável. Além disso, eles têm a responsabilidade de garantir que os dados sejam acessíveis e utilizáveis por outros pesquisadores e desenvolvedores de modelos de linguagem."
        },
        {
            "pergunta": "Qual é a importância da avaliação conjunta em modelos e ferramentas de Processamento de Linguagem Natural (PLN)?",
            "resposta": "A avaliação conjunta é importante porque permite avaliar a qualidade dos modelos e ferramentas a partir de um mesmo conjunto de dados, garantindo que os resultados sejam confiáveis e generalizáveis. Além disso, a avaliação conjunta ajuda a identificar os pontos fortes e fracos dos modelos e ferramentas, permitindo melhorias e ajustes necessários."
        },
        {
            "pergunta": "Por que a qualidade dos datasets é fundamental para a avaliação confiável de modelos e ferramentas de PLN?",
            "resposta": "A qualidade dos datasets é fundamental porque um dataset de baixa qualidade pode levar a resultados de avaliação imprecisos e não confiáveis. Isso ocorre porque os modelos e ferramentas são treinados e avaliados com base nos dados disponíveis, e se esses dados forem de baixa qualidade, os resultados também serão de baixa qualidade. Além disso, um dataset de baixa qualidade pode não representar adequadamente a realidade, o que pode levar a resultados que não sejam generalizáveis."
        },
        {
            "pergunta": "Quais são as medidas de avaliação comuns utilizadas para avaliar a qualidade dos modelos e ferramentas de classificação em PLN?",
            "resposta": "As medidas de avaliação comuns utilizadas para avaliar a qualidade dos modelos e ferramentas de classificação em PLN são a precisão, a abrangência e a medida F. A precisão mede a qualidade das classificações realizadas, enquanto a abrangência mede a qualidade da quantidade de elementos classificados. A medida F é uma média harmônica entre a precisão e a abrangência, e é utilizada para avaliar o desempenho geral dos modelos e ferramentas."
        },
        {
            "pergunta": "Qual é a diferença entre precisão e abrangência em avaliação de modelos e ferramentas de classificação em PLN?",
            "resposta": "A precisão mede a qualidade das classificações realizadas, enquanto a abrangência mede a qualidade da quantidade de elementos classificados. Em outras palavras, a precisão mede se as classificações são corretas, enquanto a abrangência mede se todos os elementos que deveriam ter sido classificados foram encontrados e classificados corretamente. Uma ferramenta pode ser muito precisa, mas ter uma baixa abrangência, o que significa que ela pode não encontrar todos os elementos que deveriam ter sido classificados."
        },
        {
            "pergunta": "Por que é importante encontrar um equilíbrio entre precisão e abrangência em avaliação de modelos e ferramentas de classificação em PLN?",
            "resposta": "É importante encontrar um equilíbrio entre precisão e abrangência porque uma ferramenta que é muito precisa, mas tem uma baixa abrangência, pode não ser útil em prática. Da mesma forma, uma ferramenta que tem uma alta abrangência, mas é imprecisa, também pode não ser útil. O equilíbrio entre precisão e abrangência é fundamental para garantir que os modelos e ferramentas sejam eficazes e generalizáveis. A medida F é uma forma de avaliar esse equilíbrio e fornecer uma visão geral do desempenho dos modelos e ferramentas."
        },
        {
            "pergunta": "Qual é o papel dos datasets na capacidade de generalização dos modelos e ferramentas de PLN?",
            "resposta": "Os datasets têm um papel fundamental na capacidade de generalização dos modelos e ferramentas de PLN. A qualidade dos dados é essencial para garantir que os modelos e ferramentas sejam treinados e avaliados de forma adequada. Além disso, os datasets podem influenciar a capacidade de generalização dos modelos e ferramentas, pois podem conter vieses ou limitações que afetem a capacidade de generalização. Portanto, é importante garantir que os datasets sejam de alta qualidade e representem adequadamente a realidade para que os modelos e ferramentas sejam eficazes e generalizáveis."
        }
    ],
    "cap-avaliacao": [
        {
            "pergunta": "O que é necessário para tomar decisões informadas sobre um produto ou sistema?",
            "resposta": "Para tomar decisões informadas sobre um produto ou sistema, é necessário ter acesso a uma avaliação baseada em fontes de informação confiáveis. Isso inclui considerar opiniões de terceiros, relatórios de controle de qualidade e análises detalhadas de funcionamento."
        },
        {
            "pergunta": "Por que a avaliação é importante em Processamento de Linguagem Natural (PLN)?",
            "resposta": "A avaliação é importante em PLN porque permite entender quando e por que os modelos acertam ou erram, o que é essencial para decidir se eles estão prontos para serem usados e para aperfeiçoá-los. Além disso, a avaliação ajuda a garantir que os sistemas de PLN sejam transparentes, justos e abrangentes."
        },
        {
            "pergunta": "Quais são as dimensões que devem ser consideradas ao avaliar um sistema de PLN?",
            "resposta": "Ao avaliar um sistema de PLN, é necessário considerar uma variedade de dimensões, incluindo o que o sistema faz, seu propósito, manutenção necessária, custos e benefícios, performance em diferentes contextos, usuários, infraestrutura computacional, riscos, considerações éticas e licença. Além disso, é importante considerar o cenário econômico, modelo de negócios, disponibilidade de recursos e aspectos sociológicos e morais."
        },
        {
            "pergunta": "Quem está envolvido na avaliação de tecnologias de linguagem?",
            "resposta": "A avaliação de tecnologias de linguagem envolve uma variedade de pessoas e grupos, incluindo pesquisadores, desenvolvedores, usuários, empresas e clientes, autoridades e reguladores. Além disso, as tecnologias de linguagem podem ter impactos em comunidades como um todo, incluindo pessoas que não usam o sistema diretamente."
        },
        {
            "pergunta": "Por que a avaliação é um processo complexo e multifacetado?",
            "resposta": "A avaliação é um processo complexo e multifacetado porque envolve considerar uma variedade de fatores e perspectivas. Além disso, a avaliação está situada em um contexto específico e deve ser feita de acordo com ele. Isso significa que não há uma receita de bolo definitiva para avaliar tecnologias de linguagem, e é necessário adaptar as abordagens e métricas às necessidades específicas de cada caso."
        },
        {
            "pergunta": "Quais são os principais métodos, técnicas e métricas utilizados na avaliação de tecnologias de linguagem?",
            "resposta": "Os principais métodos, técnicas e métricas utilizados na avaliação de tecnologias de linguagem incluem a avaliação de desempenho, análise de erro, avaliação de usabilidade, avaliação de qualidade e métricas de desempenho, como precisão, recall e F1-score. Além disso, é importante considerar a ética e a responsabilidade na avaliação de tecnologias de linguagem."
        },
        {
            "pergunta": "Quais são os principais enfoques para avaliar um sistema?",
            "resposta": "Existem três principais enfoques para avaliar um sistema, conforme mencionado por Hirschman e Thompson (1997), King (1996) e Paroubek et al. (2007). Além disso, Sparck Jones (1994) definiu quatro grupos de conceitos que devem ser aplicados em uma avaliação, incluindo o sistema avaliado e fatores de performance, tipos e níveis de definição da análise de performance, formas dos dados para teste e avaliação e estratégias para o design e a condução da avaliação."
        },
        {
            "pergunta": "O que é considerado ao avaliar o sistema avaliado e fatores de performance?",
            "resposta": "Ao avaliar o sistema avaliado e fatores de performance, devemos considerar o que é o sistema, em que ambiente ele opera, quais fatores afetam sua performance, quais seus parâmetros e configurações, quais valores foram designados a suas variáveis, quais experimentos são rodados e quais suas funções e objetivos. Isso ajuda a entender como o sistema funciona e como ele pode ser otimizado."
        },
        {
            "pergunta": "Quais são os critérios utilizados para avaliar a performance de um sistema?",
            "resposta": "Os critérios utilizados para avaliar a performance de um sistema podem incluir eficiência, efetividade ou aceitação por humanos. Além disso, é importante considerar como esses critérios serão traduzidos em métricas, quais métodos serão utilizados e a que o sistema será comparado. Isso ajuda a garantir que a avaliação seja objetiva e precisa."
        },
        {
            "pergunta": "Qual é a importância dos dados para teste e avaliação?",
            "resposta": "Os dados para teste e avaliação são cruciais para garantir que a avaliação seja precisa e representativa. Devemos considerar o tipo dos dados e quão realistas eles são, além de garantir que a distribuição dos fenômenos nos dados seja condizente com a realidade em que o sistema vai funcionar. Além disso, é importante considerar a cobertura dos fenômenos linguísticos e criar test suites para avaliar casos específicos."
        },
        {
            "pergunta": "Quais são as estratégias para o design e a condução da avaliação?",
            "resposta": "As estratégias para o design e a condução da avaliação incluem ter em mente o objetivo, a alçada e o design da avaliação em si. Além disso, é importante considerar a existência de uma referência 'padrão ouro' (gold standard) e comparar a resposta do sistema com essa referência. Isso ajuda a garantir que a avaliação seja precisa e objetiva."
        },
        {
            "pergunta": "Quais são as dicotomias que podem ser utilizadas para avaliar um sistema?",
            "resposta": "Existem dez pares de características de uma avaliação que podem ser utilizadas para avaliar um sistema, incluindo dicotomias como automático vs. manual, objetivo vs. subjetivo, quantitativo vs. qualitativo, entre outras. Essas dicotomias podem ser utilizadas em conjunto ou em paralelo para garantir que a avaliação seja completa e precisa."
        },
        {
            "pergunta": "Quais são os atributos de qualidade de um sistema que podem ser levados em conta?",
            "resposta": "Existem vários atributos de qualidade de um sistema que podem ser levados em conta, incluindo precisão, eficiência, efetividade, aceitação por humanos, entre outros. Além disso, é importante considerar a tecnologia em si e como ela é utilizada, pois uma tecnologia é tão boa quanto o uso que se faça dela."
        },
        {
            "pergunta": "Por que é importante fazer avaliações orientadas aos usuários e usuárias do sistema?",
            "resposta": "É importante fazer avaliações orientadas aos usuários e usuárias do sistema porque isso ajuda a entender como o sistema é usado, em que circunstância e para quais fins. Além disso, isso ajuda a garantir que o sistema seja desenvolvido de forma a atender às necessidades dos usuários e usuárias, o que pode melhorar a performance e a aceitação do sistema."
        }
    ],
    "cap-avaliacao-conjunta": [
        {
            "pergunta": "O que é uma avaliação conjunta e como surgiu essa prática?",
            "resposta": "Uma avaliação conjunta é uma atividade que reúne vários participantes cujos sistemas ou modelos são comparados ao executar uma tarefa comum. Essa prática surgiu na década de 2000, inicialmente chamada de evaluation contest, evaluation campaign ou joint evaluation, e mais tarde, shared task. A equipe da Linguateca, um projeto iniciado em 2000 pelo governo de Portugal, foi uma das primeiras a organizar avaliações conjuntas com o objetivo de avançar a área e avaliar seu progresso."
        },
        {
            "pergunta": "Qual foi o papel da Linguateca na organização de avaliações conjuntas?",
            "resposta": "A Linguateca foi fundamental na organização de avaliações conjuntas, pois foi responsável por lançar o livro Avaliação Conjunta: um novo paradigma no processamento computacional da língua portuguesa, que contou com contribuições da maior parte dos pesquisadores de PLN brasileiros e portugueses da época. Além disso, a Linguateca tentou desempenhar o papel de instituição com essa função, embora tivesse um pessoal e um horizonte temporal reduzidos."
        },
        {
            "pergunta": "Por que as datas mencionadas nos parágrafos anteriores podem surpreender leitoras/es mais jovens?",
            "resposta": "As datas mencionadas nos parágrafos anteriores podem surpreender leitoras/es mais jovens devido à antiguidade. A escolha de mencionar datas do século passado foi consciente, motivada pela necessidade de sinalizar que boa parte dos pontos levantados já estão (ou estiveram) no radar do PLN há tempos. Além disso, ao retomar as avaliações conjuntas realizadas há tempos, bem como avaliações realizadas mais recentemente, busca-se mostrar a quem chega no PLN de língua portuguesa que nossa língua dispõe de muitos recursos públicos e disponíveis para avaliação."
        },
        {
            "pergunta": "Qual foi o objetivo da equipe ao retomar as avaliações conjuntas realizadas há tempos?",
            "resposta": "O objetivo da equipe ao retomar as avaliações conjuntas realizadas há tempos foi trazer mais visibilidade a – e fomentar a utilização de – tanto material e conhecimento já produzido na e para a nossa língua, mas que se encontra disperso por várias publicações. Além disso, ao retomar as avaliações conjuntas, busca-se mostrar a quem chega no PLN de língua portuguesa que nossa língua dispõe de muitos recursos públicos e disponíveis para avaliação."
        },
        {
            "pergunta": "Qual foi a importância da avaliação conjunta para o PLN?",
            "resposta": "A avaliação conjunta foi fundamental para o PLN, pois permitiu que vários participantes cujos sistemas ou modelos fossem comparados ao executar uma tarefa comum. Isso permitiu que se avançasse a área e se avaliasse o progresso. Além disso, a avaliação conjunta permitiu que se identificassem os pontos fortes e fracos dos sistemas e modelos, o que foi fundamental para o desenvolvimento do PLN."
        },
        {
            "pergunta": "Qual é o objetivo principal de uma avaliação conjunta em Processamento de Linguagem Natural (PLN)?",
            "resposta": "O objetivo principal de uma avaliação conjunta é promover a investigação em uma dada tarefa, através da comparação de vários sistemas/modelos e com base em recursos e tarefas comuns, evitando reinventar a roda e aumentando o número de trocas científicas entre grupos distintos."
        },
        {
            "pergunta": "Quais são os benefícios de uma avaliação conjunta em comparação com uma avaliação \"não-conjunta\"?",
            "resposta": "Os benefícios de uma avaliação conjunta incluem a colaboração e compartilhamento de recursos, a produção de padrões de funcionamento que evitem que grupos novos tenham que começar de novo o processo de pensar a avaliação, e a criação de recursos reutilizáveis que permitam a novos grupos de PLN não precisar começar do zero nessa área."
        },
        {
            "pergunta": "Quais são os objetivos de uma avaliação conjunta, de acordo com Voorhees e Tice (2000)?",
            "resposta": "Os objetivos de uma avaliação conjunta, de acordo com Voorhees e Tice (2000), são promover a investigação numa dada tarefa e investigar se a metodologia de avaliação é adequada, e se através dela é possível definir recursos de avaliação reutilizáveis."
        },
        {
            "pergunta": "Quais são as limitações das formas de avaliação tradicionais no PLN?",
            "resposta": "As limitações das formas de avaliação tradicionais no PLN são que o conhecimento obtido não é reutilizável por uma comunidade científica, e muitas vezes, esse conhecimento morre nas organizações devido à mobilidade dos investigadores ou ao encerramento dos projetos."
        },
        {
            "pergunta": "Quando é possível aplicar o modelo de uma avaliação conjunta?",
            "resposta": "É possível aplicar o modelo de uma avaliação conjunta quando há mais de um grupo interessado e mais de um sistema numa dada área. Antes disso, a única forma de avaliação possível é a autoavaliação."
        },
        {
            "pergunta": "Qual é o objetivo principal de participar de avaliações conjuntas?",
            "resposta": "O objetivo principal é criar um ambiente que permita separar o desenvolvimento de um sistema do teste de hipóteses, além de promover a fertilização cruzada e o reconhecimento público de uma área científica. Isso é alcançado através da discussão de ideias, comparação de resultados e acesso ao estado da arte e ao resultado mínimo garantido (baseline)."
        },
        {
            "pergunta": "Quais são os benefícios de ter prazos externos e conhecimento dos resultados de outros sistemas?",
            "resposta": "Os prazos externos e o conhecimento dos resultados de outros sistemas são importantes porque fornecem um contexto para a avaliação e permitam a comparação de resultados. Isso ajuda a identificar áreas de melhoria e a promover a inovação. Além disso, a existência de prazos externos pode ajudar a manter o foco e a motivação dos participantes."
        },
        {
            "pergunta": "Qual é o papel da comunidade de pessoas na transformação de uma atividade em uma área científica respeitada?",
            "resposta": "A comunidade de pessoas desempenha um papel fundamental na transformação de uma atividade em uma área científica respeitada. A comunidade fornece um ambiente para a discussão de ideias, a troca de experiências e a observação das consequências de opções diferentes. Isso promove a fertilização cruzada e o reconhecimento público, o que é essencial para o desenvolvimento de uma área científica."
        },
        {
            "pergunta": "Quais são os procedimentos necessários para que uma avaliação conjunta seja bem-sucedida?",
            "resposta": "Para que uma avaliação conjunta seja bem-sucedida, é necessário que se baseie em procedimentos rigorosos e consensuais. Isso inclui a definição clara de objetivos, a seleção de critérios de avaliação relevantes e a implementação de um processo de avaliação justo e transparente. Além disso, é importante garantir que todos os participantes estejam cientes dos procedimentos e dos critérios de avaliação."
        },
        {
            "pergunta": "Qual é o significado do termo 'baseline' no contexto da avaliação conjunta?",
            "resposta": "O termo 'baseline' se refere ao resultado mínimo garantido que é estabelecido como referência para a avaliação. Isso fornece um ponto de partida para a comparação de resultados e permite a identificação de áreas de melhoria. O baseline é importante porque fornece um contexto para a avaliação e ajuda a garantir que os resultados sejam comparáveis."
        },
        {
            "pergunta": "Qual foi a primeira tarefa para a qual organizaram uma avaliação conjunta?",
            "resposta": "A primeira tarefa para a qual organizaram uma avaliação conjunta foi a análise morfológica, que foi realizada através das Morfolimpíadas. A análise morfológica é uma tarefa importante na linguística computacional, pois envolve a identificação e a análise das unidades morfológicas de uma língua, como palavras, raízes e afixos. A língua portuguesa é particularmente desafiadora para a análise morfológica devido ao seu paradigma flexional rico, que inclui uma variedade de conjugações verbais e declinações nominais."
        },
        {
            "pergunta": "Por que as Morfolimpíadas foram motivadas?",
            "resposta": "As Morfolimpíadas foram motivadas pelo fato de a língua portuguesa ter um paradigma flexional rico, ao mesmo tempo que havia um número razoável de grupos que tinham algum tipo de processamento morfológico em seus sistemas. Isso significa que havia uma necessidade de avaliar e comparar os diferentes sistemas de processamento morfológico para a língua portuguesa, a fim de identificar os mais eficazes e precisos."
        },
        {
            "pergunta": "Como os participantes receberam os materiais de teste?",
            "resposta": "Os participantes receberam os materiais de teste (613 textos) em três formatos: texto corrido, uma unidade (“token”) por linha, e lista de tipos. Isso permitiu que os sistemas participantes pudessem processar os textos de diferentes maneiras e avaliar sua capacidade de realizar análise morfológica em diferentes contextos."
        },
        {
            "pergunta": "Qual foi o objetivo dos sistemas participantes?",
            "resposta": "Os sistemas participantes deveriam então retornar todos os três materiais de teste morfologicamente analisados, sem saber quais palavras seriam avaliadas. Isso significa que os sistemas tinham que ser capazes de realizar análise morfológica em textos desconhecidos e retornar os resultados de forma precisa e eficaz."
        },
        {
            "pergunta": "Qual foi o resultado mais interessante das Morfolimpíadas?",
            "resposta": "O resultado mais interessante das Morfolimpíadas foi a constatação de que havia – e pensamos que ainda há – uma considerável discordância teórica sobre a forma adequada de fazer a análise morfológica do português. Isso significa que os especialistas em linguística computacional ainda não concordam sobre a melhor abordagem para realizar análise morfológica em português, o que é um desafio importante para o campo."
        },
        {
            "pergunta": "O que foi divulgado na página da Linguateca dedicada às Morfolimpíadas?",
            "resposta": "Todos os resultados, dados e programas (em Perl) usados para organizar as Morfolimpíadas foram divulgados na página da Linguateca dedicada a esta avaliação. Isso significa que os resultados e os métodos utilizados nas Morfolimpíadas estão disponíveis para que outros pesquisadores e especialistas possam aprender com eles e melhorar seus próprios sistemas de processamento morfológico."
        },
        {
            "pergunta": "O que é o CLEF e qual é o seu objetivo?",
            "resposta": "O CLEF (Cross-lingual Evaluation Forum) é um fórum de avaliação multilíngue que visa avaliar e comparar as capacidades de sistemas de processamento de linguagem natural em diferentes idiomas. O objetivo do CLEF é promover a colaboração e a inovação em áreas como recuperação de informações, resposta automática a perguntas e processamento de linguagem natural em geral."
        },
        {
            "pergunta": "Por que a Linguateca decidiu colaborar com o CLEF em 2003?",
            "resposta": "A Linguateca decidiu colaborar com o CLEF em 2003 porque, do ponto de vista da língua portuguesa, tomar parte em uma avaliação conjunta com um grande público e muita experiência parecia uma boa ideia. Além disso, a colaboração com o CLEF permitiria à Linguateca ganhar experiência em áreas nas quais não tinha nenhuma experiência anterior, como recuperação de informações e resposta automática a perguntas."
        },
        {
            "pergunta": "Quais foram as tarefas inovadoras organizadas pela Linguateca no CLEF?",
            "resposta": "A Linguateca organizou várias tarefas inovadoras no CLEF, incluindo o GeoCLEF, o GikiP e o GikiCLEF. Além disso, a Linguateca também ajudou na organização de outras tarefas, como o ImageCLEF, o WebCLEF, o ResPubliQA e o LogCLEF. Essas tarefas visavam avaliar as capacidades de sistemas de processamento de linguagem natural em áreas específicas, como processamento de texto e imagem."
        },
        {
            "pergunta": "Qual foi o resultado da colaboração da Linguateca com o CLEF em termos de participantes?",
            "resposta": "Infelizmente, a colaboração da Linguateca com o CLEF não resultou no aumento esperado de participantes de processamento em português. Na verdade, houve menos participantes de processamento em português no CLEF do que no HAREM, o que foi uma surpresa para a Linguateca."
        },
        {
            "pergunta": "Qual foi o período de tempo em que a Linguateca fez parte da organização do CLEF?",
            "resposta": "A Linguateca fez parte da organização do CLEF de 2003 a 2009, um período de 6 anos. Durante esse tempo, a Linguateca colaborou ativamente com o CLEF e organizou várias tarefas inovadoras."
        },
        {
            "pergunta": "Qual foi o objetivo do Págico?",
            "resposta": "O objetivo do Págico foi avaliar sistemas capazes de encontrar respostas não triviais a necessidades de informação complexas, em língua portuguesa, utilizando perguntas relacionadas à cultura dos países lusófonos."
        },
        {
            "pergunta": "Por que o Págico é considerado um precursor de conjuntos de dados que comparam desempenho humano e das máquinas ao responder a perguntas?",
            "resposta": "O Págico é considerado um precursor de conjuntos de dados que comparam desempenho humano e das máquinas ao responder a perguntas porque foi uma das primeiras avaliações conjuntas a comparar o desempenho de sistemas automáticos e humanos em responder a perguntas complexas em língua portuguesa."
        },
        {
            "pergunta": "Quais foram os motivos para a pouca participação no Págico?",
            "resposta": "Os motivos para a pouca participação no Págico podem ter sido a dificuldade da tarefa, o fato de a tarefa subjacente não ser uma prioridade para a maioria das pessoas que trabalhavam com PLN em português, ou a impossibilidade de disponibilizar um conjunto de treino para que as equipes treinassem seus modelos."
        },
        {
            "pergunta": "Como foram criados os tópicos do Págico?",
            "resposta": "A criação dos tópicos foi concomitante à coleta de possíveis respostas, a fim de permitir, posteriormente, o cálculo de uma 'pseudo-abrangência' (e correspondente pseudo-medida F). Os tópicos tentaram abranger todos os lugares onde se fala português e foram distribuídos entre Letras, Artes, Geografia, Cultura, Política, Esportes, Ciências e Economia."
        },
        {
            "pergunta": "Quais foram as medidas utilizadas para avaliar a participação no Págico?",
            "resposta": "As medidas utilizadas para avaliar a participação no Págico foram precisão, pseudo-abrangência, precisão relaxada (sem avaliar a justificativa), originalidade e criatividade. A originalidade recompensa as respostas corretas que foram dadas somente naquela corrida/por aquele participante, enquanto a criatividade é uma medida que pontua (uma corrida ou um participante) de forma inversamente proporcional ao número de corridas diferentes (ou participantes) que forneceram a mesma resposta."
        },
        {
            "pergunta": "Quais foram os resultados do Págico?",
            "resposta": "Os participantes humanos foram mais criativos e originais, mas é também interessante constatar que os sistemas conseguiram encontrar algumas respostas que não foram encontradas pelos participantes humanos. Todos os dados, incluindo a coleção da Wikipédia de 25 de abril de 2011 em XML, composta por 681.058 documentos, o conjunto de respostas e os resultados foram disponibilizados no recurso que chamamos Cartola."
        },
        {
            "pergunta": "O que foi a primeira ASSIN e quais foram seus objetivos?",
            "resposta": "A primeira ASSIN foi a Avaliação de Similaridade Semântica e de Inferência textual, organizada pelo NILC, e teve como objetivos avaliar a inferência textual e a similaridade semântica. Ela propôs duas tarefas separadas, ambas sobre pares de frases, e foi inspirada por iniciativas relacionadas para o inglês, mas desenvolveu um método original para obter os pares de frases iniciais com base em notícias do Google news em português do Brasil e em português de Portugal."
        },
        {
            "pergunta": "Como foram obtidos os pares de frases iniciais para a ASSIN?",
            "resposta": "Os pares de frases iniciais foram obtidos com base em notícias do Google news em português do Brasil e em português de Portugal. A ASSIN desenvolveu um método original para obter esses pares de frases, que foram selecionados e eventualmente ligeiramente modificados pela organização. Essa abordagem foi diferente das iniciativas anteriores, que se baseavam em frases criadas por peritos."
        },
        {
            "pergunta": "Quantos sistemas participaram na tarefa de similaridade e na tarefa de inferência textual da ASSIN?",
            "resposta": "Seis sistemas diferentes participaram na tarefa de similaridade, e quatro na tarefa de inferência textual da ASSIN. Além disso, a ASSIN propôs sistemas de resultado mínimo garantido (baselines) que se mostraram bastante bons."
        },
        {
            "pergunta": "O que foi a ASSIN 2 e como ela se diferenciou da ASSIN 1?",
            "resposta": "A ASSIN 2 foi uma avaliação conjunta que se caracterizou por tornar a tarefa mais simples, de duas maneiras. Ela foi organizada por um conjunto mais alargado de pesquisadores, incluindo o primeiro autor da ASSIN 1. A ASSIN 2 teve como objetivo tornar a tarefa mais fácil e comparável com outras avaliações conjuntas para o inglês. Os recursos criados para a ASSIN 2 foram tornados públicos após a conferência final e estão disponíveis na página da ASSIN."
        },
        {
            "pergunta": "Qual foi o impacto da simplificação da tarefa na ASSIN 2?",
            "resposta": "A simplificação da tarefa na ASSIN 2 levou a um aumento significativo nos números de desempenho dos sistemas participantes. Além disso, permitiu que fosse mais simples comparar os resultados com o estado da arte para outras línguas. A organização considerou que o esforço de simplificar a tarefa foi benéfico, não apenas porque os sistemas puderam ter um desempenho melhor, mas também porque o corpus da ASSIN 2 foi feito com uma estratégia de anotação comparável a de outras avaliações conjuntas para o inglês."
        },
        {
            "pergunta": "O que é o IberLEF e qual foi o seu papel em 2019?",
            "resposta": "O IberLEF (Iberian Languages Evaluation Forum) é um fórum de avaliação de linguagens ibéricas. Em 2019, foi realizado um evento no qual foram propostas três tarefas de avaliação para o português, com o objetivo de avaliar o desempenho de sistemas de processamento de linguagem natural em diferentes áreas, como reconhecimento de entidades mencionadas e extração de relações."
        },
        {
            "pergunta": "Quais foram as três tarefas propostas no IberLEF 2019?",
            "resposta": "As três tarefas propostas foram: 1) uma tarefa de reconhecimento de entidades mencionadas, que visava aumentar o tipo de dados sobre os quais essa tarefa se aplicava, incluindo documentos clínicos e da polícia; 2) uma tarefa de reconhecimento de entidades mencionadas ampliada para cobrir outros tipos de entidades; e 3) uma tarefa de extração de relações entre entidades, chamada de open information extraction, que foi pioneira para o português."
        },
        {
            "pergunta": "Quais foram as instituições participantes e quantas corridas foram enviadas?",
            "resposta": "Ao todo, participaram seis instituições nas três tarefas. Houve cinco participantes na primeira tarefa, que enviaram seis corridas, um na segunda tarefa e três na terceira tarefa, que também enviaram seis corridas."
        },
        {
            "pergunta": "O que são as coleções douradas (ou padrão-ouro) e onde podem ser encontradas?",
            "resposta": "As coleções douradas (ou padrão-ouro) são conjuntos de dados anotados que servem como referência para avaliar o desempenho de sistemas de processamento de linguagem natural. As coleções douradas do IberLEF 2019, excetuando os textos clínicos e da polícia para proteção de dados pessoais, assim como os programas de avaliação, encontram-se públicos na página do IberLEF 2019."
        },
        {
            "pergunta": "Qual foi o objetivo da tarefa de reconhecimento de entidades mencionadas e como ela se aplicava a diferentes tipos de dados?",
            "resposta": "O objetivo da tarefa de reconhecimento de entidades mencionadas era aumentar o tipo de dados sobre os quais essa tarefa se aplicava. Isso incluiu a utilização de documentos clínicos e da polícia, que foram inovações trazidas para essa tarefa. Além disso, foram distribuídos recursos já anotados para treinamento dos sistemas."
        },
        {
            "pergunta": "O que foi o IberLEF 2021 e qual foi a nova tarefa para o português?",
            "resposta": "O IberLEF 2021 foi um evento que contou com uma nova tarefa para o português, denominada Irony Detection in Portuguese (IDPT), que visava detectar ironia em textos em português. Essa tarefa foi criada pelos organizadores Corrêa et al. em 2021 e teve como objetivo avaliar a capacidade de sistemas de processamento de linguagem natural detectar ironia em textos em português."
        },
        {
            "pergunta": "Quais foram os recursos criados para a tarefa IDPT e como foram anotados?",
            "resposta": "Os organizadores criaram dois recursos para a tarefa IDPT: um conjunto de tweets e outro de notícias em português. Esses recursos foram anotados com informações sobre a presença ou ausência de ironia em cada texto, baseados em jornais e tweets brasileiros. A anotação foi feita manualmente por especialistas em linguagem e ironia."
        },
        {
            "pergunta": "Qual foi o interesse internacional na tarefa IDPT e quantos grupos participantes houve?",
            "resposta": "A tarefa IDPT recebeu interesse internacional além do lusófono, com seis grupos participantes de diferentes países. Dentre esses grupos, havia um grupo chinês e outro espanhol, demonstrando o interesse global em detectar ironia em textos em português."
        },
        {
            "pergunta": "Houve diferenças entre a detecção de ironia em notícias e tweets e quais tecnologias obtiveram o melhor desempenho em cada tipo de texto?",
            "resposta": "Sim, houve diferenças entre a detecção de ironia em notícias e tweets. Diferentes tecnologias obtiveram o melhor desempenho em cada tipo de texto, demonstrando que a detecção de ironia em textos em português é um desafio complexo que requer abordagens específicas para cada tipo de texto."
        },
        {
            "pergunta": "Quais foram os corpora utilizados para treinamento e quais jornais eletrônicos brasileiros forneceram os conjuntos de textos?",
            "resposta": "Os corpora utilizados para treinamento foram corpora já públicos, contendo 15212 tweets. Além disso, os conjuntos de textos (datasets) foram fornecidos por três jornais eletrônicos brasileiros: O Estadão, O sensacionalista e o The Piauí Herald. Esses conjuntos de textos foram utilizados para treinar os modelos de detecção de ironia em textos em português."
        },
        {
            "pergunta": "Onde estão disponíveis os dados de teste da tarefa IDPT?",
            "resposta": "Os dados de teste da tarefa IDPT estão disponíveis na página do IDPT2021. Essa página fornece acesso aos dados de teste utilizados para avaliar os modelos de detecção de ironia em textos em português, permitindo que os pesquisadores e desenvolvedores de sistemas de processamento de linguagem natural possam avaliar e melhorar seus modelos."
        },
        {
            "pergunta": "O que foi o IberLEF 2022 e qual foi a tarefa realizada?",
            "resposta": "O IberLEF 2022 foi um evento que contou com a tarefa Aspect-Based Sentiment Analysis in Portuguese (ABSAPT), que teve como objetivo identificar a opinião (positiva, neutra ou negativa) sobre aspectos (ou características) de produtos em resenhas para o TripAdvisor em português, relativos a hotelaria e turismo. Essa tarefa foi inspirada por tarefas congêneres no SemEval35."
        },
        {
            "pergunta": "Quais foram as subtarefas da tarefa ABSAPT?",
            "resposta": "A tarefa ABSAPT estava dividida em duas subtarefas: identificar primeiro o aspecto, e depois qual a opinião (em inglês, sentiment) sobre esse aspecto. Em outras palavras, a primeira subtarefa envolvia identificar as características ou aspectos dos produtos mencionados nas resenhas, e a segunda subtarefa envolvia determinar a opinião do autor da resenha sobre esses aspectos."
        },
        {
            "pergunta": "Quantos aspectos diferentes foram listados por Silva et al. (2022) e quantos estavam presentes no material de treinamento?",
            "resposta": "Silva et al. (2022) listaram 40 aspectos diferentes, dos 77 presentes no material de treinamento. Isso significa que os autores selecionaram uma parcela dos aspectos presentes no material de treinamento para serem utilizados na tarefa ABSAPT."
        },
        {
            "pergunta": "Quem foram os participantes da tarefa ABSAPT e de onde eram?",
            "resposta": "Houve cinco participantes na tarefa ABSAPT, e todos eles eram brasileiros. Isso sugere que a tarefa teve uma participação exclusiva de equipes brasileiras, o que pode ser um reflexo da importância da língua portuguesa na região."
        },
        {
            "pergunta": "Onde está disponível o conjunto de dados de treino e teste da tarefa ABSAPT?",
            "resposta": "O conjunto de dados de treino e teste da tarefa ABSAPT está disponível para uso acadêmico na página dedicada ao ABSAPT2022. Isso significa que os dados estão acessíveis para que outros pesquisadores possam utilizar e estudar, contribuindo para o avanço da área de processamento de linguagem natural."
        },
        {
            "pergunta": "O que é a avaliação DIP e qual foi o seu objetivo?",
            "resposta": "A avaliação DIP (Denominação não especificada no texto) foi uma avaliação conjunta organizada pela Linguateca e o NuPILL, com o objetivo de identificar personagens, relações familiares e características em obras literárias em português. O objetivo era avaliar a capacidade dos sistemas de processamento de linguagem natural (PLN) em realizar tarefas agregadoras, como reconhecimento de entidades mencionadas, reconhecimento de profissões e reconhecimento de relações familiares, em obras literárias completas."
        },
        {
            "pergunta": "Quais foram os recursos disponibilizados para a avaliação DIP?",
            "resposta": "Foram disponibilizados 100 obras completas em formato de texto e 100 obras em formato PDF. Além disso, foram criados padrões-ouro para 20 obras de cada conjunto, que serviram como referência para a avaliação."
        },
        {
            "pergunta": "Quantos sistemas participaram da avaliação DIP?",
            "resposta": "Embora houvesse vários interessados inicialmente, apenas um sistema participou da avaliação DIP."
        },
        {
            "pergunta": "Quais foram os resultados da avaliação DIP?",
            "resposta": "A avaliação DIP resultou na criação de um conjunto de recursos interessantes sobre leitura distante em português, incluindo métodos de avaliação inovadores. Esses recursos estão disponíveis na página do DIP e podem ser usados por equipes que desenvolvam novos sistemas baseados em treinamento."
        },
        {
            "pergunta": "Como os recursos criados na avaliação DIP podem ser usados no futuro?",
            "resposta": "Os recursos criados na avaliação DIP podem ser usados por equipes que desenvolvam novos sistemas baseados em treinamento, como foi a estratégia no caso do IberLEF 2019 em relação ao HAREM e ao ReReLEM. Isso pode ajudar a melhorar a capacidade dos sistemas de PLN em realizar tarefas agregadoras em obras literárias."
        }
    ],
    "cap-geracao-ln": [
        {
            "pergunta": "O que é a Biblioteca de Babel e como é descrita no conto de Jorge Luis Borges?",
            "resposta": "A Biblioteca de Babel é uma biblioteca fictícia descrita no conto de Jorge Luis Borges como uma estrutura hexagonal que armazena todos os textos gerados a partir da combinação de 23 caracteres do alfabeto romano, incluindo letras, espaço, vírgula e ponto. Ela contém todos os textos já escritos e que ainda serão escritos, incluindo jornais, livros e até mesmo o próprio conto de Borges."
        },
        {
            "pergunta": "Quem poderia escrever todos os documentos presentes na Biblioteca de Babel?",
            "resposta": "Em teoria, um computador poderia gerar a biblioteca de Babel, mas seria um processo extremamente lento e ineficiente. O projeto The Library of Babel é um exemplo de como isso pode ser feito de forma digital, mas ainda assim é uma tarefa monumental. A questão de quem poderia escrever todos os documentos é uma pergunta filosófica que desafia a ideia de criatividade e inteligência humana."
        },
        {
            "pergunta": "Como o projeto The Library of Babel concretiza a ideia de Borges?",
            "resposta": "O projeto The Library of Babel é uma variação digital da biblioteca descrita por Borges. Ele disponibiliza uma biblioteca de câmaras hexagonais, cada uma com 4 paredes de estantes, cinco estantes por parede e 32 volumes por estante. Cada volume consiste em um texto de 410 páginas representando combinações dos 23 caracteres. O projeto permite buscar por trechos de até 3200 caracteres e acessar páginas aleatórias, tornando-se uma concretização prática da ideia de Borges."
        },
        {
            "pergunta": "Qual é a deficiência da Biblioteca de Babel?",
            "resposta": "A deficiência da Biblioteca de Babel é que, apesar de armazenar todo o conhecimento presente em todos os livros que já ou ainda serão escritos, a maioria dos textos são sem sentido e não seguem o padrão gramatical de uma língua. Isso torna difícil encontrar e catalogar os textos que de fato contêm um significado."
        },
        {
            "pergunta": "Quem é o bibliotecário e qual é seu papel na Biblioteca de Babel?",
            "resposta": "O bibliotecário é a figura responsável por catalogar os volumes que de fato estão escritos em uma linguagem natural e contêm um significado. Ele é o responsável por encontrar e organizar os textos relevantes entre a vasta quantidade de conteúdo sem sentido. O bibliotecário onisciente seria capaz de adquirir e gerar todos os textos relevantes já escritos ou que serão escritos."
        },
        {
            "pergunta": "Seria possível criar uma máquina que se torne o bibliotecário onisciente?",
            "resposta": "A geração de linguagem natural é um campo de pesquisa que busca criar máquinas capazes de entender e gerar linguagem natural. No entanto, criar uma máquina que se torne o bibliotecário onisciente é um desafio extremamente complexo e ainda não foi alcançado. A capacidade de entender e gerar linguagem natural de forma humana é um dos maiores desafios da inteligência artificial."
        },
        {
            "pergunta": "O que é a arquitetura de GLN a partir de templates e como ela se compara às outras arquiteturas apresentadas?",
            "resposta": "A arquitetura de GLN a partir de templates é a mais simples entre as apresentadas. Ela segue a mesma divisão em fases de Seleção de Conteúdo e Realização Textual de outras arquiteturas, mas utiliza templates pré-determinados para verbalizar cada tipo de conteúdo selecionado. Isso significa que, em vez de criar um texto do zero, a arquitetura de GLN a partir de templates usa modelos pré-existentes para gerar o texto final."
        },
        {
            "pergunta": "O que são os templates utilizados na arquitetura de GLN e como eles funcionam?",
            "resposta": "Os templates são 'textos enlatados' (canned texts) que contêm marcações específicas ou tags que são substituídas por valores selecionados na primeira fase da geração. Em outras palavras, os templates são modelos de texto pré-determinados que são personalizados com informações específicas para criar o texto final. Esses modelos podem incluir marcações para indicar onde os valores devem ser inseridos, tornando o processo de geração de texto mais eficiente."
        },
        {
            "pergunta": "Quais são os processamentos ortográficos que podem ser necessários para os templates?",
            "resposta": "Os processamentos ortográficos que podem ser necessários para os templates incluem apresentar a primeira letra da frase como maiúscula se o texto em questão for uma frase completa, colocar (em certos contextos) uma frase utilizada como título em letra maiúscula, apresentar elementos específicos do texto em negrito ou itálico para indicar ênfase ou algum outro propósito, ou ainda determinar a pontuação final da frase dependendo se esta for uma afirmação, uma pergunta, um imperativo, ou se serve como introdução a uma lista detalhada. Esses processamentos ajudam a garantir que o texto final seja gramaticalmente correto e fácil de ler."
        },
        {
            "pergunta": "Quem definiu os templates como 'textos enlatados' (canned texts) e o que essa definição implica?",
            "resposta": "Reiter e Dale (2000) definiram os templates como 'textos enlatados' (canned texts). Essa definição implica que os templates são especificações de conteúdo onde a sequência de caracteres a ser usada já foi determinada, embora algum processamento ortográfico ainda possa ser necessário. Em outras palavras, os templates são modelos de texto pré-existentes que podem ser personalizados com informações específicas para criar o texto final."
        },
        {
            "pergunta": "Qual é o objetivo da arquitetura de GLN a partir de templates e como ela pode ser útil?",
            "resposta": "O objetivo da arquitetura de GLN a partir de templates é gerar texto de forma eficiente e personalizada. Essa abordagem pode ser útil em uma variedade de aplicações, como geração de relatórios, respostas automáticas a perguntas frequentes, ou criação de conteúdo para sites. A arquitetura de GLN a partir de templates permite que os desenvolvedores criem modelos de texto que podem ser personalizados com informações específicas, tornando o processo de geração de texto mais rápido e eficiente."
        },
        {
            "pergunta": "Qual é o objetivo principal da arquitetura pipeline na geração de linguagem natural?",
            "resposta": "O objetivo principal da arquitetura pipeline é gerar linguagem natural por meio de várias transformações da representação de entrada, utilizando vários módulos especialistas em diferentes funções do processo para produzir um texto final."
        },
        {
            "pergunta": "Quais são as três fases sequenciais da arquitetura pipeline de Reiter e Dale (2000)?",
            "resposta": "As três fases sequenciais da arquitetura pipeline de Reiter e Dale (2000) são: (1) Planejamento de Documento, ou Macro-planejamento, que contempla a seleção do conteúdo e sua estruturação; (2) Planejamento de Sentença, ou Micro-planejamento, que enfoca o planejamento de cada sentença do texto; e (3) Realização Textual, que é responsável pelos ajustes finais do texto."
        },
        {
            "pergunta": "O que é o Planejamento de Documento na arquitetura pipeline?",
            "resposta": "O Planejamento de Documento é a fase da arquitetura pipeline que contempla a seleção do conteúdo e sua estruturação na forma de capítulos, seções, parágrafos, sentenças, etc. Essa fase é responsável por definir a estrutura geral do texto a ser gerado."
        },
        {
            "pergunta": "Quais são as seis etapas sequenciais da arquitetura pipeline de Castro Ferreira et al. (2019)?",
            "resposta": "As seis etapas sequenciais da arquitetura pipeline de Castro Ferreira et al. (2019) são: (1) Seleção de Conteúdo, (2) Ordenação de Discurso, (3) Estruturação de Texto, (4) Lexicalização, (5) Geração de Expressões de Referência e (6) Realização Textual. Essas etapas são implementadas utilizando redes neurais."
        },
        {
            "pergunta": "Qual é o papel da Lexicalização na arquitetura pipeline de Castro Ferreira et al. (2019)?",
            "resposta": "A Lexicalização é a fase da arquitetura pipeline de Castro Ferreira et al. (2019) que visa encontrar as frases e palavras adequadas para expressar o conteúdo a ser incluído em cada frase. Essa fase é responsável por selecionar as palavras e expressões que melhor se adequam ao contexto do texto."
        },
        {
            "pergunta": "O que é o robô-jornalista 'DaMata'?",
            "resposta": "O robô-jornalista 'DaMata' é um sistema de geração de linguagem natural desenvolvido a partir da arquitetura pipeline de Castro Ferreira et al. (2019) para cobrir o desmatamento na Amazônia Brasileira. Esse sistema utiliza a arquitetura pipeline para gerar textos sobre o tema do desmatamento."
        },
        {
            "pergunta": "Qual é o objetivo principal da arquitetura ponta-a-ponta (end-to-end) em geração de linguagem natural?",
            "resposta": "O objetivo principal da arquitetura ponta-a-ponta é gerar um texto a partir de uma representação (ou outro texto) de entrada sem representações intermediárias explícitas, utilizando modelos de aprendizado profundo como ferramenta principal."
        },
        {
            "pergunta": "Quais são as principais diferenças entre a arquitetura encoder-decoder com mecanismo de atenção e a arquitetura Transformer?",
            "resposta": "A principal diferença é que a arquitetura Transformer aplica os pesos de atenção no processo de codificação, considerando os pesos de atenção de tokens antecessores e sucessores de entrada, além de ser otimizada para o processo de treinamento, processando mais tokens em menos tempo e sendo mais eficiente do que sua antecessora."
        },
        {
            "pergunta": "Qual é o papel do mecanismo de atenção na arquitetura ponta-a-ponta?",
            "resposta": "O mecanismo de atenção aumenta significativamente a adequação e fluência das traduções, permitindo que o modelo se concentre nos tokens mais relevantes da entrada para gerar a saída desejada. Além disso, o mecanismo de atenção também é utilizado para gerar a representação vetorial de um token de entrada, considerando os pesos de atenção de seus tokens antecessores e sucessores de entrada."
        },
        {
            "pergunta": "Como a arquitetura Transformer se divide em termos de módulos?",
            "resposta": "A arquitetura Transformer se divide entre um módulo codificador e outro decodificador, onde o módulo codificador é responsável por gerar a representação vetorial de entrada e o módulo decodificador é responsável por gerar a saída desejada com base na representação vetorial de entrada."
        },
        {
            "pergunta": "Qual é o impacto da arquitetura Transformer no processo de treinamento?",
            "resposta": "A arquitetura Transformer é otimizada para o processo de treinamento, permitindo que o modelo processe mais tokens em menos tempo e seja mais eficiente do que sua antecessora. Isso é alcançado através da aplicação de pesos de atenção no processo de codificação e da utilização de uma arquitetura mais eficiente para o treinamento."
        },
        {
            "pergunta": "Qual era o método tradicional de treinamento de arquiteturas neurais ponta-a-ponta para domínios específicos de GLN?",
            "resposta": "As arquiteturas neurais ponta-a-ponta eram treinadas uma única vez para um domínio específico de GLN, como a verbalização de casos de COVID-19 no Brasil. No entanto, esse método exigia uma grande quantidade de dados anotados no domínio específico, pois os pesos das redes neurais eram inicializados de forma aleatória durante o treinamento."
        },
        {
            "pergunta": "Por que o método tradicional de treinamento de arquiteturas neurais ponta-a-ponta não era eficiente?",
            "resposta": "O método tradicional de treinamento de arquiteturas neurais ponta-a-ponta não era eficiente porque exigia uma grande quantidade de dados anotados no domínio específico. Além disso, os pesos das redes neurais eram inicializados de forma aleatória durante o treinamento, o que poderia levar a resultados imprevisíveis e inconsistentes."
        },
        {
            "pergunta": "Qual é o método atual de treinamento de arquiteturas neurais ponta-a-ponta para aplicações de GLN?",
            "resposta": "O método atual de treinamento de arquiteturas neurais ponta-a-ponta para aplicações de GLN é iniciar o treinamento a partir de modelos neurais pré-treinados. Esses modelos são construídos com base em tarefas genéricas e grandes volumes de dados, o que permite que o modelo desenvolva habilidades e conhecimentos de uso geral que podem ser transferidos para tarefas posteriores, como a GLN."
        },
        {
            "pergunta": "Qual é o benefício do pré-treinamento de modelos neurais para aplicações de GLN?",
            "resposta": "O pré-treinamento de modelos neurais permite que o modelo desenvolva habilidades e conhecimentos de uso geral que podem ser transferidos para tarefas posteriores, como a GLN. Isso reduz a necessidade de grandes quantidades de dados anotados no domínio específico e melhora a eficiência do treinamento."
        },
        {
            "pergunta": "Quem são os autores que discutem o pré-treinamento de modelos neurais para aplicações de GLN?",
            "resposta": "Os autores Raffel et al. (2020) discutem o pré-treinamento de modelos neurais para aplicações de GLN e destacam os benefícios desse método para o desenvolvimento de habilidades e conhecimentos de uso geral que podem ser transferidos para tarefas posteriores."
        }
    ],
    "cap-modelos-linguagem": [
        {
            "pergunta": "Qual é o avanço significativo observado no desenvolvimento e popularização do aprendizado de representações numéricas para linguagem desde a segunda década do século XXI?",
            "resposta": "Desde a segunda década do século XXI, houve um avanço significativo no desenvolvimento e popularização do aprendizado de representações numéricas para linguagem, com modelos de linguagem computacionais sendo utilizados para representar textos escritos, fala, especificações matemáticas, código e até mesmo informações genéticas e moleculares."
        },
        {
            "pergunta": "Por que é importante representar informações essencialmente simbólicas em um formato numérico?",
            "resposta": "A resposta mais simples e direta é que os computadores gostam de números. Representar informações simbólicas em um formato numérico permite que os computadores processem e analisem essas informações de forma eficiente."
        },
        {
            "pergunta": "Como os modelos de linguagem produzidos por redes neurais representam informações simbólicas em um formato numérico?",
            "resposta": "Os modelos de linguagem produzidos por redes neurais representam informações simbólicas em um formato numérico mapeando os componentes da língua para vetores em um espaço semântico, seguindo a hipótese distribucional. Isso significa que as palavras ou componentes léxicos são representados como vetores numéricos que capturam sua semântica e contexto."
        },
        {
            "pergunta": "O que é a hipótese distribucional e como ela é utilizada nos modelos de linguagem?",
            "resposta": "A hipótese distribucional é a ideia de que o significado de uma palavra pode ser depreendido pelas palavras que coocorrem com ela. Nos modelos de linguagem, essa hipótese é utilizada para representar as palavras como vetores numéricos que capturam sua semântica e contexto. Isso permite que os modelos de linguagem gerem representações numéricas que sumarizem os contextos em que as palavras ocorrem."
        },
        {
            "pergunta": "O que é um modelo de linguagem e como ele é utilizado nos modelos de linguagem computacionais?",
            "resposta": "Um modelo de linguagem é uma simplificação de um fenômeno complexo, no caso, a língua. Nos modelos de linguagem computacionais, um modelo de linguagem é utilizado para representar a língua de forma que possa ser processada por computadores. O modelo de linguagem deve respeitar os princípios léxicos, sintáticos e semânticos da língua e deve ser capaz de completar a próxima palavra em uma sequência, considerando todas as palavras que vieram antes."
        },
        {
            "pergunta": "Como os modelos de linguagem computacionais processam e produzem linguagem?",
            "resposta": "Os modelos de linguagem computacionais processam e produzem linguagem assumindo que um texto escrito ou falado é oriundo de um processo de completação. Isso significa que o modelo de linguagem é capaz de completar a próxima palavra em uma sequência, considerando todas as palavras que vieram antes. Alguns modelos também podem considerar completar partes de uma sequência considerando palavras (ou tokens) que vieram antes ou depois do elemento que se deseja completar."
        },
        {
            "pergunta": "O que é a modelagem probabilística de linguagem em termos computacionais?",
            "resposta": "A modelagem probabilística de linguagem é a tarefa que atribui uma probabilidade a uma sequência de palavras, considerando a probabilidade de cada palavra dada as palavras anteriores na sequência."
        },
        {
            "pergunta": "Qual é a fórmula para calcular a probabilidade de uma sequência de palavras?",
            "resposta": "A fórmula para calcular a probabilidade de uma sequência de palavras é dada pela regra da cadeia da probabilidade, que pode ser expressa como: P(p_{1:i}) = P(p_1)P(p_2 | p_1)P(p_3 | p_{1:2})P(p_4 | p_{1:3}) ... P(p_i | p_{1:i-1})."
        },
        {
            "pergunta": "Qual é o problema do underflow e como pode ser resolvido?",
            "resposta": "O problema do underflow ocorre quando a multiplicação de valores menores que um resulta em um valor muito pequeno, próximo de zero. Para resolver esse problema, pode-se usar a função logarítmica para somar os termos em vez de multiplicá-los, o que ajuda a evitar o underflow."
        },
        {
            "pergunta": "O que são modelos autorregressivos ou causais e como são usados?",
            "resposta": "Modelos autorregressivos ou causais são modelos de linguagem que seguem a formulação de predizer a próxima palavra dada as palavras anteriores na sequência. Esses modelos são frequentemente usados para tarefas que envolvem geração de texto, como completar uma sequência de palavras com a próxima palavra."
        },
        {
            "pergunta": "Qual é a diferença entre um modelo n-grama e um modelo de Markov?",
            "resposta": "Um modelo n-grama é um modelo que considera a probabilidade de uma palavra dada as n palavras anteriores na sequência, enquanto um modelo de Markov é um modelo que considera apenas a palavra anterior para predizer a próxima palavra. O modelo de Markov é um caso especial de modelo n-grama, onde n=1."
        },
        {
            "pergunta": "Qual é o trade-off entre a precisão e a eficiência de um modelo n-grama?",
            "resposta": "A precisão de um modelo n-grama aumenta com o valor de n, pois mais informações são consideradas para predizer a próxima palavra. No entanto, a eficiência do modelo diminui com o valor de n, pois mais cálculos são necessários para predizer a próxima palavra. Portanto, existe um trade-off entre a precisão e a eficiência de um modelo n-grama."
        },
        {
            "pergunta": "O que é o uso de n-grams e como ele é relacionado à generalização e eficiência no cálculo da probabilidade de uma sequência de palavras?",
            "resposta": "O uso de n-grams é uma técnica utilizada para generalizar e tornar eficiente o cálculo da probabilidade de uma sequência de palavras. Isso é feito considerando a probabilidade de uma palavra aparecer em uma sequência, dada a presença de outras palavras anteriores. Essa técnica é útil para lidar com sequências de palavras que não foram vistas durante o treinamento do modelo, permitindo que o modelo generalize e faça previsões mais precisas."
        },
        {
            "pergunta": "Qual é a relação entre a probabilidade associada a um modelo de linguagem e a função que é aprendida?",
            "resposta": "A probabilidade associada a um modelo de linguagem pode ser vista como uma função que é aprendida pelo modelo. Essa função pode ser aproximada por meio de técnicas de aprendizado de máquina, como redes neurais, que são capazes de aprender e representar funções complexas. Ao aprender essa função, o modelo pode fazer previsões mais precisas sobre a probabilidade de sequências de palavras."
        },
        {
            "pergunta": "O que são redes neurais e como elas são relacionadas à aproximação universal de funções?",
            "resposta": "Redes neurais são métodos de aprendizado de máquina que são conhecidos por sua propriedade de aproximação universal de funções. Isso significa que, dada uma rede neural com ao menos uma camada escondida e um número suficiente de neurônios, ela é capaz de aproximar qualquer função contínua no espaço de interesse. Essa propriedade torna as redes neurais uma ferramenta poderosa para aprender e representar funções complexas, como a probabilidade associada a um modelo de linguagem."
        },
        {
            "pergunta": "Qual é a importância da camada escondida e do número de neurônios em uma rede neural para a aproximação universal de funções?",
            "resposta": "A camada escondida e o número de neurônios em uma rede neural são fundamentais para a aproximação universal de funções. A camada escondida permite que a rede neural aprenda representações intermediárias das entradas, o que é essencial para aprender funções complexas. Além disso, o número de neurônios é importante para garantir que a rede neural tenha capacidade suficiente para aprender e representar a função desejada. Com um número insuficiente de neurônios, a rede neural pode não ser capaz de aprender a função corretamente."
        },
        {
            "pergunta": "Quais são as referências recomendadas para entender melhor como funciona uma rede neural?",
            "resposta": "Os capítulos 5 e 6 do livro de Goodfellow et al. (2016) são uma boa introdução para entender como funciona uma rede neural. Além disso, existem muitas outras referências disponíveis que podem ajudar a entender melhor as redes neurais, incluindo artigos de pesquisa, livros e cursos online."
        }
    ],
    "cap-qa": [
        {
            "pergunta": "O que é a área de Resposta Automática a Perguntas (QA) e quais são os objetivos dos sistemas de QA?",
            "resposta": "A área de Resposta Automática a Perguntas (QA) estuda como criar sistemas capazes de responder de forma automática a perguntas em linguagem natural. Os sistemas de QA buscam a capacidade de compreender a pergunta, recuperar informações relevantes e fornecer respostas precisas e úteis. O objetivo principal dos sistemas de QA é fornecer apenas a resposta ao usuário, diferindo-se dos sistemas de Recuperação de Informação (RI) que retornam listas ranqueadas de documentos."
        },
        {
            "pergunta": "Quais são as principais diferenças entre os sistemas de QA e os sistemas de chatbots?",
            "resposta": "As principais diferenças entre os sistemas de QA e os sistemas de chatbots são a capacidade de adaptar suas respostas considerando o contexto mais amplo da conversa e a precisão em fornecer respostas informativas. Os sistemas de QA são projetados para responder a perguntas de forma isolada, ignorando o contexto mais amplo da conversa, enquanto os sistemas de chatbots visam manter um diálogo contínuo e engajante com o usuário."
        },
        {
            "pergunta": "Quais são as subáreas do Processamento de Linguagem Natural (PLN) relacionadas à área de QA?",
            "resposta": "A área de QA se relaciona diretamente com duas grandes subáreas do Processamento de Linguagem Natural (PLN), que são a compreensão da linguagem natural e a geração de linguagem natural. Além disso, QA também se relaciona com a área de Recuperação de Informação (RI), uma vez que estes sistemas utilizam algum tipo de base de conhecimento para recuperar informações relevantes para o desenvolvimento da resposta."
        },
        {
            "pergunta": "Quais são as etapas de processamento envolvidas nos sistemas de QA?",
            "resposta": "Os sistemas de QA utilizam diferentes etapas de processamento que envolvem diversas tarefas, como a compreensão da pergunta, a busca por informações relevantes em alguma base de conhecimento e a definição da resposta de saída. A compreensão da pergunta pode envolver diversas etapas, desde classificação de texto, extração de tópicos e Reconhecimento de Entidades Nomeadas (REN)."
        },
        {
            "pergunta": "Quais são as características que influenciam significativamente o funcionamento e complexidade dos sistemas de QA?",
            "resposta": "Existem diversas maneiras de classificar um sistema de QA, com base em características que influenciam significativamente seu funcionamento e complexidade. Estas classificações incluem a arquitetura do sistema, o tipo de base de conhecimento utilizada, a complexidade da pergunta e a precisão da resposta."
        },
        {
            "pergunta": "Quais são as tarefas convencionais de QA e quais são as tarefas com peculiaridades e desafios únicos?",
            "resposta": "As tarefas convencionais de QA incluem a compreensão da pergunta, a busca por informações relevantes e a definição da resposta de saída. As tarefas com peculiaridades e desafios únicos incluem ranquear uma lista de respostas fornecida para uma pergunta, correlacionar informações de múltiplas fontes de dados para fornecer uma resposta correta que requer múltiplos passos de raciocínio e responder a perguntas com base em informações visuais (sistemas multi modais)."
        },
        {
            "pergunta": "Quais são os principais critérios que diferenciam os sistemas de PR?",
            "resposta": "Os principais critérios que diferenciam os sistemas de PR estão relacionados à riqueza de possibilidades da linguagem natural, à grande amplitude de áreas de conhecimento e profundidade de detalhes que cada uma apresenta, bem como às diferentes formas de recursos de informação. Isso significa que os sistemas de PR podem variar significativamente em termos de complexidade, capacidade de processamento de linguagem e domínio de conhecimento."
        },
        {
            "pergunta": "Por que é desafiador desenvolver um sistema de PR que domine toda a complexidade da linguagem natural?",
            "resposta": "Desenvolver um sistema de PR que domine toda a complexidade da linguagem natural é desafiador devido à grande amplitude de áreas de conhecimento e profundidade de detalhes que cada uma apresenta. Além disso, a linguagem natural é rica em nuances e ambiguidades, o que torna difícil criar um sistema que possa lidar com todos os tipos de estruturas de dados e conhecimento aprofundado em todas as áreas."
        },
        {
            "pergunta": "Por que existem diferentes tipos de sistemas de PR adequados para aspectos particulares?",
            "resposta": "Existem diferentes tipos de sistemas de PR adequados para aspectos particulares porque é comum que os sistemas de PR sejam projetados para lidar com tarefas específicas ou domínios de conhecimento específicos. Isso permite que os sistemas sejam otimizados para realizar tarefas específicas de forma eficiente e eficaz, em vez de tentar lidar com todas as possibilidades da linguagem natural e do conhecimento humano."
        },
        {
            "pergunta": "Quais são as três formas de categorização de sistemas de PR mencionadas no texto?",
            "resposta": "As três formas de categorização de sistemas de PR mencionadas no texto são: (1) tipo da pergunta de entrada do sistema, (2) tipo de fonte de conhecimento e (3) domínio de conhecimento dos sistemas. Essas categorizações permitem que os sistemas de PR sejam organizados e classificados de acordo com suas características e capacidades."
        },
        {
            "pergunta": "Qual é o objetivo da seção mencionada no texto?",
            "resposta": "O objetivo da seção mencionada no texto é organizar as diferentes categorias de sistemas de PR através das três formas de categorização mencionadas anteriormente. Isso permite que os sistemas de PR sejam compreendidos e classificados de forma clara e sistemática, facilitando a compreensão e o desenvolvimento de sistemas de PR mais eficazes."
        },
        {
            "pergunta": "Quais são as principais etapas de processamento em um sistema de Processamento de Linguagem Natural (PR)?",
            "resposta": "As principais etapas de processamento em um sistema de PR incluem compreender a pergunta de entrada, recuperar informações relevantes em sua base de conhecimento e gerar linguagem natural para a resposta de saída. No entanto, essas etapas podem variar dependendo da arquitetura e abordagem do sistema."
        },
        {
            "pergunta": "Quais são as duas categorias principais de sistemas de PR?",
            "resposta": "As duas categorias principais de sistemas de PR são a abordagem modular e a abordagem end-to-end. A abordagem modular utiliza etapas explícitas de processamento, enquanto a abordagem end-to-end abstrai todas as etapas em um modelo de redes neurais profundas."
        },
        {
            "pergunta": "Quais são as três possibilidades de arquiteturas de sistemas de PR discutidas no texto?",
            "resposta": "As três possibilidades de arquiteturas de sistemas de PR discutidas no texto são: (1) uma abordagem modular que utiliza dados não estruturados como fonte de conhecimento, (2) uma abordagem modular que utiliza grafos de conhecimento como fonte de conhecimento e (3) uma abordagem end-to-end que utiliza documentos não estruturados e memória paramétrica."
        },
        {
            "pergunta": "Qual é a importância de considerar o contexto e os recursos atuais ao escolher uma abordagem para um sistema de PR?",
            "resposta": "A escolha da abordagem para um sistema de PR deve ser guiada por uma análise do contexto no qual será aplicado, dos recursos atuais e da viabilidade de desenvolver novos recursos, como conjuntos de dados para treinamento e bases de conhecimento do sistema. Isso é importante para garantir que o sistema seja eficaz e eficiente em atender às necessidades específicas do projeto."
        },
        {
            "pergunta": "Quais são as vantagens e desvantagens das abordagens modular e end-to-end em sistemas de PR?",
            "resposta": "As abordagens modular e end-to-end têm vantagens e desvantagens diferentes. A abordagem modular é mais flexível e pode ser mais fácil de implementar, mas pode ser mais complexa e requer mais recursos. A abordagem end-to-end é mais eficiente e pode ser mais fácil de treinar, mas pode ser menos flexível e requer mais dados para treinamento. A escolha da abordagem depende das necessidades específicas do projeto e dos recursos disponíveis."
        },
        {
            "pergunta": "Quais são os principais métodos de avaliação de sistemas de Processamento de Linguagem Natural (PR) e como eles são utilizados?",
            "resposta": "Os principais métodos de avaliação de sistemas de PR incluem métodos de avaliação utilizados em problemas de classificação, métodos de avaliação utilizados para Recuperação de Informação (RI) e métodos de avaliação direta das respostas de saída do sistema. Esses métodos são utilizados para avaliar o desempenho do sistema em diferentes etapas do processo de PR, como classificação de perguntas, recuperação de documentos e geração de respostas."
        },
        {
            "pergunta": "O que é um conjunto de dados (dataset) e como ele é utilizado na avaliação de sistemas de PR?",
            "resposta": "Um conjunto de dados (dataset) é um recurso de avaliação de sistema de PR que contém perguntas de entrada e respostas consideradas corretas para estas perguntas. Além disso, os conjuntos de dados podem oferecer outros recursos adicionais, como a base de conhecimento que o sistema deve utilizar para consulta, informações adicionais sobre a pergunta, como o seu tipo, entidades no texto, entre outros. Os conjuntos de dados são utilizados para testar os sistemas de PR e comparar as respostas geradas pelos diferentes sistemas."
        },
        {
            "pergunta": "Quais são as principais métricas de avaliação utilizadas para respostas curtas e precisas em sistemas de PR?",
            "resposta": "As principais métricas de avaliação utilizadas para respostas curtas e precisas em sistemas de PR incluem métricas que avaliam a capacidade do sistema de identificar corretamente a resposta exata dentro de um conjunto de respostas candidatas. Exemplos de métricas incluem a precisão, a revocação e a F1-score."
        },
        {
            "pergunta": "Quais são as principais métricas de avaliação utilizadas para respostas longas em sistemas de PR?",
            "resposta": "As principais métricas de avaliação utilizadas para respostas longas em sistemas de PR incluem métricas que determinam a similaridade entre textos, como a sobreposição de palavras, a similaridade semântica e a similaridade contextual. Exemplos de métricas incluem o BERTScore, que alinha as palavras entre as respostas do sistema e de referência com base em seus embeddings, levando em conta o contexto em que as palavras são usadas."
        },
        {
            "pergunta": "Quais são os desafios na avaliação de sistemas de PR e como eles podem ser superados?",
            "resposta": "Um dos principais desafios na avaliação de sistemas de PR é lidar com a subjetividade, especialmente em respostas longas, onde a 'correção' pode ser aberta a interpretação. Para superar esse desafio, podem ser empregadas métricas qualitativas, que envolvem a avaliação manual humana das respostas fornecida pelo sistema. Além disso, a utilização de modelos de redes neurais profundas, como o BERTScore, pode ajudar a superar o problema de sobreposição de palavras e avaliar a semelhança semântica e contextual entre as respostas."
        },
        {
            "pergunta": "Quais são os principais desafios no desenvolvimento de sistemas de Processamento de Linguagem Natural (PLN) para o idioma português?",
            "resposta": "Os principais desafios incluem a compreensão da linguagem natural, a análise semântica e a precisão das respostas. Além disso, a falta de dados e a necessidade de personalização dos modelos para contextos específicos também são desafios importantes."
        },
        {
            "pergunta": "Como os sistemas de Processamento de Linguagem Natural (PLN) para o português evoluíram ao longo do tempo?",
            "resposta": "Os sistemas de PLN para o português evoluíram significativamente ao longo do tempo, passando de sistemas simples que buscavam respostas exatas em bases de dados estruturadas para sistemas mais complexos que utilizam técnicas de aprendizado de máquina e integração com outras tecnologias, como a geração de consultas SQL. Além disso, a aplicação de modelos pré-treinados e algoritmos avançados também contribuiu para a melhoria da eficácia desses sistemas."
        },
        {
            "pergunta": "Quais são os principais sistemas de Processamento de Linguagem Natural (PLN) para o português que se destacaram ao longo do tempo?",
            "resposta": "Alguns dos principais sistemas de PLN para o português que se destacaram ao longo do tempo incluem o IdSay, o Priberam, o Págico, o DEEPAGÉ, o ClinicalQA e o BERTimbau. Esses sistemas representaram avanços significativos nas abordagens de PLN e demonstraram melhorias significativas na precisão das respostas e na capacidade de processar um espectro mais amplo de tipos de perguntas."
        },
        {
            "pergunta": "Como a integração de Processamento de Linguagem Natural (PLN) com outras tecnologias, como a geração de consultas SQL, pode melhorar a eficácia dos sistemas de PLN?",
            "resposta": "A integração de PLN com outras tecnologias, como a geração de consultas SQL, pode melhorar a eficácia dos sistemas de PLN ao permitir que os sistemas processem perguntas mais complexas e retornem respostas mais precisas. Além disso, essa integração também pode permitir que os sistemas sejam mais flexíveis e capazes de lidar com diferentes tipos de perguntas e respostas."
        },
        {
            "pergunta": "Quais são as principais tendências futuras para os sistemas de Processamento de Linguagem Natural (PLN) para o português?",
            "resposta": "As principais tendências futuras para os sistemas de PLN para o português incluem a continuação do desenvolvimento de modelos pré-treinados e algoritmos avançados, a integração com outras tecnologias, como a geração de consultas SQL, e a personalização dos modelos para contextos específicos. Além disso, a aplicação de técnicas de aprendizado de máquina e a utilização de grandes volumes de dados também são tendências importantes para o futuro dos sistemas de PLN."
        }
    ],
    "cap-dialogo-interatividade": [
        {
            "pergunta": "Qual é a importância do diálogo na existência humana, segundo Freire (1989)?",
            "resposta": "O diálogo é uma exigência existencial, ou seja, é fundamental para a existência humana. Ele é uma forma de comunicação que permite que as pessoas se relacionem, aprendam e se desenvolvam juntas."
        },
        {
            "pergunta": "Quais são as principais características do diálogo, segundo o texto?",
            "resposta": "O diálogo é uma forma de comunicação que envolve a linguagem natural, coordenação, agilidade e planejamento. Ele pode ocorrer entre dois ou mais participantes e pode variar em termos de espontaneidade, formalidade, sinceridade e naturalidade. Além disso, o diálogo está sujeito a influências sociolinguísticas, de estruturas de poder, de adequação ao ambiente e de pressuposições."
        },
        {
            "pergunta": "Como o diálogo se diferencia da forma canônica dos textos em grandes corpora?",
            "resposta": "O diálogo se diferencia da forma canônica dos textos em grandes corpora porque ele é mais espontâneo e natural. Ele envolve pausas, frases que não seguem formas idealizadas de sintaxe, sinais de compreensão e sobreposição, e clarificações. Além disso, o diálogo é influenciado pelo contexto e pelo ambiente em que ocorre."
        },
        {
            "pergunta": "Quais são os desafios de criar modelos computacionais que capturem todas as nuances do diálogo?",
            "resposta": "Os desafios de criar modelos computacionais que capturem todas as nuances do diálogo incluem a complexidade do diálogo, a variedade de características que ele envolve e a dificuldade de modelar a forma como as pessoas se comunicam de forma espontânea. Além disso, os modelos computacionais precisam ser capazes de lidar com ambiguidade, correções e outras situações que ocorrem no diálogo."
        },
        {
            "pergunta": "Quais são as limitações dos agentes conversacionais comerciais atuais?",
            "resposta": "As limitações dos agentes conversacionais comerciais atuais incluem a falta de naturalidade e riqueza em fenômenos, a alternância forçada de turnos e a dificuldade de lidar com ambiguidade e correções. Além disso, esses agentes não são capazes de modelar o diálogo de forma completa e podem se confundir em várias situações."
        },
        {
            "pergunta": "Qual é a importância de avaliar modelos de diálogo de forma apropriada?",
            "resposta": "A avaliação de modelos de diálogo é importante porque não basta otimizar uma métrica de performance para se ter um resultado satisfatório. É necessário considerar a complexidade do diálogo e a variedade de características que ele envolve. Além disso, a avaliação deve ser feita de forma apropriada para garantir que os modelos sejam capazes de lidar com as nuances do diálogo."
        },
        {
            "pergunta": "Quais são as questões imperativas quanto ao desenvolvimento e ao uso responsável de modelos de diálogo?",
            "resposta": "As questões imperativas quanto ao desenvolvimento e ao uso responsável de modelos de diálogo incluem a preservação da atividade humana construída socialmente ao longo da nossa evolução, a importância de não banalizar ou corroer essa atividade e a necessidade de considerar as implicações éticas e sociais do uso desses modelos."
        },
        {
            "pergunta": "O que é diálogo e como podemos definir essa forma de interação?",
            "resposta": "O diálogo é uma forma de interação entre dois ou mais participantes que se revezam em proferir falas (ou sinais) e, em geral, enquanto uma pessoa fala a outra está em silêncio, supostamente prestando atenção no que está sendo dito (ou gesticulado). A palavra 'diálogo' vem do grego, sendo composta pelo prefixo 'dia', que significa 'através', e 'logos' que quer dizer 'palavra' ou 'discurso'."
        },
        {
            "pergunta": "Quais são as características principais de um diálogo?",
            "resposta": "De acordo com Clark (1996a), as características principais de um diálogo incluem: co-presença, visibilidade, audibilidade, simultaneidade, interação, coordenação, adaptação, flexibilidade, espontaneidade e registro. No entanto, é importante notar que a ausência de certas características pode vir a ser um critério excludente na hora de definir diálogo."
        },
        {
            "pergunta": "Como podemos classificar as várias formas de diálogo?",
            "resposta": "Podemos classificar as várias formas de diálogo com base nas características mencionadas anteriormente. Por exemplo, conversas face a face são consideradas a forma primária de diálogo, enquanto conversas telefônicas ou por língua de sinais podem ser consideradas formas secundárias de diálogo. Além disso, podemos considerar a interatividade de uma conversa, que pode variar de uma conversa espontânea entre familiares próximos a uma entrevista com uma assimetria evidente de papéis."
        },
        {
            "pergunta": "O que é relevante quando pessoas estão conversando?",
            "resposta": "De acordo com Clark (1996a), um diálogo é uma ação conjunta entre os participantes, que precisam coordenar suas ações em busca de um propósito. A ação mais evidente é a fala (o que falar, quando falar e quando não falar) junto com seu processamento (entender o que está sendo falado e que rumo a conversa vai tomando a cada passo). Além disso, a pragmática é extremamente relevante no diálogo, pois a conversa está ocorrendo em um contexto que inclui a situação como um todo, o discurso, o ambiente físico e a bagagem de conhecimento de cada um."
        },
        {
            "pergunta": "Como a interação humano-máquina de fato ocorre?",
            "resposta": "A interação humano-máquina é uma área de estudo que busca entender como as pessoas interagem com os agentes artificiais. De acordo com Hayes (1980), a meta de criar agentes que busquem imitar o comportamento humano sequer é um ideal, dada a capacidade de adaptação dos humanos ao interlocutor. Em vez disso, ele propõe que os modelos exibam habilidades que permitam uma 'interação graciosa' com os humanos, que seja robusta a problemas de comunicação."
        },
        {
            "pergunta": "O que são os fenômenos que ocorrem em diálogos e como eles são utilizados pelas pessoas?",
            "resposta": "Os fenômenos que ocorrem em diálogos são manifestações e estratégias que as pessoas utilizam intuitivamente para se comunicar de forma eficaz. Esses fenômenos incluem atos comunicativos, atos metacomunicativos e unidades de contribuição, que são fundamentais para a compreensão e a manutenção da comunicação em um diálogo. Embora muitas pessoas os utilizem de forma intuitiva, é importante estudar e compreender esses fenômenos para desenvolver modelos de conversação bem fundamentados."
        },
        {
            "pergunta": "Qual é a importância de levar em conta a pesquisa teórica e empírica sobre os fenômenos de diálogo?",
            "resposta": "Levar em conta a pesquisa teórica e empírica sobre os fenômenos de diálogo é fundamental para desenvolver modelos de conversação bem fundamentados. Essa pesquisa, oriunda de campos como teoria do diálogo, ciência cognitiva e análise de conversação, fornece uma compreensão profunda dos fenômenos que ocorrem em diálogos e ajuda a identificar as estratégias mais eficazes para a comunicação. Além disso, essa pesquisa pode ajudar a evitar erros comuns e a desenvolver modelos de conversação mais eficazes."
        },
        {
            "pergunta": "O que é a metáfora dos dois trilhos e como ela se aplica aos diálogos?",
            "resposta": "A metáfora dos dois trilhos é uma forma de entender a estrutura de um diálogo. Segundo essa metáfora, um diálogo consiste em dois trilhos paralelos: o trilho principal, onde ocorrem os atos comunicativos, e o trilho colateral, onde ocorrem os atos metacomunicativos. Os atos comunicativos são os enunciados sobre os 'negócios oficiais' da interação, enquanto os atos metacomunicativos são os enunciados que lidam com os atos comunicativos do outro trilho de modo a manter a comunicação funcionando. Essa metáfora ajuda a entender como os diálogos são estruturados e como as pessoas se comunicam de forma eficaz."
        },
        {
            "pergunta": "O que é uma unidade de contribuição em um diálogo e como ela se relaciona com os atos comunicativos e metacomunicativos?",
            "resposta": "Uma unidade de contribuição em um diálogo é a contribuição que cada participante faz em seu turno, que pode ser desde apenas um fonema ou uma palavra até múltiplas frases em sequência, ou ainda fragmentos. Essa unidade é comumente chamada de enunciado (utterance). Os enunciados podem ser atos comunicativos, que contribuem para o tema substancial da conversa, ou atos metacomunicativos, que lidam com os atos comunicativos do outro trilho de modo a manter a comunicação funcionando. As unidades de contribuição são fundamentais para a compreensão e a manutenção da comunicação em um diálogo."
        },
        {
            "pergunta": "Como os atos metacomunicativos se relacionam com a compreensão e a manutenção da comunicação em um diálogo?",
            "resposta": "Os atos metacomunicativos são fundamentais para a compreensão e a manutenção da comunicação em um diálogo. Eles lidam com os atos comunicativos do outro trilho de modo a manter a comunicação funcionando. Os atos metacomunicativos podem ser usados para corrigir mal-entendidos, mostrar compreensão, pedir esclarecimentos, entre outros. Eles ajudam a garantir que a comunicação seja eficaz e que os participantes estejam no mesmo nível de compreensão. Além disso, os atos metacomunicativos podem ajudar a evitar conflitos e a manter a conversa fluindo de forma eficaz."
        },
        {
            "pergunta": "Quais são os principais desafios ao avaliar modelos de diálogo?",
            "resposta": "Os principais desafios incluem mensurar características subjetivas como qualidade, efetividade e engajamento, além de lidar com a multiplicidade de 'respostas certas' e a falta de um único padrão de referência. Além disso, a avaliação de modelos orientados a tarefas é mais fácil devido à presença de um objetivo claro, enquanto modelos de bate-papo não têm um objetivo tão palpável."
        },
        {
            "pergunta": "Quais são as principais iniciativas históricas na avaliação de modelos de diálogo?",
            "resposta": "Duas das principais iniciativas históricas são a PARADISE (Walker et al., 1997) e a Trindi (Bos et al., 1999). A PARADISE propõe uma hierarquia para avaliar agentes conversacionais, considerando a satisfação dos usuários, o sucesso da tarefa e a minimização de custos. A Trindi, por outro lado, elabora uma lista de critérios de comportamento do sistema que uma avaliadora ou um avaliador deve verificar."
        },
        {
            "pergunta": "Quais são as principais métricas utilizadas na avaliação automática de modelos de diálogo?",
            "resposta": "Algumas das principais métricas incluem similaridade entre o enunciado produzido pelo modelo e o que está realizado nos dados, coerência com o contexto, diversidade (entropia e inércia) e perplexidade do modelo de linguagem. Além disso, existem muitas outras métricas em uso e novas são propostas constantemente pela comunidade, pois cada aplicação pode precisar mensurar diferentes características do diálogo e da tarefa em questão."
        },
        {
            "pergunta": "Quem deve avaliar um modelo de diálogo?",
            "resposta": "A responsabilidade de garantir uma boa avaliação é, inicialmente, da pessoa física ou jurídica que o desenvolve. No entanto, é importante ter uma abordagem neutra e imparcial, o que pode ser alcançado através de experimentos, crowdsourcing ou avaliação por humanos. Além disso, é possível simular o comportamento de um humano, mas isso se torna um outro problema."
        },
        {
            "pergunta": "Quais são as principais formas de avaliação humana de modelos de diálogo?",
            "resposta": "Existem duas principais formas de avaliação humana: interativa, na qual a pessoa avalia uma conversa que ela mesma teve com um sistema, e estática, quando ela lê ou ouve uma interação do modelo com outra pessoa e julga sua qualidade. Além disso, é possível avaliar apenas um sistema de forma isolada ou capturar a preferência da avaliadora ou do avaliador ao julgá-lo em comparação a um outro sistema."
        },
        {
            "pergunta": "Quais são as principais dimensões possíveis na avaliação humana de modelos de diálogo?",
            "resposta": "Algumas das principais dimensões incluem a satisfação do usuário, a qualidade da conversa, a efetividade da comunicação, a coerência do modelo e a capacidade de lidar com erros ou imprevistos. Além disso, é possível utilizar formulários de satisfação para coletar feedback dos usuários e avaliar a qualidade do modelo."
        }
    ],
    "cap-agentes-conversacionais": [
        {
            "pergunta": "O que são agentes de conversação e como eles se relacionam com chatbots?",
            "resposta": "Agentes de conversação são sistemas de diálogo que se comunicam com usuários usando a linguagem humana. Eles podem ser divididos em duas classes: agentes orientados a tarefas, que visam resolver um problema específico, e chatbots, que simulam diálogos humanos e são mais voltados para entretenimento. Os termos 'chatbots' e 'agentes de conversação' são frequentemente usados de forma intercambiável."
        },
        {
            "pergunta": "Qual é o exemplo mais antigo de agente de conversação mencionado no texto?",
            "resposta": "O exemplo mais antigo de agente de conversação mencionado no texto é o ELIZA, criado em 1966 pelo cientista da computação Joseph Weizenbaum. O ELIZA era um agente de conversação que replicava o comportamento de um psicoterapeuta e usava padrões pré-fabricados para responder às perguntas dos usuários."
        },
        {
            "pergunta": "O que é um modelo de linguagem gerativo e como ele se relaciona com chatbots?",
            "resposta": "Um modelo de linguagem gerativo é um algoritmo que é capaz de gerar texto que faz sentido, baseado em previsões e probabilidade. Chatbots como o ChatGPT usam esses modelos para gerar respostas às perguntas dos usuários. No entanto, esses modelos não garantem que as respostas sejam corretas ou baseadas em fatos reais."
        },
        {
            "pergunta": "O que é o fenômeno da alucinação em chatbots?",
            "resposta": "A alucinação é um fenômeno em que um chatbot gera texto que parece correto, mas não é baseado em fatos reais. Isso ocorre porque os modelos de linguagem gerativos usados por chatbots são baseados em previsões e probabilidade, e não em conhecimento factual. A alucinação pode levar a respostas que parecem corretas, mas não são."
        },
        {
            "pergunta": "Quais são as limitações dos chatbots baseados em modelos de linguagem gerativos?",
            "resposta": "As limitações dos chatbots baseados em modelos de linguagem gerativos incluem a incapacidade de garantir que as respostas sejam corretas ou baseadas em fatos reais. Além disso, esses chatbots podem sofrer de alucinação, o que pode levar a respostas que parecem corretas, mas não são. Outra limitação é a incapacidade de dizer 'não sei' quando uma pergunta não pode ser respondida com certeza."
        },
        {
            "pergunta": "Quais são as estratégias para evitar as alucinações em chatbots?",
            "resposta": "Algumas estratégias para evitar as alucinações em chatbots incluem a engenharia de prompts, que envolve calibrar e alinhar as respostas anteriores com novas perguntas. Outra estratégia é usar a API do chatbot para controlar o parâmetro de temperatura, o que pode ajudar a reduzir a probabilidade de alucinação. Além disso, é possível acoplar bases de conhecimento externas ao processo de geração de texto ou interagir com o chatbot por meio de perguntas que demandem alguma tentativa de simulação de raciocínio."
        },
        {
            "pergunta": "O que é a chain-of-thought e como ela pode ser usada para evitar as alucinações em chatbots?",
            "resposta": "A chain-of-thought é um processo que envolve embutir o modelo de habilidades de explicação das respostas do chatbot. Isso pode ajudar a evitar as alucinações, pois o chatbot é forçado a explicar sua resposta passo a passo. No entanto, é importante notar que essa estratégia não é infalível e que as alucinações ainda podem ocorrer."
        },
        {
            "pergunta": "Qual é o papel dos chatbots na sociedade atual?",
            "resposta": "Os chatbots estão cada vez mais presentes na sociedade atual, e são usados para uma variedade de tarefas, desde entretenimento até assistência. No entanto, é importante ter cuidado ao usar essas tecnologias, pois elas podem ter limitações e falhas. É importante entender como os chatbots funcionam e como eles podem ser usados de forma responsável."
        },
        {
            "pergunta": "O que é a linguagem e como pode ser entendida de acordo com uma perspectiva específica?",
            "resposta": "A linguagem pode ser entendida como um conjunto de práticas relacionadas, mas que podem não compartilhar uma essência em comum. Isso significa que a linguagem é composta por atividades linguísticas heterogêneas, mas relacionadas, que podem ser vistas como 'jogos de linguagem'. Essa perspectiva sugere que a linguagem não é uma entidade única e monolítica, mas sim um conjunto de práticas que se intersectam e se influenciam mutuamente."
        },
        {
            "pergunta": "Como os modelos de linguagem atuais, como o ChatGPT e o Maritalk, funcionam e quais são suas limitações?",
            "resposta": "Os modelos de linguagem atuais, como o ChatGPT e o Maritalk, funcionam com base em previsão. Eles são treinados em grandes conjuntos de dados e aprendem a prever a próxima palavra ou sequência de palavras em um texto. No entanto, essa abordagem tem limitações, pois nem todos os jogos de linguagem ou atividades linguísticas se resumem a um jogo de previsões. Isso significa que esses modelos podem ser muito bons em algumas tarefas, como inventar uma história ou resumir um texto, mas podem não ser tão bons em outras, como fazer cálculos ou provar hipóteses."
        },
        {
            "pergunta": "Por que os agentes de conversação, como o ChatGPT e o Maritalk, se tornaram tão populares?",
            "resposta": "Os agentes de conversação, como o ChatGPT e o Maritalk, se tornaram tão populares porque permitem que as pessoas interajam com as máquinas usando sua própria língua, em vez de uma linguagem de programação. Isso significa que qualquer pessoa pode pedir que as máquinas executem certas tarefas, como criar um programa de computador ou sugerir receitas a partir de uma lista de ingredientes. Essa capacidade de interação natural e intuitiva é uma das principais razões pelas quais esses agentes se tornaram tão populares."
        },
        {
            "pergunta": "Quais são as principais diferenças entre os agentes de conversação, como o ChatGPT, o Maritalk e o BARD?",
            "resposta": "Os agentes de conversação, como o ChatGPT, o Maritalk e o BARD, têm algumas diferenças importantes. O ChatGPT é um modelo de linguagem treinado pela OpenAI, enquanto o Maritalk é um modelo de linguagem treinado pela Sabiá. O BARD, por sua vez, é uma ferramenta treinada pela Google a partir do modelo de linguagem LaMDA. Além disso, o BARD é aumentado com recuperação de informação para incluir a devolução das fontes em alguns casos. Essas diferenças podem afetar a forma como esses agentes funcionam e as tarefas que eles podem realizar."
        },
        {
            "pergunta": "Quais são as principais limitações dos agentes de conversação, como o ChatGPT e o Maritalk, e como elas podem ser superadas?",
            "resposta": "As principais limitações dos agentes de conversação, como o ChatGPT e o Maritalk, incluem a falta de compreensão profunda do significado e o contexto das palavras e frases. Além disso, esses agentes podem não ser capazes de realizar tarefas que exigem habilidades cognitivas mais complexas, como fazer cálculos ou provar hipóteses. Para superar essas limitações, é necessário desenvolver modelos de linguagem mais avançados que possam compreender melhor o significado e o contexto das palavras e frases, e que possam realizar tarefas mais complexas."
        },
        {
            "pergunta": "Quais são as limitações dos agentes de conversação em resolver tarefas de PLN?",
            "resposta": "Os agentes de conversação ainda têm dificuldade em resolver tarefas de PLN de forma eficaz, pois suas respostas podem ser difíceis de avaliar, tanto automaticamente quanto por especialistas humanos. Além disso, as métricas automáticas apresentam limitações, e a criação de conjuntos de dados que explorem todas as características desejadas em um sistema de geração de textos é um desafio."
        },
        {
            "pergunta": "Quais são as métricas automáticas utilizadas para avaliar a geração de textos?",
            "resposta": "Algumas das métricas automáticas utilizadas para avaliar a geração de textos incluem ROUGE, BLEU, BERTscore e METEOR. No entanto, essas métricas apresentam limitações, como a dificuldade em capturar aspectos semânticos e criativos do texto."
        },
        {
            "pergunta": "Por que a avaliação humana da geração de textos é cansativa e propensa a ruídos?",
            "resposta": "A avaliação humana da geração de textos é cansativa e propensa a ruídos porque os especialistas humanos precisam ler e avaliar cada resposta individualmente, o que pode ser um processo demorado e subjetivo. Além disso, os avaliadores humanos podem ter diferentes opiniões e critérios para avaliar a qualidade do texto, o que pode levar a inconsistências na avaliação."
        },
        {
            "pergunta": "Quais são as características que gostaríamos de avaliar em um sistema de geração de textos?",
            "resposta": "As características que gostaríamos de avaliar em um sistema de geração de textos incluem aspectos gramaticais e semânticos, criatividade, fluência, interesse e prazer despertado no leitor, dentre outros. A criação de conjuntos de dados que explorem todas essas características é um desafio importante para o desenvolvimento de sistemas de geração de textos eficazes."
        },
        {
            "pergunta": "Quais são os exemplos de casos em que as respostas dos agentes de conversação aguçam as nossas expectativas?",
            "resposta": "Os exemplos de casos em que as respostas dos agentes de conversação aguçam as nossas expectativas incluem a interação por meio de diálogo, auxiliar em revisões da literatura, auxiliar na escrita de código, escrita de e-mails e revisão de texto. Esses casos mostram que os agentes de conversação têm o potencial de ser úteis em uma variedade de tarefas e aplicações."
        },
        {
            "pergunta": "Quais são os desafios em mitigar vieses no ChatGPT e outros agentes de conversação?",
            "resposta": "Os desafios incluem a perpetuação de vieses sociais, mesmo quando não provocados, devido ao uso de dados e feedbacks imperfeitos. Além disso, o treinamento em uma única língua, como o inglês, pode trazer problemas éticos e culturais para outras línguas. Os vieses sociais podem estar inseridos nos textos usados para treinar os modelos, explicitamente ou implicitamente."
        },
        {
            "pergunta": "Como o treinamento do modelo de linguagem e o feedback podem afetar a apresentação de vieses sociais?",
            "resposta": "O treinamento do modelo de linguagem e o feedback podem afetar a apresentação de vieses sociais, pois os dados e feedbacks usados para treinar os modelos podem conter vieses implícitos ou explícitos. Além disso, o treinamento em uma única língua pode limitar a capacidade do modelo de entender e responder a perguntas em outras línguas de forma neutra e imparcial."
        },
        {
            "pergunta": "Quais são as limitações dos agentes de conversação, como o ChatGPT e o BARD, em relação à recuperação de informação?",
            "resposta": "Os agentes de conversação, como o ChatGPT e o BARD, têm limitações em relação à recuperação de informação, pois não são instanciados para essa tarefa e não têm acesso direto aos textos da Web. Isso pode levar a respostas incompletas ou imprecisas, especialmente quando se trata de informações específicas ou atualizadas."
        },
        {
            "pergunta": "Como os agentes de conversação, como o MariTalk e o BARD, lidam com perguntas sobre compositores e compositoras?",
            "resposta": "Os agentes de conversação, como o MariTalk e o BARD, podem ter dificuldades em lidar com perguntas sobre compositores e compositoras, especialmente quando se trata de identificar corretamente o gênero dos compositores. O MariTalk, por exemplo, pode continuar afirmando que a lista contém apenas mulheres, mesmo quando confrontado com a informação correta. O BARD, por outro lado, pode devolver uma resposta mais correta inicialmente, mas pode ter dificuldades em se ater à resposta correta quando confrontado com perguntas adicionais."
        },
        {
            "pergunta": "Quais são as implicações das limitações dos agentes de conversação em relação à recuperação de informação e à apresentação de vieses sociais?",
            "resposta": "As limitações dos agentes de conversação em relação à recuperação de informação e à apresentação de vieses sociais podem ter implicações significativas, especialmente em contextos em que a precisão e a imparcialidade são cruciais. Isso pode incluir a perpetuação de estereótipos e vieses sociais, a disseminação de informações imprecisas e a erosão da confiança nos agentes de conversação e nas tecnologias de inteligência artificial em geral."
        }
    ],
    "cap-ir": [
        {
            "pergunta": "Qual é a origem da necessidade de organizar a informação e como isso se relaciona com a existência de bibliotecas?",
            "resposta": "A necessidade de organizar a informação é inerente à espécie humana e remonta a pelo menos o ano 2600 AC, quando as primeiras bibliotecas foram criadas. Essa necessidade é fundamental para a sobrevivência e o desenvolvimento da humanidade, pois permite que as pessoas acessem e compartilhem conhecimento de forma eficiente. As bibliotecas, por sua vez, são instituições que armazenam e organizam grandes volumes de informação, tornando-a acessível ao público."
        },
        {
            "pergunta": "Quais foram os esforços realizados nos anos 1960 para automatizar o armazenamento e a busca de materiais bibliográficos?",
            "resposta": "Nos anos 1960, iniciaram-se esforços para automatizar o armazenamento e a busca de materiais bibliográficos através da computação. Esses esforços incluíram o desenvolvimento de sistemas de gerenciamento de bibliotecas, que permitiam a catalogação e a busca de livros e outros materiais de forma mais eficiente. Além disso, também foram desenvolvidas tecnologias de processamento de linguagem natural, que permitiam a busca de textos por palavras-chave e frases."
        },
        {
            "pergunta": "O que é a área de recuperação de informação (RI) e qual é o seu objetivo central?",
            "resposta": "A área de recuperação de informação (RI) é uma disciplina que se concentra em encontrar material relevante a partir de grandes coleções de dados não estruturados, geralmente texto. O objetivo central da RI é a busca, ou seja, a tarefa de encontrar material relevante a partir de uma consulta de um usuário. Isso envolve a desenvolvimento de algoritmos e técnicas para indexar, buscar e recuperar informações de forma eficiente e precisa."
        },
        {
            "pergunta": "Qual é a tarefa conhecida por recuperação ad hoc e como ela se relaciona com a RI?",
            "resposta": "A recuperação ad hoc é a tarefa de encontrar material relevante a partir de uma consulta de um usuário, sem conhecimento prévio do conteúdo da coleção de dados. Essa tarefa é comumente conhecida por recuperação ad hoc e é o objetivo central da área de recuperação de informação (RI). A recuperação ad hoc envolve a desenvolvimento de algoritmos e técnicas para buscar e recuperar informações de forma eficiente e precisa, considerando as necessidades e preferências do usuário."
        },
        {
            "pergunta": "Quais são os tipos de dados que podem ser aplicados à RI e qual é o foco deste capítulo?",
            "resposta": "A RI pode ser aplicada a diferentes tipos de dados não estruturados, como imagem, áudio, vídeo, etc. No entanto, o foco deste capítulo é informações textuais, que são o tipo de dado mais comum e amplamente utilizado em aplicações de RI. As informações textuais incluem documentos, artigos, livros, etc., e são fundamentais para a busca e recuperação de informações em muitas áreas, como a ciência, a tecnologia e a educação."
        },
        {
            "pergunta": "O que é representado pelo retângulo laranja no processo de Recuperação de Informação (RI)?",
            "resposta": "O retângulo laranja representa o sistema de Recuperação de Informação (RI), que é responsável por processar a consulta da usuária e retornar os resultados relevantes."
        },
        {
            "pergunta": "Quais são as etapas do processo de RI que ocorrem offline?",
            "resposta": "As Etapas 1 e 2 do processo de RI ocorrem offline. Na Etapa 1, os documentos da coleção são pré-processados, e na Etapa 2, esses documentos são indexados. Essas etapas precisam ser concluídas antes que o sistema possa receber consultas."
        },
        {
            "pergunta": "Qual é o objetivo do pré-processamento dos documentos na Etapa 1?",
            "resposta": "O objetivo do pré-processamento dos documentos na Etapa 1 é preparar os documentos para serem indexados. Isso envolve a remoção de stopwords, a conversão de letras maiúsculas para minúsculas, a remoção de pontuação e outros processos para tornar os documentos mais fáceis de serem processados pelo sistema de RI."
        },
        {
            "pergunta": "Como a consulta da usuária é processada pelo sistema de RI?",
            "resposta": "A consulta da usuária é processada pelo sistema de RI da seguinte forma: na Etapa 3, o sistema executa sobre a consulta as mesmas operações de pré-processamento que foram aplicadas na Etapa 1. Em seguida, na Etapa 4, o texto pré-processado da consulta é utilizado para buscar no índice os documentos que mais bem atendam a consulta."
        },
        {
            "pergunta": "Como os resultados da consulta são retornados à usuária?",
            "resposta": "Os resultados da consulta são retornados à usuária sob a forma de uma lista ordenada. Isso significa que os documentos mais relevantes para a consulta são listados primeiro, seguidos dos documentos menos relevantes."
        },
        {
            "pergunta": "Qual é o problema central da Recuperação de Informação (RI) e como os Modelos Clássicos abordam essa questão?",
            "resposta": "O problema central da RI é casar a consulta com os documentos que a satisfazem. Os Modelos Clássicos abordam essa questão aplicando um casamento puramente léxico, o que significa que uma consulta somente recupera documentos que possuam alguma de suas palavras-chave. No entanto, essa abordagem tem um impacto negativo na revocação, pois não permite a recuperação de documentos que contenham sinônimos das palavras-chave da consulta."
        },
        {
            "pergunta": "O que é a Modificação Automática de Consultas (AQM) e qual é sua importância na área de Recuperação de Informação?",
            "resposta": "A Modificação Automática de Consultas (AQM) é uma sub-área de pesquisa que visa solucionar o problema da sinonímia na recuperação de informação. A AQM é importante porque permite a recuperação de documentos que contenham sinônimos das palavras-chave da consulta, melhorando a revocação e a precisão da recuperação de informação. Além disso, a AQM é um ponto forte de interseção entre a Recuperação de Informação e o Processamento de Linguagem Natural (PLN), pois os métodos adotados em PLN para solucionar o problema da sinonímia encontram uma ótima área de aplicação na AQM."
        },
        {
            "pergunta": "Quais são as principais abordagens para a Modificação Automática de Consultas (AQM) e como elas são classificadas?",
            "resposta": "As principais abordagens para a Modificação Automática de Consultas (AQM) são classificadas de acordo com a taxonomia introduzida por Carpineto e Romano (2012). Essa taxonomia divide as abordagens em categorias, como expansão de consultas, expansão de documentos, etc. Além disso, existem trabalhos mais recentes que se concentram em expandir os documentos, como o trabalho de Nogueira et al. (2019), que enriquece os documentos com consultas que poderiam ser feitas com o intuito de recuperar o documento."
        },
        {
            "pergunta": "Como a expansão de documentos pode ser útil na Recuperação de Informação e quais são os benefícios dessa abordagem?",
            "resposta": "A expansão de documentos pode ser útil na Recuperação de Informação porque permite a recuperação de documentos que contenham informações relevantes, mesmo que elas não estejam explícitas na consulta. Além disso, a expansão de documentos pode melhorar a precisão da recuperação de informação, pois permite a consideração de informações contextuais e relacionadas. O trabalho de Nogueira et al. (2019) é um exemplo de como a expansão de documentos pode ser aplicada na prática, enriquecendo os documentos com consultas que poderiam ser feitas com o intuito de recuperar o documento."
        },
        {
            "pergunta": "Qual é o papel da Processamento de Linguagem Natural (PLN) na Modificação Automática de Consultas (AQM) e como os métodos de PLN podem ser aplicados na AQM?",
            "resposta": "O Processamento de Linguagem Natural (PLN) desempenha um papel importante na Modificação Automática de Consultas (AQM), pois os métodos de PLN podem ser aplicados para solucionar o problema da sinonímia na recuperação de informação. Os métodos de PLN, como a análise de sentimento, a identificação de entidades nomeadas e a extração de informações, podem ser usados para expandir as consultas e os documentos, melhorando a precisão da recuperação de informação. Além disso, a AQM é um ponto forte de interseção entre a Recuperação de Informação e o PLN, pois os métodos adotados em PLN para solucionar o problema da sinonímia encontram uma ótima área de aplicação na AQM."
        }
    ],
    "cap-ie": [
        {
            "pergunta": "Qual é o objetivo principal da Extração de Informação (EI)?",
            "resposta": "O objetivo principal da EI é obter informação estruturada de dados não-estruturados, permitindo que os dados sejam organizados e analisados de forma mais eficiente. Isso é alcançado através da aplicação de técnicas de processamento de linguagem natural e aprendizado de máquina para identificar e extrair informações relevantes de textos."
        },
        {
            "pergunta": "Quais são as principais tarefas de interesse na Extração de Informação (EI)?",
            "resposta": "As principais tarefas de interesse na EI incluem o Reconhecimento de Entidades Nomeadas (REN), a Extração de Relações (ER) e a Extração de Eventos (EE). Essas tarefas são fundamentais para a análise semântica de textos e permitem identificar entidades, relacionamentos e eventos mencionados nos textos."
        },
        {
            "pergunta": "O que é o Reconhecimento de Entidades Nomeadas (REN) e qual é sua importância?",
            "resposta": "O REN é a tarefa de identificar e classificar entidades mencionadas em textos através de designadores rígidos como nomes próprios, expressões temporais e espécies biológicas. Essa tarefa é considerada um primeiro passo na análise semântica de um texto, pois permite identificar as entidades às quais se faz referência nele. A importância do REN reside em sua capacidade de fornecer uma base para a análise semântica de textos e permitir a identificação de entidades relevantes."
        },
        {
            "pergunta": "O que é a Extração de Relações (ER) e qual é sua importância?",
            "resposta": "A ER é a tarefa de identificar relacionamentos semânticos entre duas ou mais entidades, ou seja, identificar 'quem fez o que para quem e quando'. Essa tarefa é fundamental para o entendimento da informação relatada no texto e sua identificação é passo essencial para a estruturação da mesma. A importância da ER reside em sua capacidade de fornecer uma base para a construção de bases de conhecimento e permitir a identificação de relações relevantes entre entidades."
        },
        {
            "pergunta": "O que é a Extração de Eventos (EE) e qual é sua importância?",
            "resposta": "A EE é a tarefa de identificação de uma menção a um evento em uma sentença e, se existirem, extração de outras informações sobre o evento. Um evento pode ser entendido como uma ocorrência específica envolvendo participantes. A importância da EE reside em sua capacidade de fornecer uma base para a análise de eventos mencionados em textos e permitir a identificação de informações relevantes sobre esses eventos."
        },
        {
            "pergunta": "Quais são as principais aplicações da Extração de Informação (EI)?",
            "resposta": "As principais aplicações da EI incluem a construção de bases de conhecimento, a resposta automática a perguntas, a sumarização, a recuperação de informação e mais. A EI também tem aplicações em áreas como a inteligência artificial, a ciência da computação e a linguística computacional."
        },
        {
            "pergunta": "Quais foram os primeiros trabalhos que abordaram o problema de Extração de Informações (EI) e como eles se desenvolveram?",
            "resposta": "Os primeiros trabalhos que abordaram o problema de EI surgiram no final da década de 1970. Esses trabalhos tinham como modelo geral a aplicação de regras para a identificação de informações especificadas em um gabarito. Tais sistemas empregavam analisadores sintáticos (parsers) e regras definidas especificamente para o domínio e gênero textual estudado. Exemplos desses trabalhos incluem os de Sager (1978), Sager et al. (1987), DeJong (1979) e Cowie (1983)."
        },
        {
            "pergunta": "O que foi a conferência MUC e como ela contribuiu para o desenvolvimento da área de EI?",
            "resposta": "A conferência MUC (Message Understanding Conference) foi uma série de conferências realizadas entre 1987 e 1997, promovidas pela Agência de Projetos de Pesquisa Avançada de Defesa (DARPA). Essas conferências representaram um esforço em avançar a tecnologia de EI e consistiam de tarefas de avaliação conjunta de métodos desenvolvidos por pesquisadores para problemas propostos pelos organizadores. A MUC-6, ocorrida em 1995, introduziu a tarefa de Reconhecimento de Entidades Nomeadas (REN) com o intuito de ser uma tarefa de uso prático, independente de domínio e que poderia ser realizada automaticamente em um futuro próximo."
        },
        {
            "pergunta": "Como a abordagem baseada em dados influenciou o desenvolvimento da área de EI?",
            "resposta": "A década de 1990 viu um crescimento de abordagens baseadas em dados, a partir da análise de corpora. Tais esforços foram impulsionados pelos resultados positivos na área, como o trabalho de Hearst (1992). Métodos baseados em dados passaram a explorar o emprego de análise estatística e aprendizado de máquina na construção de padrões para a extração de relações. Além disso, a proliferação de métodos supervisionados aplicados à Extração de Relações (ER) e ao REN também foi observada."
        },
        {
            "pergunta": "Quais são as principais tendências atuais na área de EI?",
            "resposta": "A aplicação de métodos baseados em redes neurais, em particular deep learning e grandes modelos de linguagem, às tarefas de Processamento de Linguagem Natural é uma tendência atual da área. Além disso, a geração de grandes conjuntos de dados por supervisão fraca também é uma área de pesquisa em desenvolvimento. A Wikipédia e Freebase são fontes comuns usadas para obter anotações de entidades e relações em textos."
        },
        {
            "pergunta": "Qual é a importância da conceituação de relação e entidade na área de EI?",
            "resposta": "A conceituação de relação e entidade é fundamental na área de EI, pois define o que é extraído e como é extraído. A concordância entre as definições de Entidade e Relação é necessária para que a tarefa de EI seja realizada de forma eficaz. A próxima seção discute a conceituação de relação adotada e o conceito de entidade, que são essenciais para entender a área de EI."
        },
        {
            "pergunta": "O que é Extração de Informação Aberta (EIA) e como ela se difere da Extração de Relações?",
            "resposta": "A Extração de Informação Aberta (EIA) é a tarefa de extrair informações estruturadas de documentos sem necessitar da pré-definição do contexto da tarefa, ou seja, das relações e tipos de entidade de interesse. A principal diferença entre a EIA e a Extração de Relações é que a EIA não depende de uma especificação prévia do domínio de aplicação e das relações alvo a serem identificadas, enquanto a Extração de Relações sim. Isso significa que a EIA visa eliminar a necessidade de pré-definição do contexto da tarefa, tornando-a mais flexível e geral."
        },
        {
            "pergunta": "Quais são as principais aplicações da Extração de Informação Aberta?",
            "resposta": "A Extração de Informação Aberta tem várias aplicações, incluindo processar e estruturar o conhecimento a partir de grandes volumes de dados disponíveis na Web, seguindo o paradigma da Web como um Corpus (WaC). Além disso, a EIA pode ser usada para extrair informações estruturadas de documentos, o que pode ser útil em várias áreas, como a mineração de dados, a análise de texto e a inteligência artificial."
        },
        {
            "pergunta": "Quais são os principais desafios da Extração de Informação Aberta para a língua portuguesa?",
            "resposta": "Os principais desafios da Extração de Informação Aberta para a língua portuguesa incluem a falta de recursos linguísticos disponíveis para orientar o desenvolvimento de pesquisas para a língua, bem como a necessidade de desenvolver métodos e sistemas específicos para a língua portuguesa. Além disso, a área ainda é relativamente recente e precisa de mais estudos e desenvolvimentos para alcançar o mesmo nível de maturidade que a Extração de Informação Aberta para a língua inglesa."
        },
        {
            "pergunta": "Quais são os principais avanços na área de Extração de Informação Aberta para a língua portuguesa?",
            "resposta": "Os principais avanços na área de Extração de Informação Aberta para a língua portuguesa incluem o desenvolvimento de métodos e sistemas específicos para a língua portuguesa, como o trabalho de Souza e Claro (2014), Pereira e Pinheiro (2015) e Barbosa et al. (2016). Além disso, houve um crescimento recente no número de estudos sobre a tarefa e os resultados obtidos por esses estudos, com desenvolvimentos de métodos, construção de corpus e avaliação de sistemas disponíveis."
        },
        {
            "pergunta": "Por que a área de Extração de Informação Aberta para a língua portuguesa é importante?",
            "resposta": "A área de Extração de Informação Aberta para a língua portuguesa é importante porque a língua portuguesa é uma das línguas mais faladas do mundo e tem uma grande quantidade de textos disponíveis na Web. Além disso, a Extração de Informação Aberta pode ser usada para extrair informações estruturadas de documentos, o que pode ser útil em várias áreas, como a mineração de dados, a análise de texto e a inteligência artificial. Portanto, é importante desenvolver métodos e sistemas específicos para a língua portuguesa para aproveitar ao máximo as informações disponíveis."
        },
        {
            "pergunta": "Qual foi o primeiro evento que estabeleceu a avaliação sistemática de sistemas de Extração de Informação (EI)?",
            "resposta": "A avaliação sistemática de sistemas de EI foi estabelecida primeiramente nas conferências MUC, em particular na sua segunda edição, com o estabelecimento de gabaritos-padrão que deveriam ser utilizados por todos os sistemas participantes e a adoção de métricas de qualidade, baseadas naquelas usadas na área de recuperação de informação."
        },
        {
            "pergunta": "Quais são as métricas de qualidade utilizadas para avaliar a tarefa de extração de relações?",
            "resposta": "As métricas de qualidade utilizadas para avaliar a tarefa de extração de relações são precisão e cobertura, também denominada de Recall ou Revocação. A precisão reflete a qualidade das extrações, enquanto a cobertura reflete quão abrangente o sistema é em suas extrações."
        },
        {
            "pergunta": "Qual é a fórmula para calcular a precisão de um sistema de EI?",
            "resposta": "A precisão de um sistema de EI pode ser calculada como: P = # (relacionamentos corretamente extraídos) / # (relacionamentos extraídos pelo sistema)"
        },
        {
            "pergunta": "Qual é a fórmula para calcular a cobertura de um sistema de EI?",
            "resposta": "A cobertura de um sistema de EI pode ser calculada como: R = # (relacionamentos extraídos) / # (relacionamentos no corpus)"
        },
        {
            "pergunta": "Qual é a medida F1 e como ela é calculada?",
            "resposta": "A medida F1 é a média harmônica entre a precisão e a cobertura. Ela pode ser calculada como: F1 = 2 * P * R / (P + R)"
        },
        {
            "pergunta": "Quais são as peculiaridades da avaliação de sistemas de Extração de Informação Aberta (EIA)?",
            "resposta": "A avaliação de sistemas de EIA possui algumas peculiaridades, como a dificuldade de avaliar a cobertura e a precisão devido à falta de uma referência do conjunto total de relacionamentos a serem identificados. Além disso, a avaliação de EIA muitas vezes recorre a avaliações qualitativas das saídas dos sistemas e comparação direta por humanos das extrações obtidas."
        },
        {
            "pergunta": "Qual é a métrica de rendimento (yield) e como ela é calculada?",
            "resposta": "A métrica de rendimento é o número de extrações válidas de um dado sistema. Ela pode ser calculada como: Y = P' * # (extrações realizadas), onde P' é a precisão do sistema calculada sobre uma amostra aleatória das extrações realizadas."
        },
        {
            "pergunta": "Quais são as iniciativas para avaliar os sistemas de EIA em língua portuguesa?",
            "resposta": "Uma avaliação conjunta foi promovida durante o Fórum Ibérico de Avaliação de Línguas (IberLEF) em 2019, utilizando o corpus proposto por Glauber et al. (2018). Além disso, outra abordagem de avaliação foi idealizada por Malenchini et al. (2019), que se focou na avaliação extrínseca dos sistemas de EIA através de sua contribuição na tarefa de respostas automáticas a perguntas."
        }
    ],
    "cap-mt": [
        {
            "pergunta": "O que é tradução automática e como ela funciona?",
            "resposta": "A tradução automática (TA) é um processo que envolve a tradução de um texto eletrônico de uma língua para outra sem intervenção humana. Ela funciona por meio da análise e interpretação da língua-fonte e da geração da língua-alvo, com o objetivo de produzir uma saída semanticamente equivalente à entrada. A TA utiliza modelos estatísticos e neurais para realizar essa tarefa, e é amplamente utilizada em todo o mundo por governos, indústria da tradução, consumidores finais e em pesquisas."
        },
        {
            "pergunta": "Quais são as principais abordagens para a tradução automática?",
            "resposta": "As principais abordagens para a tradução automática incluem abordagens baseadas em regras, exemplos, estatísticas e, mais recentemente, a TA neural. Cada uma dessas abordagens tem suas próprias vantagens e desvantagens, e a escolha da abordagem certa depende do contexto e do objetivo da tradução. A abordagem baseada em regras utiliza regras linguísticas pré-definidas para realizar a tradução, enquanto a abordagem baseada em exemplos utiliza exemplos de traduções pré-existentes para aprender a traduzir. A abordagem estatística utiliza estatísticas para aprender a traduzir, e a abordagem neural utiliza redes neurais para realizar a tradução."
        },
        {
            "pergunta": "Quando surgiu a tradução automática e como ela evoluiu ao longo dos anos?",
            "resposta": "A tradução automática surgiu no final dos anos 1950 e início dos anos 1960, com os experimentos de Georgetown. No entanto, é possível encontrar referências a tentativas de tradução automática no século XVII. Ao longo dos anos, a TA evoluiu significativamente com o avanço de modelos estatísticos e neurais. Atualmente, a TA é amplamente utilizada em todo o mundo por governos, indústria da tradução, consumidores finais e em pesquisas em uma variedade de aplicações."
        },
        {
            "pergunta": "Qual é o papel da tradução automática na sociedade atual?",
            "resposta": "A tradução automática desempenha um papel importante não apenas no âmbito comercial, mas também no âmbito social e político. Ela é amplamente utilizada em diversas aplicações de comunicação, que incluem tradução de textos, áudio e vídeo. Além disso, a TA é utilizada em pesquisas e desenvolvimento de tecnologias, como a inteligência artificial e o processamento de linguagem natural. O impacto da TA pode ser observado em nossa sociedade, e é por isso que a avaliação da TA é fundamental para garantir a qualidade da tradução."
        },
        {
            "pergunta": "Por que a avaliação da tradução automática é importante?",
            "resposta": "A avaliação da tradução automática é importante porque garante a qualidade da tradução. Com a ampla utilização da TA atualmente, é fundamental avaliar a precisão e a eficácia da tradução para garantir que ela atenda às necessidades dos usuários. A avaliação da TA envolve a comparação da tradução automática com a tradução humana, e é utilizada para identificar erros e melhorar a qualidade da tradução. Além disso, a avaliação da TA é fundamental para garantir que a tradução seja semanticamente equivalente à entrada, e que ela atenda às necessidades dos usuários."
        },
        {
            "pergunta": "Qual é a importância da avaliação da qualidade da tradução no mundo globalizado de hoje?",
            "resposta": "A avaliação da qualidade da tradução é fundamental no mundo globalizado de hoje, pois a tradução é um processo multifacetado que envolve fatores cognitivos, linguísticos, sociais, culturais e técnicos. A avaliação da qualidade da tradução ajuda a garantir que as traduções sejam precisas, coerentes e adequadas para o público-alvo, o que é essencial para a comunicação eficaz em um mundo cada vez mais interconectado."
        },
        {
            "pergunta": "O que é a avaliação da qualidade da tradução (AQT) e como ela se relaciona com a avaliação da tradução automática (ATA)?",
            "resposta": "A avaliação da qualidade da tradução (AQT) é o processo de avaliar a qualidade das traduções, tanto humanas quanto automáticas. Já a avaliação da tradução automática (ATA) é um subcampo da AQT que se concentra exclusivamente na avaliação da qualidade dos sistemas de tradução automática. Enquanto a AQT abrange uma ampla gama de traduções, a ATA se concentra especificamente na avaliação da eficácia e precisão dos sistemas de tradução automática."
        },
        {
            "pergunta": "Por que a avaliação da qualidade da tradução é um processo complexo?",
            "resposta": "A avaliação da qualidade da tradução é um processo complexo porque envolve fatores cognitivos, linguísticos, sociais, culturais e técnicos. Além disso, a tradução é um processo criativo que requer habilidades e conhecimentos específicos, o que torna difícil definir e medir a qualidade da tradução de forma objetiva. A complexidade da avaliação da qualidade da tradução também é agravada pela falta de consenso sobre o que é e como ela deve ser feita."
        },
        {
            "pergunta": "Quais são as principais abordagens para a avaliação da qualidade da tradução?",
            "resposta": "Existem várias abordagens para a avaliação da qualidade da tradução, incluindo a avaliação baseada em critérios, a avaliação baseada em métricas e a avaliação baseada em modelos. A avaliação baseada em critérios envolve a definição de critérios específicos para avaliar a qualidade da tradução, enquanto a avaliação baseada em métricas envolve a utilização de métricas quantitativas para avaliar a precisão e a coerência da tradução. A avaliação baseada em modelos envolve a utilização de modelos estatísticos e linguísticos para avaliar a qualidade da tradução."
        },
        {
            "pergunta": "Qual é a importância de realizar a avaliação dos sistemas de tradução automática?",
            "resposta": "A avaliação dos sistemas de tradução automática é fundamental para garantir que esses sistemas sejam precisos e eficazes. A avaliação ajuda a identificar os pontos fortes e fracos dos sistemas de tradução automática, o que permite melhorar a qualidade da tradução e aumentar a confiança dos usuários. Além disso, a avaliação dos sistemas de tradução automática ajuda a garantir que esses sistemas sejam utilizados de forma responsável e ética, o que é essencial para a comunicação eficaz em um mundo cada vez mais interconectado."
        },
        {
            "pergunta": "Qual é o futuro da tradução automática (TA) e como a globalização e a internet estão afetando sua evolução?",
            "resposta": "O futuro da TA parece muito promissor, com a globalização e a internet criando mais conteúdo todos os dias e surgindo casos de uso nos quais a TA pode ser útil. A TA está bem encaminhada para se tornar a tecnologia mais importante para aprimorar a produtividade dos tradutores humanos."
        },
        {
            "pergunta": "Qual é a diferença entre a TA neural (NMT) e seu antecessor, o PSMT, e como a NMT está mudando o paradigma da área?",
            "resposta": "A NMT é uma mudança de paradigma na área, com um aumento impressionante na qualidade em comparação com o PSMT. A NMT é capaz de lidar melhor com conteúdo gerado pelo usuário e está sendo aplicada de forma útil em ambientes de tradução de fala e discurso falado."
        },
        {
            "pergunta": "Como os modelos de linguagem em larga escala (LLMs) estão sendo utilizados na TA e quais são suas limitações?",
            "resposta": "Os LLMs estão sendo amplamente utilizados na TA, proporcionando melhorias significativas na qualidade e na fluidez das traduções geradas. No entanto, a qualidade da tradução depende de vários fatores, como a disponibilidade de dados de treinamento de alta qualidade e a compreensão do contexto e nuances linguísticas. Além disso, os LLMs podem ser sensíveis a preconceitos presentes nos dados de treinamento."
        },
        {
            "pergunta": "Qual é o papel dos linguistas profissionais na interação com a saída da tradução automática e como a demanda por esses profissionais está mudando?",
            "resposta": "A demanda por linguistas profissionais que possam interagir com a saída da tradução automática está aumentando, pois os clientes corporativos estão buscando soluções personalizadas e adaptadas ao seu conteúdo. Além disso, a interação humana com a tradução automática ainda é altamente demandada, pois a qualidade da tradução automática ainda não atingiu os níveis da tradução humana."
        },
        {
            "pergunta": "Por que é importante testar a qualidade dos sistemas de TA e como isso pode afetar a confiabilidade desses sistemas?",
            "resposta": "A necessidade de testar a qualidade dos sistemas de TA se tornou essencial, pois a TA está se tornando ubíqua em nosso dia a dia. Uma boa prática na avaliação da TA é essencial para evitar afirmações exageradas e fornecer aos usuários um feedback honesto. Isso pode afetar a confiabilidade dos sistemas de TA e a confiança dos usuários nesses sistemas."
        }
    ],
    "cap-as": [
        {
            "pergunta": "Qual é o objetivo principal do processo de resumir?",
            "resposta": "O objetivo principal do processo de resumir é transmitir uma mensagem o mais próximo do original, valorizando o conteúdo mesmo em detrimento da forma. Isso envolve selecionar o que é mais importante do evento, reestruturar o fato acontecido ou vivenciado dentro da narrativa sintetizada e considerar o tempo ou limite textual disponível para a construção da narrativa resumida."
        },
        {
            "pergunta": "Como a liberdade estilística na produção da síntese pode afetar a narrativa?",
            "resposta": "A liberdade estilística na produção da síntese pode permitir que o autor acrescente e evidencie suas avaliações e perspectivas, mas não deve veicular conteúdo falso. A narrativa deve representar o evento real, mesmo que com uma abordagem mais criativa ou interpretativa."
        },
        {
            "pergunta": "O que é reescrita e como ela se relaciona com o processo de resumir?",
            "resposta": "A reescrita é o trabalho que é feito por um autor por meio de operações na linguagem que abrangem diferentes aspectos do texto, conduzindo-o à sua modificação. O processo de resumir está compreendido na reescrita, pois envolve a seleção e reorganização de informações para criar uma versão resumida do texto original."
        },
        {
            "pergunta": "Qual é a definição de sumarização automática (SA) e como ela se relaciona com o processamento de linguagem natural (PLN)?",
            "resposta": "A sumarização automática (SA) é a produção automatizada de versões reduzidas de outros textos, resultando em sumários. A SA é uma tarefa importante no processamento de linguagem natural (PLN), pois envolve a análise e seleção de informações relevantes em textos para criar uma versão resumida que seja útil e precisa."
        },
        {
            "pergunta": "Quais são as tendências e perspectivas metodológicas da sumarização automática?",
            "resposta": "As tendências e perspectivas metodológicas da sumarização automática incluem a recuperação de informação em textos, a resolução de problemas linguísticos complexos, a democratização da Web e a influência dos Large Language Models (LLMs) em PLN. Além disso, a sumarização automática pode ser aplicada em diversas áreas, como compreensão de textos, recuperação da informação, indexação, mineração de textos e sistemas de pergunta-resposta."
        },
        {
            "pergunta": "Quais são as motivações para a sumarização automática em PLN?",
            "resposta": "As motivações para a sumarização automática em PLN incluem a necessidade de processar grandes volumes de informação, a democratização da Web, a influência dos Large Language Models (LLMs) e a aplicação em diversas áreas, como compreensão de textos, recuperação da informação, indexação, mineração de textos e sistemas de pergunta-resposta."
        },
        {
            "pergunta": "Quais são as limitações e desafios da sumarização automática?",
            "resposta": "As limitações e desafios da sumarização automática incluem a dificuldade em lidar com problemas linguísticos complexos, a necessidade de desenvolver algoritmos mais precisos e eficientes, a influência da língua e da cultura na interpretação de textos e a necessidade de considerar as necessidades e preferências dos usuários."
        },
        {
            "pergunta": "Qual é o objetivo da avaliação de desempenho dos sistemas de SA?",
            "resposta": "O objetivo da avaliação de desempenho dos sistemas de SA é medir a qualidade e a informatividade dos sumários produzidos, considerando critérios como a preservação de informações relevantes, gramaticalidade, coesão e coerência."
        },
        {
            "pergunta": "Qual é a métrica de avaliação amplamente utilizada para medir a informatividade dos sumários?",
            "resposta": "A métrica de avaliação amplamente utilizada para medir a informatividade dos sumários é a ROUGE (Recall-Oriented Understudy for Gisting Evaluation), que compara a quantidade de n-gramas em comum entre um sumário automático e um ou mais sumários de referência."
        },
        {
            "pergunta": "Quais são as principais variantes da métrica ROUGE?",
            "resposta": "As principais variantes da métrica ROUGE são ROUGE-1, ROUGE-2, ROUGE-3 e ROUGE-4, que medem a precisão, revocação e medida-f (média harmônica entre precisão e revocação) dos sumários."
        },
        {
            "pergunta": "Qual é o problema da métrica ROUGE em relação à avaliação da qualidade dos sumários?",
            "resposta": "A métrica ROUGE aborda somente a capacidade de seleção de conteúdo dos sistemas, ignorando aspectos de qualidade linguística como coerência e gramaticalidade, o que pode levar a avaliações incompletas ou imprecisas."
        },
        {
            "pergunta": "Quais são as propriedades linguísticas introduzidas pela DUC para avaliar a qualidade dos sumários automáticos?",
            "resposta": "As propriedades linguísticas introduzidas pela DUC para avaliar a qualidade dos sumários automáticos são: coesão, coerência, clareza referencial, gramaticalidade e foco."
        },
        {
            "pergunta": "Qual é o método da pirâmide para avaliar a qualidade dos sumários?",
            "resposta": "O método da pirâmide é um método que utiliza um conjunto de sumários de referência para extrair unidades de conteúdo do sumário (SCUs) e avaliar a qualidade dos sumários automáticos com base na presença e peso dessas unidades."
        },
        {
            "pergunta": "Por que a produção de sumários é considerada uma tarefa intelectual?",
            "resposta": "A produção de sumários é considerada uma tarefa intelectual porque sofre influência da familiaridade com o assunto, atitude e disposição do produtor, e pode depender dos interesses de quem o produz, dos interesses dos leitores e da importância subjetiva que ele atribui às informações textuais."
        },
        {
            "pergunta": "Qual é o problema da dependência da sobreposição de unidades de n-gramas entre sumários de referência e candidatos?",
            "resposta": "A dependência da sobreposição de unidades de n-gramas entre sumários de referência e candidatos pode levar a avaliações incompletas ou imprecisas, pois não considera a qualidade linguística e a coerência dos sumários."
        },
        {
            "pergunta": "Quais são as três etapas principais para produzir um sumário?",
            "resposta": "As três etapas principais para produzir um sumário são: análise, transformação e síntese. Na etapa de análise, os textos-fonte são interpretados e representados de forma formal. Na etapa de transformação, os segmentos dos textos-fonte são ranqueados e selecionados com base em algum critério de relevância. Na etapa de síntese, o sumário é gerado em língua natural a partir do conteúdo selecionado."
        },
        {
            "pergunta": "O que acontece na etapa de transformação do processo de produção de um sumário?",
            "resposta": "Na etapa de transformação, os segmentos dos textos-fonte são ranqueados e selecionados com base em algum critério de relevância. Isso significa que os segmentos mais relevantes são selecionados e incluídos no sumário, enquanto os menos relevantes são excluídos. O objetivo é alcançar a taxa de compressão desejada para o sumário."
        },
        {
            "pergunta": "Quais são os métodos utilizados na etapa de síntese do processo de produção de um sumário?",
            "resposta": "Na etapa de síntese, podem ser utilizados métodos de tratamento de correferência, fusão, linearização, justaposição e ordenação de sentenças. Esses métodos permitem que o sumário seja gerado em língua natural a partir do conteúdo selecionado. A correferência é utilizada para garantir que as referências sejam consistentes, a fusão combina informações relacionadas, a linearização organiza as sentenças em uma ordem lógica, a justaposição combina sentenças relacionadas e a ordenação de sentenças garante que o sumário seja apresentado de forma clara e lógica."
        },
        {
            "pergunta": "Por que é importante considerar a arquitetura genérica do processo de produção de um sumário?",
            "resposta": "A arquitetura genérica do processo de produção de um sumário é importante porque ela fornece uma estrutura para entender como os diferentes métodos e técnicas podem ser utilizados para produzir um sumário. Além disso, ela permite que os desenvolvedores de algoritmos de sumarização criem soluções mais eficazes e eficientes, pois podem aproveitar as vantagens de diferentes abordagens e combinar técnicas de forma mais eficaz."
        },
        {
            "pergunta": "Quais são os recursos adicionais disponíveis para complementar a leitura sobre o processo de produção de um sumário?",
            "resposta": "Para complementar a leitura sobre o processo de produção de um sumário, estão disponíveis o Apêndice D, que discute aspectos relevantes ao pré-processamento dos textos para SA, e um notebook virtual com os algoritmos apresentados no capítulo. Esses recursos adicionais fornecem informações mais detalhadas e práticas sobre como produzir um sumário e podem ser úteis para aqueles que desejam aprender mais sobre o assunto."
        },
        {
            "pergunta": "Qual é o contexto da aplicação da Análise de Sumarização (SA) ao português brasileiro?",
            "resposta": "A aplicação da Análise de Sumarização (SA) ao português brasileiro acompanhou as tendências metodológicas e de aplicações da literatura internacional. Isso significa que a SA no português brasileiro foi influenciada pelas abordagens e técnicas desenvolvidas em outros países e contextos linguísticos."
        },
        {
            "pergunta": "Quais são alguns recursos desenvolvidos para a Análise de Sumarização (SA) em português brasileiro?",
            "resposta": "Alguns recursos desenvolvidos para a Análise de Sumarização (SA) em português brasileiro incluem o TeMário, um corpus de textos jornalísticos com sumários, e o CSTNews, um corpus de textos jornalísticos com diferentes camadas de anotação linguística e sumários humanos. Outros recursos incluem o OpiSums-PT15, um corpus de opiniões escritas em português brasileiro, e a ferramenta CSTTool, que utiliza o modelo discursivo Cross-document Structure Theory para solucionar desafios linguísticos da sumarização multidocumento."
        },
        {
            "pergunta": "Quais são as principais diferenças entre as abordagens tradicionais e as abordagens mais recentes da Análise de Sumarização (SA)?",
            "resposta": "As abordagens tradicionais da Análise de Sumarização (SA) se baseavam em métodos superficiais, como a identificação de palavras-chave, para gerar sumários. Já as abordagens mais recentes utilizam modelos de língua mais robustos, como os LLMs (Large Language Models), que podem gerar sumários com mais qualidade linguística. Além disso, as abordagens mais recentes também se baseiam em técnicas de aprendizado de máquina e processamento de linguagem natural para melhorar a precisão e a eficiência da SA."
        },
        {
            "pergunta": "Quais são os principais desafios da Análise de Sumarização (SA) em diferentes domínios?",
            "resposta": "Os principais desafios da Análise de Sumarização (SA) em diferentes domínios incluem a dependência de língua e domínio, que pode ser custosa do ponto de vista do processamento de linguagem natural. No entanto, os LLMs podem ajudar a resolver esse problema. Além disso, a SA em diferentes domínios também pode ser desafiadora devido à falta de dados e recursos específicos para cada domínio."
        },
        {
            "pergunta": "Quais são os principais grupos de pesquisa e instituições que trabalham com a Análise de Sumarização (SA) em português brasileiro?",
            "resposta": "Os principais grupos de pesquisa e instituições que trabalham com a Análise de Sumarização (SA) em português brasileiro incluem o Núcleo Interinstitucional de Linguística Computacional (NILC), o Sistemas de Informação e Banco de Dados (SINBAD) da Universidade Federal de Campina Grande e o Grupo de pesquisa em sistemas inteligentes (GSI) da Universidade Tecnológica Federal do Paraná. Esses grupos têm desenvolvido pesquisas e projetos em SA utilizando diferentes abordagens e técnicas."
        }
    ],
    "cap-complexidade-textual": [
        {
            "pergunta": "O que é complexidade textual e em quais áreas é estudada?",
            "resposta": "A complexidade textual é um tópico que é estudado em várias áreas, incluindo estudos do discurso, educação, psicolinguística, linguística cognitiva, fonoaudiologia e processamento de linguagens naturais (PLN). A complexidade textual se refere à dificuldade ou facilidade com que um texto pode ser compreendido por um leitor ou ouvinte. Ela depende de vários fatores, incluindo a escolha de palavras, a estrutura do texto e o conhecimento prévio do leitor."
        },
        {
            "pergunta": "O que é inteligibilidade textual e como ela se relaciona com a complexidade?",
            "resposta": "A inteligibilidade textual é a facilidade com que um texto pode ser compreendido por um leitor ou ouvinte. Ela é inversamente correlacionada com a complexidade, ou seja, quanto mais complexo um texto for, menos inteligível ele será. A inteligibilidade depende de vários fatores, incluindo a escolha de palavras, a estrutura do texto e o conhecimento prévio do leitor. É importante notar que a inteligibilidade não é o mesmo que a legibilidade, que se refere à facilidade de ler um texto devido a fatores como o tamanho e tipo da fonte, cor, estruturação em itens, etc."
        },
        {
            "pergunta": "Por que a complexidade é sempre relativa e como isso afeta a compreensão de um texto?",
            "resposta": "A complexidade é sempre relativa porque depende do conhecimento prévio, habilidade de leitura, interesse e motivação do leitor ou ouvinte. O que pode ser complexo para uma pessoa pode ser fácil para outra. Isso significa que a complexidade de um texto não é uma característica objetiva, mas sim uma característica subjetiva que depende do leitor ou ouvinte. Portanto, é importante considerar o público alvo do texto e adaptar a linguagem e a estrutura do texto para atender às suas necessidades e habilidades."
        },
        {
            "pergunta": "Qual é a diferença entre inteligibilidade e legibilidade?",
            "resposta": "A inteligibilidade se refere à facilidade com que um texto pode ser compreendido por um leitor ou ouvinte, enquanto a legibilidade se refere à facilidade de ler um texto devido a fatores como o tamanho e tipo da fonte, cor, estruturação em itens, etc. Em outras palavras, a legibilidade é uma característica que afeta a facilidade de ler um texto, mas não necessariamente a compreensão do texto. A inteligibilidade, por outro lado, é uma característica que afeta a compreensão do texto e depende de vários fatores, incluindo a escolha de palavras, a estrutura do texto e o conhecimento prévio do leitor."
        },
        {
            "pergunta": "Por que é importante considerar o público alvo do texto ao avaliar a complexidade e a inteligibilidade?",
            "resposta": "É importante considerar o público alvo do texto ao avaliar a complexidade e a inteligibilidade porque a complexidade e a inteligibilidade são características relativas que dependem do conhecimento prévio, habilidade de leitura, interesse e motivação do leitor ou ouvinte. Se o texto é destinado a um público específico, é importante adaptar a linguagem e a estrutura do texto para atender às suas necessidades e habilidades. Isso pode incluir a escolha de palavras e frases simples, a estruturação do texto de forma clara e lógica, e a inclusão de exemplos e ilustrações para ajudar a explicar conceitos complexos."
        },
        {
            "pergunta": "Qual é a relação entre a complexidade de um texto e o leitor?",
            "resposta": "A complexidade de um texto existe apenas a partir do ponto de vista específico de quem está lendo. Isso significa que a complexidade é subjetiva e depende do conhecimento de mundo, experiência, habilidade de leitura e grau de interesse do leitor no texto. Portanto, pessoas diferentes podem achar o mesmo texto complexo e simples, dependendo de suas características individuais."
        },
        {
            "pergunta": "O que é o Indicador de Alfabetismo Funcional (INAF) e qual é sua importância?",
            "resposta": "O INAF é um levantamento que avalia o nível de letramento da população adulta do Brasil. Ele é realizado a cada dois anos, desde 2001, e classifica a população em cinco níveis de letramento: analfabeto, rudimentar, básico, proficiente e pleno. Embora a classificação seja relativamente arbitrária, o INAF fornece um retrato geral dos potenciais leitores adultos do Brasil e ajuda a identificar áreas de melhoria na educação e no desenvolvimento de políticas públicas."
        },
        {
            "pergunta": "Qual é a diferença entre alfabetização e letramento?",
            "resposta": "Alfabetização refere-se ao processo mecânico de reconhecer os grafemas e ligá-los aos fonemas, enquanto letramento é o uso social desse processo. Em outras palavras, alfabetização é a habilidade de ler e escrever, enquanto letramento é a capacidade de usar a leitura e a escrita de forma eficaz em diferentes contextos sociais. O INAF avalia o nível de letramento da população, e não apenas a alfabetização."
        },
        {
            "pergunta": "O que é o estado de fluxo e como ele se relaciona com a leitura?",
            "resposta": "O estado de fluxo é um conceito que descreve a experiência de estar completamente engajado em uma atividade. No contexto da leitura, o estado de fluxo ocorre quando o nível de dificuldade do texto é adequado para o nível de proficiência do leitor. Se o texto for muito simples, o leitor pode se sentir entediado, enquanto que se o texto for muito difícil, o leitor pode se sentir desmotivado. O estado de fluxo é importante porque ele pode aumentar a motivação e o engajamento do leitor."
        },
        {
            "pergunta": "Qual é a importância da motivação para a leitura?",
            "resposta": "A motivação é fundamental para a leitura eficaz. Se o leitor não estiver motivado, ele pode não se engajar com o texto e não alcançar os objetivos de leitura. A motivação pode ser influenciada por fatores como o interesse no assunto, a habilidade de leitura e o nível de dificuldade do texto. O estado de fluxo é um conceito que pode ajudar a entender como a motivação se relaciona com a leitura."
        },
        {
            "pergunta": "O que é a análise automatizada da complexidade de textos e quais são suas aplicações práticas?",
            "resposta": "A análise automatizada da complexidade de textos, também conhecida como ARA (Automatic Readability Assessment), é uma área de pesquisa que visa desenvolver métodos para medir automaticamente a complexidade de textos ou sentenças. Suas aplicações práticas incluem ajudar a indicar material de leitura adequado para uma dada série escolar e contribuir para um melhor entendimento dos processos de leitura e compreensão em populações com processamento típico e atípico de linguagem."
        },
        {
            "pergunta": "Quais são as abordagens de predição e medição da complexidade de textos?",
            "resposta": "De acordo com Graesser et al. (2011), as abordagens de predição e medição da complexidade de textos podem ser divididas em duas categorias: uma que se baseia em métricas clássicas, como o Índice Flesch, e outra que se baseia em métricas mais avançadas, como o Coh-Metrix. Além disso, também existem abordagens que utilizam rastreamento ocular para capturar o processo de leitura de estudantes."
        },
        {
            "pergunta": "Quais são os desafios para a aplicação dos métodos de análise automatizada da complexidade de textos?",
            "resposta": "Um dos grandes desafios para a aplicação dos métodos de análise automatizada da complexidade de textos é a criação de corpora grandes e balanceados, anotados com as classes de interesse, por professores ou linguistas. Além disso, também é necessário converter os textos em valores numéricos para serem usados nas fases de treinamento e avaliação dos métodos."
        },
        {
            "pergunta": "Qual é a crítica frequente à abordagem de anotação da complexidade que usa preditores com base em corpus com julgamento de especialistas?",
            "resposta": "A crítica frequente à abordagem de anotação da complexidade que usa preditores com base em corpus com julgamento de especialistas é que a anotação não é baseada no desempenho real da leitura de estudantes. Isso significa que a anotação pode não refletir a complexidade real do texto para os leitores."
        },
        {
            "pergunta": "Qual é o corpus Touchstone Applied Science Associates (TASA) e qual é sua importância?",
            "resposta": "O corpus Touchstone Applied Science Associates (TASA) é um grande corpus de textos em inglês que foi anotado com a métrica DRP (Degrees of Reading Power) e com avaliações de leitura de estudantes. É importante porque é um dos poucos corpora que atende à crítica de que a anotação da complexidade deve ser baseada no desempenho real da leitura de estudantes."
        },
        {
            "pergunta": "Quais são as principais fontes de métricas para a tarefa de predição da complexidade textual e sentencial?",
            "resposta": "As principais fontes de métricas para a tarefa de predição da complexidade textual e sentencial incluem fórmulas clássicas, linguísticas, psicolinguísticas e de rastreamento ocular. Cada uma dessas fontes fornece métricas específicas que podem ser usadas para medir a complexidade de textos e sentenças."
        },
        {
            "pergunta": "Quais são as principais tarefas que os grandes modelos de linguagem podem realizar para avaliar e tratar a complexidade de textos?",
            "resposta": "Os grandes modelos de linguagem podem realizar tarefas como simplificação de textos, explicação das operações de simplificação utilizadas, identificação de entidades nomeadas e eliminação de informações desnecessárias. Além disso, eles podem ser utilizados para avaliar a complexidade de textos e fornecer sugestões para melhorar a clareza e a compreensão dos mesmos."
        },
        {
            "pergunta": "Quais são os modelos de linguagem utilizados no experimento de simplificação de textos?",
            "resposta": "Os modelos de linguagem utilizados no experimento foram o Bard da Google, o Copilot da Microsoft e o ChatGPT 3.5 da OpenAi. Esses modelos foram escolhidos por serem alguns dos mais avançados e capazes de realizar tarefas de linguagem natural."
        },
        {
            "pergunta": "Qual foi o prompt fornecido aos modelos de linguagem para realizar a simplificação de textos?",
            "resposta": "O prompt fornecido aos modelos foi: 'Simplifique o seguinte texto para que um aluno do quarto ano do ensino fundamental consiga compreender, após a simplificação, forneça passo a passo os detalhes das mudanças e motivos para fazer as adaptações no texto:', seguido do texto a ser simplificado. Esse prompt foi projetado para avaliar a capacidade dos modelos de simplificar textos complexos e fornecer explicações claras das operações realizadas."
        },
        {
            "pergunta": "Quais são as principais diferenças entre as simplificações realizadas pelos modelos de linguagem e as simplificações feitas por humanos?",
            "resposta": "As principais diferenças entre as simplificações realizadas pelos modelos de linguagem e as simplificações feitas por humanos são a capacidade de manter entidades nomeadas e a profundidade das explicações fornecidas. Os modelos de linguagem tendem a eliminar informações desnecessárias e simplificar o texto de forma mais superficial, enquanto as simplificações feitas por humanos tendem a manter mais informações e fornecer explicações mais detalhadas."
        },
        {
            "pergunta": "Quais são as limitações dos modelos de linguagem na tarefa de simplificação de textos?",
            "resposta": "As limitações dos modelos de linguagem na tarefa de simplificação de textos incluem a capacidade de eliminar informações importantes, a falta de contexto e a possibilidade de 'alucinações' ou erros na explicação das operações realizadas. Além disso, os modelos de linguagem podem não ser capazes de capturar a nuances e a complexidade do texto original, o que pode resultar em simplificações que não são totalmente precisas ou claras."
        }
    ],
    "cap-aes": [
        {
            "pergunta": "O que é a Correção Automática de Redação (CAR) e como ela é definida?",
            "resposta": "A Correção Automática de Redação (CAR) é uma aplicação do Processamento de Linguagem Natural (PLN) que envolve o processo de avaliação e atribuição de nota em textos escritos em prosa, via programas computacionais. Ela é definida como uma solução completa que abarca as áreas de Automated Essay Scoring (AES) e Automated Essay Evaluation (AEE), que são distintas, mas complementares e, às vezes, com alguma intersecção."
        },
        {
            "pergunta": "Quando surgiu a correção automática de redações e como ela se desenvolveu ao longo do tempo?",
            "resposta": "A correção manual de redações é uma prática bastante antiga, mas a correção automática de redações data da década de 60, em inglês. Para o português, essa tecnologia é ainda mais recente. A área de Automated Essay Scoring (AES) surgiu como uma aplicação independente, enquanto a área de Automated Essay Evaluation (AEE) se desenvolveu posteriormente, com o objetivo de automatizar o retorno ou feedback para o aluno, colaborando para o processo de aprendizagem da escrita."
        },
        {
            "pergunta": "Quais são as principais diferenças entre a Avaliação Automática de Redação (AAR) e a Correção Automática de Redação (CAR)?",
            "resposta": "A Avaliação Automática de Redação (AAR) é traduzida do inglês como Automated Essay Scoring (AES) e se refere à atribuição de nota para redações, enquanto a Correção Automática de Redação (CAR) é associada ao termo Automated Essay Evaluation (AEE) e abarca não apenas a atribuição de nota, mas também o retorno ou feedback para o aluno, colaborando para o processo de aprendizagem da escrita."
        },
        {
            "pergunta": "Quais são as três etapas básicas que uma aplicação de Correção Automática de Redação (CAR) deve contemplar para ser considerada uma solução completa?",
            "resposta": "Uma aplicação de Correção Automática de Redação (CAR) deve contemplar as seguintes três etapas básicas: 1) identificação de desvios no texto, 2) atribuição de nota e 3) retorno ou feedback para o aluno. Cada uma dessas etapas pode ser vista como uma aplicação independente no Processamento de Linguagem Natural (PLN), mas, para ser considerada uma solução completa, a aplicação deve abarcar todas as três etapas."
        },
        {
            "pergunta": "O que é a redação escolar e quais são os principais gêneros e tipos textuais avaliados na Correção Automática de Redação (CAR)?",
            "resposta": "A redação escolar é o objeto de estudo da Correção Automática de Redação (CAR) e envolve a avaliação de textos escritos em prosa. Os principais gêneros e tipos textuais avaliados incluem textos narrativos, descritivos, expositivos e argumentativos, entre outros. Além disso, os critérios avaliados incluem a estrutura, a coesão, a coerência e a linguagem utilizada no texto."
        },
        {
            "pergunta": "O que é a Correção Automática de Redação (CAR) e quais são as etapas envolvidas nesse processo?",
            "resposta": "A Correção Automática de Redação (CAR) é um processo que envolve a detecção e identificação de desvios em textos. As etapas da CAR incluem a detecção de desvios, que pode ser feita por meio de duas abordagens distintas: baseada em regras (abordagem simbólica) e baseada em modelos estatísticos (abordagem estatística). A detecção de desvios é uma etapa importante da CAR, pois permite identificar erros gramaticais e de uso em textos."
        },
        {
            "pergunta": "Quais são as principais diferenças entre as abordagens simbólica e estatística para a detecção de desvios em textos?",
            "resposta": "As abordagens simbólica e estatística para a detecção de desvios em textos têm diferenças significativas. A abordagem simbólica é baseada em regras e é mais adequada para identificar desvios gramaticais, que são mais comuns em falantes nativos da língua. Já a abordagem estatística é baseada em modelos estatísticos e é mais eficaz para capturar desvios de uso, que são mais comuns em não-nativos da língua."
        },
        {
            "pergunta": "Por que a abordagem simbólica ainda é amplamente utilizada para a detecção de desvios em textos, apesar de ser considerada obsoleta para tarefas mais complexas?",
            "resposta": "A abordagem simbólica ainda é amplamente utilizada para a detecção de desvios em textos porque ela permite mostrar o erro ao aluno, explicar por que está errado e fazer sugestões de correção. Além disso, a abordagem simbólica é mais fácil de implementar e entender do que a abordagem estatística, o que a torna mais acessível para muitos usuários."
        },
        {
            "pergunta": "Quais são os recursos disponíveis para a língua portuguesa para a detecção de desvios em textos?",
            "resposta": "Existem vários recursos disponíveis para a língua portuguesa para a detecção de desvios em textos, incluindo o CoGroo e o LanguageTool. Esses recursos são repositórios de regras gramaticais para a língua portuguesa e têm versões livres, gratuitas e de código-aberto. Além disso, existem plataformas de correção de redação que desenvolveram seu próprio conjunto de recursos linguísticos e regras gramaticais."
        },
        {
            "pergunta": "Quais são os tipos de desvios mais comuns em redações e como podem ser caracterizados?",
            "resposta": "Os tipos de desvios mais comuns em redações incluem erros gramaticais e de uso. Esses desvios podem ser caracterizados por meio de regras simbólicas ou expressões regulares, que são usadas para definir padrões de linguagem. A caracterização de desvios em redações é importante para desenvolver sistemas de correção automática de redação que sejam eficazes e precisos."
        },
        {
            "pergunta": "Qual é o objetivo principal da atribuição de nota a uma redação?",
            "resposta": "O objetivo principal da atribuição de nota a uma redação é avaliar a qualidade e o conteúdo da redação, seja de forma global ou por meio de notas individuais para cada critério de avaliação. Isso ajuda a identificar as áreas fortes e fracas da redação e fornecer feedback construtivo para o autor."
        },
        {
            "pergunta": "Quais são as abordagens mais comuns para atribuir notas a redações?",
            "resposta": "As abordagens mais comuns para atribuir notas a redações incluem o uso de corpus rotulado, que são conjuntos de redações que já foram avaliadas manualmente e possuem indicação de nota e/ou adequação da redação em relação ao critério avaliado. Além disso, as técnicas de aprendizado supervisionado por classificação ou regressão também são frequentemente utilizadas."
        },
        {
            "pergunta": "Qual foi uma das primeiras ferramentas estáveis para a atribuição de notas em redações?",
            "resposta": "O Project Essay Grade (PEG) foi uma das primeiras ferramentas estáveis para a atribuição de notas em redações, desenvolvido em 1973 por Ajay et al. Essa ferramenta foi projetada para avaliar redações universitárias curtas em inglês e apresentou boa performance no contexto aplicado."
        },
        {
            "pergunta": "Quais fatores contribuíram para o reaquecimento da área de AES na década de 90?",
            "resposta": "Os avanços tecnológicos de hardware e software na década de 90 contribuíram para o reaquecimento da área de AES. Isso permitiu o desenvolvimento de novas soluções e abordagens para a atribuição de notas em redações, incluindo o uso de deep learning e Transformers."
        },
        {
            "pergunta": "Por que é importante conhecer o contexto e modelo de correção para realizar a atribuição de nota de forma efetiva?",
            "resposta": "Conhecer o contexto e modelo de correção é fundamental para realizar a atribuição de nota de forma efetiva, pois isso permite que as estratégias de avaliação sejam adaptadas às necessidades específicas da redação. Além disso, diferentes estratégias podem ser reaproveitadas e combinadas para a avaliação de redações de modelos de correção distintos."
        },
        {
            "pergunta": "Qual é a última etapa da Correção Automática de Redação (CAR) e como ela evoluiu ao longo do tempo?",
            "resposta": "A última etapa da Correção Automática de Redação (CAR) é o fornecimento de um feedback para o aluno. Até pouco tempo atrás, a correção automática produzia basicamente uma nota como resultado da avaliação da redação. No entanto, com o tempo, surgiu a necessidade de explicar ou justificar essa nota, e as pesquisas mais recentes vêm focando em aspectos mais complexos e profundos da língua, que vão além da superficialidade do texto."
        },
        {
            "pergunta": "Como os primeiros trabalhos de Correção Automática de Redação (CAR) forneciam feedbacks sobre as características e propriedades linguísticas do texto?",
            "resposta": "De acordo com Shermis; Burstein (2013), os primeiros trabalhos de Correção Automática de Redação (CAR) se limitavam a dar feedbacks sobre as características e propriedades linguísticas do texto, como a quantidade de conectivos, variação lexical, quantidade de palavras de conteúdo, tamanho médio das palavras, frases e parágrafos, dentre outros."
        },
        {
            "pergunta": "Qual é a principal diferença entre o feedback fornecido por um corretor manual e um corretor automático?",
            "resposta": "A principal diferença entre o feedback fornecido por um corretor manual e um corretor automático é que o corretor manual fornece um feedback mais personalizado e detalhado, enquanto o corretor automático fornece um feedback mais sistematizado e limitado a estatísticas básicas do texto."
        },
        {
            "pergunta": "Quais são as lacunas identificadas nos trabalhos de Correção Automática de Redação (CAR) para o português, de acordo com Lima et al. (2023)?",
            "resposta": "De acordo com Lima et al. (2023), as lacunas identificadas nos trabalhos de Correção Automática de Redação (CAR) para o português incluem o baixo detalhamento nos feedbacks retornados pelos modelos de avaliação, o que geralmente não tem utilidade pedagógica para o aluno."
        },
        {
            "pergunta": "Como as plataformas de Correção Automática de Redação (CAR) estão utilizando os modelos gerativos, como o ChatGPT, para fornecer feedbacks aos alunos?",
            "resposta": "Algumas plataformas de Correção Automática de Redação (CAR) estão utilizando os modelos gerativos, como o ChatGPT, para fornecer feedbacks aos alunos de forma mais personalizada e detalhada. Esses modelos podem gerar automaticamente as devolutivas a partir de elementos encontrados ou não encontrados no texto, instanciando palavras ou trechos do texto da redação."
        },
        {
            "pergunta": "Quais são as opiniões divididas sobre a correção automática de redações?",
            "resposta": "As opiniões sobre a correção automática de redações são divididas entre estudantes, escritores, professores de redação, bancas de avaliação em série, especialistas em Linguística Computacional, cientistas de dados e desenvolvedores de sistemas. Existem aqueles que defendem a correção automática como uma ferramenta útil e eficiente, enquanto outros a veem como uma ameaça à correção humana e aos valores pedagógicos e educacionais."
        },
        {
            "pergunta": "Quais são as vantagens dos corretores ortográficos e gramaticais?",
            "resposta": "Os corretores ortográficos e gramaticais são ferramentas úteis que podem ajudar a melhorar a qualidade da escrita. Eles podem detectar erros de ortografia e gramática, sugerindo correções e melhorando a clareza e a precisão do texto. Além disso, essas ferramentas podem ser embutidas em outras soluções, como o pacote Office, o Gdrive, redes sociais e teclados de smartphones, tornando-as acessíveis e fáceis de usar."
        },
        {
            "pergunta": "Quais são os principais prós e contras da correção automática de redações?",
            "resposta": "Os principais prós da correção automática de redações incluem a eficiência e a rapidez na detecção de erros, a capacidade de processar grandes volumes de texto e a objetividade na avaliação. No entanto, os principais contras incluem a possibilidade de erros de interpretação, a falta de compreensão do contexto e a subversão dos valores pedagógicos e educacionais da avaliação manual. Além disso, a correção automática pode não ser capaz de capturar a complexidade e a nuances do texto, o que pode levar a avaliações injustas ou incompletas."
        },
        {
            "pergunta": "Quais são as questões éticas relacionadas à correção automática de redações?",
            "resposta": "As questões éticas relacionadas à correção automática de redações incluem a possibilidade de discriminação e injustiça na avaliação, a falta de transparência e responsabilidade na tomada de decisões e a subversão dos valores pedagógicos e educacionais da avaliação manual. Além disso, a correção automática pode ser usada para fins de vigilância e controle, o que pode levantar preocupações sobre a privacidade e a liberdade de expressão."
        },
        {
            "pergunta": "O que é a correção híbrida e como ela pode ser usada na avaliação de redações?",
            "resposta": "A correção híbrida é uma abordagem que combina a correção manual e a correção automática de redações. Essa abordagem pode ser usada para aproveitar as principais potencialidades de cada tipo de correção, reconhecendo-se também suas limitações. A correção híbrida pode ser usada para detectar erros de ortografia e gramática, sugerir correções e melhorar a clareza e a precisão do texto, enquanto a correção manual pode ser usada para avaliar a complexidade e a nuances do texto, garantindo que a avaliação seja justa e completa."
        }
    ],
    "cap-fake-news": [
        {
            "pergunta": "Quais são as consequências das notícias falsas para a sociedade?",
            "resposta": "As notícias falsas podem ter consequências graves para a sociedade, incluindo a perda de vidas, a destruição de reputações e a interferência nos rumos de povos e nações. Além disso, elas podem afetar decisões políticas, a saúde física e mental da população e causar danos econômicos e sociais. A propagação de notícias falsas pode levar a uma perda de confiança nas instituições e nos meios de comunicação, o que pode ter consequências graves para a democracia e a sociedade em geral."
        },
        {
            "pergunta": "Como a tecnologia contribui para a propagação de notícias falsas?",
            "resposta": "A tecnologia, incluindo dispositivos e serviços como smartphones, redes sociais e mensageiros instantâneos, tem permitido que notícias falsas se espalhem rapidamente, alcançando milhares de pessoas em pouco tempo. Isso ocorre porque a tecnologia facilita a disseminação de informações, independentemente de sua veracidade, e pode criar um efeito de rede que ajuda a propagar notícias falsas. Além disso, a Inteligência Artificial pode produzir sistemas que criam desinformação como efeito colateral."
        },
        {
            "pergunta": "Qual é o papel da detecção automática de notícias falsas na sociedade?",
            "resposta": "A detecção automática de notícias falsas tem um papel importante na sociedade, pois pode ajudar a prevenir a propagação de informações enganosas e proteger a população de seus efeitos negativos. A detecção automática pode ser usada para identificar notícias falsas em tempo real, permitindo que as autoridades e os meios de comunicação tomem medidas para corrigir a informação e evitar danos. Além disso, a detecção automática pode ajudar a melhorar a qualidade da informação disponível na internet e a promover a confiança nas instituições e nos meios de comunicação."
        },
        {
            "pergunta": "Quais são os desafios da detecção automática de notícias falsas?",
            "resposta": "Os desafios da detecção automática de notícias falsas incluem caracterizar o estilo de escrita de notícias falsas e verdadeiras, explicar e mapear a distribuição dessas notícias e como elas afetam as várias camadas da sociedade. Além disso, é necessário desenvolver sistemas computacionais que possam identificar automaticamente notícias falsas com precisão e eficiência. Isso requer a criação de algoritmos e modelos que possam aprender a partir de dados e adaptar-se a novas situações."
        },
        {
            "pergunta": "Como o Processamento de Linguagem Natural (PLN) pode ajudar na detecção automática de notícias falsas?",
            "resposta": "O PLN pode ajudar na detecção automática de notícias falsas ao fornecer técnicas e ferramentas para analisar e processar textos. O PLN pode ser usado para caracterizar o estilo de escrita de notícias falsas e verdadeiras, identificar padrões e anomalias nos textos e desenvolver sistemas que possam identificar automaticamente notícias falsas. Além disso, o PLN pode ser usado para criar corpora de treinamento e testar sistemas de detecção automática de notícias falsas."
        },
        {
            "pergunta": "Quais são os principais corpora de notícias falsas para o português do Brasil?",
            "resposta": "Os principais corpora de notícias falsas para o português do Brasil incluem Fake.br, FakeRecogna 2.0, FakeTweet.Br, Bracis2019FakeNews, FakeWhastApp.BR e FACTCK.BR. Esses corpora variam em termos de tamanho, período de coleta e características, como a presença de informações temporais e classes de veracidade avaliadas."
        },
        {
            "pergunta": "Qual é o maior corpus conhecido de notícias falsas para o português do Brasil?",
            "resposta": "O maior corpus conhecido de notícias falsas para o português do Brasil é o FakeRecogna 2.0, com mais de 50.000 notícias. No entanto, é importante notar que a análise dos metadados pode revelar informações diferentes da descrição do artigo principal do corpus, como no caso do FakeRecogna 2.0, que inclui notícias publicadas em 2002, apesar de ter sido coletado entre 2019 e 2023."
        },
        {
            "pergunta": "Quais são as características variadas dos corpora de notícias falsas para o português do Brasil?",
            "resposta": "Os corpora de notícias falsas para o português do Brasil variam em termos de características, como a presença de informações temporais, classes de veracidade avaliadas e tipos de dados incluídos. Alguns corpora incluem notícias em seu sentido mais estrito, enquanto outros incluem postagens em redes sociais ou dados de aplicativos de mensagens instantâneas. Além disso, alguns corpora fornecem metadados e atributos linguísticos adicionais, o que enriquece as possibilidades de análise."
        },
        {
            "pergunta": "Qual é a importância das informações temporais nos corpora de notícias falsas?",
            "resposta": "As informações temporais são importantes nos corpora de notícias falsas porque os sistemas computacionais de detecção de notícias falsas podem ser fortemente ancorados nos tópicos de determinado período. Isso significa que a presença de informações temporais pode ajudar a melhorar a eficácia dos modelos de detecção de notícias falsas."
        },
        {
            "pergunta": "Quais são os benefícios da inclusão de metadados e atributos linguísticos nos corpora de notícias falsas?",
            "resposta": "A inclusão de metadados e atributos linguísticos nos corpora de notícias falsas pode enriquecer as possibilidades de análise e fornecer aos pesquisadores informações estruturadas que podem ser diretamente utilizadas nos modelos de classificação. Além disso, a inclusão prévia desses recursos pode facilitar o processo de pesquisa e gerar maior consistência nos resultados."
        },
        {
            "pergunta": "Qual é o objetivo principal dos estudos que exploram a identificação de notícias falsas de forma automática?",
            "resposta": "O objetivo principal é desenvolver métodos computacionais capazes de identificar notícias falsas de forma automática, utilizando abordagens baseadas em linguagem verbal e não verbal. Isso pode ser alcançado por meio da análise de sinais e atributos que podem indicar se uma informação é falsa, e da criação de métodos computacionais que possam detectar esses sinais."
        },
        {
            "pergunta": "Qual é a taxonomia do comportamento enganoso proposta por Zhou (2005) e como ela ajuda a organizar o comportamento enganoso?",
            "resposta": "A taxonomia do comportamento enganoso proposta por Zhou (2005) se divide em Linguagem Verbal e Não Verbal. Essa taxonomia ajuda a organizar o comportamento enganoso, auxiliando na sistematização de sinais e atributos que podem indicar se uma informação é falsa e na criação de métodos computacionais. A Linguagem Verbal se refere ao conteúdo escrito e falado da língua, enquanto a Linguagem Não Verbal se refere às características exibidas enquanto uma pessoa produz conteúdo enganoso, como paralinguística e comportamento proxêmico-cinésico."
        },
        {
            "pergunta": "O que é paralinguística e como ela se relaciona com a detecção de notícias falsas?",
            "resposta": "A paralinguística é um subgrupo da linguagem não verbal que se refere a propriedades do discurso que não estão diretamente relacionadas ao conteúdo falado. Isso inclui propriedades relacionadas à voz, como o tom de voz e pausas na fala, relacionados ao teclado (velocidade de digitação, quantidade de vezes que apagou a mensagem, erros de digitação por proximidade de teclas), participatório (demora entre as respostas, mudança de assunto) e sequencial (de quem veio a iniciação de conversas). A paralinguística pode ser usada como um indicador para detectar notícias falsas, pois pode revelar características do comportamento do autor que não estão presentes no conteúdo escrito."
        },
        {
            "pergunta": "Qual é a diferença entre as abordagens baseadas em linguística e as abordagens baseadas em conteúdo para a detecção de notícias falsas?",
            "resposta": "As abordagens baseadas em linguística utilizam pistas provenientes do texto para fazer a diferenciação entre conteúdo falso ou verdadeiro. Assume-se que os produtores de conteúdo enganoso podem inconscientemente deixar rastros linguísticos que sinalizam a falsidade. Já as abordagens baseadas em conteúdo buscam simular a metodologia de checagem de fatos, avaliando cada informação apresentada, muitas vezes remetendo a abordagens baseadas em grafos de conhecimento."
        },
        {
            "pergunta": "Por que as pesquisas de PLN costumam atuar mais diretamente no grupo de Linguagem Verbal?",
            "resposta": "As pesquisas de PLN costumam atuar mais diretamente no grupo de Linguagem Verbal porque é mais fácil analisar e processar o conteúdo escrito e falado da língua. Além disso, a Linguagem Verbal é mais facilmente acessível e pode ser analisada por meio de técnicas de processamento de linguagem natural. A Linguagem Não Verbal, por outro lado, é mais difícil de analisar e requer técnicas mais avançadas de processamento de sinais e imagem."
        },
        {
            "pergunta": "Quais são os impactos da incidência de conteúdo enganoso nas sociedades?",
            "resposta": "A incidência de conteúdo enganoso pode ter impactos significativos nas sociedades, incluindo a disseminação de informações falsas sobre a saúde, a economia e a política, o que pode levar a prejuízos econômicos, violência e polarização social. Além disso, a desinformação pode afetar a capacidade de diálogo e a coesão social, contribuindo para o adoecimento psicológico e o afastamento entre grupos."
        },
        {
            "pergunta": "Qual é o exemplo de estudo empírico sobre a pandemia de COVID-19 mencionado no texto?",
            "resposta": "O estudo empírico conduzido por Galhardi et al. (2020) sobre a pandemia de COVID-19, que analisou os relatos de usuários enviados ao aplicativo brasileiro 'Eu Fiscalizo' durante o período de março a abril de 2020. Foi observado que 65% dos relatos propagavam orientações caseiras e incorretas para evitar a disseminação do vírus, e 20% continham métodos para curar a doença."
        },
        {
            "pergunta": "O que é a 'infodemia' e como ela se relaciona com a pandemia de COVID-19?",
            "resposta": "A 'infodemia' é um termo usado para descrever a disseminação de informações falsas ou enganosas sobre uma doença ou pandemia. No contexto da pandemia de COVID-19, a 'infodemia' se refere à disseminação de informações falsas sobre a doença, o que pode levar a prejuízos econômicos, violência e polarização social. A Organização Mundial de Saúde (OMS) usou o termo 'infodemia' para descrever a situação em que o mundo estava enfrentando, além de uma pandemia do coronavírus, uma pandemia de desinformação."
        },
        {
            "pergunta": "Como as notícias falsas podem afetar a economia?",
            "resposta": "As notícias falsas podem afetar a economia de várias maneiras, incluindo a disseminação de informações falsas sobre empresas ou investimentos, o que pode levar a prejuízos econômicos para os investidores. Além disso, as notícias falsas podem afetar a confiança dos consumidores e a estabilidade do mercado, o que pode levar a uma recessão econômica."
        },
        {
            "pergunta": "Quais são os estudos que simularam a disseminação de desinformação em redes sociais?",
            "resposta": "Os estudos de Azzimonti e Fernandes (2023) e Oliveira (2023) simularam a disseminação de desinformação em redes sociais, utilizando uma rede social sintética baseada no Twitter. Os estudos mostraram que robôs programados para promover visões tendenciosas podem influenciar uma pequena parte dos usuários, o que pode levar à disseminação de notícias falsas e à polarização social."
        },
        {
            "pergunta": "Qual é o papel da Inteligência Artificial na detecção automática de notícias falsas?",
            "resposta": "A Inteligência Artificial (IA) pode desempenhar um papel importante na detecção automática de notícias falsas, pois pode ser treinada para identificar padrões e características de notícias falsas. A IA pode ser utilizada para analisar grandes quantidades de dados e identificar notícias falsas em tempo real, o que pode ajudar a prevenir a disseminação de desinformação."
        },
        {
            "pergunta": "Quais são os exemplos de softwares disponibilizados para o público em geral para detectar notícias falsas?",
            "resposta": "O sistema FakeCheck, desenvolvido por Monteiro et al. (2018), é um exemplo de software disponibilizado para o público em geral para detectar notícias falsas. O sistema é treinado com o corpus Fake.br e pode ser utilizado em qualquer navegador da web."
        }
    ],
    "cap-saude": [
        {
            "pergunta": "Qual é o papel da tecnologia na área da saúde e como o Processamento de Linguagem Natural (PLN) contribui para melhorar o diagnóstico, o tratamento e a gestão de pacientes?",
            "resposta": "A tecnologia tem sido fundamental para melhorar a área da saúde, e o PLN é uma ferramenta importante para analisar grandes volumes de dados não estruturados gerados em ambientes clínicos. Isso permite a extração de informações valiosas que podem ser usadas para melhorar o diagnóstico, o tratamento e a gestão de pacientes. Além disso, o PLN pode ser usado para identificar padrões e relacionamentos entre os dados, permitindo uma melhor compreensão da condição do paciente e a construção de modelos preditivos para prever possíveis complicações ou doenças."
        },
        {
            "pergunta": "Quais são os oito tipos de atividades socio-semióticas no domínio da medicina e como elas são representadas por meio de textos?",
            "resposta": "Os oito tipos de atividades socio-semióticas no domínio da medicina são: 1) instruir e regular o comportamento, 2) documentar fatos e experiências, 3) compartilhar experiências, 4) organizar a produção de conhecimento, 5) construir realidade ficcional, 6) executar procedimentos cirúrgicos, 7) interagir com pacientes e familiares, e 8) compartilhar informações em fóruns online. Essas atividades são representadas por meio de textos como bulas de medicamentos, cartilhas, normativas, manuais de instrução de equipamentos, questionários aplicados ao paciente, registros de exames clínicos, relatos de profissionais da saúde, notas de evolução de enfermagem, sumários de alta, boletins médicos, e notas em texto livre em campos próprios do prontuário eletrônico do paciente."
        },
        {
            "pergunta": "Como o PLN pode ser usado para extrair informações valiosas de artigos acadêmicos e narrativas clínicas?",
            "resposta": "O PLN pode ser usado para extrair informações valiosas de artigos acadêmicos e narrativas clínicas por meio de técnicas como extração de ontologias, identificação de padrões e relacionamentos entre os dados, e construção de modelos preditivos. Isso pode ser feito por meio de algoritmos de processamento de linguagem natural que podem analisar grandes volumes de texto e identificar informações relevantes. Além disso, o PLN pode ser usado para identificar termos técnicos e complexos em diferentes áreas da saúde e permitir que as informações sejam compartilhadas de forma mais clara e precisa."
        },
        {
            "pergunta": "Qual é o papel da linguagem verbal nas atividades socio-semióticas no domínio da medicina?",
            "resposta": "A linguagem verbal tem um papel fundamental nas atividades socio-semióticas no domínio da medicina. Ela é usada para instruir e regular o comportamento, documentar fatos e experiências, compartilhar experiências, organizar a produção de conhecimento, construir realidade ficcional, executar procedimentos cirúrgicos, interagir com pacientes e familiares, e compartilhar informações em fóruns online. A linguagem verbal é usada para transmitir informações, expressar sentimentos e emoções, e estabelecer relações entre os profissionais da saúde e os pacientes."
        },
        {
            "pergunta": "Como o PLN pode ser usado para melhorar a compreensão da condição do paciente e prever possíveis complicações ou doenças?",
            "resposta": "O PLN pode ser usado para melhorar a compreensão da condição do paciente e prever possíveis complicações ou doenças por meio de técnicas como identificação de padrões e relacionamentos entre os dados, construção de modelos preditivos, e extração de informações valiosas de narrativas clínicas. Isso pode ser feito por meio de algoritmos de processamento de linguagem natural que podem analisar grandes volumes de texto e identificar informações relevantes. Além disso, o PLN pode ser usado para identificar termos técnicos e complexos em diferentes áreas da saúde e permitir que as informações sejam compartilhadas de forma mais clara e precisa."
        },
        {
            "pergunta": "O que é o Registro Eletrônico de Saúde (RES) e como ele afeta a quantidade de dados gerados relativos à atenção aos pacientes?",
            "resposta": "O Registro Eletrônico de Saúde (RES) é um sistema que armazena informações sobre a saúde dos pacientes de forma eletrônica. Com o advento do RES, a quantidade de dados gerados relativos à atenção aos pacientes aumentou significativamente, pois os prontuários eletrônicos podem conter dados estruturados, semiestruturados ou não estruturados, todos eles oferecendo uma grande quantidade de informações sobre o paciente."
        },
        {
            "pergunta": "Quais são as características únicas dos dados clínicos presentes nas narrativas clínicas em texto livre que dificultam sua análise e interpretação?",
            "resposta": "Os dados clínicos presentes nas narrativas clínicas em texto livre apresentam características únicas que dificultam sua análise e interpretação, pois são frequentemente apresentados em linguagem médica especializada, repleta de termos técnicos, jargões e abreviaturas que podem variar entre os distintos profissionais de saúde. Além disso, esses textos também podem conter erros de digitação, ortografia ou gramática, tornando a interpretação ainda mais complexa."
        },
        {
            "pergunta": "Quais são os diferentes tipos de texto presentes nas narrativas clínicas e quais são os desafios específicos de cada um?",
            "resposta": "Existem diferentes tipos de texto presentes nas narrativas clínicas, como notas de evolução de enfermagem, sumários de alta e notas de ambulatório. Cada um desses tipos de texto apresenta desafios específicos em termos do tipo de linguagem e também da relevância das informações registradas. Por exemplo, as notas de evolução de enfermagem podem ser mais descritivas e detalhadas do que outros tipos de texto, enquanto os sumários de alta podem fornecer informações importantes sobre a condição atual do paciente e seu histórico de tratamento."
        },
        {
            "pergunta": "Por que a anotação manual de narrativas clínicas é necessária e quais são as limitações dessa abordagem?",
            "resposta": "A anotação manual de narrativas clínicas é necessária porque os modelos de Processamento de Linguagem Natural (PLN) precisam de dados anotados para treinar e melhorar sua precisão. No entanto, a anotação manual de narrativas clínicas requer tempo e recursos, o que dificulta a construção de grandes datasets para treinamento de modelos de PLN. Além disso, a anotação manual pode ser sujeita a erros e inconsistências, o que pode afetar a precisão dos modelos de PLN."
        },
        {
            "pergunta": "Quais são as soluções para contornar as limitações da anotação manual de narrativas clínicas e melhorar a precisão dos modelos de PLN?",
            "resposta": "Uma solução para contornar as limitações da anotação manual de narrativas clínicas é utilizar modelos genéricos para pré-processamento, sendo a saída avaliada manualmente. Outra solução é utilizar corpora de narrativas clínicas anotadas, como o corpus DepClin-Br, que foi desenvolvido por uma equipe de cientistas da computação e linguistas. Além disso, a construção de modelos de PLN específicos para o domínio clínico pode ajudar a melhorar a precisão e relevância dos modelos de PLN."
        }
    ],
    "cap-saude-mental": [
        {
            "pergunta": "Quais são os principais transtornos de saúde mental que afetam a sociedade moderna e qual é o impacto deles na expectativa de vida?",
            "resposta": "Os principais transtornos de saúde mental que afetam a sociedade moderna são a depressão e a ansiedade. Segundo dados do Global Burden of Disease Study de 2019, a depressão é o segundo fator que mais impacta na queda da expectativa de vida da população global. Isso significa que a depressão tem um impacto significativo na saúde e bem-estar das pessoas, afetando não apenas a qualidade de vida, mas também a expectativa de vida."
        },
        {
            "pergunta": "Qual é a prevalência da depressão no Brasil e como ela mudou ao longo dos anos?",
            "resposta": "De acordo com Lopes et al. (2022), a prevalência da depressão no Brasil cresceu 36,7% de 2013 a 2019, indo de 7,9% em 2013 para 10,8% em 2019. Isso indica que a depressão é um problema crescente no país e que é necessário tomar medidas para prevenir e tratar essa condição."
        },
        {
            "pergunta": "Como a pandemia de Covid-19 afetou a saúde mental das pessoas?",
            "resposta": "Durante a pandemia de Covid-19, uma pesquisa realizada em maio de 2020 com 483 adultos constatou que 70,3% tinham sintomas de depressão e 67,2% tinham sintomas de ansiedade. Isso sugere que a pandemia teve um impacto significativo na saúde mental das pessoas, aumentando a prevalência de transtornos de saúde mental."
        },
        {
            "pergunta": "Como as redes sociais são utilizadas por pessoas com transtornos de saúde mental?",
            "resposta": "Diversos estudos demonstram que indivíduos com transtornos de saúde mental são usuários regulares de redes sociais em proporção similar à população em geral. Além disso, esses indivíduos frequentemente recorrem às redes sociais em busca do suporte de outros usuários com problemas semelhantes. Isso sugere que as redes sociais podem ser uma ferramenta útil para as pessoas com transtornos de saúde mental se conectarem com outros e buscar apoio."
        },
        {
            "pergunta": "Qual é o objetivo dos estudos que buscam detectar transtornos de saúde mental em redes sociais?",
            "resposta": "O objetivo desses estudos é identificar casos de maior gravidade e eventualmente sinalizar a necessidade de um indivíduo buscar ajuda por meio da detecção computacional de transtornos de depressão e ansiedade a partir de textos em português, em geral provenientes de redes sociais. Isso pode ajudar a prevenir e tratar essas condições, melhorando a saúde mental das pessoas."
        },
        {
            "pergunta": "Qual é o objetivo principal da detecção de depressão e ansiedade a partir de textos?",
            "resposta": "O objetivo principal é desenvolver recursos linguístico-computacionais capazes de identificar sinais de depressão e ansiedade em textos, especialmente em plataformas de redes sociais, para ajudar no diagnóstico e tratamento desses transtornos de saúde mental."
        },
        {
            "pergunta": "Quais são os principais domínios e idiomas utilizados nos estudos de detecção de depressão e ansiedade?",
            "resposta": "Os principais domínios são Reddit e Twitter/X, e o idioma mais comum é o inglês. No entanto, há estudos que se concentram em outros idiomas, como o português, como o estudo de Mann et al. (2020) e o corpus SetembroBR (Santos et al., 2024)."
        },
        {
            "pergunta": "Qual é a importância do pareamento entre indivíduos diagnosticados e suas contrapartidas no grupo de controle?",
            "resposta": "O pareamento é fundamental para garantir que os dois grupos sejam minimamente comparáveis e evitar diferenças indesejáveis que possam levar ao aprendizado de padrões espúrios. Isso pode ser feito por meio de características como gênero, faixa etária, comportamento na rede social, etc."
        },
        {
            "pergunta": "Por que a detecção precoce de transtornos de saúde mental é importante?",
            "resposta": "A detecção precoce é importante porque pode ajudar a prevenir ou reduzir a gravidade dos sintomas, melhorando a qualidade de vida dos indivíduos afetados. Além disso, a detecção precoce pode reduzir o impacto dos transtornos de saúde mental na sociedade como um todo."
        },
        {
            "pergunta": "Quais são as limitações dos corpora textuais rotulados com informações sobre saúde mental?",
            "resposta": "As limitações incluem a natureza sensível do problema, considerações éticas associadas, e a proteção da privacidade e do direito de arrependimento dos usuários. Além disso, alguns corpora podem não ser totalmente acessíveis devido a restrições de uso e compartilhamento de dados."
        },
        {
            "pergunta": "Como os corpora textuais rotulados com informações sobre saúde mental são geralmente disponibilizados?",
            "resposta": "Os corpora são geralmente disponibilizados como coleções de identificadores numéricos a partir dos quais as postagens integrais podem ser recuperadas diretamente a partir da API da plataforma, caso ainda sejam disponibilizadas pelos seus autores. Isso pode acarretar algum esforço de programação e, em alguns casos, pode não ser gratuito."
        }
    ],
    "cap-direito": [
        {
            "pergunta": "Qual é o objetivo do capítulo em relação ao trabalho computacional com textos produzidos na esfera do Direito?",
            "resposta": "O objetivo do capítulo é apresentar algumas perspectivas e desafios no âmbito de trabalhos que exploram materiais produzidos em português, considerando somente o cenário do Direito Brasileiro."
        },
        {
            "pergunta": "Por que o Direito de cada país tem especificidades linguísticas e culturais que repercutem sobre seus textos, discursos e tipo de vocabulário?",
            "resposta": "O Direito de cada país tem especificidades linguísticas e culturais que repercutem sobre seus textos, discursos e tipo de vocabulário porque as leis e regulamentações são criadas em um contexto histórico e cultural específico, o que influencia a linguagem e o vocabulário utilizado."
        },
        {
            "pergunta": "Quais são os dois cenários textuais apresentados no capítulo para ilustrar a exploração do vocabulário jurídico?",
            "resposta": "Os dois cenários textuais apresentados no capítulo são as leis e sentenças judiciais."
        },
        {
            "pergunta": "O que é o reconhecimento terminológico (RT) e como é utilizado no contexto do Direito?",
            "resposta": "O reconhecimento terminológico (RT) é um trabalho que envolve a identificação e análise de termos e conceitos jurídicos em textos, utilizando técnicas da Linguística de Corpus e do PLN. É utilizado no contexto do Direito para melhorar a compreensão e a interpretação de textos jurídicos."
        },
        {
            "pergunta": "Quais são os desafios de trabalhar com textos antigos em português no contexto do Direito?",
            "resposta": "Os desafios de trabalhar com textos antigos em português no contexto do Direito incluem a normalização e padronização da apresentação escrita das palavras antigas, a variabilidade da escrita e a necessidade de transcrição e decifração de manuscritos."
        },
        {
            "pergunta": "Qual é o objetivo da análise de conteúdos em sentenças judiciais via Análise de Sentimentos?",
            "resposta": "O objetivo da análise de conteúdos em sentenças judiciais via Análise de Sentimentos é identificar padrões de sentenças judiciais favoráveis ou desfavoráveis a um determinado assunto, o que pode ser útil para profissionais do Direito que buscam entender como um determinado tribunal já vem decidindo sobre um assunto específico."
        },
        {
            "pergunta": "Quais são as três fontes de textos jurídicos brasileiros apresentadas no capítulo?",
            "resposta": "As três fontes de textos jurídicos brasileiros apresentadas no capítulo são o Estatuto da Criança e do Adolescente (ECA), a Constituição do Brasil de 1988 (CF88) e um conjunto de Sentenças Judiciais dos Juizados Especiais Cíveis."
        },
        {
            "pergunta": "Qual é a utilidade da técnica de Análise de Sentimentos para os profissionais do Direito?",
            "resposta": "A técnica de Análise de Sentimentos pode ser muito útil para os profissionais do Direito, pois permite identificar padrões de sentenças judiciais favoráveis ou desfavoráveis a um determinado assunto, o que pode ajudar a reduzir o trabalho de leitura individual de cada sentença e a encontrar a informação necessária de forma mais eficiente."
        },
        {
            "pergunta": "Qual é a relação entre o Direito e a língua?",
            "resposta": "A relação entre o Direito e a língua é intensa, pois o Direito se manifesta através da língua e as palavras que emprega conferem e confirmam a sua existência peculiar como uma prática social e área de conhecimento. A linguagem jurídica é uma área de estudos específica conhecida como jurilinguística, que explica que a linguagem jurídica compreende diversas 'espécies' de práticas que se subdividem conforme uma dada finalidade e foco."
        },
        {
            "pergunta": "Quais são as diferentes espécies de linguagem jurídica?",
            "resposta": "As diferentes espécies de linguagem jurídica incluem a linguagem judiciária, forense ou processual, que se refere às sentenças produzidas em processos judiciais; a linguagem legislativa, que se refere aos textos de leis, decretos e portarias; e outras formas de linguagem jurídica que adotam usos diferenciados e vocabulário diferenciado. Cada tipo de suporte e/ou instrumento jurídico tende a adotar usos diferenciados e um vocabulário diferenciado."
        },
        {
            "pergunta": "Qual é a importância da consideração dos elementos linguísticos e modos de dizer próprios da linguagem jurídica?",
            "resposta": "A consideração dos elementos linguísticos e modos de dizer próprios da linguagem jurídica é importante porque pode nos ajudar a desempenhar tarefas de um modo mais produtivo. Além disso, a linguagem jurídica é uma área de estudos específica que pode nos ajudar a entender melhor a relação entre o Direito e a língua."
        },
        {
            "pergunta": "Como a linguagem jurídica varia de um país para outro?",
            "resposta": "A linguagem jurídica varia de um país para outro devido à especificidade de discursos envolvida e à cultura jurídica de cada país. No Brasil, por exemplo, a linguagem jurídica é regida pelo sistema da civil law, o que significa que uma lei escrita tem preponderância sobre a jurisprudência. Além disso, a linguagem jurídica brasileira é influenciada pela jurisprudência nas decisões judiciais, principalmente quando agrupadas pelos tribunais e transformadas em súmulas."
        },
        {
            "pergunta": "Qual é o papel do poder legislativo e do poder executivo na produção e execução das leis no Brasil?",
            "resposta": "No Brasil, o poder legislativo é responsável por produzir as leis, que são discutidas e votadas, e então aprovadas para entrarem em vigor. O poder executivo, por sua vez, é responsável por executar as leis aprovadas. Além disso, o poder judiciário é responsável por verificar e direcionar a aplicação das leis."
        },
        {
            "pergunta": "O que é uma súmula e qual é seu papel no sistema jurídico brasileiro?",
            "resposta": "Uma súmula é um tipo de documento que consiste em um verbete que registra a interpretação pacífica ou majoritária adotada por um Tribunal a respeito de um tema específico. No sistema jurídico brasileiro, as súmulas têm um papel importante, pois são utilizadas para orientar as decisões judiciais e garantir a uniformidade da jurisprudência."
        }
    ]
}