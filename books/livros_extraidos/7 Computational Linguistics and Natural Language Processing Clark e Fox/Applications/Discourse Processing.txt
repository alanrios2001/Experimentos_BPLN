“9781405155816_4_021” — 2010/5/8 — 12:16 — page 599 — #1
21 Discourse Processing
RUSLAN MITKOV
This chapter discusses concepts related to the computational processing of dis-course, and theories and methods related to or employed in this task. Section 1introduces basic notions of discourse and Section 2 illustrates discourse structurethrough examples from different genres. Section 2 also covers topic segmen-tation and outlines major works such as Hearst’s TextTiling approach to text
segmentation.
Section 3 examines theories and formalisms which deal with the computational
treatment of coherence relations. Hobbs’s coherence theory and rhetorical struc-tural theory (RST) as popular theories of organizing text in terms of coherencerelations are introduced. Another popular theory which models local coherence,namely centering theory, is discussed in greater detail.
Section 4 covers anaphora, which make a vital contribution to the cohesion
of discourse and whose interpretation is crucial for discourse understanding. Aconsiderable part of this section is dedicated to the computational processing ofanaphora. The process of anaphora resolution with its distinct stages is coveredand major algorithms for anaphora resolution are outlined.
Section 5 discusses the role of discourse processing in NLP applications and
ﬁnally Section 6 points to references for further reading.
1 Discourse: Basic Notions and Terminology
Natural language texts do not normally consist of isolated pieces of text or sen-tences but of sentences which form a uniﬁed whole and which make up whatwe call discourse. According to the Longman dictionary,
1discourse is (1) a serious
speech or piece of writing on a particular subject, (2) serious conversation or dis-cussion between people, or (3) the language used in particular types of speech orwriting. What Longman presumably mean by ‘serious’ (but do not explicitly say)is that the text produced is not a random collection of symbols or words, but (a)related and (b) meaningful sentences which have a particular communicative goal.

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 600 — #2
600 Ruslan Mitkov
The reference to ‘related’ and ‘meaningful’ sentences has to do with the fact thatdiscourse is expected to be both cohesive and coherent.
Discourse typically manifests cohesion, which is about the way textual units are
linked together. Cohesion occurs where the interpretation of some element in thediscourse is dependent on that of another and involves the use of abbreviatedor alternative linguistic forms which can be recognized and understood by thehearer or the reader, and which refer to or replace previously mentioned items inthe spoken or written text.
Consider the following extract from Jane Austen’s Pride and Prejudice (Austen
1995: 23):
(1) Elizabeth looked archly, and turned away. Herresistance had not injured her
with the gentleman.
Although it is not stated explicitly, it is normal to assume that the second sen-
tence is related to the ﬁrst one and that herrefers to Elizabeth . It is this reference
which ensures the cohesion between the two sentences. If the text is changedby replacing herwith hisin the second sentence or the whole second sentence is
replaced with This chapter is about discourse processing, cohesion does not occur any
more: the interpretation of the second sentence in both cases no longer dependson the ﬁrst sentence. In the above example it is the use of the pronoun her(see also
the section on anaphora below) that secures cohesion, but lexical cohesion is also
possible through simple repetition of words, synonyms, or hypernyms.
2
Whereas cohesion in texts is more about linking sentences or, more generally,
textual units through cohesive devices such as anaphors and lexical repetitions,discourse is also expected to manifest coherence, which is about it making sense.
More speciﬁcally coherence has to do with the meaning relations between twounits and how two or more units combine to produce the overall meaning of aspeciﬁc discourse.
(2) George passed his exam. He scored the highest possible mark.(3) George passed the exam. He enjoyed red wine.Whereas example (2) makes perfect sense in that the second sentence elaborates
on the fact the George excelled in his exam, example (3) sounds a bit odd andsomehow lacks overall meaning: most readers may even ﬁnd the two sentences in(3) unrelated. Leaving aside the hypothetical possibility/explanation that, becauseGeorge has passed his exam and he likes red wine, he is likely to treat himself toa nice bottle of Rioja red wine, readers would ﬁnd (3) hardly coherent as opposedto (2) where the second sentence is in a meaningful relation to the ﬁrst one in thatit elaborates on the fact presented in the ﬁrst sentence.
Discourse could take the form of a monologue where a writer or speaker is the
author of the text.
3A particular form of discourse is the dialogue (see Chapter 16,
COMPUTATIONAL MODELS OF DIALOGUE , for more on dialogue), where there is
an interaction or conversation,4usually between two participants. Another form
of discourse is the multiparty discourse which usually takes place at meetings.

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 601 — #3
Discourse Processing 601
2 Discourse Structure
Discourse structure is a type of structure used to “refer to the structure of somepostulated unit higher than the sentence, for example the paragraph or somelarger entity such as episode or topic unit” (Halliday & Hasan 1976: 10). It is safeto say that every genre has its own discourse structure (Halliday & Hasan 1976).
5
In fact, under normal circumstances texts are organized logically and follow a spe-ciﬁc structure of discourse topics. The stereotypical sequence of discourse topics ischaracteristic of certain documents and genres and accounts for the way texts areorganized and segmented by topics.
2.1 Text organization
As an illustration of how a speciﬁc discourse is organized, consider the follow-ing entry
6from The Hamlyn Pocket Dictionary of Wines which was written for the
explicit discourse goal of deﬁning Flagey-Echezeaux (Paterson 1980: 2).
(4) Flagey-Echezeaux (France). Important red wine township in the Cote
de Nuits with two front-ranking vineyards, Echezeaux and GrandsEchezeaux. The ﬁrst produces a ﬁne rich, round wine and the second,which is not a single vineyard but a group, is also capable of producingﬁne wine but, like other divided properties, the quality of its wine is vari-able. The lesser wines of Flagey-Echezeaux are entitled to the appellationVosne-Romanee.
The author does not randomly order the sentences in the text, but rather plans
an overall organizational framework within which the individual sentences areproduced. In the case of the above example, the framework chosen is typical ofdeﬁnitions. Here, the author ﬁrst identiﬁes Flagey-Echezeaux by describing itssuperordinate (‘important red wine township in the Cote de Nuits’), and thenintroduces two of its constituents (Echezeaux and Grands Echezeaux). Next, adescription about each of the vineyards is provided in turn and ﬁnally the authorpresents additional information about Flagey-Echezeaux in the last sentence. Theway the above discourse is organized can be explained in terms of rhetorical pred-
icates, where these are the means by which a writer or speaker can describeinformation and organize the text. In this particular example the ﬁrst sentence cor-responds to the rhetorical predicate identiﬁcation (‘important red wine township
in the Cote de Nuits’), followed by constituency (‘with two front-ranking vine-
yards’). Building on Grimes’s (1975) and Williams’s (1893) predicates, McKeownobserves typical patterns of rhetorical predicates referred to as schemata which
she uses to analyze text and then to apply these as text organization strategiesin natural language generation (for more on natural language generation, seeChapter 20,
NATURAL LANGUAGE GENERATION ). McKeown’s schemata include
attributive schema, identiﬁcation schema, and constituency schema. Each schema

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 602 — #4
602 Ruslan Mitkov
consists of a stereotypical sequence of rhetorical predicates with the identiﬁcation
schema featuring a possible sequence of the following predicates: identiﬁcation,
attributive, ampliﬁcation,a n d particular illustration. For details, see McKeown (1985).
It is widely accepted that scientiﬁc papers and their abstracts have a predeﬁned
structure which can be exploited in NLP applications. In its simplest form, a scien-tiﬁc paper can be considered to have a problem–solution structure (Hutchins 1977),
but, in general, a more detailed organization can be identiﬁed in scientiﬁc papers:background information about the domain tackled in the paper, the problem to be
addressed in the paper, the solution to the problem, evaluation of the solution and
conclusion (Swales 1990). These sections are normally referred to as moves orrhetor-
ical predicates . Research in the structure of abstracts conﬁrms that, by and large,
these moves hold, but it is not unusual for abstracts to include only a subset of themoves (Salanger-Meyer 1990; Or˘ asan 2001).
Another example of how text is organized are online medical patient informa-
tion leaﬂets (Connor 2006). Such leaﬂets exhibit typical discourse structure in thatdifferent topics are not presented randomly but follow an established order. Inline with Clerehan and Buchbinder (2006), Connor (2006) identiﬁes the structurein terms of rhetorical moves or predicates. The leaﬂets ﬁrst provide backgroundinformation on the medicine (rhetorical predicate inform ), then give a summary on
the use of the medicine (describe), provide dosage instructions (instruct), followedby an outline/explanation of the beneﬁt of the medicine ( explain). These leaﬂets
next feature information on the side effects (realized by the rhetorical predicateinform ), and then proceed with information about monitoring ( suggest ), constraints
on patient behavior (inform, warn ), storage instructions ( instruct), and the descrip-
tion of circumstances when medical advice is necessary. Finally disclaimers foronline contents are displayed.
2.2 Text segmentation algorithm
The examples above illustrate how discourse can be organized and segmented onthe basis of different topics. For many practical NLP implementations the auto-matic segmentation of text according to the topics covered is crucial. Among thedifferent algorithms which have been developed to address this task, the TextTil-
ing(Hearst 1994; 1997b) approach is perhaps the best known. Hearst’s algorithm
does not attempt to capture any hierarchical relations and dependencies that holdbetween topic segments (discourse units) as rhetorical structure theory does.
7
Instead, it splits documents into a linear sequence of multi-paragraph segments,each of which focuses upon a distinct subtopic within the main topic of the dis-course. Its name derives from the fact that the goal of TextTiling is to partition
texts into contiguous, non-overlapping subtopic segments, broadly resemblingtiles, that are assumed to occur within the scope of one or more overarching maintopics, which span the length of the text.
Under this approach, the structure of a document is treated as a sequence
of subtopical discussions that occur in the context of one or more main topic

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 603 — #5
Discourse Processing 603
discussions. This conception is illustrated in Hearst (1997) by the example of a21-paragraph science news article called ‘Stargazers’, whose main topic is theexistence of life on earth and other planets. Its contents can be described asconsisting of nine subtopic discussions, presented below. In the example, thenumbers indicate paragraphs within the document so that paragraphs 1–3 present‘Intro – the search for life in space,’ paragraphs 4–5 present ‘The Moon’s chemicalcomposition,’ etc.
1–3 Intro – the search for life in space4–5 The Moon’s chemical composition6–8 How early Earth–Moon proximity shaped the Moon
9–12 How the Moon helped life evolve on Earth
13 Improbability of the Earth–Moon system
14–16 Binary/trinary star systems make life unlikely17–18 The low probability of non-binary/trinary systems19–20 Properties of Earth’s Sun that facilitate life
21 Summary
The approach assumes that a particular set of lexical items is in use during the
course of a given subtopic discussion and, when the subtopic changes, a signiﬁ-cant proportion of the vocabulary changes too. The method assumes three broadcategories of lexical items to be found within a text:(1) words that occur frequently throughout the text, which are often indicative
of its main topic(s);
(2) words that are less frequent but more uniform in distribution, which do not
provide much information about the divisions between discussions;
(3) groups of words that are ‘clumped’ together with high density in some
parts of the text and low density in other parts. These groups of words areindicative of subtopic structure.
The problem of subtopic segmentation is thus the problem of determining
where these clusters of words in the third category begin and end. Hearst presentsablock comparison algorithm that addresses this task. Here, a block is deﬁned as a
sequence of ncontiguous sentences. Pairs of blocks are compared for their overall
lexical similarity and the ‘gap’ between each pair of blocks is assigned the result-inglexical score. The more words that the blocks have in common, the higher the
score assigned to the gap between them. If a gap with a low lexical score is fol-lowed and preceded by gaps with high lexical scores, then the low-scoring gapis likely to indicate a shift in vocabulary that corresponds to a subtopic change.In its initial implementation, the blocks are represented by vectors and the lex-ical score is computed as the normalized inner product of the two vectors. TheTextTiling system identiﬁes subtopic shifts at gaps whose lexical score falls below
some threshold. Evaluation of the TextTiling approach to subtopic segmentation
revealed that performance levels were encouraging. When assessed on its ability

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 604 — #6
604 Ruslan Mitkov
to classify gaps between blocks as subtopic shifts, different conﬁgurations of thesystem obtained F-scores ranging from 0.61 to 0.705, compared to human judges’reliability scores on the same data (F-score of 0.77). When computing F-score, truepositives are gaps between blocks that both the TextTiling system and a human
annotator agree to be either points of subtopic shift or gaps that simply happen tolie within a subtopic segment.
Since its original statement, several authors have experimented with differ-
ent parameters of the original TextTiling algorithm described in Hearst (1994).
Alternate methods for scoring the gaps between text blocks, including vocabulary
introductions and lexical chains , were applied in Hearst (1997). Choi et al. (2001)
successfully applied latent semantic analysis (LSA) in their variant of the TextTiling
algorithm, achieving signiﬁcantly improved performance levels from the system.Under this approach, the improvement is brought about by using LSA to replaceeach word used in the text blocks with appropriate groups of words that co-occur with them. The initial vectors are thus converted into matrices of vectorswhose similarity is computed by taking the sum of the cosine scores betweeneach of them. In this way, the approach can more accurately reduce the computeddepth of gaps between blocks that contain non-matching but semantically relatedwords.
Kozima (1993) describes a similar approach to discourse segmentation in which
each block of text in a document is assigned a lexical cohesion proﬁle , deﬁned as a
record of the lexical cohesiveness (semantic similarity) of the words in the block.Low levels of lexical cohesiveness within a block imply that it spans discoursesegments and contains a subtopic shift.
Numerous alternative approaches to discourse segmentation, based on differ-
ent assumptions and methodologies, have been introduced since the inception ofTextTiling. Crowe (1996) describes an approach in which references to events areidentiﬁed in texts and each paragraph that mentions an event is then concate-nated to form a representative discourse segment. Kan et al. (1998) implementsan approach to discourse segmentation based on lexical chains combined witha weighting scheme established in a supervised training step. Paragraphs in thetext are then assigned a weight with reference to the type of chain link occur-ring within them and their position with respect to other links. Paragraphs witha high aggregate weight are considered likely to represent the beginning of a newsubtopic segment in the text. The authors report that this segmentation methodoutperforms the TextTiling algorithm. Utiyama and Isahara (2001) present a prob-
abilistic method exploiting a graph search algorithm to ﬁnd the most likely topicsegmentation of an input text.
Supervised approaches to discourse segmentation have been scarce due to a
lack of available training data. Litman and Passoneau (1995) present one super-vised approach exploiting a decision tree classiﬁer in the context of transcribeddialogues. The method uses information about prosodic cues, cue phrases, andanaphora within blocks of text. System performance compared favorably with thatof human annotators.

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 605 — #7
Discourse Processing 605
3 Discourse Coherence
In the previous section we discussed the issue of discourse structure. In factthe way that units are connected into meaningful relationships is another fac-tor which contributes to the acceptability of the structure of a speciﬁc discourse.In the next section we shall discuss theories for text coherence which haveplayed an inﬂuential part in different computational models of discourse and NLPapplications.
3.1 Hobbs’s theory of coherence relations
In Hobbs’s model (1979), discourse structure comprises text units and coherence
relations between them. Consider the following example, where the second sen-
tence is an elaboration of the ﬁrst one as it expands on the ﬁrst without giving
additional instructions:
(5) Go down Washington Street. Just follow Washington Street three blocks to
Adams Street.
Building on the work by Grimes (1975) and Halliday and Hasan (1976), Hobbs
proposed a set of 12 coherence relations, including cause, evaluation, background,
parallel,a n d elaboration. Hobbs’s work goes beyond previous similar or related
work in that he proposes an inference mechanism for reasoning about thecoherence relations.
As an illustration, Hobbs (1979) considers the text below, where the two sen-
tences are claimed to be in an elaboration relation by virtue of the second sentenceelaborating on the ﬁrst one.
(6) John can open Bill’s safe. He knows the combination.Following the outlines in Hobbs (1979) and Lochbaum et al. (2000), the proposi-
tions expressed by the two sentences can be represented as follows:
can(John, open(Safe))know(he, combination(Comb,y))
Here John, Safe and, Comb are literals and heand yare variables. Then, by
employing domain axioms which express general facts about generating statesof affairs and more speciﬁc facts about the relation between safes and combina-tions, it is possible to reason from the ﬁrst of these propositions to the followingproposition:
know(John, cause(do(John, a), open(Safe)))‘John knows that his doing some action a opens the safe.’

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 606 — #8
606 Ruslan Mitkov
Similarly, the following proposition can be pragmatically implied from the
second proposition:
know(he, cause(dial(z, Comb, y), open(y)))‘He knows that z’s dialing the combination of y causes it to open.’
The requirements of the elaboration relation will be satisﬁed by these two propo-
sitions if dialing the combination of the safe is recognized as a specialization ofdoing some action which will open the safe. The second statement thus elaborateson the ﬁrst by providing this additional property of how John can open the safe.
Hobbs’s work differs from previous approaches not only because of the central
role it assigns to reasoning, but also because it incorporates reference resolutionas a by-product of reasoning about coherence. For example, to recognize that theelaboration relation held in the foregoing example, the variables heand zhave to be
identiﬁed with John and the variable ywith the safe. Therefore the anaphor hein
the second sentence was resolved to the antecedent John as part of the process of
reasoning about coherence.
3.2 Rhetorical structure theory
Rhetorical structure theory (RST) is a model of text organization initially pro-
posed by Mann and Thompson (1987) and later used by a number of NLPresearchers (e.g., Marcu 1997; 2000; see section 5). In RST relations are deﬁnedto hold between two non-overlapping spans or text units called the nucleus and
thesatellite. The nucleus is expected to be more central or topical to the author and
can be interpreted independently. The satellite is less central and usually its inter-pretation is connected with the nucleus. Mann and Thompson deﬁne a set of 25rhetorical relations. These relations include the evidence relation which applies to
two text spans where the satellite provides evidence for the claim contained in thenucleus. Following the authors’ convention, the arrow in Figure 21.1 representstheevidence relation by means of an asymmetric arrow.
In this example, the unit representing the second sentence is in an evidence rela-
tion with the unit representing the ﬁrst sentence and increases the reader’s beliefin the claim expressed in this unit.
In the evidence relation the satellite supports the nucleus, but does not con-
tribute to it. This is different to the circumstance relation where the satellite sets a
framework (e.g., temporal or spatial) within which the nucleus can be interpreted.Following Mann and Thompson (1987), consider the example:
(7) Probably the most extreme case of Visitors Fever I have ever witnessed was
a few summers ago when I visited the relatives in the Midwest.
In this example the satellite (‘when I visited the relatives in the Midwest’) pro-
vides a temporal framework for interpreting the nucleus (the preceding part of thesentence).

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 607 — #9
Discourse Processing 607
Geor ge is ver y tired. He has not slept all ni ght lon g.
Figure 21.1 Example of the RST relation evidence.
As another example of a relation, consider the following text:(8) The next LREC conference will take place in Marakesh, Morroco. I shall
give you more details in due course, but now is a good time to reserve ahotel.
In this example, the units represented by the two clauses contained in the
second sentence are in a justify relation with the unit represented by the ﬁrst sen-
tence. They tell readers why the writer believes he has the right to say unit one(‘I shall give you more details in due course’) without giving more details, and inparticular without providing the dates for the conference.
In the original version of RST (Mann & Thompson 1987), relations are formally
deﬁned by a set of constraints on the nucleus (N) and satellite (S) which explain the
goals and beliefs of the writer (W) and reader (R), and by the effect on the reader.
By way of example, the justify relation is deﬁned as follows:
Relation name: JustifyConstraints on N: R might not believe N to a degree satisfactory to WConstraints on S: R believes S or will ﬁnd it credibleConstraints on theN+S combination: R’s comprehending S increases R’s belief of NEffect: R’s belief in N is increased
There are different sets of rhetorical relations in RST and variations of the theory.
By way of example, the RST TreeBank (Carlson et al., 2001) features 78 distinctrelations, grouped into 16 classes.
3.3 Centering
Centering is a theory about local discourse coherence and is based on the idea that
each utterance features a topically most prominent entity called the center. Centering
theory regards utterances8which continue the topic of preceding utterances as more
coherent than utterances which feature a topic shift (or ﬂag up an impending shift).
The main idea of centering theory (Grosz et al., 1983; 1995) is that certain enti-
ties mentioned in an utterance are more central than others and this imposes

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 608 — #10
608 Ruslan Mitkov
constraints on the use of referring expressions and in particular on the use of pro-nouns. It is argued that the coherence of a discourse depends on the extent towhich the choice of the referring expressions conforms to the centering properties.
As an illustration, consider the following examples:(9) Discourse A(a) John works at Barclays Bank.
(b) He works with Lisa.
(c) John is going to marry Lisa.
(d) He is looking forward to the wedding.
Discourse B(a) John works at Barclays Bank.
(b) He works with Lisa.
(c) John is going to marry Lisa.
(e) She is looking forward to the wedding.
Centering predicts that discourse B is less coherent than discourse A. In both
examples the discourse entity realized by John is the center in utterances (9b) and(9c),
9but whilst in (9d) the center remains the same, utterance (9e) shifts the cen-
ter to the discourse entity realized by Lisa. The shift in center and the use of apronominal form to realize the new center contribute to making B less coherentthan A. In utterance (9d), unlike (9e), it is the center of utterances (9b) and (9c)which has been pronominalized.
Discourses consist of continuous discourse segments. A discourse segment D con-
sists of a sequence of utterances U
1,U2,...,UN. Each utterance Uin D is assigned
a set of potential next centers known as forward looking centers Cf (U, D)10which
correspond to the discourse entities evoked by the utterance. Each utterance (otherthan the ﬁrst) in a segment is assigned a single center deﬁned in centering theoryas the backward looking center
11Cb (U ). The backward looking center Cb (U )i sa
member of the set Cf (U ) and is the discourse entity the utterance U is about. The
Cb entity connects the current utterance to the previous discourse: it focuses on anentity that has already been introduced. A central claim of centering is that eachutterance has exactly one backward looking center.
12
The set of forward looking centers Cf ( U) is partially ordered according to their
discourse salience. The highest-ranked element in Cf (U ) is called the preferred cen-
terCp (U ) (Brennan et al., 1987). The preferred center in a current utterance UN
( d e n o t e da sC p( UN)) is the most likely backward looking center of the following
utterance (denoted as Cb (U N+1)). Discourse entities in subject position are pre-
ferred over those in object position, which are preferred over discourse entities insubordinate clauses or those performing other grammatical functions.
13
Grosz et al. (1995) deﬁne three types of transition relations across pairs of
utterances.(1) Center continuation:C b (U
N+1)=Cb (U N), i.e., the backward looking cen-
ter of the utterance UN+1is the same as the backward looking center in the

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 609 — #11
Discourse Processing 609
utterance UNand this entity is the preferred center of Cf (U N+1). In this case
Cb (U N+1) is the most likely candidate for Cb (U N+2).
(2) Center retaining:C b (UN+1)=Cb (U N), but this entity is not the most highly
ranked element in Cf (U N+1). In this case, therefore, Cb (U N+1)i sn o tt h e
preferred candidate for Cb( UN+2) and, although it is retained as Cb in UN+1,
it is not likely to ﬁll that role in UN+2.
(3) Center shifting :C b(U N+1)̸=Cb (U N)
Brennan et al. (1987) distinguish between smooth-shift orshifting -1(if Cb(U N+1)=
Cp(U N+1)) and rough-shift or simply shifting (if Cb (U N+1)̸=Cp(U N+1)).
To exemplify the theory, here are two very simple discourses differing in their
last sentences from Discourses A and B:
(9) Discourse C(a) John works at Barclays Bank.
(b) He works with Lisa.
(c) John is going to marry Lisa.
(f) Lisa has known him for two years.
Discourse D(a) John works at Barclays Bank.
(b) He works with Lisa.
(c) John is going to marry Lisa.
(g) She has known John for two years.
Sentence (9c) exhibits center continuation; the backward looking centers of (9b)
and (9c) and the forward looking centers of sentences (9a), (9b), and (9c) are listedas follows:
(9) (a) John works at Barclays Bank.
Cb unspeciﬁed
14
Cf ={John, Barclays Bank}
(b) He works with Lisa.
Cb = JohnCf ={John, Lisa}
(c) John is going to marry Lisa.
Cb = JohnCf ={John, Lisa}
In sentence (9f), we have center retaining:
15
(9) (f) Lisa has known him for two years.
Cb = JohnCf ={Lisa, John}

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 610 — #12
610 Ruslan Mitkov
Whereas in (g) we have a center shift.
(g) She has known John for two years.
Cb = LisaCf ={Lisa, John}
In addition to the transitions outlined above, centering theory also includes two
rules which state:
Rule 1: If some element of Cf (U
N) is realized as a pronoun in UN+1, then Cb
(UN+1) must also be realized as a pronoun.
Rule 2: Transition states are ordered in terms of preference. The continue
transition is preferred to the retain transition, which is preferred to the
shift transition.16
Rule 1 stipulates that if there is only one pronoun in an utterance, then this pro-noun should be the (backward looking) center. It is reasonable to assume that ifthe next sentence also contains a single pronoun, then the two pronouns core-fer. The center is the most preferred discourse entity in the local context which isto be referred to by a pronoun.
17The use of a pronoun to realize the backward
looking center indicates that the speaker/writer is talking/writing about the samething. Psycholinguistic research (Hudson-D’Zmura 1988; Gordon et al., 1993) andcross-linguistic research (Kameyama 1985; 1986; 1998; Di Eugenio 1990; Walkeret al., 1994) have validated that Cb is preferentially realized by a pronoun (e.g., inEnglish) or by equivalent forms such as zero pronouns in other languages (e.g.,Japanese).
Rule 2 provides an underlying principle for coherence of discourse. Frequent
shifts detract from local coherence, whereas continuation contributes to coherence.Maximally coherent segments are those which do not feature changes of center,concentrate on one main discourse entity (topic) only, and therefore require lessprocessing effort.
Rule 2 is used as a preference in anaphora resolution (Brennan et al., 1987;
Walker 1989). As an illustration, consider the following discourse:
(9) Discourse E(h) Although Jenny was in a hurry, she was glad to bump into Kate.
(i) She told her some exciting news.
This discourse segment consists of the following utterances:
U1=Jenny was in a hurry
U2=she was glad to bump into Kate
U3=She told her some exciting news
The discourse entity ‘Jenny’ is both the backward looking center of the second
utterance Cb (U
2) and the preferred center Cp (U 2) on the list of forward looking

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 611 — #13
Discourse Processing 611
centers. Since continuation is preferred over retaining (Rule 2, see above), center-ing favors ‘Jenny’ as both Cb (U
3)a n dC p( U3), therefore predicting sheas ‘Jenny’
and heras ‘Kate’ (the instantiations she=‘Kate’ and her=‘Jenny’ would have sig-
naled retaining since in this case we would have had Cp ( U3)=‘Kate,’ Cb (U 3)=
‘Jenny’).18
Centering has proved to be a powerful tool in accounting for local coherence and
has been used successfully in anaphora resolution. However, as with every theoryin linguistics, it has its limitations (see also Kehler 1997). For instance, the originalcentering model only accounts for local coherence of discourse. In an anaphoraresolution context, when the candidates for the antecedent of an anaphor in thecurrent utterance U
Khave to be identiﬁed, centering proposes that the discourse
entities in the immediately preceding utterance UK−1be considered. Centering,
however, does not offer a solution for resolving anaphors in UKwhose antecedents
can be found only in UK−2(or even further back in the discourse). To overcome this
restriction, Hahn and Strube (1997) put forward an alternative centering modelthat extends the search space for antecedents.
Walker (1998) goes even further and argues that the restriction of centering to
operate within a discourse segment should be abandoned in favor of a new modelintegrating centering into the global discourse structure. To this end it is proposedthat a model of attentional state, the so-called cache model , be combined with the
centering algorithm.
For more work on centering see the references in Section 6.
4 Anaphora Resolution
(1) Elizabeth looked archly, and turned away. Herresistance had not injured her
with the gentleman.
We saw in this example how the pronoun herserves as a link and ensures cohe-
sion between the two sentences. Such words which point to previous items ofdiscourse and contribute to its cohesion are referred to as anaphors.T h eu n d e r -
standing of a discourse usually involves the understanding of anaphors whoseinterpretation depends on either previous sentences or preceding words of thecurrent sentence. The interpretation of anaphors (made possible by anaphora reso-
lution, see Section 4.2) is of vital importance and has attracted considerable interestin the area of computational discourse.
4.1 Anaphora: linguistic fundamentals
We deﬁne anaphora as the linguistic phenomenon of pointing back to a previously
mentioned item in the text. The word or phrase ‘pointing back’19is called an
anaphor and the entity to which it refers or for which it stands is its antecedent.
When the anaphor refers to an antecedent and when both have the same referent

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 612 — #14
612 Ruslan Mitkov
in the real world, they are termed coreferential . Therefore coreference is the act of
referring to the same referent in the real world.
Consider the following example from Huddleston (1984):(10) The Queen is not here yet but sheis expected to arrive in the next half an
hour.
In this example, the pronoun sheis an anaphor, the Queen is its antecedent and she
and the Queen are coreferential. Note that the antecedent is not the noun Queen
but the noun phrase (NP) the Queen . The relation between the anaphor and the
antecedent is not to be confused with that between the anaphor and its referent;in the example above the referent is the Queen as a person in the real world (e.g.,
Queen Elizabeth) whereas the antecedent is the Queen as a linguistic form.
A speciﬁc anaphor and more than one of the preceding (or following) noun
phrases may be coreferential, thus forming a coreferential chain of discourse entities
which have the same referent. For instance in (11), Sophia Loren ,she, the actress,a n d
herare coreferential. Coreference partitions discourse into equivalence classes of
coreferential chains and in (11) the following coreferential chains can be singledout:{Sophia Loren, she, the actress, her}, {Bono, the U2 singer },{a thunderstorm},
and{ap l a n e }.
(11) Sophia Loren says shewill always be grateful to Bono. The actress revealed
that the U2 singer helped hercalm down during a thunderstorm while
traveling on a plane.
Note that not all varieties of anaphora have a referring function. Consider verbanaphora, for example.
(12) When Manchester United swooped to lure Ron Atkinson away from the
Albion, it was inevitable that his midﬁeld prodigy would follow,a n di n
1981 he did.
This sentence features the verb anaphor didwhich is a substitution for the
antecedent followed but does not have a referring function and therefore we can-
not speak of coreference between the two. Also, the anaphor and the antecedentmay refer but may still not be coreferential, as in the case of identity-of-senseanaphora:
20
(13) The man who gave his paycheck to his wife was wiser than the man who
gave itto his mistress (Karttunen 1969).
as opposed to identity-of-reference anaphora:
(14) This man gave his paycheck to his wife in January; in fact, he gave itto her
in person.

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 613 — #15
Discourse Processing 613
In (13) the anaphor itand the antecedent his paycheck are not coreferential whereas
in (14) they are.
Bound anaphora is another example where the anaphor and the antecedent are
not coreferential.
(15) Every speaker had to present hispaper.
Anaphora normally operates within a document (e.g., article, chapter, book),whereas coreference can be taken to work across documents. We have seen thatthere are varieties of anaphora that do not involve coreference. It is also possibleto have coreferential items that are not anaphoric with cross-document coreference
being an obvious example: two mentions of the same person in two differentdocuments will be coreferential, but will not stand in an anaphoric relation.
The most widespread type of anaphora is pronominal anaphora. Pronominal
anaphora can be exhibited by personal, possessive, or reﬂexive pronouns (‘A kneejerked between Ralph’s legs and hefell sideways busying himself with hispain
as the ﬁght rolled over him’) as well as by demonstrative pronouns (‘This was
more than he could cope with’). Relative pronouns are regarded as anaphoric too.First- and second-person singular and plural pronouns are usually used in a deic-tic manner
21(‘Iwould like youto show me the way to San Marino’), although
their anaphoric function is not uncommon in reported speech or dialogues asdemonstrated by the use of Iin the last sentence of (16).
Lexical noun phrase anaphors take the form of deﬁnite noun phrases also called
deﬁnite descriptions ,a n d proper names. Although pronouns, deﬁnite descriptions,
and proper names are all considered to be deﬁnite expressions, proper names anddeﬁnite descriptions, unlike pronouns, can have a meaning independent of theirantecedent. Furthermore, deﬁnite descriptions do more than just refer. They con-vey some additional information as in (16) where the reader can learn more aboutRoy Keane through the deﬁnite description Alex Ferguson’s No.1 player .
(16) Roy Keane has warned Manchester United he may snub their pay deal.
United’s skipper is even hinting that unless the future Old Trafford Pack-
age meets hisdemands, hecould quit the club in June 2000. Irishman
Keane , 27, still has 17 months to run on hiscurrent £23,000-a-week con-
tract and wants to commit himself to United for life. Alex Ferguson’s No.1
player conﬁrmed: “If it’s not the contract I want, I won’t sign.”
In this text, Roy Keane has been referred to by anaphoric pronouns (he ,his,him-
self, I), but also by deﬁnite descriptions ( United’s skipper, Alex Ferguson’s No. 1
player) and a proper name modiﬁed by a common noun (Irishman Keane). On theother hand, Manchester United is referred to by the deﬁnite description the club and
by the proper name United .
Noun phrase anaphors may have the same head as their antecedents ( the
chapter and this chapter ) but the relation between the referring expression and
its antecedent may be that of synonymy ( a shop ...the store ), generalization/
hypernym ( a boutique ...the shop,a l s o Manchester United ...the club as in (16)) or

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 614 — #16
614 Ruslan Mitkov
specialization/hyponym (a shop ...the boutique ,a l s o their pay deal ...his current
£23,000-a-week contract as in (16)).22Proper names usually refer to antecedents
which have the same head ( Manchester United ...United ) with exact repetitions not
being uncommon.
According to the form of the anaphor, anaphora occurs as verb anaphora
(‘Stephanie balked ,a s didMike’) or adverb anaphora (‘We shall go to McDonalds
and meet you there’. Zero anaphora, which is typical of many languages such as
Romance, Slavonic and oriental languages, is also exhibited. Consider the examplein Spanish:
(17) Gloria is very tired. ( She) has been working all day long.
Gloria está muy cansada. ØHa estado trabajando todo el día.
In the last example Østands for the omitted anaphor she.
Nominal anaphora arises when a referring expression – pronoun, deﬁnite noun
phrase, or proper name – has a non-pronominal noun phrase as antecedent. Thismost important and frequently occurring class of anaphora has been researchedand covered extensively, and is well understood in the NLP literature. Broadlyspeaking, there are two types of nominal anaphora: direct and indirect. Direct
anaphora links anaphors and antecedents by such relations as identity, synonymy,
generalization, and specialization (see above). In contrast, indirect anaphora links
anaphors and antecedents by relations such as part-of (‘Although the store had
only just opened, the food hall was busy and there were long queues at the tills’)
or set membership (‘Only a day after heated denials that the Spice Girls were
splitting up, Melanie C declared she had already left the group’). Resolution of
indirect anaphora normally requires the use of domain or world knowledge. Indi-rect anaphora is also known as associative orbridging anaphora.
23For more on the
notions of anaphora and coreference, and on the different varieties of anaphora,see Hirst (1981) and Mitkov (2002).
4.2 Anaphora resolution
The process of determining the antecedent of an anaphor is called anaphora reso-
lution. In anaphora resolution the system has to determine the antecedent of theanaphor. For identity-of-reference nominal anaphora, any preceding NP which iscoreferential with the anaphor is considered as the correct antecedent. On the otherhand, the objective of coreference resolution is to identify all coreferential chains.
However, since the task of anaphora resolution is considered successful if any ele-ment of the anaphoric (coreferential) chain preceding the anaphor is identiﬁed,annotated corpora for automatic evaluation of anaphora systems require markupof anaphoric (coreferential) chains and not only anaphor-closest antecedent pairs.
The process of automatic resolution of anaphors consists of the following main
stages: (1) identiﬁcation of anaphors, (2) location of the candidates for antecedents,and (3) selection of the antecedent from the set of candidates on the basis ofanaphora resolution factors.

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 615 — #17
Discourse Processing 615
4.2.1 Identiﬁcation of anaphors In pronoun resolution, only the anaphoric
pronouns have to be processed further, therefore non-anaphoric occurrences ofthe pronoun itas in (18) have to be recognized by the program.
(18) It must be stated that Oskar behaved impeccably.When a pronoun has no referential role, and it is not interpreted as a bound
variable (‘Every man loves his mother’), then it is termed pleonastic. Therefore,grammatical information as to whether a certain word is a third-person pronounwould not be sufﬁcient: each occurrence of ithas to be checked in order to ﬁnd
out ﬁrst if it is referential or not. Several algorithms for identiﬁcation of pleonasticpronouns have been reported in the literature (Paice & Husk 1987; Lappin & Leass1994; Evans 2000; 2001; Boyd et al., 2005).
The search for anaphoric noun phrases can be even more problematic. Deﬁnite
noun phrases are potentially anaphoric, often referring back to preceding nounphrases, as The Queen does in (19):
(19) Queen Elizabeth attended the ceremony. The Queen delivered a speech.
It is important to bear in mind that not every deﬁnite noun phrase is necessarilyanaphoric. Typical examples are deﬁnite descriptions which describe a speciﬁc,unique entity, or deﬁnite descriptions used in a generic way. In (20) the NP The
Duchess of York is not anaphoric and does not refer to the Queen .
(20) The Queen attended the ceremony. The Duchess of York was there too.
As in the case of the automatic recognition of pleonastic pronouns, it is importantfor an anaphora resolution program to be able to identify those deﬁnite descrip-tions that are not anaphoric. Methods for identiﬁcation of non-anaphoric deﬁnitedescriptions have been developed by Bean and Riloff (1999), Vieira and Poesio(2000), and Muñoz (2001).
Finally, proper names are regarded as potentially anaphoric to preceding proper
names that partially match in terms of ﬁrst or last names (e.g., John White . . .John . . . Mr White).4.2.2 Location of the candidates for antecedents Once the anaphors have been
detected, the program has to identify the possible candidates for their antecedents.The vast majority of systems only handle nominal anaphora, since processinganaphors whose antecedents are verb phrases, clauses, sentences, or sequences ofsentences is a more complicated task. Typically in such systems all noun phrases(NPs) preceding an anaphor within a certain search scope are initially regarded ascandidates for antecedents.
The search scope takes a different form depending on the processing model
adopted and may vary in size depending on the type of anaphor. Since anaphoric
relations often operate within/are limited to a discourse segment,
24the search

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 616 — #18
616 Ruslan Mitkov
scope is often set to the discourse segment which contains the anaphor. Anaphora
resolution systems which have no means of identifying the discourse segmentboundaries usually set the search scope to the current and N preceding sentences,
with N depending on the type of anaphor. For pronominal anaphors, the searchscope is usually limited to the current and two or three preceding sentences. Def-inite noun phrases, however, can refer further back in the text and, for suchanaphors, the search scope is normally larger. Approaches which search thecurrent or the linearly preceding units to locate candidates for antecedents arereferred to by Cristea et al. (2000) as linear models as opposed to the hierarchical
models which consider candidates from the current or the hierarchically preceding
discourse units such as the discourse-VT model based on veins theory (Cristeaet al., 1998; also see Section 6). Cristea et al. (2000) show that, compared withlinear models, the search scope of the discourse-VT model is smaller, whichmakes it computationally less expensive, and potentially more accurate in pick-ing out the potential candidates. However, in fact, the automatic identiﬁcationof veins cannot, at present, be performed with satisfactory accuracy and there-fore this model is not yet sufﬁciently attractive for practical anaphora resolutionsystems.4.2.3 The resolution algorithm: factors in anaphora resolution Once the ana-
phors have been detected, the program will attempt to resolve them by selectingtheir antecedents from the identiﬁed sets of candidates. The resolution rules basedon the different sources of knowledge and used in the resolution process (con-stituting the anaphora resolution algorithm), are usually referred to as anaphora
resolution factors . These factors can be constraints which eliminate certain noun
phrases from the set of possible candidates. The factors can also be preferences
which favor certain candidates over others. Constraints are considered to be oblig-atory conditions that are imposed on the relation between the anaphor and itsantecedent. Therefore, their strength lies in discounting candidates that do sat-isfy these conditions; unlike preferences, they do not propose any candidates.Typical constraints in anaphora resolution are gender and number agreement,
25
c-command constraints,26and selectional restrictions. Typical preferences are
recency (the most recent candidate is more likely to be the antecedent), centerpreference in the sense of centering theory (the center of the previous clause isthe most likely candidate for antecedent), or syntactic parallelism (candidateswith the same syntactic function as the anaphor are the preferred antecedents).However, it should be made clear that it is not difﬁcult to ﬁnd examples whichdemonstrate that such preferences are not absolute factors since very often theyare overriden by semantic or real-world constraints.
27Approaches making use of
syntactic constraints, such as Hobbs (1976, 1978) and Lappin and Leass (1994) orthe knowledge-poor counterpart of the latter (Kennedy & Boguraev 1996), havebeen particularly successful and have received a great deal of attention, one of thereasons for this is that such constraints are good at ﬁltering antecedent candidatesat intra-sentential (within the sentence) level.

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 617 — #19
Discourse Processing 617
4.3 Algorithms for anaphora resolution
Anaphora resolution algorithms can be broadly classed into rule-based andmachine learning (ML) approaches. Initially it was the rule-based approaches suchas Hobbs’s naïve algorithm (Hobbs 1976; 1978) and Lappin and Leass’s (1994) res-olution of anaphora procedure (henceforth RAP) which gained popularity (seebelow). In recent years there has been a considerable amount of work reportedon machine learning (ML) approaches to pronoun (and, in general, to anaphoraand coreference) resolution (Soon et al., 2001; Ng & Cardie 2002; Müller et al.,2002; Strube & Müller 2003). Ge et al.’s statistically enhanced implementation ofHobbs’s algorithm has previously been reported to perform better than Hobbs’soriginal algorithm itself (Ge et al., 1998), even outperforming Lappin and Leass’sRAP (Preiss 2002c) and it is fair to say that ML approaches to anaphora resolu-tion are/have been an important direction of research. However, the results froma number of studies (Barbu 2001; Preiss 2002a; Stuckardt 2002; 2004; 2005) suggestthat ML algorithms for pronoun resolution do not necessarily perform better thantraditional rule-based approaches.
In this section we shall brieﬂy outline ﬁve popular rule-based approaches: two
approaches based on full parsing and three based on partial parsing. Histori-cally, the approaches based on partial parsing (referred to as ‘knowledge-poorapproaches’) were proposed in the 1990s and followed those applying to theoutput of full parsers. Most knowledge-poor algorithms share a similar pre-processing methodology. They do not rely on a parser to process the input andinstead use POS taggers and NP extractors. Nor do any of the methods make use ofsemantic or real-world knowledge. The drive towards knowledge-poor and robustapproaches was further motivated by the emergence of cheaper and more reliablecorpus-based NLP tools such as POS taggers and shallow parsers, alongside theincreasing availability of corpora and other NLP resources (e.g., ontologies). For ahistorical outline of anaphora resolution algorithms, see Mitkov (2002).4.3.1 Approaches based on full parsing4.3.1.1 Hobbs’s naïve algorithm Hobbs’s (1976; 1978) naïve algorithm
28operates
on fully parsed sentences. The original approach assumes that the surface parse-trees represent the correct grammatical structure of the sentence with all adjunctphrases properly attached and that they feature ‘syntactically recoverable omittedelements’ such as elided verb phrases and other types of zero anaphors or zeroantecedents. Hobbs also assumes that an NP node directly dominates an N-barnode, with the N-bar identifying a noun phrase without its determiner. Hobbs’salgorithm traverses the surface parse-tree in a left-to-right and breadth-ﬁrst fash-ion, looking for a noun phrase of the correct gender and number. Parse-trees ofprevious sentences in the text are traversed in order of recency. Hobbs’s algo-rithm was not implemented in its original form, but later implementations reliedeither on manually parsed corpora (Ge et al., 1998; Tetreault 1999) or a full parser(Dagan & Itai 1991; Lappin & Leass 1994; Baldwin 1997).

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 618 — #20
618 Ruslan Mitkov
4.3.1.2 Lappin and Leass’s RAP Lappin and Leass’s (1994) algorithm29termed
resolution of anaphora procedure (RAP), operates on syntactic representationsgenerated by McCord’s slot grammar parser (McCord 1990; 1993). It relies onsalience measures derived from syntactic structure as well as on a simple dynamicmodel of attentional state to select the antecedent of a pronoun from a list of NPcandidates. RAP consists of the following components: an intrasentential syntac-tic ﬁlter, a morphological ﬁlter, a procedure for identifying pleonastic pronouns,an anaphor binding algorithm which handles reﬂexive and reciprocal pronouns,a procedure for assigning values to several salience parameters for an NP , a pro-cedure for identifying anaphorically linked NPs as an equivalence class, and adecision procedure for selecting the preferred candidate for antecedent. The algo-rithm does not employ semantic information or real-world knowledge in selectingfrom the candidates.4.3.2 Approaches based on partial parsing4.3.2.1 Mitkov’s knowledge-poor approach Mitkov’s robust pronoun resolution
approach
30(Mitkov 1996; 1998) works from the output of a text processed by a
part-of-speech tagger and an NP extractor, locates noun phrases which precedethe anaphor within a distance of two sentences and checks for gender and numberagreement. The resolution algorithm is based on a set of boosting and impedingindicators applied to each antecedent candidate. The boosting indicators assigna positive score to an NP , reﬂecting a likelihood that it is the antecedent of thecurrent pronoun. In contrast, the impeding ones apply a negative score to an NP ,reﬂecting a lack of conﬁdence that it is the antecedent of the current pronoun. Ascore is calculated based on these indicators and the discourse referent with thehighest aggregate value is selected as the antecedent.4.3.2.2 Kennedy and Boguraev’s approach Kennedy and Boguraev (1996)
31report
on a modiﬁed version of Lappin and Leass’s (1994) RAP which does not requirefull syntactic parsing but applies to the output of a part-of-speech tagger enrichedwith annotations of grammatical function. They use a phrasal grammar for iden-tifying NP constituents and, similar to Lappin and Leass (1994), employ saliencepreferences to rank candidates for antecedents. The general idea is to constructcoreference equivalence classes that have an associated value based on a set often factors. An attempt is then made to resolve every pronoun to one of the pre-viously introduced discourse referents by taking into account the salience valueof the class to which each possible antecedent belongs. It should be pointed outthat Kennedy and Boguraev’s approach is not a simple knowledge-poor adap-tation of RAP . It is rather an extension, given that some of the factors usedare unique.4.3.2.3 Baldwin’s CogNIAC CogNIAC (Baldwin 1997)
32makes use of limited
knowledge and resources and its pre-processing includes sentence detection,

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 619 — #21
Discourse Processing 619
part-of-speech tagging and recognition of base forms of noun phrases, as wellas basic semantic category information such as gender and number (and, in onevariant, partial parse-trees). The pronoun resolution algorithm employs a set of‘high-conﬁdence’ rules which are successively applied to the pronoun under con-sideration. The processing of a pronoun terminates after the application of theﬁrst relevant rule. The original version of the algorithm is non-robust, a pronounbeing resolved only if a speciﬁc rule can be applied. The author also describes arobust extension of the algorithm, which employs two additional weak rules to beapplied if no others are applicable.4.3.3 Comparing pronoun resolution algorithms Mitkov and Hallett (2007)
compare the above ﬁve algorithms using the evaluation workbench, an envi-ronment for comparative evaluation of rule-based anaphora resolution algo-rithms (Mitkov 2000; Barbu & Mitkov 2001). The evaluation was conductedon 2,597 anaphors from the three corpora, each one of them covering a dif-ferent genre: technical manuals, newswire, and literary texts. The evaluationresults show that, on the whole, Lappin and Leass’s algorithm performedbest (success rate 60.65 percent), followed closely by Hobbs’s naïve algorithm(60.07 percent). Mitkov’s approach was third and emerged as the best per-forming knowledge-poor algorithm (57.03 percent), followed by Kennedy andBoguraev’s method (52.08 percent), both systems surpassing Baldwin’s CogNIAC(37.66 percent).
These results conﬁrm the results from previous studies (Mitkov et al., 2002)
that fully automatic pronoun resolution is more difﬁcult than previous work hadsuggested. The results also depart signiﬁcantly from the results reported in theauthors’ papers describing their algorithms. In fact, they are much lower than theoriginal results reported. Mitkov and Hallett believe that the main reason for thisis the fact that all algorithms implemented in the evaluation workbench operatein a fully automatic mode, whereas in their original form they relied on someform (to a lesser or higher degree) of post-editing of the output of their parsers,which of course favored the performance of the algorithm. As a result, some ofthe implemented algorithms could not beneﬁt from speciﬁc rules which requiredmore accurate pre-processing, such as identiﬁcation of pleonastic pronouns oridentiﬁcation of gender or animacy, identiﬁcation of clauses within sentences, etc.Another important reason for the lower performance could have been the factthat the evaluation corpus of technical manuals used in their study was takenin its original format and as such featured texts that were frequently broken intonon-narrative sections.
Mitkov and Hallett’s results suggest that the best-performing pronoun reso-
lution algorithms score slightly higher than 60 percent if they operate in a fullyautomatic mode. These results are comparable to those reported in a related inde-pendent study carried out by Preiss (2002b) which evaluates Lappin and Leass’salgorithm with different parsers and which reports an average success rate of61 percent when the pre-processing is done with Charniak’s parser. In addition,

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 620 — #22
620 Ruslan Mitkov
different versions of Mitkov’s algorithm were also evaluated on technicalmanuals in Mitkov et al. (2002) where the performance of the original, non-optimized version of this algorithm was comparable to the results reported inthis paper.
5 Applications
5.1 Text organization and discourse segmentation
applications
The way in which discourse is organized has been exploited in numerous NLPprojects. As mentioned earlier, McKeown (1985) applies text organization strate-gies in the form of schemata of rhetorical predicates innatural language generation.
This explicit structure of scientiﬁc papers in terms of moves (background infor-
mation, problem, solution, evaluation, conclusion) was used by Teufel (1999) andTeufel and Moens (2002) to produce summaries automatically. Their work was
based on the assumption that these moves can be identiﬁed not only in the fullpapers, but also in scientiﬁc abstracts. The automatic summarization task is a two-
stage procedure: identiﬁcation of important sentences followed by recognition ofmoves also referred to as the rhetorical roles of the extracted sentences. For both
stages, a Bayesian classiﬁer inspired by the work described in Kupiec et al. (1995)is used. In order to classify sentences as worthy of inclusion in the summaryand to identify their rhetorical roles, a set of low-level properties of sentences areextracted and used as features for the classiﬁer. Examples of features include thepresence of indicator phrases, the presence of indicator phrases which correspondto a particular rhetorical class, and sentence relative location. For training data, acorpus of 80 scientiﬁc articles was annotated with information about the rhetoricalcategory of each sentence. Teufel and Moens (2002) report an accuracy of 66 per-cent for the classiﬁcation of sentences as being worth including in a summary. Inaddition, the classiﬁer can identify the correct rhetorical role for 64 percent of thecorrectly extracted sentences.
Mitkov and Corpas (2008) employ the identiﬁcation of rhetorical predicates to
enhance the performance of a third generation translation memory system (Pekar
& Mitkov 2007). This TM system attempts to match sentences (segments) not onlyin terms of syntax but also in terms of semantics. With semantic processing beingfar from perfect, if two sentences (segments) under consideration for a semanticmatch are also labeled with the same rhetorical predicate, the probability of theirmatching is increased.
The identiﬁcation of subtopic segments within a text has been exploited in
many different NLP applications, especially those concerned with processing longdocuments. These applications include high-level discourse planning in natu-ral language generation, summarization of topic structure, visualization and thedevelopment of reading aids, information retrieval, and the efﬁcient navigation ofdocuments.

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 621 — #23
Discourse Processing 621
In the area of information access, Tombaugh et al. (1987) found that a more efﬁ-
cient way to read long texts on computer screens is to divide the screen into twowindows. The ﬁrst window displays information about the content of the overalldocument and provides readers with visual cues about the locations of portions ofpreviously read text. This information can be presented as a ﬁne-grained hyper-linked table of contents that assists readers in their recall and ability to quicklylocate information that has already been read once. The second window of theinterface displays the segment of text associated with the hyperlink selected in theﬁrst. Hearst (1997) notes that the effectiveness of such systems is improved whenthey are capable of tokenizing long texts in terms of subtopic segments as opposedto sentences or paragraphs.
Choi (2000) uses discourse segmentation to improve document navigation for
users who have visual impairment. Under his approach, telegraphic text com-pression methods are applied to sentences and topic segments in a text in order tocreate an index of entries that are read to the visually impaired user. The user maythen select entries of interest to be read more fully.
Hearst (1997) also demonstrates that discourse segmentation could be useful in
presenting a search engine’s results to a user. In the implemented TileBar system,
returned documents are presented visually, in the form of bars in which differentsubtopic segments are represented by ‘tiles’ color-coded according to their rele-vance to the input query. Clicking on a tile in the bar allows the user to access thefull document, with the browser initially displaying the discourse segment thatcorresponds to the clicked tile.
Mooney et al. (1990) use linear discourse segmentation in the generation of
explanatory text. The authors argued that hierarchical structures of the kindderived under approaches such as RST do not apply well to the generation ofextended explanations in which it is important to accurately obtain the high-level structure of the hierarchy. They show that discourse segmentation is moreappropriate for high-level discourse planning in a natural language generationsystem.
Discourse segmentation has also been used to facilitate automatic text sum-
marization. It is useful when summarizing long documents to identify differenttopic segments within the input text and then apply a summarization method toeach topic in turn, concatenating and returning those outputs in the ﬁnal sum-mary. Barzilay and Elhadad (1997) used an approach to discourse segmentation toproduce a model of topic progression for a text for this purpose.
Harabagiu and Lacatusu (2005) employ ﬁve different topic representation
approaches, including the TextTiling algorithm (Hearst 1997) to generate multi-
document summaries. Boguraev and Neff (2000) propose a summarizationmethodology based on linear discourse segmentation. Topic shifts are detected in atext and integrated into a linguistically aware summarizer which exploits salienceby picking different chains of cohesively connected segments. Angheluta et al.(2002) use generic topical cues for identifying the thematic structure of a text toextract summaries in the form of tables of contents. Following this work, Moens(2008) also describes a system which segments a text into topics and subtopics

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 622 — #24
622 Ruslan Mitkov
and which she uses for the generation of tables of contents . Each segment is charac-
terized by important key terms which are extracted from it as well as by its startand end position in the text. A table of contents is built using the hierarchical andsequential relationships between topical segments identiﬁed in a text. The table ofcontents generator employs linguistic theories (deterministically and probabilis-tically modeled) related to the topic and comment of a sentence. It also utilizespatterns of thematic progression in text. The system is applied to English texts(news, web, and encyclopedic texts).
5.2 Applications of discourse coherence theories
Rhetorical structure theory (RST) was initially developed with text generation inmind (Mann & Thompson 1987) but later received considerable attention from anumber of NLP researchers. RST was widely used in summarization, mainly dueto Marcu’s (1997; 2000) work.
The distinction between nuclei and satellites made in rhetorical structure theory
was successfully employed to produce summaries automatically (Ono et al., 1994;Marcu 1997; Corston-Oliver 1998). The underlying idea of these methods is thata summary can be produced from the rhetorical structure tree by keeping onlythe nuclei and removing the satellites. According to RST, the understanding ofnuclei does not depend on the satellites, and therefore the resulting summary is acoherent text. This method is difﬁcult to use because it is not trivial to determinethe type of each span and the relations between them. In addition, the method alsorequires the organization of the spans into a tree structure.
Ono et al. (1994) determine relations between sentences or blocks of sentences
in Japanese texts using linguistic clues such as connectives, anaphoric expressions,and idiomatic expressions. These relations are used to evaluate the importance ofeach sentence and to produce a summary. The method was evaluated on 30 edito-rial articles and 42 technical papers that were annotated with information whichindicated the important sentences. The proposed method extracts 60 percent of themost important sentences from editorials and 74 percent from technical papers.The authors argue that their method is very useful because the generated abstractcontains well-connected units rather than fragmentary sentences.
A formalized algorithm to build rhetorical structure (RS) trees was proposed by
Marcu (1997). The algorithm operates in three stages. First, it determines the tex-tual units and the discourse markers, and hypothesizes a set of relations betweenthe units. On the basis of these elements, in the second step all the trees satisfyinga set of constraints are built. The last step of the algorithm is to choose the besttree among the ones constructed in the previous step. A corpus analysis was usedto identify the set of constraints and discourse markers used by the algorithm, aswell as how the best tree should be built. Once the tree is constructed, it is pos-sible to determine a partial order between the units of the text, which in turn canbe used to produce summaries of the desired length. The approach was evalu-ated on ﬁve texts, and comparison between the units annotated by human judges;

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 623 — #25
Discourse Processing 623
those selected by the program as important reported a recall of 52.77 percent andprecision of 50 percent.
The algorithm proposed by Marcu can prove very slow in building the tree
as it tries all the possible combinations of units. This problem is addressed byCorston-Oliver (1998), who proposes an algorithm to build RS trees which com-bines a backtracking algorithm with a greedy approach. In addition, the algorithmdoes not rely only on cue phrases, as is the case in most of the previous work, butinstead it combines them with syntactic analysis and logical form. Even thoughCorston-Oliver’s work is presented in the context of automatic summarization, thepaper contains only a preliminary evaluation of the quality of the RST produced.
Rhetorical information is also used by Alonso i Alemany and Fuentes Fort (2003)
to improve the performance of a summarizer based on lexical chains. Instead of
building the RS tree of the source, and producing a summary based on the tree,rhetorical relations between textual units are used to further compress the text byremoving the satellites of the selected sentences. In addition, the argumentativestructure of the text is used to boost a lexical cohesion method. The advantage ofthis method is that it only requires the identiﬁcation of the spans and the relationsbetween them, and not the building of the whole tree.
Whereas the most popular application of centering theory has been in anaphora
resolution algorithms (Brennan et al., 1987; Walker 1989; Tetreault 1999; 2001), itwas also successfully used in automatic summarization to produce and evaluatesummaries. Barzilay and Lapata (2005b) discuss how local coherence can be usedin multi-document summarization by building an entity grid which captures thetransitions between sentences. Or˘ asan (2006) takes a similar approach to that used
in text generation by Karamanis and Manurung (2002) and produces summarieswhich minimize the number of violations of the continuity principle.
33Evaluation
of the summaries produced reveals that the summaries are more informative thanthose produced using other methods, but not necessarily more coherent as initiallyassumed. The justiﬁcation for this is that centering theory is a theory of local cohe-sion, so it cannot deal very well with sentences extracted from different parts of adocument.
Hasler (2007) uses centering theory to evaluate summaries. Even though, in
her case, the summaries are analyzed manually, the proposed method could beeasily implemented with a view to conducting automatic analysis. The evalu-ation method assigns scores to summaries on the basis of a preferred orderingof transitions which occur between utterances within them. Pairs of summarieswere compared by judges using the proposed method, and an agreement rate of70 percent was observed.
Miltsakaki and Kukich (2004) apply centering theory to improve the task of essay
scoring. They experiment with the e-rater essay scoring system and ﬁnd that the use
of rough-shift transitions as a measure of local discourse coherence contributesto the task of essay evaluation. More speciﬁcally, a metric based on rough-shift(see Section 3.3) improves the performance of e-rater signiﬁcantly, in that it betterapproximates human scores and provides the option of giving additional instruc-tional feedback to the student. A by-product of this project is the conﬁrmation that

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 624 — #26
624 Ruslan Mitkov
the rough-shift transition, insufﬁciently covered in the literature, reliably accountsfor incoherence.
5.3 Anaphora resolution applications
Anaphora resolution has been extensively applied in NLP . The successful iden-tiﬁcation of anaphoric or coreferential links is vital to a number of applicationssuch as machine translation, automatic abstracting, dialogue systems, questionanswering, and information extraction.
The interpretation of anaphora is crucial for the successful operation of a machine
translation system. In particular, it is essential to resolve anaphoric relations when
translating into languages which mark the gender of pronouns. Unfortunately, themajority of MT systems developed do not adequately address the problems ofidentifying the antecedents of anaphors in the source language and producing theanaphoric ‘equivalents’ in the target language. As a consequence, only a limitednumber of MT systems have been successful in translating discourse, rather thanisolated sentences. One reason for this situation is that, in addition to anaphoraresolution itself being a very complicated task, translation adds a further dimen-sion to the problem in that the reference to a discourse entity encoded by a sourcelanguage anaphor by the speaker (or writer) has not only to be identiﬁed by thehearer (translator or translation system), but also re-encoded in a different lan-guage. This complexity is partly due to gender discrepancies across languages, tonumber discrepancies of words denoting the same concept, to discrepancies in thegender inheritance of possessive pronouns, and discrepancies in target-languageanaphor selection (Mitkov & Schmidt 1998).
Anaphora resolution in information extraction could be regarded as part of the
coreference resolution task which takes the form of merging partial data objectsabout the same entities, entity relationships, and events described at differentdiscourse positions. The importance of coreference resolution in information extrac-
tion has led to the inclusion of the coreference resolution task in the Message
Understanding Conferences (MUC-6 and MUC-7). This in turn gave considerableimpetus to the development of coreference resolution algorithms and as a resultseveral new systems emerged (Baldwin et al., 1995; Kameyama 1997; Gaizauskas& Humphreys 2000).
Researchers in text summarization are increasingly interested in anaphora res-
olution since techniques for extracting important sentences are more accurate ifanaphoric references of indicative concepts are taken into account as well. Moregenerally, coreference and coreferential chains have been extensively exploited forabstracting purposes. Baldwin and Morton (1998) describe a query-sensitive doc-ument summarization technique which extracts sentences containing phrases thatcorefer with expressions in the query. Azzam et al. (1999) use coreferential chainsto produce abstracts by selecting a ‘best’ chain to represent the main topic of atext. The output is simply the concatenation of sentences from the original docu-ment which contain one or more expressions occurring in the selected coreferential

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 625 — #27
Discourse Processing 625
chain. Boguraev and Kennedy (1997) employ their anaphora resolution algorithm(Kennedy & Boguraev 1996) in what they call ‘content characterization’ of tech-nical documents. Or˘ asan (2006; 2009), Mitkov et al. (2007), and Kabadjov (2007)investigated the effect of anaphora resolution on text summarization. The resultsof these studies suggest that fully automatic anaphora resolution in spite of itslow performance still has a beneﬁcial, albeit limited, effect on text summariza-tion. An interesting observation from these studies is that, once the success rate ofanaphora resolution reaches levels closer to 80 percent, summarization is almostguaranteed to improve and that the performance of a summarizer also dependson how anaphoric knowledge is incorporated.
It should be noted that cross-document coreference resolution has emerged as
an important trend due to its role in cross-document summarization . Bagga and
Baldwin (1998) describe an approach to cross-document coreference resolutionwhich extracts all sentences containing expressions coreferential with a speciﬁcentity (e.g., John Smith) from each of several documents. In order to establish
cross-document coreference and, in this particular application, decide whetherthe documents discuss the same entity (i.e., the same John Smith), the authors
employ a vector space model to resolve ambiguities between people having thesame name. Witte et al. (2005) identify both within-document and cross-documentcoreference chains in order to establish the most important entities within a docu-ment or across documents and produce a summary on the basis of one or severaldocuments.
Coreference resolution has proven to be helpful in question answering (QA).
Morton (1999) retrieves answers to queries by establishing coreference linksbetween entities or events in the query and those in the documents.
34The
sentences in the searched documents are ranked according to the coreference rela-tionships, and the highest-ranked sentences are displayed to the user. Anaphoraresolution is employed for question answering in Harabagiu et al. (2001a). Watsonet al. (2003) demonstrate experimentally that anaphora resolution is highly rele-vant to open domain QA. Recent experiments employing anaphora resolution inQA are reported in Negri and Koulekov (2007) and in Bouma et al. (2007).
Mitkov et al. (2007) also conducted a study to investigate the impact of anaphora
resolution on term extraction and text categorization. As in the case of the results
on summarization, fully automatic anaphora resolution with performance in therange of 50 percent has a positive, albeit limited, effect. Finally, Hendrickx et al.(2008) report similar impact of coreference resolution with regard to information
extraction .
6 Further Reading
Kehler (1995) shows how the recognition of coherence relations affects the inter-pretation of a variety of linguistic phenomena including verb phrase ellipsis,gapping, tense, and pronominal reference. In later work, Kehler et al. (2008: 2)revisit Hobbs’s theory which maintains that the mechanisms supporting pronoun

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 626 — #28
626 Ruslan Mitkov
interpretation are “driven predominantly by semantics, world knowledge, andinference, with particular attention drawn to how these are used to establish thecoherence of a discourse.” On the basis of three new experimental studies, theauthors evaluate a coherence-driven analysis with respect to four previously pro-posed interpretation biases – grammatical role parallelism, thematic roles, implicitcausality, and subjecthood – and argue that the coherence-driven analysis canexplain the underlying source of the biases and predict in what contexts evidencefor each will surface.
Radev (2000) introduces cross-structure theory (CST), which describes relation-
ships between two or more sentences from different source documents relatedto the same topic. CST is related to rhetorical structure theory (RST) but takes into
account the features of multi-document structure and does not have an underlyingtree representation or make assumptions about writers’ intentions. There are 18domain-independent relations such as identity ,equivalence, subsumption ,contradic-
tion, overlap, fulﬁlment ,a n d elaboration between text spans. Radev argues that being
aware of these relations during multi-document summarization could help to min-imize redundancy or the inclusion of contradictions from different sources, andtherefore improve the quality of the summary. Radev et al. (2003) developed theCSTBank, a corpus annotated for CST which could be useful for multi-documentsummarization as it provides a theoretical model for issues that arise when tryingto summarize multiple texts.
Strube (1998) proposes an alternative framework to centering by replacing the
backward looking center and the centering transitions with an ordered list ofsalient discourse entities (referred to as S-list). The S-list ranking gives preference
to hearer-old over hearer-new discourse entities (Prince 1981) and can account forthe difference in salience between deﬁnite NPs (usually hearer-old) and indeﬁniteNPs (usually hearer-new). In contrast to centering, Strube’s model can also handleintra-sentential anaphora.
Kibble (2001) discusses a reformulation of the centering transitions. Instead of
deﬁning a total preference ordering, the author argues that a partial orderingemerges from the interaction between ‘cohesion’ (maintaining the same cen-ter), ‘salience’ (realizing the center as subject), and Strube and Hahn’s notion of‘cheapness’ (realizing the anticipated center of a following utterance as subject).
A corpus-based study (Poesio et al., 2000; 2004) investigates the validity of the
claim that each utterance has exactly one backward looking center (apart from theﬁrst utterance in the discourse segment) and of the claim stating that, if any Cf(U
N) is pronominalized in UN+1, then Cb (U N+1) must also be pronominalized. It
found that both these claims are subject to frequent violation. The authors exper-imented with different deﬁnitions of utterances (Suri & McCoy 1994; Kameyama1998) such as sentences or ﬁnite clauses, and also treating adjuncts as embeddedutterances. They allowed a discourse entity to serve as a Cb of an utterance evenif it was only indirectly referred to by a bridging reference. This led to fewer vio-lations of the ﬁrst claim but to more violations of the second. The study concludesthat texts can be coherent even if the above claims do not hold since coherence canbe achieved by other means such as rhetorical relations.

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 627 — #29
Discourse Processing 627
Mitkov and Or˘ asan (2004) report on another corpus-based study whose aim is
to evaluate speciﬁc conventions of centering theory and to establish whether theyshould be revisited. In particular, their study explores the relation between dis-course coherence and several parameters such as the deﬁnition of an utterance,the varieties of anaphora considered, the forms of the discourse entities, and thetype of genre. The results obtained in this study point to a number of interestingobservations which merit further investigation. Mitkov and Or˘ asan ﬁnd that thecentering theory considers a discourse more coherent in English, when posses-sive pronouns and zero pronouns are also counted as discourse units. In this casedeﬁning the utterance as a coordinate clause would by and large result in bettercoherence than if only sentences were considered. In most cases counting indirectrealizations yielded improved coherence. The study also found that the sample ofnewswire texts was more coherent than the sample of encyclopedic texts. Whereasthe authors cautiously regard the study carried out and the results obtained as pre-liminary, they believe that they indicate the value of revisiting some of the currentconventions in centering theory – e.g., reconsidering issues like the best deﬁni-tion of an utterance or a discourse entity, and asking whether indirect realizationshould be counted as well.
Cristea et al. (1998) propose veins theory (VT) as a generalization of center-
ing theory by extending the applicability of centering rules from local to globaldiscourse. The authors deﬁne veins over discourse structure trees similar to thetrees used in RST which delimit domains of referential accessibility for each unitin a discourse. Once identiﬁed, reference chains can be extended across seg-ment boundaries, thus enabling the application of centering theory over the entirediscourse.
Walker et al. (1998) is a good collection of papers on centering.For a detailed account on anaphora resolution, the reader is referred to Mitkov
(2002). For the latest research on anaphora resolution, see the proceedings of therecent Discourse Anaphora and Anaphor Resolution Colloquium (DAARC) con-ferences or the volumes based on these conferences (e.g., Branco 2007; Branco et al.,2005).
For work and publications on dialogue , see Chapter 16 of this book,
COMPU -
TATIONAL MODELS OF DIALOGUE . Recently there has been growing interest in
multiparty dialogue (e.g., Purver et al., 2006; 2007; Gupta et al., 2007; Ehlen et al.,
2008; Hawes et al., 2008).
Of the surveys on the computational treatment of discourse, chapter 21 in
Jurafsky and Martin (2009) is worth reading.
ACKNOWLEDGMENT
I am greatly indebted to Constantin Or˘ asan, Gloria Corpas, Elena Lloret, Laura Pack-
Hagan, Erin Phillips, and especially Richard Evans for their help, comments, andsuggestions.

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 628 — #30
628 Ruslan Mitkov
NOTES
1 www.ldoceonline.com/2 Cohesion is exhibited at grammatical level through the use of anaphora, ellipsis,
or substitution ( grammatical cohesion), or at lexical level through the repetitions of
words (lexical cohesion ). As with many linguistic phenomena, grammatical cohesion
and lexical cohesion cannot always be regarded as clear-cut distinctions and there areborderline cases (Halliday & Hasan 1976: 6).
3 Monologues are usually intended for a reader or readership (in the case of a writer) or
hearer or audience (in the case of a speaker) but may not be intended for any readershipor audience (as in the case of personal diaries). Monologues may have more than oneauthor.
4 Dialogue usually involves freer interchange and turn taking.5 This includes informal, spontaneous conversation which includes the principles of tak-
ing of turns. See Chapter 16,
COMPUTATIONAL MODELS OF DIALOGUE , for more on
dialogue.
6 This example was used for illustrative purposes in McKeown (1985).7 Rhetorical structure theory (RST) (see Section 3.2 of this chapter) and other dis-
course theories such as discourse representation theory (Kamp & Reyle 1993) representhierarchical structures and relations, and do not merely reﬂect linear sequences oftopics.
8 In very broad terms, we can think of an utterance as a ﬁnite clause or a sentence (in
fact, this is one of the various parameters that need to be deﬁned when employingcentering).
9 Centering does not assign a center to the ﬁrst utterance of a discourse segment.
10 To simplify notation, I shall drop D which denotes the discourse segment of which the
utterance is part.
11 The backward looking center is often referred to simply as the center. However, the
qualiﬁcation ‘backward looking’ is in line with the requirement that the backwardlooking center of a current utterance establishes a link to the previous utterance andmust be on its list of forward looking centers.
12 Apart from the initial utterance of a discourse segment.13 This statement is valid for English and for a number of other languages.14 According to Grosz et al. (1995), the ﬁrst utterance in a discourse segment is not
assigned a center. It could be argued that there are cases where the most salient elementis clearly identiﬁable even in the ﬁrst utterance (e.g., with cleft constructions).
15 Note that if there is one pronoun, it realizes the center (see below, rule 1).16 As deﬁned by Brennan et al. (1987), smooth-shift is preferred to rough-shift.17 Deleted as a zero pronoun in languages exhibiting extensive use of zero pronouns such
as Japanese, Italian, Spanish, and Bulgarian.
18 Note that sheand herinU
3cannot be coreferential (see Section 4.1 for deﬁnition of
coreferential).
19 The word (phrase) ‘pointing back’ is also called a referring expression if it has a
referential function.
20 In identity-of-sense anaphora, the anaphor and the antecedent do not correspond to
the same referent in the real world but to ones of a similar description.

“9781405155816_4_021” — 2010/5/8 — 12:16 — page 629 — #31
Discourse Processing 629
21 Deictic expressions are those words whose interpretation is derived from speciﬁc
features of the utterance (e.g., who the speaker is, who the addressee is, where andwhen the utterance takes place), and not from previously introduced words, as is thecase with anaphors.
22 It should be noted that these are only the basic relationships between the anaphoric
deﬁnite NP and the antecedent but not all possible relations.
23 Note that some authors consider synonymy, generalization, and specialization as
examples of indirect anaphora.
24 Discourse segments are stretches of discourse in which the sentences are addressing
the same topic (Allen 1995).
25 However, Barlow (1998) and Mitkov (2002) point out that there are a number of
exceptions.
26 A node A c-commands a node B if and only if (1) A does not dominate B, (2) B does
not dominate A, and (3) the ﬁrst branching node dominating A also dominates B(Haegeman 1994). Therefore, in a tree generated by the rules S →AB, A →E, B→
CD, C →F, and D →G, A c-commands B, C, F, D, and G, B c-commands A and E, C
c-commands D and G, and D c-commands C and F.
27 Mitkov (2002) explains that constraints and preferences usually work in combina-
tion towards the goal of identifying the antecedent. Applying a speciﬁc constraint orpreference alone may not result in the tracking down of the antecedent.
28 The original algorithm handles personal and possessive pronouns whose antecedents
are NPs.
29 The original algorithm handles third-person pronouns, including reﬂexives and recip-
rocals, whose antecedents are NPs.
30 The original algorithm handles third-person personal pronouns whose antecedents
are NPs.
31 The original algorithm handles personal, reﬂexive, and possessive third-person pro-
nouns whose antecedents are NPs.
32 The original algorithm handles third-person personal pronouns whose antecedents are
NPs.
33 The continuity principle is the most general of the four principles proposed in Kibble
and Power (2000) to redeﬁne centering theory and requires that two consecutive utter-ances have at least one entity in common. The ﬁnal version of Kibble and Power (2000)does not use any name for this principle, but it is referred to as continuity principle inKaramanis and Manurung (2002).
34 The coreference relationships that Morton’s system supports are identity, part–whole,
and synonymy.

