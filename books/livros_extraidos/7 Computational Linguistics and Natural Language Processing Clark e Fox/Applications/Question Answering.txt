“9781405155816_4_022” — 2010/5/8 — 12:17 — page 630 — #1
22 Question Answering
BONNIE WEBBER AND NICK WEBB
1 What is Question Answering?
Questions are asked and answered every day. Question answering (QA)technology aims to deliver the same facility online. It goes further than the morefamiliar search based on keywords (as in Google, Yahoo, and other search engines),
in attempting to recognize what a question expresses and to respond with anactual answer. This simpliﬁes things for users in two ways. First, questions donot often translate into a simple list of keywords. For example, the question
(1) Which countries did the pope visit in the 1960s?
does not simply translate to the keywords ‘countries,’ ‘pope,’ ‘visit,’ ‘1960s’because a search on those keywords will only ﬁnd documents (web pages) thatcontain the words ‘countries’ (or ‘country,’ if the search engine recognizes plurals),
‘pope,’ and ‘1960s,’ and not words or phrases that denote particular countries(such as ‘United Kingdom,’ or the ‘United States’), or the pope (‘head of theCatholic church,’ for example), or a date within the 10-year time span between‘1960’ and ‘1970.’ A much more complex set of keywords is needed in order to getanywhere close to the intended result, and experience shows that people will notlearn how to formulate and use such sets.
Second, QA takes responsibility for providing answers, rather than a searchable
list of links to potentially relevant documents (web pages), highlighted by snippets
of text that show how the query matched the documents. While this is not muchof a burden when the answer appears in a snippet and further document access isunnecessary, QA technology aims to move this from being an accidental propertyof search to its focus.
In keyword search and in much work to date on QA technology, the information
seeking process has been seen as a one-shot affair: the user asks a question, and thesystem provides a satisfactory response. However, early work on QA (Section 1.1)did not make this assumption, and newly targeted applications are hindered byit: while a user may try to formulate a question whose answer is the information

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 631 — #2
Question Answering 631
they want, they will not know whether they have succeeded until something hasbeen returned for examination. If what is returned is unsatisfactory or, while notthe answer, is still of interest, a user needs to be able to ask further questions thatare understood in the context of the previous ones. For these target applications,QA must be part of a collaborative search process (Section 3.3).
In the rest of this section, we give some historical background on QA sys-
tems (Section 1.1), on dialogue systems in which QA has played a signiﬁcantrole (Section 1.2), and on a particular QA task that has been a major driver ofthe ﬁeld over the past 8 years (Section 1.3). Section 2 describes the current state ofthe art in QA systems, organized around the de facto architecture of such systems.Section 3 discusses some current directions in which QA is moving, including thedevelopment of interactive QA. We close with some pointers to further reading.
1.1 Early question answering systems
Early QA systems were developed to enable users to ask interesting questionsabout well-structured data sets such as baseball statistics, personnel data, or chem-ical analyses of lunar rock and soil samples. (Simmons (1965) provides an earlysurvey.) These early QA systems essentially attached a front end and a back end toa database system. The front end performed parsing and interpretation, mappingquestions phrased in everyday terms onto a form that speciﬁed a computation to
be carried out over the database – for example, the question
(2) What is the average concentration of aluminum in high-alkali rocks?
would be mapped to a computation that identiﬁes the high-alkali rocks in thedatabase, ﬁnds the aluminum concentration in each, and then computes anaverage over those values.
Given the potential complexity of such queries, differences between the sys-
tem’s and the user’s underlying models of the data, as well as users’ frequentlack of awareness of what information is actually in the database, early QAdevelopment focused on such issues as:•mapping user questions to computable database queries;
•handling questions that could not be parsed or that could not be interpreted asa valid query;
•resolving syntactic and referential ambiguities detected in questions;
•handling differences in how user and system conceptualized the domain (e.g.,user queries about the age of lunar rocks versus system data on potassium/rubidium and uranium isotope ratios, as well as differences between what userand system believed to be true in the domain (Kaplan 1982; Mays et al., 1982;Pollack 1986; Webber 1986);
•identifying, in the case of distributed databases, what information needed tobe imported from where, in order to answer the user’s question (Hendrix et al.,1978).

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 632 — #3
632 Bonnie Webber and Nick Webb
User–system interactions designed to resolve ambiguities or reconcile mis-
matches between user and system beliefs about the domain showed that satisfyinga user’s information needs required the user to do more than just ask questionsand the system to do more than just answer them. But systems were still viewedas question answerers, carrying out other types of interactions on an ‘as needed’basis.
This ﬁrst foray into database QA was essentially abandoned in the late 1980s for
two reasons – one technical, and one social. Technically, considerable effort wasneeded to guarantee an effective and reliable mapping between user questionsand database queries. Not only did the correct mapping depend on the structureof the particular database, but many disparately phrased user questions neededto be mapped onto the same database query. Even worse, questions that differedonly minimally needed to be mapped onto very different database queries. Theonly solution to these problems available at the time was more and more mappingrules that had to be written by hand by system experts. As a solution, this wasneither scalable nor portable. The social problem involved the lack of a signiﬁcantaudience for the technology: ordinary people lacked access to large data sets, andmanagers whose companies maintained large data sets lacked sufﬁcient interestin accessing the data themselves. Companies such as Symantic which developedstate-of-the-art software for database QA (Hendrix 1986) ended up abandoning it.
With the advent of the web, this social problem disappeared, and machine
learning techniques that have proved so useful in other areas of language technol-ogy are beginning to be applied to the problem of learning (complex) mappingsbetween user questions and database queries (Mooney 2007; Zettlemoyer &Collins 2007). While it is still early days, this does mean that the growing num-ber of databases containing rich and useful information may again be primed foraccess through natural language questions.
1.2 Question answering in dialogue systems
QA was also a feature of early systems whose main purpose in interacting withusers in natural language was something other than answering their questions.In one of the earliest of such dialogue systems , SHRDLU (Winograd 1973), users
could converse with an animated robot (with a visible arm) that could both act (inresponse to user requests) and reﬂect on its actions (in response to user questions).More generally, users could question SHRDLU about the state of its world (e.g.,where objects were, what objects were where) or about its previous actions or itsplans (e.g., why or when it performed some action), which SHRDLU could answerin terms of the history it maintained of its goals and actions.
Interactions with SHRDLU, and with another animated robot system called the
Basic Agent (Vere & Bickmore 1990), were through typed text. Later systems sup-ported limited speech interaction (Allen et al., 1996; Lemon et al., 2001; Eliasson2007). Because these robots did not have any goals of their own, apart from thoseadopted in response to user requests/commands, and because no mechanism was

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 633 — #4
Question Answering 633
provided for user–robot collaboration, the dialogic capability of these systems didnot extend to asking questions of the user or to making requests themselves.
Other more recent dialogue systems, which take on other roles with respect to
the user, also include QA capabilities in their repertoire. In intelligent tutoring sys-
tems, the system has goals in its role of tutor – e.g., assessing the student’s knowl-edge, correcting the student’s errors, and imparting information that the student ismissing. Dialogues can thus involve the system introducing and describing a topicfor tutoring, after which it might ask the student a question about some aspect ofthe problem, explain why the student’s response is correct or incorrect, and/orremind the student of something already said previously during the interaction.Here, it is the student who answers questions or says he/she does not know, butit is still the system’s job to determine if an answer is correct. Again, the earliesttutoring systems, like S
OPHIE (Brown & Burton 1975), interacted through typed
text, while later systems such as ITSPOKE (Litman & Forbes-Riley 2006) allow forspoken interaction.
The most industrially relevant role played by dialogue systems has been in
information provision and user assistance, such as in helping users to plantravel (Goddeau et al., 1994), book ﬂights (Seneff 2002), or ﬁnd an appropriaterestaurant (Walker et al., 2004), or in routing user telephone calls (Chu-Carroll &Carpenter 1999) or handling directory inquiries (de Roeck et al., 2000). All suchtasks involve the system getting sufﬁcient information from the user to fully orpartially instantiate some form (often called a frame) which the system can eval-
uate (just like a database query) and present the results to the user as a basis forfurther interaction. In such cases, the user’s information needs may be anywherefrom completely formed to vague. They may or may not be able to be satisﬁedgiven the underlying data, and queries may need to be reformulated on the basisof additional knowlege and relaxed constraints. Dialogues can thus involve thesystem asking the user questions related to values of frame elements; the userspecifying such values (either precisely or vaguely); the system listing and/ordescribing the results (when too numerous to list); the user choosing some itemfrom among the results or modifying or replacing some already speciﬁed values;and the system requesting conﬁrmation of its understanding.
A key emerging element of dialogue approaches is their inherent generality –
the potential for subdialogue structures independent of task or application (suchas for error correction or clariﬁcation) that will, in the future, allow them to beseamlessly integrated with QA systems (Section 3.3). For more detailed discus-sion of the issues and technology underlying dialogue systems, see Chapter 16,
COMPUTATIONAL MODELS OF DIALOGUE .
1.3 Question answering in TREC
Returning to straight QA systems, the advent of the web has made increasingamounts of information accessible to people. To ﬁnd the bits of interest, peopleare using search methods pioneered in text retrieval (a ﬁeld revitalized by the

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 634 — #5
634 Bonnie Webber and Nick Webb
web), but scaled up to nearly instant response to vast numbers of users. In thiscontext, QA too has found a second life, with the idea that users would ﬁnd notjust relevant documents (web content), but the particular information they areseeking.
This vision of QA differs signiﬁcantly from QA over databases, where (as noted
in Section 1.1) questions map onto computations carried out over a database of
known structure. Instead, in what has been called open domain question answer-
ing, the answer to a question must be found and extracted rather than computed ,
as a natural extension to text retrieval. Advances in open domain QA have beenaccelerated by its adoption within the Text Retrieval Conference (TREC).
Initially, TREC QA focused on ‘factoid’ questions – questions of who, what ,where,
and, to some extent, when , that can be answered by a short word or phrase. From
1999 to 2007, TREC QA advanced on several fronts, to address increasingly largedocument collections, increasingly complex questions, and increasingly complexevaluation strategies. While the basic approach to factoid QA is now well under-stood, challenges remain. This basic approach and the current state of the artare described in Section 2, and some challenges that are now beginning to beaddressed, in Section 3.
2 Current State of the Art in Open Domain QA
A basic QA system involves a cascade of processes that takes a user’s questionas input and responds in the end with an answer or rank-ordered list of topanswer candidates, along with an indication of the source of the information (seeFigure 22.1). This embodies the de facto paradigm for QA characterized by Pa¸ sca
(2007) as:•retrieve potentially relevant documents;
•extract potential answers (called here answer candidates);
•return top answer(s).
We discuss question typing in Section 2.1, query construction and text retrieval in
Section 2.2, text processing for answer candidates (including weighting, ranking, and
ﬁltering candidates) in Section 2.3, and answer rendering in Section 3.3. Perfor-
mance evaluation is brieﬂy discussed in Sections 2.4 and 3.4, and at greater lengthin Chapter 11,
EVALUATION OF NLP SYSTEMS .
2.1 Question typing
Questions generally undergo two initial processes to identify what type of infor-mation is being sought ( question typing) and in what piece of text it is likely to be
found ( query construction ). Although these processes can be carried out in parallel,
query construction is so intimately tied up with text retrieval , that we will discuss
them together in Section 2.2. Question typing aims to associate a label (QType) with

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 635 — #6
Question Answering 635
questionquery
constructiondocumentcollection
answer question
typingdocument
retrievalanswer candidate
processinganswer candidate
extraction
Figure 22.1 Basic QA system architecture.
a question, indicating the kind of information being sought – e.g., the meaningof an abbreviation (
ABBREV ) or of a word or phrase ( DEFINITION ), the name of the
person who has or had some particular property or set of properties ( PERSON ), etc.
For more on the location of speciﬁc entities in text, see Section 3 on name extrac-tion in Chapter 18,
INFORMATION EXTRACTION . These labels provide testable
semantic constraints on possible answer candidates. Labels assigned by question
typing have also been used to support text retrieval through predictive annotation
(Section 2.2), as well as to support the processing involved with identifying andranking answer candidates. For example, a system that has done text retrieval onshort passages may ﬁlter out ones that lack anything of the appropriate type as ananswer candidate (Section 2.3).
While manually constructed rules have been used for question typing –f o r
example,•If the question starts with Who orWhom ,Q T y p ei s
PERSON .
•If the question starts with Where,Q T y p ei s LOCATION .
state-of-the art systems (Li & Roth 2006; Moschitti et al., 2007) use probabilisticclassiﬁers, P(QType |Q), where the question Q is represented as a set of features.
The features used have become more sophisticated over time, now extending tosyntactic and semantic, as well as lexical features.
Using syntactic features alone, Li and Roth (2006) achieve state-of-the-art perfor-
mance, using the SNoW classiﬁer (Carlson et al., 1999) over the question types inthe widely available TREC 10 and 11 question sets (Voorhees 2002; available fromhttp://l2r.cs.uiuc.edu/∼cogcomp/). This corpus of 1,000 manually annotated

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 636 — #7
636 Bonnie Webber and Nick Webb
questions has both coarse-grain (i.e., ABBREVIATION ,ENTITY ,DESCRIPTION ,
HUMAN ,LOCATION ,NUMERIC ) question types, and a further 50 reﬁnements of
those categories (so for example the coarse-grain category HUMAN can be rep-
resented by four ﬁne-grain categories: GROUP ,INDIVIDUAL ,TITLE ,a n d DESCRIP -
TION ). For the coarse-grain question types, Li and Roth (2006) achieve 92.5 percent
accuracy, and 85 percent on the ﬁne grain. Using semantic information, includingnamed entities and some manually extracted word class information, did not sig-niﬁcantly improve classiﬁcation accuracy over the coarse-grain question types, butboosted accuracy over the ﬁne-grain categories to 89.3 percent.
Such accuracy ﬁgures disguise the fact that a question can often have more than
one type of answer. For example, is What is an SME? an
ABBREV question for which
an appropriate strategy would involve patterns commonly used to relate abbrevi-ations with their full-text forms, or is it a
DESCRIPTION question, for which an
appropriate strategy would involve patterns commonly used in deﬁning terms? Awho question may be answered by a
PERSON (Who won the Masters tournament in
1985?), or an ORGANIZATION (Who won the Nobel Peace Prize in 1999?), or a COUN -
TRY (Who won the World Cup in 2006?). When a question is asked, a system may
know neither which kind of information the user is seeking nor what informationthe corpus contains. Allowing questions to have more than one possible type per-mits the system to ﬁrst see what kind of answers the corpus supports and, if morethan one, provide the user with answers that cover the different alternatives. Itis only because TREC has required systems to produce a single minimal answerto a question that work on QA has by and large ignored the possibilities of suchhelpful responses.
2.2 Query construction and text retrieval
In open domain QA, questions are always answered with respect to a corpus oftexts. This can be as vast and diverse as the web or more specialized, such as thecollection of biomedical abstracts in MedLine (www.ncbi.nlm.nih.gov/PubMed)or the collection of news articles in the AQUAINT corpus (Voorhees & Tice 2000).Because both the types of queries that one can construct and their success inretrieving text with good answer candidates reﬂect characteristics of the corpusand its access methods, it makes sense to discuss query construction and text retrieval
together. More speciﬁcally, the kind of query a system constructs depends onwhich of two forms of retrieval is used to ﬁnd text that might contain an answerto a user’s question: relevance-based retrieval orpattern-based retrieval. These are
discussed in the next subsections.2.2.1 Relevance-based retrieval Inrelevance-based retrieval, queries are inter-
preted as requests for texts relevant to a topic. Relevance may be assessed in terms
of
MATCHING a Boolean combination of terms or proximity to a weighted vec-
tor of terms or language model, just as in standard text retrieval (Manning et al.,2008). The problem, as already noted, is that QA demands answers, rather than the

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 637 — #8
Question Answering 637
texts that contain them, and such answers are generally expressed very locally in
a text. (With complex questions, the evidence supporting an answer may actuallybe distributed across one or even several texts. This is discussed in Section 3.2.)Although standard word-based indexing of texts supports very fast and efﬁcientretrieval, such indexing characterizes texts in terms of their gross lexical proper-ties, not by local information. For example, a news article on the 1987 Herald of
Free Enterprise ferry disaster may mention in passing ‘It was the worst peacetime
disaster involving a British ship since the Titanic sank in 1912.’ While this sentence
contains an answer to the question
(3) When did the Titanic sink?
this local information might simply be noise with respect to more prominent lexi-cal properties of the article, such that it might not be retrieved on a relevance-basedsearch for ‘ Titanic ’ and ‘sink,’ or it might not be ranked high enough for the answer
candidate extraction process to ever get to it (Section 2.3). So, with respect to agiven question, text retrieval using relevance-based methods is not guaranteed to
return texts with a high likelihood of having an answer somewhere within them.And if text retrieval fails to return texts containing answers, any subsequent pro-cessing might as well not happen. Attempts to improve this situation are usuallyfound under the rubric information retrieval for question answering (IR4QA).
A simple and widely adopted solution to the locality problem in relevance-
based retrieval is simply to break texts into a set of separate passages, each ofwhich is separately indexed for text retrieval ( passage retrieval ), assuming that there
is a better correlation between standard metrics of relevance and answers in short
texts.
Other text retrieval techniques used instead of, or in addition to, segmentation
into smaller passages involve extensions to what is indexed. In predictive annotation
(Prager et al., 2000), texts are indexed not just by the position of each (non-stop)word in the text, but also by the position of each of 20 types of named entities(called here QA-tokens ), each of which could answer a question of a given type.
For example, predictive annotation of the sentence
(4) Sri Lanka boasts the highest per capita income in South Asia.
would index the QA-token
COUNTRY $ as occuring over the ﬁrst two words of the
sentence, and the QA-token PLACE $ as occuring over the last two. (Techniques
used here are similar to those used in information extraction, as discussed inChapter 18,
INFORMATION EXTRACTION .)
QA-tokens are used as well in query construction. When user questions are
mapped to queries, not just keywords from the question are included in the query,but also an appropriate QA-token (or disjunction thereof) for the question type.For example, the where question
(5) Where is the capital of Sri Lanka?

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 638 — #9
638 Bonnie Webber and Nick Webb
would be mapped to a query comprising the keywords ‘capital,’ ‘Sri,’ and ‘Lanka,’along with a disjunct of the QA-tokens (
PLACE $,COUNTRY $,STATE $,NAME $), all
of which can potentially answer a where question. Such a query would cause to
be retrieved any passage containing the sentence in (4), although, as Prager notes,it is important to discard the match between
COUNTRY $ and the annotation of ‘Sri
Lanka’ as COUNTRY $, in order to answer the question correctly. (Even where QA-
tokens, or answer types, are not themselves indexed, they may still be used to ﬁlterout retrieved texts or passages that lack any instance of the QA-token/answertype.)
While predictive annotation reduces ‘false positives’ in text retrieval by demand-
ing passages that contain candidate answers of particular types, they can also bereduced by indexing not just words and their positions but how the words areused. This is illustrated in the Joost system (Bouma et al., 2005), which indexes
each word along various linguistic dimensions (e.g., part of speech, dependencyrelations, etc.), as well as indexing the type and span of each named entity. Usinga genetic algorithm to optimize various weightings, Bouma and his colleaguesfound a 10 percent improvement in passage retrieval on a test set of 250 questions,measured by MRR ( mean reciprocal ranking) over passages containing a correct
answer. Moreover, Morton (2005) found that when third-person pronouns wereresolved and indexed by the full NPs with which they corefer, the frequency ofboth ‘false positives’ and ‘false negatives’ could be reduced by narrowing thedistance between potential answer candidates and terms from the question (alltreated as full NPs through coreference resolution).
1
In all this work, the basic idea is that the more frequently a system can rely on
top-ranked passages to contain a correct answer, the fewer passages need to beexamined in the next stage of processing. It should be remembered, however, thatany ranking in text retrieval is a ranking on texts, not on the answers they maycontain. A separate process of answer candidate ranking is carried out at a laterstage (Section 2.3).
A survey of passage retrieval techniques for QA can be found in Tellex et al.
(2003).2.2.2 Pattern-based retrieval The other type of text retrieval used in QA is
pattern-based retrieval. This relies on the quoted string search capabilities of search
engines, reﬂecting the assumption that, although natural language usually pro-vides multiple ways to express the same information, it is expressed in the sameor a similar way to the question somewhere in the corpus. Pattern-based retrievalalso differs from relevance-based retrieval in taking the result of retrieval to bethesnippet returned as evidence for the match (cf. Section 1) rather than a pointer
to the text that contains it. Thus pattern-based retrieval does not derive any ben-eﬁt from breaking texts into smaller passages, as does relevance-based retrieval,because it targets snippets rather than their source.
Pattern-based retrieval systems differ from one another in the kinds of string
patterns they use in performing the match. The well-known AskMSR system

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 639 — #10
Question Answering 639
(Brill et al., 2002) used different sets of string patterns for different types ofquestions. For where is questions, for example, retrieval patterns were generated
with ‘is’ in every possible position – e.g.
(6) Where is the Vale of Tears?
is the Vale of Tearsthe is Vale of Tearsthe Vale is of Tearsthe Vale of is Tearsthe Vale of Tears is
Patterns usually reﬂect a direct relationship between questions and their answers.For example, since wh-questions in English (where, when, who, what, which,why) usually involve fronting the question phrase (and, in some cases, an auxiliary
as well), as in:
(7) When was the telephone invented?(8) In which American state is Iron Mountain located?(9) What is the largest whale?
their answers are likely to be found non-fronted, directly to the right of thestrings:•the telephone was invented in <answer>
•Iron Mountain is located in <answer>
•the largest whale is <answer>
Other syntactic relationships predict other string patterns, such as:•invented the telephone in <answer>
•the telephone, invented in <answer>
•in<answer> the telephone was invented
•Iron Mountain, located in <answer>
•<answer> is the largest whale
•<answer> is the largest of the whales
As Lin (2007) notes, a quoted string search does not guarantee that the snippet
returned from a search engine as evidence will actually contain a ﬁller for the ques-tion phrase (i.e., an answer). There are several reasons for this. First, the answermaterial may be outside the boundaries of the snippet. Secondly, since searchengines (as of this writing) ignore punctuation and case, a quoted string can alsomatch over successive sentences and/or clauses (i.e., across ﬁnal punctuation),producing false positive matches that have to be ﬁltered out later on, as in
(10) He was six when the telephone was invented. In 1940,h e...
which a search engine would ﬁnd as a match for the string the telephone was
invented in .

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 640 — #11
640 Bonnie Webber and Nick Webb
More recently, researchers have shown how lexical resources such as WordNet
(Miller et al., 1990) and FrameNet (Baker et al., 1998) can be used to construct addi-tional string patterns that can be used in pattern-based retrieval (Kaisser & Webber2007). Again, one must be aware that any ranking here is the search engine’s rank-ing on the documents, not on any answer candidates contained in the snippets.The ranking of answer candidates themselves is discussed in Section 2.3, afteranswer candidate extraction.
2.3 Processing answer candidates
Once a set of passages or snippets has been retrieved, a system must determinewhat, if anything, in each separate passage or snippet might serve as an answerto the question (answer candidate extraction) and how good an answer it might be(answer candidate evaluation). The latter requires assessing the candidate and itstextual context, possibly comparing it as well with the other, separately extractedcandidates. Candidates are then ranked based on this assessment, with the top-scoring candidate (or top N scoring candidates) presented to the user. Techniquesfor these different aspects of answer candidate processing are described below.
Two kinds of patterns are used for extracting answer candidates from pas-
sages or snippets. They can be extracted directly, using string patterns that identify
the contents of a particular bounded span as an answer candidate, or they canbe extracted using structured patterns, from the output of parsing, semantic role
labeling, and/or interpreting the passages or snippets.
String patterns are the simplest. They can be derived directly from the user’s
question (exactly as in pattern-based retrieval, Section 2.2.2), authored manually,or computed by bootstrapping from questions with known answers. To illustratethe latter, known [entity, location] pairs such as [Taj Mahal, Agra], [Grant’s Tomb,
New York City], etc., can be used to retrieve texts that suggest the followingpatterns for identifying the answer to location questions:
<NAME> [is|are] located in <ANSWER>.
<NAME> in<ANSWER>,
<NAME> [lies|lie] on <ANSWER>.
In each case, the answer candidate is bounded on its left and right by an identiﬁ-able word or symbol. Such patterns can also be characterized by their reliability –how often they pick up a candidate of the right type, as opposed to a randomstring. Because patterns are so simple, they are rarely completely reliable, as inexample 11, where ‘the background’ is identiﬁed as an answer candidate. Becauseparentheticals, adverbial modiﬁers, and adjuncts can occur so freely within a sen-tence, no set of patterns can reliably capture all desired answer candidates, as inexample 12, where the adverbials ‘all the way up’ and ‘all the way down’ blockthe ‘located in’ pattern from matching.
(11) ‘Where are the Rocky Mountains?’
<NAME> in<ANSWER>

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 641 — #12
Question Answering 641
‘Denver’s new airport, topped with white ﬁberglass cones in imitation ofthe Rocky Mountains in the background, continues to lie empty.’
(12) ‘Where are the Rocky Mountains?’
<NAME> [is|are] located in <ANSWER>.
‘The Rocky Mountains are located all the way up to Alaska and all theway down to Mexico.’
To reduce false positives (as in example 11), answer candidates found by
string patterns are commonly ﬁltered by tests derived from the question type(Section 2.1). For example, for a where question, <ANSWER> must be classiﬁable
as a
LOCATION . For a who question, it must be classiﬁable as a PERSON , or in some
cases, a PERSON or a COMPANY . Named entity recognition, often in conjunction
with WordNet and/or Wikipedia categories, has been used in this ﬁltering.
Answer candidate extraction rarely produces a single candidate answer, so one
or more best candidates need to be selected and displayed to the questioner. Toproduce a ranking of such candidates so as to be able to select the best, their qualityneeds to be assessed. The simplest and most common method of doing so involvescomputing their frequency of occurrence within the set of answer candidates (Brillet al., 2002). To reﬂect true frequency, this requires recognizing answer candidatesthat should be treated as equivalent, such as ‘Clinton,’ ‘Bill Clinton,’ ‘WilliamJefferson Clinton,’ and ‘ex-President Clinton,’ rather than as distinct candidates.Online resources such as Wikipedia are useful in this regard.
A more complex method can be used when correct answers to questions occur
only infrequently in the corpus. This method involves assessing the probabilitythat an answer candidate is the correct answer to the question (Xu et al., 2002; Koet al., 2007). Since lack of sufﬁcient data prevents such probabilities from beingcomputed precisely, they are approximated using such features as the ways inwhich the context of answer A
imatches question Q, whether A iis correctly typed
for the argument role that the question word plays in Q, whether A iis supported
as an answer by online resources such as gazetteers, WordNet, Wikipedia and/orthe snippets returned from web search. Probabilities based on these approxima-tions have been shown to indeed increase the ranking of good answers, therebyimproving system performance.
Result of answer candidate extraction and evaluation is a rank-ordered list of
answer candidates, of which the top (or top N) is/are presented as the answer(s),with each supported by either a document ID or a speciﬁc piece of text meant toserve as evidence. In the case of answer candidates that occur multiple times, eachwith different support, there has as yet been no attempt to assess the quality ofthat evidence – whether one piece of text might provide either stronger or clearersupport for an answer.
2.4 Evaluating performance in QA
Factoid QA in TREC treats every question as having a single correct answer,although it may be described in different ways (e.g., a distance question with an

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 642 — #13
642 Bonnie Webber and Nick Webb
answer given in miles or in kilometers, with a precise value or an approximateone, etc.). In early QA tracks, groups were allowed to return a list of ﬁve possibleanswers for each question, in rank-order, so that a question could be scored bythe reciprocal rank of the ﬁrst correct answer: if it ranks ﬁrst, it gets a score of 1;
if it ranks second, it gets a score of 1/2; etc. If only the ﬁfth answer is correct, itgets a score of 1/5. Otherwise it scores a 0. Over a set of questions, then, a systemcould be assessed in terms of mean reciprocal rank (MRR) – i.e., the average of the
reciprocal rank score of the question set. MRR is still often used to measure systemperformance, as it is more lenient than assessing only a single answer.
Although recall and precision are also used in assessing system performance,
because they do not take account of the ranked position of answers and becauseonly the top-ranked correct answer to a given question actually matters (not howmany correct answers have been returned for the question), coverage and answer
redundancy are sometimes used as alternatives to recall and precision for assessing
retrieval in QA (Roberts & Gaizauskas 2004).•Coverage is the proportion of the question set for which a correct answer can be
found within the top npassages retrieved for each question.
•Answer redundancy is the average number, per question, of passages within the
topnretrieved that contain a correct answer.
Coverage is preferable to recall because, since factoid questions are taken to have a
single answer, it does not make sense to worry about recall (i.e., the proportion of
documents containing that answer).
These metrics all assume that every question has a correct answer (or different
descriptions of the correct answer), so at issue is where judgments of correctnesscome from. When evaluating QA systems, one must keep the document collectionﬁxed, so that differences in performance do not simply reﬂect differences in thecorpus. Because it is expensive to ﬁnd all possible answers (or possible ways tophrase every answer) in a large corpus, a method called pooling was developed.
As used in large, multi-system QA evaluations, pooling involves collecting the
rank-ordered results from all runs of all the participating systems; selecting N runsper system; taking the top X documents from each of those N runs and mergingthem into a judgment pool for that query, eliminating duplicates, and, ﬁnally, man-ually assessing correctness judgments on only these pooled answers. Importantly,pooling has been found not to be biased towards systems that contribute to thepool – that is, there is no performance beneﬁt to be gained by participating in thepooling system. On the other hand, pooling makes it harder to assess new tech-niques which may produce answers that may be correct but do not occur in thepool formed from earlier results.
To allow for the development of such techniques, Lin and Katz (2006) attempted
to ﬁnd all possible answers in the AQUAINT corpus for a subset of questions fromTREC, while, more recently, Kaisser and Lowe (2008) have used the Mechanical
Turk to create an even larger set of 8,107 [question, document id, answer, answer
source sentence] tuples for the over 1,700 TREC questions from 2002 to 2006.

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 643 — #14
Question Answering 643
More discussion of evaluation methods used in QA can be found in Chapter 11,
EVALUATION OF NLP SYSTEMS .
3 Current Directions
QA technology is moving in several different directions, but here we will focuson the three that we ﬁnd most important: (1) extending the relationship betweenquestion and corpus; (2) broadening the range of questions that can be answered;and (3) deepening the relationship between user and system. We brieﬂy discusseach in turn, and then follow with a brief discussion of the consequences forevaluation.
3.1 Extending the relation between question and corpus
We have mentioned several times that open domain QA involves ﬁnding answers
in text, while database QA involves computing them, usually from an assort-
ment of other, simpler facts. Section 2.2 showed that evidence that enabled ananswer to be located could take the form of words from the user’s question, pos-sibly augmented by the syntactic relations between them, or the form of lexicaland/or syntactic variations on the question that could be predicted from hand-made resources such as WordNet and FrameNet or from patterns found throughtext mining. As Kaisser and Webber (2007) showed, however, this is not enough:while a corpus as large as the web might yield an answer phrased similarly toa given question, especially if the answer is widely discussed, this is much lesslikely with a less extensive corpus or a popular topic. The corpus may still be ableto serve as the source of an answer, but only through inference.
Textual entailment captures the intuition that one piece of text – in this case, text
containing a correct answer to a given question – can be inferred from another. Todate, progress has been gauged by the PASCAL Recognising Textual Entailment(RTE) Challenge (Bar-Haim et al., 2006; Dagan et al., 2008b).
Formally, the textual entailment task is to determine, given some text, T,a n d
hypothesis, H, whether the meaning of Hcan be inferred from the meaning of T.
For QA, Hwould be derived from a question such as:
(13) How many inhabitants does Slovenia have?
and Twould be an answer passage such as:
In other words, with its 2 million inhabitants, Slovenia has only 5.5 thousandprofessional soldiers.
Recognizing the value of RTE to QA, the RTE Challenge includes manuallyannotated T–H data for training and testing, based on question–answer pas-
sage pairs from TREC (http://trec.nist.gov) and CLEF QA (http://clef-qa.itc.it)

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 644 — #15
644 Bonnie Webber and Nick Webb
(see Chapter 10, LINGUISTIC ANNOTATION , for a discussion on annotating
language resources). To create the T–H pairs, annotators extract from the answer
passages answers of the correct type – but not necessarily the correct answer.The original question with the selected answer term included becomes H,a n d
the answer passage becomes T. If, from example 13, annotators chose the answer
‘2 million inhabitants’ from T, they could create a positive entailment pair, with
‘Slovenia has 2 million inhabitants’ as H. Alternatively, if they chose ‘5.5 thou-
sand’ from T, they could create a negative entailment pair with ‘Slovenia has only
5.5 thousand inhabitants’ as H.
The RTE challenge has tested systems on a 50–50 mix of positive and neg-
ative T–H pairs, where 50 percent would be the baseline accuracy. For the
second RTE challenge, most systems obtained between 55 percent and 61 percent(Bar-Haim et al., 2006), where the majority of approaches drew on some combi-nation of lexical overlap (often using WordNet; Miller et al., 1990), semantic rolelabeling, using FrameNet (Baker et al., 1998), and extensive use of backgroundknowledge. The top-performing system was LCC’s GROUNDHOG (Hickl et al.,2006), which achieved an accuracy of 75.4 percent. Around 10 percent of this scorewas obtained by expanding the training set of positive and negative entailmentexamples by following Burger and Ferro (2005), collecting around 100,000 positiveexamples consisting of the headline and ﬁrst sentence of newswire texts. To cre-ate negative examples, they extracted pairs of sequential sentences that includedmentions of the same named entity from a newswire corpus. Sample testing withhuman annotators determined that both of these example sets were accurate to theninetieth percentile.
Although, intuitively, we might expect a deeper level of analysis to be required
to achieve high accuracy in the RTE task, systems that have employed suchanalysis have so far failed to improve over the 60 percent baseline performanceachieved by simple lexical matching, a technique already exploited in QA. Anyfuture breakthrough in RTE is likely to be quickly and widely adopted in QA andelsewhere.
3.2 Broadening the range of answerable questions
We noted in Section 1.3 that open domain QA has primarily addressed factoid
questions of who, what ,when ,how many,a n d where – in particular, ones that can
be answered with a word or short phrase from the corpus. Little effort has goneinto discovering systematic ways of answering why and how questions, which usu-
ally require longer answers. This may primarily have to do with the problem ofevaluating such answers. And completely ignored are polar (‘yes’/‘no’) questionssuch as
(14) Is pinotage another name for pinot noir?(15) Is calcium citrate better absorbed and a more effective treatment for
osteoporosis than calcium carbonate?

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 645 — #16
Question Answering 645
Scenario 1: The al-Qaida Terrorist GroupYour division chief has ordered a detailed report on the al-Qaida Terrorist Group due in threeweeks. This report should present information regarding the most essential concerns, includ-ing who are the key ﬁgures involved with al-Qaida along with other organisations, countries,and members that are afﬁliated, any trades that al-Qaida has made with organisations orcountries, what facilities they possess, where they receive their ﬁnancial support, what capa-bilities they have (CBW program, other weapons, etc.) and how have they acquired them,what is their possible future activity, how their training program operates, who their newmembers are.
Figure 22.2 An ARDA scenario (from Small & Strzalkowski 2009).
because ﬁnding and presenting good evidence for an answer is more importantthan the answer itself and because the problem of assessing the quality of evidencehas not really been addressed.
Nevertheless, there are now efforts to move beyond factoid QA in order to
address questions that require information from more than a single source text orfrom multiple paragraphs within a text, possibly coupled with inference as above.These are called complex questions, the simplest form of which Bouma et al. (2005)
have called ‘which questions,’ as in
(16) Which ferry sank southeast of the island Utö?
Answering this requires combining evidence that some entity sank southeast ofthe island Utö with evidence that the entity is a ferry. Complex QA often requiresadditional knowledge to process (from the data, from a model of the world, orfrom the user), and is used to accumulate evidence to support a particular positionor opinion. This moves QA signiﬁcantly away from the text retrieval dominatedparadigm of factoid QA. The US ARDA AQUAINT program has been a majordriver in the development of systems to address complex questions, often in thecontext of an explicit task or scenario , such as in Figure 22.2.
Complex QA has been approached in different ways, two of which we describe
here: (1) decomposing the problem into subproblems and then dealing with eachone in turn; and (2) more complex document/passage indexing.
In question decomposition, a complex question is reduced to a set of subques-
tions, using linguistic and/or domain-speciﬁc knowledge. The subquestions aremeant to be ones that can be answered by existing factoid QA technology. Forexample, the START system (Katz 1997) syntactically decomposes the question
(17) Who was the third Republican president?
into the sequence of questions Q1 ( Who were the Republican presidents?), followed
by Q2 (Who is/was the third of them? ), where ‘them’ stands for the list of answers
to Q1. Saquete et al. (2004) perform similar syntactic decomposition in answeringtemporal questions such as

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 646 — #17
646 Bonnie Webber and Nick Webb
(18) Where did Bill Clinton study before going to Oxford University?
which gets split into the pair of questions Q1 ( Where did Bill Clinton study? )a n dQ 2
(When did Bill Clinton go to Oxford University?), with a constraint that the answer toQ1 fall in the period before the time associated with Q2.
The problem with syntactic decomposition is that it quickly becomes computa-
tionally expensive to generate and evaluate all syntactically legal decompositionsof a complex question. For the question
(19) What American companies have introduced a generic drug in a European
country?
possible subquestions include What companies have introduced a drug in a European
country?, What American companies have introduced a generic drug? ,a n ds oo n .O n e
really only wants to generate those which have answers in the corpus, and that isthe goal of the second way of addressing complex questions – complex indexing.
In Section 2.2, we mentioned various ways in which text is indexed for QA –
by words (or stems), by named entities, by part-of-speech tags, and by depen-dency relations. Even more complex indexing is used, either alone or withquestion decomposition, in the parameterized annotations used in START (Katz
et al., 2005) and elsewhere, as a way of answering complex questions. Parame-ters take the form of domain-speciﬁc templates with parameterized slots – such asthe following, from Katz et al. (2005):
In<year>, <group type>< group name> carried out a <event type> in
<country>, involving <agent type>< agent name>.
When used to index a passage, slots would be replaced by appropriate namedentities. Berrios et al. (2002) use similar templates to index paragraphs in medicaltextbooks, in order to answer common clinical questions, including:
(20) What is the <Pathophysiology/Etiology> of<Manifestation/Pathology >?
(21) How can <Pharmacotherapy >be used to treat <Disease/Syndrome>?
Here the parameters are disjunctions of Uniﬁed Medical Language System(UMLS) categories (www.nlm.nih.gov/research/umls/) that can be ﬁlled by aterm or phrase that belongs to any one of them.
Templates such as these function in a similar way to syntactic decomposition,
in that they can collect lists of responses for each subquery, then perform con-straint propagation to ﬁnd responses that satisfy all parts of the original query.However, these templates need to be built for each domain or data resource underconsideration by the QA system – a signiﬁcant overhead.
The LCC system (Lacatusu et al., 2005) achieves question decomposition in a
similar way. Top-down decomposition is driven by syntactic information. Bottom-up decomposition is achieved by using predictive question–answer pairs (stored

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 647 — #18
Question Answering 647
in a question–answer base, and referred to as QUABs) to expand the answer space.These QUABs are selected from a database of 10,000 question–answer pairs cre-ated ofﬂine by human annotators. So for a question such as What has been the
impact of job outsourcing programs on India’s relationship with the US?, those QUABswhich closely match the question, and are therefore adjudged to anticipate theuser’s information need, are presented to the user. Both methods of decomposi-tion are powerful approaches, but require a high degree of manual knowledgeconstruction – creating templates or the construction of question–answer pairs –in order to be effective. Some initial work has been undertaken to incorporateexisting large-scale knowledge sources, such as CYC (Curtis et al., 2005), intothe complex QA scenario, without yielding signiﬁcant improvements in eitherdomain-speciﬁc or open domain QA, possibly due to the incompleteness of theinformation represented in these knowledge sources.
3.3 Relation between user and system
Open domain QA has basically assumed a single correct answer to every factoidquestion, very much a reﬂection of its origins in IR, which abstracts individualrelevancy judgments away to those of an average user, to serve as a gold standard .
But it should be clear that different correct answers will be appropriate for dif-ferent users. The most obvious example of this comes in the degree of speciﬁcitydesirable in answering where is questions, such as:
(22) Where is the Verrazano-Narrows Bridge?(23) Where is the Three Gorges Dam?
where someone in North America might appreciate a speciﬁc answer to the ﬁrst(e.g., New York City, or between Brooklyn and Staten Island) and a general answerto the second (e.g., China), while for someone in Asia, the reverse might be true(e.g., New York, and Western Hubei Province).
Addressing another aspect of tailoring responses to users, Quarteroni and
Manandhar (2009) show how a user’s age and assumed reading level can be usedto choose the most appropriate among possible correct answers to a question.Alternatively, rather than trying to choose the most appropriate among possiblecorrect answers, a different approach would be to offer all of them, allowing usersto choose for themselves, as in the answer model for the question W h e r ei sG l a s g o w ?
given in Figure 22.3.
Dalmas and Webber (2007) show that a similar approach can be taken in pre-
senting answers to ambiguous questions, such as the famous question Where is the
Taj Mahal? (the tomb? the casino/hotel? the local Indian restaurant? etc.)
But the main way of enriching the relationship between user and QA system
involves the use of interaction. In Section 1.2 we described early systems that couldengage in dialogue in the process of providing answers to user questions. Suchsystems relied on complete descriptions of the target domain in order to choosethe next dialogue move, and so could only work in conceptually simple domains

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 648 — #19
648 Bonnie Webber and Nick Webb
ManchesterBritain
Glasgow London
GlasgowScotland London
Figure 22.3 An answer model for the question: Where is Glasgow? (Dalmas & Webber
2007), showing both Scotland and Britain as possible answers. Here →conveys a part-of
relation, and a dashed line, an equivalence relation.
such as travel planning (Goddeau et al., 1994; Seneff 2002), ﬁnding an appropri-ate restaurant (Walker et al., 2004), automated banking (Hardy et al., 2005), etc.In more complex applications, a complete description of the domain is unlikely,and systems must be able to ask questions of their own in order to work towardsproviding the user with the information he/she is after. On the other hand, as userquestions become more complex, it becomes more difﬁcult to anticipate all possi-ble ways of answering them, and systems require dialogue to help guide them towhat it is that the user wants. This is the issue that we focus on, under the rubricinteractive question answering orIQA.
IQA can be seen as a process in which the user is a continual part of the informa-
tion loop – as originator of the query, arbitrator over information relevance, andconsumer of the ﬁnal product. This mode of operation is useful for both factoidand complex QA, but perhaps provides greatest beneﬁt in those cases where theuser’s information need is still vague, or involves complex multifaceted concepts,or reﬂects misconceptions or opinions – all cases where the expected informationcontent returned is complex, with a degree of variability not present in factoidQA. Interactive QA systems borrow from dialogue systems their interaction, theiremphasis on completion of user task, their handling of incomplete or underspeci-ﬁed (as well as overspeciﬁed) user input and the constraint and relaxation phasesof the query process, while remaining focused on large or open domains, such aslaw, biomedicine, or international politics.
Rather than attempting to resolve complex question ambiguity independently
of either the user or the context, both can be used to generate follow-up queriesto the user that can serve as signiﬁcant directional tools to guide the informationseeking process. To achieve this requires an understanding of the context of theuser’s query and some knowledge about the domain of inquiry, or at least thedomain as represented by results returned by the speciﬁc query.
In the previous section, we described the use of predictive question–answer
pairs (QUABs) to expand the range of answerable questions. They are also used inthe interactive QA system FERRET (Harabagiu et al., 2005) to support interaction.When the user asks a question, those QUABs that relate to the query are offeredto the user, as a way of expanding the search space. These QUABs can address

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 649 — #20
Question Answering 649
not only subquestions decomposed from the original question, but also otherinformation that may be tangentially related to the question. However, this datais often created manually, and thus is an expensive overhead. HITIQA (Small &Strzalkowski, 2009) exempliﬁes an IQA system that does not rely on ap r i o r i knowl-
ege of the domain. Rather, HITIQA creates just-in-time semantic representationsof retrieved passages, using domain-independent ‘frames’, which are approxi-mations of the underlying relationships between entities. While less precise ortargeted than the parameterized annotations used in the START system (Katz et al.,2005) described in Section 3.2, they are sufﬁcient for capturing the essence of aparagraph.
A similar framing process is applied to the questions posed to the system, cre-
ating a goal frame. This allows for systematic comparison between the analyst’squestion and the retrieved text passages and for selection of instantiated framesfor answer passages. HITIQA uses interactive dialogue with the analyst to nego-tiate the scope of the answer, and experiments with several groups of analystsshowed that users accept system suggestions regarding the current question scopeas well as those that relate to other aspects of their task, thus speeding up theirwork. Dialogue is enabled through the frame-based representation of content andmore speciﬁcally through the mismatches between the goal frame and the dataframes in the answer space. This is best illustrated through the example interac-tion taken from a live demonstration to the ARDA AQUAINT community in 2005in Figure 22.4.
This interaction exploits the goal frame that was created for the question
(Figure 22.5) and the frames built for each of the retrieved passages (Figures 22.6and 22.7). The passages in Figures 22.6 and 22.7 are represented by one-conﬂictdata frames in the analyst’s answer space (i.e., where there is one conﬂict betweenthe data frame and the question frame). The conﬂicts on the relation attribute were,respectively: status vs. retirement and status vs. private accounts. These relations
Analyst: “What is the status of the Social Security System?”HITIQA: “Would you be interested in information on retirement relative to your question?”HITIQA: “Do you want to see information on private accounts?”HITIQA: “Thank you, Please view your answer”[final selection of passages are displayed]Analyst: “Yes”Analyst: “Yes”[exact match answer passages are displayed]
Figure 22.4 Example interaction taken from a live demonstration to the ARDA
AQUAINT community in 2005.
RELATIONS statusSocial Security System ORGANIZATION
Figure 22.5 Goal frame for the question: What is the status of the Social Security system?

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 650 — #21
650 Bonnie Webber and Nick Webb
Last week, the General Accounting Office, the auditing arm of Congress, issued a chilling report. Withoutbadly needed reform, Social Security is rapidly headed for financial ruin. By 2017, when baby boomersare well into retirement, the program will start running annual deficits that will eventually bankrupt thesystem, the GAO warned.
Social Security is sound for  today’s seniors and for those nearing retirement, but it needs to be fixed foryounger workers – our children and grandchildren. The goverment has made promises it cannot afford topay for with the current pay-as-you-go system.
RELATION retirementGeneral Accounting Office, Social Security, Congress, GAO2017
ORGANIZATION
RELATION retirement
Social Security ORGANIZATIONDATE
Figure 22.6 Two cluster seed passages and their corresponding frames relative to the
retirement clariﬁcation question.
One reform approach, favored by Bush, would let workers divert some of the 12.4% they and theiremployers pay in Social Security taxes into private accounts in exchange for cuts in guaranteed SocialSecurity benefits. In the short term, however, it would take $1 trillion out of the system, as taxes diverted tothe private accounts wouldn’t be available to pay benefits for current retirees.
On Jan. 11, Bush kicked off his new campaign by telling a town hall meeting that younger workers shouldbe able to take some of their payroll tax and “set it aside in the form of a personal savings account.” SocialSecurity only provides returns of about 2% a year after inflation, and private accounts, says the President,could top that easily if they were invested even partially in stocks.
RELATION private accountsSocial Security, GAOBush12.4%, 1 trillion
private accountsSocial Security, town hallBush2%Jan. 11ORGANIZATION
PERSONNUMBER
RELATIONORGANIZATION
PERSONNUMBERDATE
Figure 22.7 Two cluster passages and their corresponding frames relative to the private
accounts clariﬁcation question.

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 651 — #22
Question Answering 651
were obtained from the passage’s cluster relations: one cluster containing retire-ment (Figure 22.6) and the other cluster containing private accounts (Figure 22.7).From the system’s viewpoint each of these clusters represents an alternative inter-pretation of the user’s question about the status of Social Security, and so each isoffered to the user through automatically generated clariﬁcation questions. Thisdata-driven approach allows the system to work effectively in any domain andwith any text genre, without necessitating costly knowledge engineering.
3.4 Evaluating extended QA capabilities
As QA broadens and deepens to include the extensions described in this section,so evaluation methods need to develop so that we can continue to assess how wella method performs and/or how much better it is than the alternative(s).
In Section 2.4, we described some of the metrics used and the issues involved
in evaluating system performance on factoid questions, where it is assumed thatspeciﬁc gold standard answers can be identiﬁed and agreed upon prior to evalua-
tion. (See also Chapter 11,
EVALUATION OF NLP SYTEMS .) Here, for each way of
extending QA from where it is today, one can consider whether existing evaluationmethods sufﬁce or whether new ones are needed.
Extending the relation between question and corpus is a matter of textual entail-
ment (Section 3.1), so methods used in assessing correctness in that task should
apply here as well, provided that QA systems identify the minimal set of sen-tences within a document that entails the provided answer (rather than simply apointer to the document).
For answering why and how questions, nugget methods (Dang & Lin 2007)
should sufﬁce initially, though one might also want to assess temporal, causal,and/or subsumption relations that should be seen to hold between the nuggets.Evaluating performance on answering polar questions could also be taken to be a
matter of textual entailment, where providing the minimal set of evidential sen-tences would again be crucial for evaluation, not least for user acceptance. Forwhat Bouma et al. (2005) have called which questions, standard QA metrics should
sufﬁce, though, as with textual entailment, systems should probably also identifythe set of sentences (within a document or across multiple documents) that wereused in computing the answer.
It is in the more complex relation between user and system that one ﬁnds
a challenge for QA evaluation. But even here, there is a gradient. Quarteroniand Manandhar (2009) assess a QA system’s ability to tailor answers to a user’sage and reading level by asking outside assessors to provide judgments on theanswers. For simple information needs, such as a list of the names of the N upcom-ing conferences on language technology, along with their locations and acceptancerates (Bertomeu et al., 2006) – i.e., the sort of information that one might be able toget from a database if such a database existed – evaluation metrics developed indialogue system technology such as task completion, time to task completion, anduser satisfaction (Walker et al., 2000) would probably be equally effective here. But

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 652 — #23
652 Bonnie Webber and Nick Webb
unlike the setup where assessors are considered the ﬁnal consumers of answers,as they are in a gold standard model, here motivated users must become systemassessors.
This is even more true as user information needs become more complex and
users may not even know what information might be relevant. Here therehas been an attempt to develop objective evaluation criteria – in the ciQAtrack at TREC (www.umiacs.umd.edu/∼jimmylin/ciqa/), but see also iCLEF(Gonzalo & Oard 2004; Gonzalo et al., 2006, and ACLIA (Advanced Cross-lingualInformation Access) track (http://aclia.lti.cs.cmu.edu/wiki/moin.cgi/Home) inNTCIR. We want to argue, however, that ciQA and its evaluation criteria donot reﬂect the true character of users attempting to solve complex informationneeds.
A key element in a successful evaluation paradigm is the deﬁnition of the cen-
tral concepts. Evaluation metrics have gained increased prominence in recentyears, as they often inﬂuence the research agenda. Unfortunately, ciQA’s viewof complex interactive QA reﬂects TREC’s IR bias, and consequently fails to show-case the true potential of the interaction process. In particular, it assumes thatthe user’s information need does not have to be expressed via natural languagequeries: it can be expressed in a template that represents the question in canonicalform. Only additional context (the narrative ) is provided in natural language. For
example:
Template: What evidence is there for transport of <drugs> from <Bonaire> to
<the United States>?Narrative: The analyst would like to know of efforts made to discourage narco trafﬁckers
from using Bonaire as a transit point for drugs to the United States. Speciﬁcally, theanalyst would like to know of any efforts by local authorities as well as the internationalcommunity.
This narrative elaborates on the information need, providing context or a ﬁner-
grained statement of interest, or a focus on particular topical aspects. Whilethis narrative could be used automatically by systems that adopt a questiondecomposition approach, or could be exploited by a motivated user, to drive aninteraction around tangential issues surrounding the central concepts, ciQA eval-uation has not been designed to discriminate between any attention being paid tothe narrative or not.
This template approach seems similar to the use of such templates in some of
the decomposition approaches discussed in Section 3.2, but with a fundamentaldifference. Whereas those approaches attempt to process large amounts of datainto a form that has the potential to match a range of queries, here the templates areknown ap r i o r i , greatly reducing the problem space. Within IR, when evaluation
was ﬁxed on a known data set, and ﬁxed set of associated queries, performanceincreased over time, but again many approaches failed to scale when the data setincreased, or substantially changed in form and content.

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 653 — #24
Question Answering 653
A second problem with ciQA with respect to showcasing the true potential of
interaction is its limited deﬁnition of the term. In particular, it allowed only a sin-gle interaction (2006) or single ﬁve-minute period of interaction (2007). This singleinteraction or period of interaction reﬂects the mistaken view (deriving from anidealization adopted in IR) that interaction involves no more than reﬁning theoriginal query to better reﬂect the available data. However, as Wizard-of-Oz exper-iments into the use of interaction in QA continue to show (cf. Bertomeu 2007),success and satisfaction can depend on support for a variety of different inter-changes and answer strategies, including clariﬁcation subdialogues, explanationsubdialogues, responses that overanswer a question (providing more information
and more kinds of information than the user actually requested), being unawareof their relevance when the question was originally asked.
Since it is clear that neither the type of interaction supported in ciQA nor its
evaluation criteria (in terms of the ‘information nuggets’ contained in the systemresponses) are a fair representation of the capabilities of a true interactive system, itshould be no surprise that the results of the ﬁrst ciQA event were largely negative;standard IR approaches performed as well as complex QA approaches. Of the 12individual runs in 2006, only two scored higher than the manual baseline sentenceretrieval system. Thus Kelly and Lin (2007) note that ‘interaction doesn’t appearto help much (at present),’ although they acknowledge both that the evaluationdesign could be ﬂawed (in limiting interaction to a single turn, and limiting thetime for that interaction), and that the types of interaction deployed (a variant ofrelevance feedback) are not appropriate for this kind of task. Indeed, Lin (2007)points to a truism that should be very revealing. Current IR paradigms maximizerecall – by returning more of the same . Complex QA, in contrast, values novelty –
what information the user has not seen, that may present another aspect of thescenario. Some systems, such as HITIQA (Small & Strzalkowski 2009), explicitlyencode this assumption, remembering in the dialogue history both what usershave seen, and what they have indicated they like, versus what does not interestthem. This does not have to be explicit: interest can be gauged by a user copyinginformation into a report, or dismissing a window after just a few seconds, forinstance.
Perhaps a more useful indicator of evaluation is related work by Kelly et al.
(2009), which addresses the issue of instability of traditional evaluation metricsin multi-user environments, and describes the use of questionnaires to evaluate arange of IQA systems, as a method of garnering effective user feedback about thesystems themselves, and involving users in a subjective evaluation process. Keyis the ability to discriminate between the resulting systems on the basis of severalhypotheses, such as effort and efﬁciency. However, the assumption here, reﬂectinga fact that the ﬁeld will have to face fairly soon, is that effective evaluation of com-plex interactive QA systems can only truly be achieved by subjective evaluationsby motivated users. This is certainly more expensive, and less statistically clear cutthan previous evaluation paradigms for QA, but may lead us to real discoveries ofstrategies for complex interactive question answering.

“9781405155816_4_022” — 2010/5/8 — 12:17 — page 654 — #25
654 Bonnie Webber and Nick Webb
4 Further Reading
A survey of work on database QA can be found in Webber (1986), while themost comprehensive description of a database QA system remains Woods (1978).Prager (2007) provides the best introduction to open domain QA, while an incisivecomponent-by-component performance analysis of an open domain QA systemcan be found in Moldovan et al. (2003). For collections of papers on open domainQA, the reader is referred to Maybury (2003) and Strzalkowski and Harabagiu(2006), and to special issues of the Journal of Applied Logic (on Questions and
Answers: Theoretical and Applied Perspectives 5:1, March 2007) and of the Jour-
nal of Natural Language Engineering (cf. Webb & Webber 2009, a special issue on
interactive question answering). Papers on QA appear regularly in conferencessponsored by the Association for Computational Linguistics (ACL) and by theSpecial Interest Group on Information Retrieval (SIGIR).
NOTE
1 Even when pronoun resolution is not used to support enhanced indexing, but only
answer candidate extraction and/or evaluation, it has been shown to be of beneﬁt andworth the cost (Vicedo & Ferrández 2000).

