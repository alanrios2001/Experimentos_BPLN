“9781405155816_4_003” — 2010/5/8 — 11:41 — page 74 — #1
3 Statistical Language
Modeling
CIPRIAN CHELBA
Many practical applications such as automatic speech recognition, statisticalmachine translation, and spelling correction resort to variants of the well-established source-channel model for producing the correct string of words W
given an input speech signal, sentence in foreign language, or typed text withpossible mistakes, respectively. A basic component of such systems is a statis-tical language model which estimates the prior probability values for strings ofwords W.
1 Introduction to Statistical Language Modeling
A statistical language model estimates the prior probability values P(W)for
strings of words Win a vocabulary Vwhose size is in the tens, or hundreds of
thousands. Typically the string Wis broken into sentences, or other segments such
as utterances in automatic speech recognition, which are assumed to be condition-ally independent. For the rest of this chapter, we will assume that Wis such a
segment, or sentence.
Estimating full-sentence language models is computationally hard if one seeks
a properly normalized probability model
1over strings of words of ﬁnite length in
V∗. A simple and sufﬁcient way to ensure proper normalization of the model is
to decompose the sentence probability according to the chain rule and make surethat the end-of-sentence symbol </s> is predicted with non-zero probability in
any context. With W=w
1,w2,...,wnwe get:
(1) P(W)=n∏
i=1P(w i|w1,w2,...,wi−1)
Since the parameter space of P(w k|w1,w2,...,wk−1)is too large, the language
model is forced to put the context Wk−1=w1,w2,...,wk−1into an equivalence
class determined by a function Φ(Wk−1).A sar e s u l t ,

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 75 — #2
Statistical Language Modeling 75
(2) P(W)∼=n∏
k=1P(w k|Φ(Wk−1))
The word strings encountered in a practical application are of ﬁnite length. The
probability distribution P(W)should assign probability 0.0 to strings of words of
inﬁnite length, and thus sum up to 1.0 over the set of strings of ﬁnite length –the support of P(W). From a modeling point of view in a practical situation,
the text gets broken into sentences, and the language model needs to predict thedistinguished end-of-sentence symbol </s>. It can be easily shown that if the lan-
guage model is smooth, i.e., P(w
k|Φ(Wk−1))> ϵ > 0,∀wk,Wk−1, then we also have
P(</s> |Φ(Wk−1))> ϵ > 0,∀W k−1which in turn ensures that the model assigns
probability 1.0 to the set of word sequences of ﬁnite length.
Research in language modeling consists of ﬁnding appropriate equivalence
classiﬁers Φand methods to estimate P(w k|Φ(Wk−1)).
The most successful paradigm in language modeling uses the (n−1)-gram
equivalence classiﬁcation, that is, deﬁnesΦ(W
k−1).=wk−n+1 ,wk−n+2 ,...,wk−1
Once the form Φ(Wk−1)is speciﬁed, only the problem of estimating
P(w k|Φ(Wk−1))from training data remains.
In most cases, n=3, which leads to a trigram language model. The latter
has been shown to be surprisingly powerful and, essentially, all attempts toimprove on it in the last 30 years have failed. The one interesting enhancement,facilitated by maximum entropy estimation methodology, has been the use oftriggers (Rosenfeld 1994) or of singular value decomposition (Bellegarda 1997)(either of which dynamically identify the topic of discourse) in combination withn-gram models. Other widespread choices are class-based language models,which further restrict the equivalence class of the context to Φ(W
k−1)to use
explicit equivalence classes on words, [w]; one can also predict the next word
using its class: P(w k|[W k−1])=P([w k]|[W k−1])·P(w k|[wk]), if the class member-
ship[w]is a function of w– a case referred to as hard clustering; if the class mem-
bership is ambiguous – soft clustering – one needs to sum over all possible classassignments for both the context and predicted words.
1.1 Measures of language model quality
1.1.1 Perplexity A statistical language model can be evaluated by how well it
predicts a string of symbols Wt– commonly referred to as test data – generated by
the source to be modeled.
Assume we compare two models M1and M2using the same vocabulary2V.
They assign probability PM1(Wt)and PM2(Wt), respectively, to the sample test
string Wt. The test string has been neither used nor seen at the estimation step of
either model and it was generated by the same source that we are trying to model.‘Naturally,’ we consider M
1to be a better model than M2ifPM1(Wt)>P M2(Wt).

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 76 — #3
76 Ciprian Chelba
A commonly used quality measure for a given model Mis related to the entropy
of the underlying source and was introduced under the name of perplexity(PPL) (Jelinek 1997):(3) PPL(M) =exp(
−1
NN∑
k=1ln[PM(wk|Wk−1)])
To give intuitive meaning to perplexity, it represents the number of guesses themodel needs to make in order to ascertain the identity of the next word, whenrunning over the test word string from left to right. It can be easily shown thatthe perplexity of a language model that uses the uniform probability distributionover words in the vocabulary Vequals the size of the vocabulary; a good language
model should of course have lower perplexity, and thus the vocabulary size is anupper bound on the perplexity of a given language model.
Very likely, not all words in the test string W
tare part of the language model
vocabulary. It is common practice to map all words that are out-of-vocabulary to adistinguished unknown word symbol, and report the out-of-vocabulary (OOV)rate on test data – the rate at which one encounters OOV words in the teststring W
t– as yet another language model performance metric besides perplex-
ity. Usually the unknown word is assumed to be part of the language modelvocabulary – open vocabulary language models – and its occurrences are countedin the language model perplexity calculated in equation (3). A situation far lesscommon in practice is that of closed vocabulary language models where all wordsin the test data will always be part of the vocabulary V.
1.1.2 Task-speciﬁc measures In many practical applications the language
model is used as part of a larger statistical system, and the metrics for evaluatingsuch systems are dictated by the problem at hand. Typical applications that uselanguage models include, but are not restricted to: speech recognition, machinetranslation, spelling correction, case restoration (true-casing), spam ﬁltering, andother text classiﬁcation applications.
Although the language model performance in such a system is still reasonably
correlated with perplexity, this correlation is not always strong. A particular classof systems is that using the source-channel paradigm, e.g., speech recognition ormachine translation: given an input sequence Fof continuous or discrete valued
symbols, one wishes to determine the most likely word sequence Wthat would
give rise to it:(4) arg max
WP(W|F)=arg max
WP(W)·P(F|W )
For such situations the discriminative power of the language model relative to thechannel model P(F|W )is what matters most, rather than performance in isolation
on correct text, as measured by perplexity.

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 77 — #4
Statistical Language Modeling 77
An intuitive explanation is that, during the search for the maximum scor-
ing word sequence in equation (4), the decoder examines a large set of wordsequences, each being accompanied by the channel model score P(F|W ). Depend-
ing on the channel model quality, the language model may be asked the prob-ability of n-grams that are rarely seen on correct text, and yet they may receive
a reasonably high language model probability under the maximum likelihoodestimate, instead of a strong negative vote of conﬁdence.
Recent work demonstrates that signiﬁcant performance gains can be obtained
by training the language model in this way (Collins 2000). For the particularcase of speech recognition, Chelba (2006) proposes an alternative to perplex-ity named acoustic-sensitive perplexity as an objective function for evaluatinglanguage models used in a source-channel setup.
1.2 Smoothing
Since the language model is meant to assign non-zero probability to unseen stringsof words (or, equivalently, ensure that the cross-entropy of the model over anarbitrary test string is not inﬁnite), a desirable property is that:(5) P(w
k|Φ(Wk−1)) > ϵ > 0,∀wk,Wk−1
This is also known as the smoothing requirement.
A large body of work has accumulated over the years on various smooth-
ing methods for n-gram language models that ensure this to be true. The two
most widespread smoothing techniques are probably Kneser–Ney (1995) and Katz(1987). Goodman (2001) is an excellent overview that is highly recommended toany practitioner of language modeling.
A simple smoothing method for discrete probability models due to Jelinek and
Mercer (1980) is recursive linear interpolation among relative frequency estimatesof different orders f
k(·),k=0...nusing a recursive mixing scheme (see Figure 3.1).
LetUbe the vocabulary in which the predicted random variable u(not
necessarily a word) takes values.P
n(u|z 1,...,zn)=
λ(z1,...,zn)·Pn−1(u|z 1,...,zn−1)+(1−λ(z1,...,zn))·fn(u|z 1,...,zn),
P−1(u)=uniform (U)
where:•z
1,...,znis the context of order nwhen predicting u;

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 78 — #5
78 Ciprian Chelba
fn (u|z1 ... zn)
fn–1 (u|z1 ... zn–1)
f0 (u)
P–1 (u) = 1/|U|P0 (u)Pn–1 (u|z1 ... zn–1)Pn (u|z1 ... zn)
Figure 3.1 Recursive linear interpolation.
•fk(u|z 1,...,zk)is the order-k relative frequency estimate for the conditional
probability P(u|z 1,...,zk):
fk(u|z 1,...,zk)=C(u, z1,...,zk)/C(z 1,...,zk),k=0...n,
C(u, z1,...,zk)=∑
zk+1∈Zk+1...∑
zn∈ZnC(u, z1,...,zk,zk+1...zn),
C(z 1,...,zk)=∑
u∈UC(u, z1,...,zk);
•λ(z1,...,zk)∈[0, 1], k=0...nare the interpolation coefﬁcients.
Theλ(z1,...,zk)coefﬁcients are grouped into equivalence classes – tied – based
on the range into which the count C(z 1,...,zk)falls; the count ranges for each
equivalence class – also called buckets – are set such that a statistically sufﬁ-cient number of events (u|z
1,...,zk)fall in that range. The approach is a standard
one (Jelinek & Mercer 1980). In order to determine the interpolation weights, weapply the deleted interpolation technique:(1) split the training data in two sets – development and cross-validation respec-
tively;
(2) get the relative frequency – maximum likelihood – estimates
f
k(u|z 1,...,zk),k=0...nfrom development data;
(3) employ the expectation-maximization (EM) algorithm (Dempster et al., 1977)
for determining the maximum likelihood estimate from cross-validation dataof the tied interpolation weights λ(C(z
1,...,zk)).
The cross-validation data cannot be the same as the development data; if this werethe case, the maximum likelihood estimate for the interpolation weights would beλ(C(z
1,...,zk))=0, disallowing the mixing of different order relative frequency
estimates and thus performing no smoothing at all.
It is a simple exercise to cast such a language model as a back-off n-gram
model (Katz 1987).
As a ﬁnal comment on LM smoothing, most of the techniques currently in use
(Katz, Kneser–Ney, etc.) have been developed for tasks using relatively small

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 79 — #6
Statistical Language Modeling 79
ARPA LM format with back-off rule for probability calculation:p(wd3|wd1,wd2)=
if(trigram exists) p_3(wd1,wd2,wd3)
else if(bigram w1,w2 exists) bo_wt_2(w1,w2) *p(wd3|wd2)
else p(wd3|w2)
p(wd2|wd1)=
if(bigram exists) p_2(wd1,wd2)
else bo_wt_1(wd1) *p_1(wd2)
All probs and back-off weights (bo_wt) are given in log10 form.Everything before the beginning of the data mark is a commentBeginning of data mark: \data\ on a line by itselfEnd of data mark: \end\ on a line by itselfThe data block is thus encoded as follows:\datangram 1=nr # number of unigrams
ngram 2=nr # number of bigrams
ngram 3=nr # number of trigrams
\1-grams:p_1 wd bo_wt_1\2-grams:p_2 wd1 wd2 bo_wt_2\3-grams:p_3 wd1 wd2 wd3\end\
Figure 3.2 ARPA format for language model representation.
amounts of training data (1–100 million words). While the importance of LMsmoothing cannot be overemphasized, the impact of a particular choice for thesmoothing technique used in building a language model may become less impor-tant as large amounts of training data become available for a task of interest (Brantset al., 2007).
1.3 Language model representation in practice
A commonly accepted way of representing back-off n-gram models is the ARPA
format described in Figure 3.2, which we consider to be self-explanatory.
In many practical situations, including automatic speech recognition, it is
convenient to represent an n-gram language model as a ﬁnite state machine
(FSM) that drives the decoding (search) process. An excellent starting point is theOpenFst toolkit (Allauzen et al., 2007).
In such a representation, the transitions are labeled with words in the lan-
guage model vocabulary, and the costs on such arcs are the language modelprobabilities in an appropriate representation (usually as log-probabilities sincethat also has computational advantages in terms of the precision of ﬂoating point

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 80 — #7
80 Ciprian Chelba
operations); the states in the FSM are the n-gram contexts of the LM. We note that,
due to the smoothing constraint (see equation (5)), all VN−1contexts of length
N−1 would have to be represented, which is intractable for common vocabulary
sizes of 100,000 words or more. A widespread approximation used for represent-ing back-off language models as FSMs is to use as states only the contexts listedin the ARPA representation of the language model and to add back-off transitionswhose cost is the back-off weight. This reduces the LM state space drastically,at a small cost in modeling accuracy – the LM representation is not exact, andnon-deterministic.
As increasing amounts of training data become available, it becomes of par-
ticular interest to reduce the number of parameters, as well as efﬁciently storelanguage models. The number of n-grams in an LM can be reduced while having
the least possible impact on model perplexity by using pruning techniques. Theyrange from simple count cut-off pruning (discarding n-grams whose count in the
training data is below a certain threshold) to the more sophisticated entropy-basedpruning techniques (Seymore & Rosenfeld 1996; Stolcke 1998). The interactionbetween pruning and various smoothing techniques deserves a more carefulstudy, in particular for more aggressive pruning regimes. One such example is therapid deterioration of Kneser–Ney models with entropy pruning (Siivola et al.,2007).
The use of a trie for storing n-gram back-off language models is well estab-
lished: the CMU (Rosenfeld 1995), SRILM (Stolcke 2002) toolkits, as well as others(Whittaker & Raj 2001; Hsu & Glass 2008) all rely on it in one form or another.Its reﬁnement using array indexes instead of pointers for the trie representationis also an established idea – it was implemented in later versions of the CMU(Clarkson & Rosenfeld 1997) toolkit, as well as the more recent MITLM (Hsu &Glass 2008). The array sizes are known in advance and can be pre-allocated toavoid memory fragmentation.
The quantization of LogP/BoW values is also an established procedure for
reducing the storage requirements. The work in Whittaker and Raj (2001) appliesboth techniques, and in addition makes the important connection between LMpruning and compression/quantization. By representing LogP/BoWs on a vari-able number of bits (codewords) at each n-gram order, quantizing recursively
the differences between actual LogP value and quantized back-off estimate, andremoving redundant n-grams using a similar criterion to Stolcke (1998) and
Seymore and Rosenfeld (1996), the authors show that the LM performance on anASR task can be preserved while dramatically reducing the memory footprint ofthe model.
More recent approaches (Harb et al., 2009) enhance the standard use of integer
arrays to represent the trie by applying block compression techniques in order toreduce the storage requirements for both skeleton and payload. The compressionmethods used are lossless.
In a signiﬁcant departure from traditional techniques, randomized encoding
schemes (Talbot & Osborne 2007; Talbot & Brants 2008) achieve excellent compres-sion performance by using a lossy representation of the model. These can storeparameters in constant space per n-gram independent of either vocabulary size

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 81 — #8
Statistical Language Modeling 81
orn-gram order, but return an incorrect value for a ‘random’ subset of n-grams
of tunable size: the more errors allowed, the more succinct the encoding. In thecase of Talbot and Brants (2008), n-grams can also be looked up in constant time
independent of the compression rate. On the other hand, these schemes cannoteasily store a list of all future words for each n-gram context as required by certain
applications, e.g., when representing the language model as an FSM.
We conclude our introduction to language modeling (in particular n-gram
models) here. The rest of the chapter contains a presentation of the structured lan-guage model (Chelba & Jelinek 2000). This novel language modeling approachattempts to leverage n-gram modeling techniques in order to exploit the syn-
tactic structure exhibited by natural language. Section 2 outlines the underlyingprobabilistic model and its estimation from training data annotated with syntac-tic parse-tree information. Section 3 details experiments in an automatic speechrecognition setup, followed by Section 4 which presents reﬁnements to the orig-inal formulation in an attempt to better capture the syntactic dependencies inlanguage. Section 5 compares our approach to related ones, followed by conclu-sions suggesting research directions for language modeling, and the structuredlanguage model in particular.
2 Structured Language Model
As outlined in equation (2), a language model predicts the next word in a string ofwords based on an equivalence classiﬁcation of the word preﬁx Φ(W
k−1).
From a theoretical point of view, the ﬁnite-order Markov assumption on which
t h ee s t i m a t i o no fn-gram models rests is inadequate for modeling the dependen-cies in natural language. The main criticism is that such a model operates on thesurface of the word string, and ignores the more complex dependencies exhibitedin natural language syntax – best described using parse-trees. A more expressiveformal language that is able to take into account such dependencies is the class ofcontext-free grammars (CFGs).
The structured language model (SLM) we present addresses this problem by
using a parser for classifying the word preﬁx hierarchically and proposing severalpossible equivalence classiﬁcations Φ
l(Wk−1),l=1...Nfor a given word preﬁx
Wk−1, each accompanied by a probability P(Φl(Wk−1)|W k−1).
We wish to emphasize that the encoding for a word sequence together with a
parse-tree used by the SLM is different from that provided by a CFG. The excel-lent study in Abney et al. (1999) contrasts CFGs and the class of probabilisticpush-down automata (to which the SLM belongs) from a learning, and powerof expression, point of view.
The remaining part of the SLM presentation is structured as follows: the basic
idea behind the hierarchical method for organizing the word preﬁx is outlined inthe next section. One main constraint imposed on it by the incremental operationof the language model in a speech recognizer – see equation (1) – is that it has toproceed left to right through the word sequence.

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 82 — #9
82 Ciprian Chelba
ended_VBD cents_NNS aftercents_NPof_PP
loss_NPloss_NPended_VP’
with_PP
contract_NP
with_IN a_DT loss_NN of_IN 7_CD the_DT contract_NN
Figure 3.3 Partial parse.
Section 2.2 presents a model that assigns probability to each possible pair
consisting of a word sequence and parse. This is then used to assign probabil-ityP(Φ
l(Wk−1)|W k−1)to each equivalence classiﬁcation of Wk−1considered by
the model, and ﬁnally mix them to get a word-level probability P(w k|Wk−1),a s
described in Section 2.4. A few shortcuts are introduced to make the compu-tation feasible. Section 2.6 describes two successive stages of model parametersre-estimation. Finally, Section 2.7 presents experiments carried out on the UPennTreebank corpus and compares the results of our approach to those obtained fromthe 3-gram model.
2.1 Basic idea and terminology
Consider predicting the word after in the sentence: the contract ended
with a loss of 7 cents after trading as low as 9 cents .A 3 -
gram approach has to predict after from (7, cents) whereas it is intuitively
clear that the strongest predictor would be (contract, ended) which is
outside the reach of even 7-grams.
The linguistically correct partial parse of the word history when predicting
after is shown in Figure 3.3; the word ENDED is called the headword of the
constituent (ENDED ended ( WITH with (... ))) and ENDED is an exposed head-
word when predicting after – topmost headword in the largest constituent that
contains it. A model that uses the two most recent exposed headwords wouldpredict after from
CONTRACT, ENDED , in agreement with our intuition. Another intu-
itive argument in favor of the headword prediction is the fact that the headwordcontext is invariant to the removal of the (
OFof ( CENTS 7 cents)) constituent –
yielding a correct sentence – whereas the trigram predictor is not invariant to thistransformation.
Our working hypothesis is that syntactic structure ﬁlters out irrelevant words
and points to the important ones – exposed headwords – thus providing an equiv-alence classiﬁcation of the word preﬁx and enabling the use of long-distanceinformation when predicting the next word.

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 83 — #10
Statistical Language Modeling 83
h_0 = (h_0.word, h_0.tag) h_{–1} h_{–m} = (<s>, SB) h_{–2}
(<s>, SB) ....... (w_r, t_r) (w_{p –1}, t_{p –1}) (w_p, t_p) (w_{p+1}, t_{p+1}) ........ (w_k, t_k) w_{k+1}.... </s>
Figure 3.4 A word-and-parse k-preﬁx.
(<s>, SB) (w_1, t_1) ..................... (w_n, t_n) (</s>, SE)(</s>, TOP)
........(</s>, TOP’)
(</s>, TOP’)
Figure 3.5 Complete parse.
The SLM will attempt to build the syntactic structure incrementally while
traversing the sentence left to right. It will assign a probability to every wordsequence Wand parse T– every possible POS tag assignment, binary branching
parse, non-terminal label, and headword annotation for every constituent of T.
The probability assignment is based on a simple encoding of the (W,T)pair that
is described in the next section.2.1.1 Word sequence and parse encoding LetWbe a sentence of length n
words to which we have prepended <s> and appended </s> so that w
0=<s> and
wn+1=</s>.L e t Wk=w0...wkbe the word k-preﬁx of the sentence and WkTk
the word-and-parse k-preﬁx. To stress this point, a word-and-parse k-preﬁxcontains only those binary subtrees whose span is completely included inthe word k-preﬁx, excluding w
0=<s>. Single words along with their POS
tag can be regarded as root-only trees. Figure 3.4 shows a word-and-parsek-preﬁx; h_0...h_{-m} are the exposed heads, each head being a pair (head-
word, non-terminal label), or (word, POS tag) in the case of a root-only tree.A complete parse – Figure 3.5 – is deﬁned to be any binary parse of the(<s>, SB) (w
1,t1)...( wn,tn)(</s>,SE)3sequence with the restrictions that:
•(</s>, TOP) is the only allowed head;
•(w1,t1)...( wn,tn)(</s>, SE) forms a constituent headed by (</s>,
TOP’).
Note that in a complete parse (w1,t1)...( wn,tn)need not form a constituent4but,
for the parses where it does, there is no restriction on which of its words is theheadword or what the non-terminal label is that accompanies the headword. Toclarify this point, the constituents available at the time the end-of-sentence </s>
is predicted are attached to </s> under the TOP’ non-terminal tag. For example, a

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 84 — #11
84 Ciprian Chelba
valid SLM parse is one in which each word in the sentence is a separate constituentof span length one – corresponding to the regular n-gram model.
The model operates by means of three modules:
•WORD-PREDICTOR predicts the next word w
k+1given the word-and-parse
k-preﬁx and then passes control to the TAGGER;
•TAGGER predicts the POS tag of the next word tk+1given the word-and-
parse k-preﬁx and the newly predicted word and then passes control to theCONSTRUCTOR;
•CONSTRUCTOR grows the already existing binary branching structure byrepeatedly generating transitions from the set (unary, NTlabel), (adjoin-left,NTlabel), or (adjoin-right, NTlabel) until it passes control to the PREDICTORby taking a null transition. NTlabel is the non-terminal label assigned to thenewly built constituent and {left, right} speciﬁes where the new headword isinherited from.
5
The operations performed by the CONSTRUCTOR are illustrated in
Figures 3.6–3.8 and they ensure that all possible binary branching parses withall possible headword and non-terminal label assignments for the w
1...wkword
<s>T_{–m}h_{–1} h_0 h_{–2}
......... T_{–2} T_{–1} T_0
Figure 3.6 Before an adjoin operation.
...............T’_0
T_{–1} T_0<s>T’_{–1} <–T_{–2}h_{–1} h_0h’_{–1} = h_{–2}
T’_{–m+1}<–<s>h’_0 = (h_{–1}.word, NTlabel)
Figure 3.7 Result of adjoin-left under NTlabel.
...............T’_{–1} < – T_{–2} T_0h_0h_{–1}
<s>T’_{–m+1}<–< s >h’_{–1} = h_{–2}
T_{–1}h’_0 = (h_0.word, NTlabel)
Figure 3.8 Result of adjoin-right under NTlabel.

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 85 — #12
Statistical Language Modeling 85
PREDICTOR TAGGER
PARSERpredict word
tag word
adjoin_{left,right}null
Figure 3.9 Language model operation as a ﬁnite state machine.
Transition t; // a CONSTRUCTOR transition
predict (<s>, SB);do {
//WORD-PREDICTOR and TAGGERpredict (next_word, POStag);//CONSTRUCTORdo {
if (h_{-1}.word != <s>) {
if (h_0.word == </s>) {
t = (adjoin-right, TOP’);
} else {
if (h_0.tag is in set of NTlabels)
t = [(adjoin-{left,right}, NTlabel), null];
else
t = [(unary, NTlabel), (adjoin-{left,right}, NTlabel), null];
}
} else {
if (h_0.tag is in set of NTlabels)
t = null;
else
t = [(unary, NTlabel), null];
}
} while(t != null) //done CONSTRUCTOR
} while(!(h_0.word==</s> && h_{-1}.word==<s>))t = (adjoin-right, TOP); //adjoin <s>_SB; DONE;
Figure 3.10 SLM operation.
sequence can be generated. The ﬁnite state machine in Figure 3.9 presents asimpliﬁed operation of the model. Algorithm (10) below formalizes the abovedescription of the sequential generation of a sentence with a complete parse. Theunary transition is allowed only when the most recent exposed head is a leaf ofthe tree – a regular word along with its POS tag – hence it can be taken at mostonce at a given position in the input word string. The second subtree in Figure 3.4provides an example of the structure that results from a unary transition followedby a null transition.

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 86 — #13
86 Ciprian Chelba
It is easy to see that any given word sequence with a complete parse – see
Figure 3.5 – and headword annotation is generated by a unique sequence of modelactions.
6Conversely, a generative model running according to algorithm (10) can
only generate a complete parse.
2.2 Probabilistic model
The language model operation provides an encoding of a given word sequencealong with a parse-tree (W,T)into a sequence of elementary model actions. In
order to obtain a correct probability assignment P(W ,T)one has to simply assign
proper conditional probabilities to each transition in the ﬁnite state machine thatdescribes the model – see Figure 3.9.
The probability P(W ,T)of a word sequence Wand a complete parse Tcan be
calculated as:P(W ,T)=
n+1∏
k=1[
P(w k|Wk−1Tk−1)·P(tk|Wk−1Tk−1,wk)·P(
Tk
k−1⏐⏐W
k−1Tk−1,wk,tk)]
P(
Tk
k−1|Wk−1Tk−1)
=Nk∏
i=1P(
pki⏐⏐⏐W
k−1Tk−1,wk,tk,pk1...pki−1)
where:•W
k−1Tk−1is the word-parse (k−1)-preﬁx;
•wkis the word predicted by WORD-PREDICTOR;
•tkis the tag assigned to wkby the TAGGER;
•Tk
k−1is the parse structure attached to Tk−1that generates Tk=Tk−1∥Tk
k−1;w e
use∥to denote this particular concatenation operation;
•Nk−1 is the number of operations the CONSTRUCTOR executes at position
kof the input string before passing control to the WORD-PREDICTOR (the
Nk-th operation at position k is the null transition); Nkis a function of T;
•pkidenotes the i-th CONSTRUCTOR action carried out at position k in the word
string: pki∈{(adjoin-left, NTtag) ,(adjoin-right, NTtag)},1 ≤
i<Nk,pki=null, i=Nk.
Note that each(
Wk−1Tk−1,wk,tk,pk1...pki−1)
deﬁnes a valid word-parse
k-preﬁx WkTkat position kin the sentence, i=1...Nk.
To ensure a proper probabilistic model over the set of complete parses for any
sentence W, certain CONSTRUCTOR and WORD-PREDICTOR probabilities must
be given speciﬁc values:7
•P(null|W kTk)=1, ifh_{-1}.word = <s> andh_{0} ̸=(</s>, TOP’) –
that is, before predicting </s> – ensures that (<s>, SB) is adjoined in the
last step of the parsing process;

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 87 — #14
Statistical Language Modeling 87
•P((adjoin-right, TOP) |WkTk)=1, if h_0 = (</s>, TOP’) and
h_{-1}.word = <s>P((adjoin-right, TOP’)|W
kTk)=1, if h_0 = (</s>, TOP’) and
h_{-1}.word ̸=<s>
both ensure that the parse generated by our model is consistent with thedeﬁnition of a complete parse;
•∃ϵ>0s.t.∀W
k−1Tk−1,P(w k=</s>|W k−1Tk−1)≥ϵensures that the model
halts with probability 1, and thus is a proper probability model over stringsof words of ﬁnite length; once the end of sentence symbol </s>is gener-
ated, the model wraps up (completes) the parse with probability 1. In practicesmoothing (see equation (5)) makes sure this requirement is met.
2.2.1 Model component parameterization In order to be able to estimate the
model components we need to make appropriate equivalence classiﬁcations ofthe conditioning part for each component. The equivalence classiﬁcation shouldidentify the strong predictors in the context and allow reliable estimates from atreebank. Our choice relies heavily on exposed heads: the experiments in Chelba(1997) show that exposed heads are good predictors for the WORD-PREDICTORcomponent of the language model; Collins (1996) shows that they are useful forhigh accuracy parsing, making them the favorite choice for the CONSTRUCTORmodel as well; our experiments showed that they are also useful in the TAGGERcomponent model.
8
P(w k|Wk−1Tk−1)=P(w k|[W k−1Tk−1])=P(w k|h0,h−1) (6)
P(tk|wk,Wk−1Tk−1)=P(tk|wk,[Wk−1Tk−1])=P(tk|wk,h0.tag, h−1.tag) (7)
P(
pki|WkTk)
=P(
pki|[W kTk])
=P(
pki|h0,h−1)
(8)The above equivalence classiﬁcations are limited by the severe data sparsenessproblem faced by the 3-gram model and by no means do we believe that theycannot be improved upon, especially that used in CONSTRUCTOR model (8).Richer equivalence classiﬁcations should use a probability estimation method thatdeals better with sparse data than the one presented in Section 2.2.2.
It is worth noting that the 3-gram model belongs to the parameter space of our
model: if the binary branching structure developed by the parser were alwaysright-branching – the null transition has probability 1 in the CONSTRUCTOR
model – and we mapped the POS tag vocabulary to a single type, then our modelwould become equivalent to a trigram language model.2.2.2 Modeling tool All model components – WORD-PREDICTOR, TAG-
GER, CONSTRUCTOR – are conditional probabilistic models of the typeP(u|z
1,z2,...,zn)where u,z1,z2,...,znbelong to a mixed set of words, POS
tags, NTtags, and CONSTRUCTOR actions (u only). The smoothing technique
of Jelinek and Mercer (1980) has been used for estimating all models.

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 88 — #15
88 Ciprian Chelba
2.3 Pruning strategy
Since the number of parses for a given word preﬁx Wkgrows faster than exponen-
tial9with k,Ω(2k), the state space of our model is huge even for relatively short
sentences. We thus have to prune most parses without discarding the most likelyones for a given preﬁx W
k. Our pruning strategy is a synchronous multi-stack
search algorithm.
Each stack contains hypotheses – partial parses – that have been constructed
bythe same number of PREDICTOR and the same number of CONSTRUCTOR opera-
tions. The hypotheses in each stack are ranked according to the ln (P(W k,Tk))score,
highest on top. The ordered set of stacks containing partial parses with the samenumber of PREDICTOR operations but different number of CONSTRUCTORoperations is referred to as a stack-vector.
The amount of search is controlled by two parameters:
•the maximum stack depth – the maximum number of hypotheses the stack cancontain at any given time;
•log-probability threshold – the difference between the log-probability score ofthe topmost hypothesis and the bottommost hypothesis at any given state ofthe stack cannot be larger than a given threshold.
Figure 3.11 shows schematically the operations associated with the scanning of anew word w
k+1.10First, all hypotheses in a given stack-vector are expanded with
the following word. Then, for each possible POStag the following word can take,we expand the hypotheses further. Due to the ﬁnite stack size, some are discarded.We then proceed with the CONSTRUCTOR expansion cycle, which takes place intwo steps:(1) ﬁrst all hypotheses in a given stack are expanded with all possible CON-
STRUCTOR actions excepting the null transition. The resulting hypotheses
are sent to the immediately lower stack of the same stack-vector – samenumber of WORD-PREDICTOR operations and exactly one more CON-STRUCTOR move. Some are discarded due to ﬁnite stack size;
(2) after completing the previous step, all resulting hypotheses are expanded
with the null transition and sent into the next stack-vector. Pruning can still
occur due to the log-probability threshold on each stack.
2.4 Left-to-right perplexity
To maintain a left-to-right operation of the language model, the probabilityassignment for the word at position k+1 in the input sentence was made using:
P(w
k+1|Wk)=∑
Tk∈SkP(w k+1|WkTk)·ρ(Wk,Tk), (9)
ρ(Wk,Tk)=P(W kTk)/∑
Tk∈SkP(W kTk)

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 89 — #16
Statistical Language Modeling 89
(k) (k’) (k+1)
p parser opk+1 predict.
p+1 parser k+1 predict.P_k parserk+1 predict.
word predictorand tagger
parser adjoin/unary transitionsnull parser transitionsp parser op
k+1 predict.
p+1 parser 
k+1 predict.
P_k parser
k+1 predict.
P_k+1 parserk+1 predict.0 parser op
k+1 predict.0 parser op
k+1 predict.0 parser opk predict.
p parser opk predict.
p+1 parserk predict.
P_k parserk predict.
P_k+1parserk+1 predict.
Figure 3.11 One search extension cycle.
where Skis the set of all parses present in our stacks at the current stage k.T h i s
leads to the following formula for evaluating the perplexity:
L2R-PPL =exp(
−1/NN∑
i=1ln[
P(
wi|Wi−1)])
(10)
2.5 Separate left-to-right word predictor in the
language model
An important observation is that the next-word predictor probability P(w k+1|
WkTk)in (9) need not be the same as the WORD-PREDICTOR probability (6) used
to extract the structure Tk.T h u s P(w k+1|WkTk)can be estimated separately. To be
more speciﬁc, we can in principle have a WORD-PREDICTOR model componentthat operates within the parser model whose role is to strictly extract syntacticstructure and a second L2R-WORD-PREDICTOR model that is used only for theleft to right probability assignment:(11) P
2(wk+1|Wk)=∑
Tk∈SkPWP(wk+1|WkTk)·ρ(Wk,Tk),

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 90 — #17
90 Ciprian Chelba
ρ(Wk,Tk)=P(W kTk)/∑
Tk∈SkP(W kTk) (12)In this case the interpolation coefﬁcient given by (12) uses the regular WORD-PREDICTOR model whereas the prediction of the next word for the pur-pose of word-level probability assignment is made using a separate modelP
WP(wk+1|WkTk).
2.5.1 Initial parameters Each model component – WORD-PREDICTOR, TAG-
GER, CONSTRUCTOR – is initialized from a set of parsed sentences – a treebank –after the parses undergo headword percolation and binarization, as explainedbelow.
Using the same notation as in the previous section, each binary parse-tree
(W,T)with headword annotation is decomposed into its derivation d(W,T).
Separately for each m-th model component, we then:
•gather joint counts C
(m)(u(m),z(m))from the derivations that make up the
development data – about 90 percent of the training data;
•estimate the interpolation coefﬁcients on joint counts gathered from cross-validation data – the remaining 10 percent of the training data – using the EMalgorithm (Dempster et al., 1977). The buckets used for tying the interpolationweights are determined heuristically.
These are the initial parameters used with the re-estimation procedure
described in the previous section.2.5.1.1 Headword percolation In order to obtain training data for our model, we
need to binarize the UPenn Treebank-style parse-trees and percolate headwords(Marcus et al., 1993). The procedure used was to ﬁrst percolate headwords using acontext-free (CF) rule-based approach and then binarize the parses by again usinga rule-based approach. Inherently a heuristic process, we were satisﬁed with theoutput of an enhanced version of the procedure described in Collins (1996).
The procedure ﬁrst decomposes a parse tree from the treebank into its phrase
constituents, identiﬁed solely by the non-terminal/POS labels. Within each con-stituent we then identify the headword position and then, in a recursive thirdstep, we ﬁll in the headword position with the actual word percolated up from theleaves of the tree.
The headword percolation procedure is based on rules for identifying the head-
word position within each constituent. They are presented in Table 3.1.
11Let
Z→Y1...Ynbe one of the context-free (CF) rules that make up a given parse.
We identify the headword position as follows:•identify in the ﬁrst column of the table the entry that corresponds to the Z
non-terminal label;
•search Y
1...Ynfrom either left or right, as indicated in the second col-
umn of the entry, for the Yilabel that matches the regular expressions

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 91 — #18
Statistical Language Modeling 91
Table 3.1 Headword percolation rules
TOP right _SE _SBADJP right <∼QP|_JJ|_VBN|∼ADJP|_$|_JJR><ˆ∼PP|∼S|∼SBAR|_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB>
ADVP right <_RBR|_RB|_TO|∼ADVP><ˆ∼PP|∼S|∼SBAR|_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB>
CONJP left _RB <ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
FRAG left <ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
INTJ left <ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
LST left _LS <ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
NAC right <_NNP|_NNPS|∼NP|_NN|_NNS|∼NX|_CD|∼QP|_VBG><ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
NP right <_NNP|_NNPS|∼NP|_NN|_NNS|∼NX|_CD|∼QP|_PRP|_VBG><ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
NX right <_NNP|_NNPS|∼NP|_NN|_NNS|∼NX|_CD|∼QP|_VBG><ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
PP left _IN _TO _VBG _VBN ∼PP
<ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
PRN left ∼NP ∼PP ∼SBAR ∼ADVP ∼SINV ∼S∼VP
<ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
PRT left _RP <ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
QP left <_CD|∼QP>< _NNP|_NNPS|∼NP|_NN|_NNS|∼NX>< _DT|_PDT>
<_JJR|_JJ>< ˆ_CC|_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
RRC left ∼ADJP ∼PP ∼VP <ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
S right ∼VP <∼SBAR|∼SBARQ|∼S|∼SQ|∼SINV>
<ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
SBAR right <∼S|∼SBAR|∼SBARQ|∼SQ|∼SINV><ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
SBARQ right ∼SQ ∼S∼SINV ∼SBAR <ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
SINV right <∼VP|_VBD|_VBN|_MD|_VBZ|_VB|_VBG|_VBP >∼S∼SINV
<ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
SQ left <_VBD|_VBN|_MD|_VBZ|_VB| ∼VP|_VBG|_VBP>
<ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
UCP left <ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
VP left <_VBD|_VBN|_MD|_VBZ|_VB| ∼VP|_VBG|_VBP>
<ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
WHADJP right <ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
WHADVP right _WRB <ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
WHNP right _WP _WDT _JJ _WP$ ∼WHNP
<ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
WHPP left _IN <ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
X right <ˆ_.|_,|_”|_“|_‘|_’|_:|_LRB|_RRB >
listed in the entry; the ﬁrst matching Yiis going to be the headword of
the(Z(Y1...)...( Yn...)) constituent; the regular expressions listed in one
entry are ranked in left-to-right order: ﬁrst we try to match the ﬁrst one, ifunsuccessful we try the second one, and so on.

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 92 — #19
92 Ciprian Chelba
Z
Z’
Z’
Z’BZ
Z’
Z’
Z’A
Y_n Y_k Y_1 Y_n Y_k Y_1
Figure 3.12 Binarization schemes.
A regular expression of the type <_CD|~QP> matches any of the con-
stituents listed between angular parentheses. <. . .> are used for constituent
types which are desired as headwords, whereas <^ ... > are used for con-
stituent types which are not acceptable as headwords. For example, the<^_.|_,|_’’|_‘‘|_‘|_’|_:|_LRB|_RRB> regular expression will match any
constituent that is not– list begins with <^– among any of the elements in the list
between <^and>, in this case any constituent which is not a punctuation mark is
eligible to be a headword. The terminal labels – POS tags – have _prepended to
t h e m–a si n_CD; the non-terminal labels have the ~preﬁx – as in ~QP; |is merely
a separator in the list.2.5.1.2 Binarization Once the position of the headword within a constituent is
identiﬁed to be k, we binarize the constituent – equivalent with a CF production
of the type Z→Y
1,...Yn, where Z,Y1,...Ynare non-terminal labels or POS tags
(only Yican be a POS tag) – as follows: a ﬁxed rule is used to decide which of the
two binarization schemes in Figure 3.12 to apply depending only on the value ofZ. The intermediate nodes created by the above binarization schemes receive thenon-terminal label Z
′.
The choice among the two schemes is made according to the list of rules pre-
sented in Table 3.2, based on the identity of the label on the left-hand side of a CFrewrite rule. Notice that whenever k=1o rk =n– a case which is very frequent –
the two schemes presented above yield the same binary structure.
Another problem when binarizing the parse-trees is the presence of unary pro-
ductions. Our model allows unary productions of the type Z→Yonly, where Z
is a non-terminal label and Yis a POStag. The unary productions Z→Ywhere
both Zand Yare non-terminal labels were deleted from the treebank, only the Z
constituent being retained: (Z (Y (.) (.))) becomes (Z (.) (.)).
Binarization brings the training data parse-trees to Chomsky normal form,
and renders them suitable for the next stage of parameter estimation in ourstatistical model. Each tree is fed as input to the ﬁnite state machine that describesthe model – see Figure 3.9. Each transition generates an n-gram event for one of
the model components.
2.6 Model parameter re-estimation
As outlined in Section 2.4, the word-level probability assigned to a training/testset by our model is calculated using the proper word-level probability assignment

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 93 — #20
Statistical Language Modeling 93
Table 3.2 Binarization rules
## first column : constituent label## second column: binarization type : A or BTOP AADJP BADVP BCONJP AFRAG AINTJ ALST ANAC BNP BNX BPP APRN APRT AQP ARRC ASBSBAR BSBARQ BSINV BSQ AUCP AVP AWHADJP BWHADVP BWHNP BWHPP AXB
in equation (9). An alternative which leads to a deﬁcient probability model is tosum over all the complete parses that survived the pruning strategy.
The estimation procedure of the SLM parameters takes place in two stages:
(1) the N-best training algorithm (see Section 2.6.1) is employed to increase
the training data ‘ likelihood’ calculated using the deﬁcient sum-probability
assignment. The initial parameters for this ﬁrst estimation stage are gath-ered from a treebank. The perplexity is still evaluated using the formula inequation (9);
(2) estimate a separate L2R-WORD-PREDICTOR model such that the likelihood
of the training data according to the probability assignment in equation (11)is increased. The initial parameters for the L2R-WORD-PREDICTOR compo-nent are obtained by copying the WORD-PREDICTOR estimated at stage one.

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 94 — #21
94 Ciprian Chelba
As a ﬁnal step in reﬁning the model we have linearly interpolated the structured
language model (9) with a trigram model.
Section 2.6.1 presents the basic idea behind the N-best training stage.
Section 2.6.2 presents the training of a separate L2R-WORD-PREDICTOR model –the second re-estimation stage.2.6.1 N-best EM re-estimation We would like to estimate the model compo-
nent probabilities (6–8) such that the likelihood of the training data is increased.Since our problem is one of maximum likelihood estimation from incompletedata – the parse structure along with POS/NT tags and headword annotation fora given observed sentence is hidden – our approach makes use of the expectation-maximization algorithm (EM) (Dempster et al., 1977). Two speciﬁc modiﬁcationswe make are:•E-step: instead of scanning all the hidden events allowed – parses T–f o ra
given observed one – sequence of words W– we restrict the algorithm to oper-
ate with N-best hidden events;
12the N-best are determined using the search
strategy described in Section 2.3. For a presentation of different modiﬁcationsto the EM, the reader is referred to Byrne et al. (1998).
•M-step: assuming that the count ranges and the corresponding interpolationvalues for each order are kept ﬁxed to their initial values – see Section 2.5.1 –the only parameters to be re-estimated using the EM algorithm are the maximalorder counts C
(m)(u,z1,...,zn)for each model component. The interpolation
scheme outlined in Section 2.2.2 is then used to obtain a smooth probabilityestimate for each model component.
The derivation of the re-estimation formulas, the initial parameter values andfurther comments and experiments on the ﬁrst model re-estimation stage arepresented in Chelba and Jelinek (2000).2.6.2 Second stage parameter re-estimation Once the model is trained accord-
ing to the procedure described in the previous section, we proceed into asecond stage of parameter re-estimation. In order to improve performance, wedevelop a model to be used strictly for word prediction – see (11) – differentfrom the WORD-PREDICTOR model (6). We will call this new component theL2R-WORD-PREDICTOR.
In order to train this fourth model component, the key step is to recognize in (11)
a hidden Markov model (HMM) with ﬁxed transition probabilities – althoughdependent on the position in the input sentence k– speciﬁed by the ρ(W
k,Tk)
values.
The E-step of the EM algorithm (Dempster et al., 1977) for gathering joint
counts C(m)(y(m),x(m)), L2R-WORD-PREDICTOR-MODEL, is the standard one
whereas the M-step uses the same count smoothing technique as that describedin Section 2.6.1. The second re-estimation step operates directly on the L
L2R(C,Pθ)
likelihood.

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 95 — #22
Statistical Language Modeling 95
The second re-estimation pass is seeded with the WORD-PREDICTOR model
joint counts C(m)(y(m),x(m))resulting from the ﬁrst parameter re-estimation pass
(see Section 2.6.1).
2.7 UPenn Treebank perplexity results
During the original development of the SLM we chose to work on the UPennTreebank corpus (Marcus et al., 1993) – a subset of the WSJ (Wall Street Journal) cor-
pus. This is a well-known corpus and the existence of a manual treebank makes itideal for our experiments.
Unless speciﬁed otherwise in a speciﬁc section, the vocabulary sizes were:
(1) word – also WORD-PREDICTOR operation – vocabulary: 10,000, open – all
words outside the vocabulary are mapped to the <unk> token;
(2) POStag – also TAGGER operation – vocabulary: 40,000, closed;(3) non-terminal tag vocabulary: 52,000, closed;(4) CONSTRUCTOR operation vocabulary: 107,000, closed.
The training data was split into:
(1) development set (929,564 words (sections 00–20));
(2) cross-validation set (73,760 words (sections 21–2));
(3) testset (82,430 words (sections 23–4).
The development and cross-validation sets were used strictly for initializing the
model parameters as described in Section 2.5.1 and then with the re-estimationtechniques described in Sections 2.6.1 and 2.6.2.
The parameters controlling the search – see Section 2.3 – were set to:
maximum-stack-depth =1 0a n d LnP-threshold = 6.91.
As explained in Section 2.6.1, the ﬁrst stage of model parameter re-estimation
re-evaluates the maximal order counts for each model component.
Each iteration involves parsing the entire training data which is a time-
consuming process – about 60 hours of Sun Sparc Ultra-2 CPU-time. Table 3.3shows the results of the re-estimation iterations; E0-3 denote iterations of the
re-estimation procedure described in Section 2.6.1; L2R0-5 denote iterations of
the re-estimation procedure described in Section 2.6.2. A deleted interpolation tri-gram model derived from the same training data had perplexity 167.14 on thesame test data.
Simple linear interpolation between our model and the trigram model:
Q(w
k+1/Wk)=λ·P(w k+1/wk−1,wk)+(1−λ)·P(w k+1/Wk)
yielded a further improvement in PPL, as shown in Table 3.4. The interpolationweight was estimated on check data to be λ=0.36. An overall relative reduction
of 11 percent over the trigram model has been achieved.

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 96 — #23
96 Ciprian Chelba
Table 3.3 Parameter re-estimation results
Iteration DEV set TEST set
number L2R-PPL L2R-PPL
E0 24.70 167.47
E1 22.34 160.76
E2 21.69 158.97
E3 21.26 158.28
L2R0 (=E3) 21.26 158.28
L2R5 17.44 153.76
Table 3.4 Interpolation with trigram results
TEST set PPLIteration
number λ=0 . 0 λ=0 . 3 6 λ=1 . 0
E0 167.47 152.25 167.14
E3 158.28 148.90 167.14
L2R0 (=E3) 158.28 148.90 167.14L2R5 153.76 147.70 167.14
2.7.1 Maximum depth factorization of the model The word-level probability
assignment used by the SLM can be thought of as a model factored over differentmaximum reach depths. Let D(T
k)be the depth in the word preﬁx Wkat which the
headword h−1.word can be found:
(13) P(w k+1|Wk)=d=k∑
d=0P(d|W k)·P(w k+1|Wk,d),
where:
P(d|W k)=∑
Tk∈Skρ(Wk,Tk)·δ(D(T k),d)
P(w k+1|Wk,d)=∑
Tk∈SkP(T k|Wk,d)·P(w k+1|Wk,Tk)
P(T k|Wk,d)=ρ(Wk,Tk)·δ(D(T k),d)/P(d|W k)
We can interpret equation (13) as a linear interpolation of models that reach backto different depths in the word preﬁx W
k. The expected value of D(T k)shows how

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 97 — #24
Statistical Language Modeling 97
Table 3.5 Maximum depth evolution
during training
Iteration number Expected depth E[D]
E0 3.35
E1 3.46
E2 3.45
far the SLM reaches in the word preﬁx:
ESLM[D]=1/Nk=N∑
k=0d=k∑
d=0d·P(d|W k) (14)
For the 3-gram model we have E3−gram [D]=2. We evaluated the expected depth
of the SLM using the formula in equation (14). The results are presented inTable 3.5.
It can be seen that the memory of the SLM is considerably higher than that of
the 3-gram model – whose depth is 2.
Figure 3.13 shows
13the distribution P(d|W k), averaged over all positions kin
the test string:P(d|W )=1/N
N∑
k=1P(d|W k)
It can be seen that the SLM makes a prediction which reaches farther than the
3-gram model in about 40 percent of cases, on the average.2.7.1.1 Non-causal ‘Perplexity’ Attempting to calculate the conditional perplex-
ity by assigning to a whole sentence the probability:
P(W|T
∗)=n∏
k=0P(
wk+1⏐⏐W
kT∗
k)
, (15)where T
∗=argmax TP(W ,T)– the search for T∗being carried out according to
our pruning strategy – is not valid because it is not causal: when predicting wk+1
we would be using T∗which was determined by looking at the entire sentence.
In order to have a valid perplexity calculation we would need to factor in theuncertainty of guessing the preﬁx of the ﬁnal best parse T
∗
kbefore predicting w k+1,
based solely on the word preﬁx Wk.
However, the perplexity value calculated using (15) is an indication of the lower
bound for the achievable perplexity of our model; for the above search parametersand E0 model statistics this bound was 98, corresponding to a relative reductionof 40 percent over the perplexity of the 3-gram model. For comparison, the value

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 98 — #25
98 Ciprian Chelba
0 5 10 15 20 2500.10.20.30.40.50.60.7
depthP(depth)Depth distribution according to P(T/W)
E[depth(E0)] = 3.35
E[depth(E1)] = 3.46
Figure 3.13 Structured language model maximum depth distribution.
when conditioning on the manual parses in the UPenn Treebank – which was usedto get the E0 statistics – was 115. This shows that the parses found by our modelare better predictors – under the exposed heads parameterization – than those inthe treebank.
This suggests that a better parameterization in the SLM – one that reduces
the entropy H(ρ(T
k|Wk))of guessing the ‘ good ’ parse given the word preﬁx –
would lead to a better model. Indeed, as we already pointed out, the trigrammodel is a particular case of our model for which the parse is always right-branching and we have no POS/NT tag information, leading to H(ρ(T
k|Wk))=0
and a standard 3-gram WORD-PREDICTOR. The 3-gram model is thus anextreme case of the structured language model: one for which the ‘ hidden’
structure is a function of the word preﬁx. Our result shows that better mod-
els can be obtained by allowing richer ‘hidden’ structure – parses – and thata promising direction of research is to ﬁnd the best compromise between thepredictive power of the WORD-PREDICTOR – measured by H(w
k+1|Tk,Wk))–
and the ease of guessing the most desirable hidden structure Tk|Wk–
measured by H(ρ(T k|Wk))– on which the WORD-PREDICTOR operation is based.
The re-estimation procedure we presented is one such method.

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 99 — #26
Statistical Language Modeling 99
3 Speech Recognition Lattice Rescoring Using the
Structured Language Model
The SLM was developed primarily for use in speech recognition. A simple way toevaluate a complex language model in such applications is a two-pass recognitionapproach:•a computationally cheap decoding step is run as the ﬁrst pass;
•a set of hypotheses is retained as an intermediate result;
•a more sophisticated recognizer is run over these in a second pass – usuallyreferred to as the rescoring pass.
The search space in the second pass is much more restricted compared to the ﬁrstpass so we can afford to use better – usually also computationally more intensive –acoustic and/or language models.
The two most popular two-pass strategies differ mainly in the number of inter-
mediate hypotheses saved after the ﬁrst pass and the form in which they arestored.
In the so-called N-best rescoring method,
14a list of complete hypotheses along
with acoustic/language model scores are retained and then rescored using morecomplex acoustic/language models.
Due to the limited number of hypotheses in the N-best list, the second pass
recognizer might be too constrained by the ﬁrst pass so a more comprehensivelist of hypotheses is often needed. The alternative preferred to N-best list rescor-ing is lattice rescoring (Aubert et al., 1994). The intermediate format in which thehypotheses are stored is a directed acyclic graph in which the nodes are a subsetof the language model states in the composite hidden Markov model and the arcsare labeled with words. Typically, the ﬁrst pass acoustic/language model scoresassociated with each arc – or link – in the lattice are saved and the nodes containtime alignment information.
Compared to N-best lists, lattices typically offer a higher density of paths for a
given graph size (measured in number of arcs/nodes). For both cases one can cal-culate the ‘oracle’ word error rate (WER): the word error rate along the hypothesiswith the minimum number of errors. The oracle WER decreases with the numberof hypotheses saved; thinking of it as an ‘ achievable’ WER is misleading, since there
may not exist an AM/LM in the class of models used that can actually select theword sequence attaining it.
Of course, a set of N-best hypotheses can be assembled as a lattice, the difference
between the two being just in the number of different hypotheses – with differ-ent time alignments – stored in the lattice. One reason which makes the N-bestrescoring framework attractive is the possibility to use ‘ whole sentence’ language
models: models that are able to assign a score only to complete sentences dueto the fact that they do not operate in a left-to-right fashion. The drawbacks arethat the number of hypotheses explored is too small and their quality reﬂects the

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 100 — #27
100 Ciprian Chelba
models used in the ﬁrst pass. To clarify the latter assertion, assume that the secondpass language model is dramatically different from the one used in the ﬁrst passand that if we extracted the N-best list using the second (better) language model,different kinds of errors, speciﬁc to this language model, would be observed. Inthat case, simple rescoring of the N-best list generated using the weaker languagemodel may prevent the stronger language model from showing its merits.
It is thus desirable to have as complete a sample as possible of the possible word
hypotheses – not biased towards a given model – and one of a manageable size.This is what makes lattice rescoring the chosen method in our case.
There are several reasons that make A
∗appealing for lattice decoding using the
SLM:•the lattice can be conceptually structured as a preﬁx tree of hypotheses –the time alignment is taken into consideration when comparing two-wordpreﬁxes;
•the algorithm operates with whole preﬁxes x, making it ideal for incorporating
language models whose memory is the entire utterance preﬁx;
•a reasonably good overestimate h(y|x) and an efﬁcient way to calculate h
L(x)
are readily available using the n-gram language model.
We have applied the SLM for rescoring lattices on both Wall Street Journal (WSJ )
and Switchboard (SWB). The word error rate reduction over a state-of-the-art3-gram model was:•1% absolute (10.6% to 9.6%, 9% relative) on WSJ;
•1% absolute (41.3% to 40.3%, 2% relative) on SWB.
We have also evaluated the perplexity reduction relative to a standard deletedinterpolation 3-gram trained under the same conditions as the SLM. We have achieved
a relative reduction in PPL of 15 percent and 5 percent on WSJ and SWB
respectively.
4 Richer Syntactic Dependencies
The statistical parsing community have used various ways of enriching the depen-dency structure underlying the parameterization of the probabilistic model usedfor scoring a given parse-tree (Collins 1999; Charniak 2000). Recently, such mod-els (Charniak 2001; Roark 2001a) have been shown to outperform the SLM interms of both PPL and WER on the UPenn Treebank and WSJ corpora respec-
tively. In Chelba (2001), a simple way of enriching the probabilistic dependenciesin the CONSTRUCTOR component of the SLM also showed better PPL and WERperformance; the simple modiﬁcation to the training procedure brought the WERperformance of the SLM to the same level with the best reported in Roark (2001a).

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 101 — #28
Statistical Language Modeling 101
160180PPL
121314WER
204060LR−error
2030LP−error
baseline PA OP OP+PA h–2+PA h–2+OP h–2+OP+PA h–2
Figure 3.14 Comparison of PPL, WER, labeled recall/precision error.
The work reported in Xu et al. (2002) presents three simple ways of enriching
the syntactic dependency structure in the SLM, extending on the work in Chelba(2001). The results show that indeed a good parser (as measured by LP/LR) ishelpful in reducing the PPL and WER. Another remarkable fact is that a lan-guage model exploiting elementary syntactic dependencies obviates the need for a3-gram model in N-best rescoring.
In particular, the best-performing enriching scheme achieved a 0.4 percent abso-
lute WER reduction over the performance of the standard SLM. Overall, theenriched SLM achieves 10 percent relative reduction in WER over the 3-grammodel baseline result.
The enriched SLM outperformed the 3-gram used to generate the lattices and
N-best lists, without interpolating it with the 3-gram model. Although the N-bestlists are already highly restricted by the 3-gram model during the ﬁrst recognitionpass, this fact still shows the potential of a good grammar-based language model,especially when the SLM was trained on 20 million words of WSJ, while the lattice
3-gram model was trained on 45 million words of WSJ text. Our results are not
indicative of the performance of SLM as a ﬁrst pass language model.
It is very interesting that labeled recall and language model performance
(WER/PPL) are well correlated. Figure 3.14 compares PPL, WER ( λ=0.0 at train-
ing iteration 0), and labeled precision/recall error (100-LP/LR) for all models.Overall, the labeled recall is well correlated with the WER and PPL values. Webelieve that these results show that improvements in parser accuracy are expectedto lead to improvements in WER.

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 102 — #29
102 Ciprian Chelba
5 Comparison with Other Approaches
The SLM shares many features with both class-based language models (Brownet al., 1992) and skip n-gram language models (Rosenfeld 1994); an interesting
approach combining class-based language models and different order skip-bigrammodels is presented in Saul and Pereira (1997). It seems worthwhile to make twocomments relating the SLM to these approaches.
The smoothing involving NT/POS tags in the WORD-PREDICTOR is similar
to a class-based language model using NT/POS labels for classes. We depart,however, from the usual approach by not making the conditional independence
assumption P(w
k+1|wk,c l a s s (wk))=P(w k+1|class(wk)). Also, in our model, the ‘ class
assignment’ – through the heads exposed by a given parse Tkfor the word preﬁx
Wkand its ‘weight’ ρ(Wk,Tk), see equation (9) – is:
•highly context-sensitive – it depends on the entire word preﬁx Wk;
•is randomized – a few equivalence classes are extracted from each context, eachweighted by ρ(W
k,Tk);
•and is syntactically motivated through the operations of the CONSTRUCTOR.
We also found the POS/NT labels in the PREDICTOR equivalence classiﬁcation tobe useful for better word prediction.
Recalling the depth factorization of the model in equation (13), our model can
be viewed as a skip n-gram where the probability of a skip P(d
0,d1|Wk)–d0,d1are
the depths at which the two most recent exposed headwords h0,h1can be found,
similar to P(d|W k)– is highly context-sensitive. Note that the hierarchical scheme
for organizing the word preﬁx allows for contexts that do not necessarily consistof adjacent words, as in regular skip n-gram models.
6 Conclusion
Among the directions which we consider worth exploring in the future forimproving the structured language model, there are:•automatic induction of the SLM initial parameter values;
•study of other binarization schemes, in particular left-corner ones promise tobe extremely suitable to the SLM operation;
•better integration of the 3-gram model and the SLM;
•better parameterization of the model components;
•study of the interaction between SLM and other language modeling techniquessuch as cache and trigger or topic language models.
In the broader scope, a couple of important and promising directions in
statistical language modeling are:

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 103 — #30
Statistical Language Modeling 103
•language model adaptation: how to leverage data sources that may not befully matched to a scenario of interest; ﬁnding relevant data to be used in anadaptation setup (Berger & Miller 1998) is of course equally important;
•discriminative training for language models – n-gram or more general ones –
used in a source-channel paradigm.
Of particular interest is a direction worth highlighting separately: scalability.
Scaling to very large training data sets and language models, along the linesof Brants et al. (2007), is a signiﬁcant disruption as far as modeling techniquesare concerned. Many techniques developed on small to medium data sets (10–100 million words) may lose their edge, while others prove valuable by beingmore effective learners. Computational issues aside, the rate at which a givenmodel improves when presented with increasing amounts of relevant trainingdata becomes a critical aspect in such a data-rich regime. Despite its simplis-tic treatment of natural language, the n-gram model is extremely successful in
practice, not least because of its ability to easily make use of large amounts ofdata. Techniques that attempt to model language better need to balance modelingpower with simplicity in order to scale well with increasing amounts of data.
Finally, it is worth putting in perspective the fact that in a source-channel
approach (see equation (4)), the language model is nothing more than a prior guess
on the word sequence W, which completely disregards the input signal F.A s
such, its ability to reduce the conditional entropy H(W|F)is expected to be lim-
ited, especially when the channel model is poor and provides limited informationabout the Wstring that gave rise to the signal F. An intuitive explanation is that
the language model is our best guess for what the user might say or type at theinput of an automatic speech recognition or statistical machine translation systemrespectively, before the input speech, or sentence F, is revealed!
From this point of view, it is unrealistic to expect a large impact on performance
coming from the language model alone; it can do so to the extent that it comple-ments the channel model well. Speculatively comparing the impact of similar-sizelanguage models across different applications (ASR, SMT, spelling correction) andsimilar tasks seems to support this view. The SMT noisy channel constrains thechoice of output words much more than the ASR one: the latter starts from a pres-sure wave instead of a sentence in foreign language. As such, the sensitivity ofSMT accuracy to LM performance is generally higher than in ASR. The same rela-tionship seems to hold when comparing similar LMs used in spelling correctionand SMT, the former being a more constrained noisy channel.
ACKNOWLEDGMENT
Parts of this chapter appeared in Computer Speech & Language 14(4):283–332, October 2000,
and are used with the permission of Elsevier Limited.

“9781405155816_4_003” — 2010/5/8 — 11:41 — page 104 — #31
104 Ciprian Chelba
NOTES
1 We note that in some practical systems the constraint on using a properly normalized
language model is sidestepped at a gain in modeling power and simplicity.
2 Language models estimated on different vocabularies cannot be directly compared
using perplexity, since they model completely different probability distributions.
3 SB/SE is a distinguished POS tag for sentence begin and end respectively.4 The set of complete parses is a superset of the parses in the UPenn Treebank which
insist that (w
1,t1)...( wn,tn)forms a constituent.
5 Obviously, the headword origin after a unary transition is fully determined.6 This will prove very useful in initializing our model parameters from a treebank – see
Section 2.5.1.
7 The set of constraints on the probability values of different model components is
consistent with algorithm 10.
8 Since the word to be tagged is in itself a very strong predictor for the POS tag, we limit
the equivalence classiﬁcation of the TAGGER model to include only the NTlabels ofthe two most recent exposed heads.
9 Thanks to Bob Carpenter, Lucent Technologies Bell Labs, for pointing out this
inaccuracy in our paper (Chelba & Jelinek 1998).
10 P
kis the maximum number of adjoin operations for a k-length word preﬁx; since the
tree is binary we have Pk=k−1.
11 The origin of this table of rules is not clear, they are attributed to Magerman and Black.12 For a better understanding, this procedure can be thought of as analogous to a com-
promise between the forward-backward and Viterbi re-estimation for hidden Markovmodels.
13 The non-zero value of P(1|W )is due to the fact that the prediction of the ﬁrst word in
a sentence is based on context of length 1 – sentence begin – in both SLM and 3-grammodels.
14 The value of N is typically 100–1,000.

