“9781405155816_4_010” — 2010/5/8 — 11:55 — page 238 — #1
10 Linguistic Annotation
MARTHA PALMERAND NIANWEN XUE
1 Introduction
The recent availability of linguistically annotated electronic text has revolution-ized the ﬁelds of natural language processing and machine translation. Thecreation of the Penn Treebank (Marcus et al., 1993) and the word-sense annotatedSemcor (Miller 1995; Fellbaum et al., 1998) showed how even limited amounts ofannotated data can result in major improvements in complex natural languageunderstanding systems. These and other annotated corpora have led to the train-ing of stochastic natural language processing components which have resultedin high-level improvements for parsing and word-sense disambiguation (WSD),similar to the improvements for part-of-speech tagging attributed to the annota-tion of the Brown corpus and, more recently, the British National Corpus (BNC)(Burnard 2000b). These successes have encouraged the development of an increas-ingly wide variety of corpora with richer and more diverse annotation. Theseinclude the Automatic Content Extraction (ACE) annotations (named entity tags,nominal entity tags, coreference, semantic relations and events); semantic annota-tions, such as more coarse-grained sense tags (Palmer et al., 2007); semantic rolelabels as in PropBank (Palmer et al., 2005), NomBank (Meyers et al., 2004), andFrameNet (Baker et al., 1998); and pragmatic annotations, such as coreference(Poesio & Vieira 1998; Poesio 2004), temporal relations as in TimeBank(Pustejovsky et al., 2003; 2005), the Opinion corpus (Wiebe et al., 2005), and thePenn Discourse Treebank (Miltsakaki et al., 2004b), to name just a few.
The depth of representation that NLP systems currently aspire to is in fact
deﬁned by the availability of corresponding linguistic annotations. For machinelearning systems to be trained to produce transformations that add substantiallynew information to textual input, they have to ﬁrst be exposed to similar informa-tion in context. In most cases these systems do an admirable job of automaticallyreproducing the same types of annotation, assuming the annotation has beencarried out consistently. The higher the inter-annotator agreement, and the greaterthe consistency and coherence of the original annotation, the higher the probability

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 239 — #2
Linguistic Annotation 239
of acceptable performance of the trained systems. Not surprisingly, there is nowan insatiable demand for more and more annotated data: the same types of anno-tations for different genres and different languages; newer, richer annotationsfor the original languages; parallel annotations of parallel corpora; the mergingof annotations that were ﬁrst done independently; and new formatting for pre-existing annotations that makes them easier to merge. For today’s NLP systems,the annotation deﬁnes the task, and increasingly rich annotations are the keyto more sophisticated systems. Clearly annotation work needs to become muchmore widely distributed to cope with this need. The ﬁeld requires a better under-standing of reliable annotation processes for several different types of linguisticannotation that can be readily ported.
It is tempting to assume that recent advances in semi-supervised and unsu-
pervised machine learning (see Chapter 8) may eventually obviate the needfor linguistic annotation, but this is not likely. Even unsupervised systems relyon manually annotated data for evaluation purposes. The ready portability ofthese systems to other genres and languages will simply increase the clamor foradditional annotation, albeit in smaller amounts than would be necessary forsupervised approaches. Meanwhile, applications that are aiming at the highestpossible accuracy levels continue to rely on supervised machine learning.
In this chapter we ﬁrst present details of several different speciﬁc annota-
tion projects and then review the basic elements that must be considered toachieve consistent annotation, which are generally applicable to different typesof annotation. These include:•target phenomena deﬁnition;
•corpus selection;
•annotation efﬁciency and consistency;
•annotation infrastructure;
•annotation evaluation;
•the use of machine learning for pre-processing and sampling.
2 Review of Selected Annotation Schemes
Covering every individual annotation scheme in every language is beyond thescope of this chapter. In this section we review a representative set of widely usedresources that range from syntactic annotation to pragmatic annotation, including:Syntactic structure, e.g., treebanking Associating a manual syntactic parse (a
complicated structure) with every sentence in a corpus consisting of a set ofdocuments. Whether the target structure is dependency structure or phrase struc-ture, this is an unusually difﬁcult type of annotation that requires in-depthtraining of annotators. A pre-processing step that involves tokenization, end-of-sentence detection and part-of-speech tagging is usually involved. Because of the

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 240 — #3
240 Martha Palmer and Nianwen Xue
labor-intensive nature of treebanking, it is usually done with single annotation –one person looks at each sentence.Independent semantic classiﬁcation, e.g., sense tagging Based on a pre-existing sense
inventory or set of semantic classes, every instance of a speciﬁc lemma (a wordform corresponding to a unique lexical entry) in a corpus is manually taggedwith its relevant sense, or class. This usually involves the same pre-processingas treebanking, including part-of-speech tagging, and is typically done as double-blind annotation (two independent taggers) with adjudication of discrepancies. Itrequires only minimal training.Semantic relation labeling, e.g., semantic role labeling This is a more complex task,
since it involves identifying a target relation and one or more participants in thatrelation. Semantic role labeling often begins with a corpus of parsed sentences,and the arguments associated with distinct subcategorization frames of verbs aregiven consistent label names according to a predeﬁned lexical resource of framedescriptions. This is also typically done as double-blind annotation, and can beapplied to predicative nouns as well as verbs, or to other types of relations, suchas discourse relations and temporal relations. Training must include familiarizingthe annotators with the parses, if provided, and the relation descriptions.Discourse relations Since they typically involve relations between sentences or
sentence fragments, discourse relations can be viewed as an additional type ofsemantic relation. For example, the Penn Discourse Treebank (PDTB), funded byNSF, is based on the idea that discourse connectives such as and, but, then,while,
...can be thought of as predicates with associated argument structures (Miltsakaki
et al., 2004a).Temporal relations Our ﬁnal example of semantic relations consists of temporal
relations, such as those found in TimeBank. Given a corpus where both nominaland verbal events and their participants have been identiﬁed, relations betweenthe events, such as temporal and subordinating relations, are identiﬁed andlabeled using a predeﬁned set of relationship types.Coreference tagging References to entities in a document are identiﬁed as men-
tions, and mentions of the same entity are linked as being coreferent, or membersof a coreference set. These can include pronouns, nominal entities, named enti-ties, elided arguments, and events. Techniques for annotating and evaluatingentire sets of coreferences are signiﬁcantly more complex than techniques forstraightforward class-labeling or relation-labeling tasks.Opinion tagging The annotation of opinions, evaluations, emotions, sentiments,
and other private states in text is collectively described as opinion tagging or senti-ment tagging. At its simplest this could be seen as a type of semantic classiﬁcation

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 241 — #4
Linguistic Annotation 241
task, but since the tagging typically includes ﬁlling in several different featurevalues rather than simply assigning class labels, it is discussed separately.
These different types of annotation are all described in more detail below.
2.1 Syntactic structure, e.g., treebanking
The dramatic improvement in natural language parsing achieved during the lasttwo decades or so has been generally attributed to the emergence of statisticaland machine learning approaches (Collins 1999; Charniak 2000). However, sta-tistical and machine learning methods are only possible with the availability oflarge-scale treebanks, corpora of hand crafted syntactic trees. The Penn Treebank(PTB) (Marcus et al., 1993) played a special role in providing a shared data set onwhich competing parsing approaches are trained and tested. In our view, thereare two main factors that contributed to the success of the Penn Treebank. Theﬁrst one has to do with its size. Although not the ﬁrst syntactically annotated cor-pus, the Penn Treebank is the ﬁrst one that covers over two million words of text.Statistical approaches to natural language parsing require large quantities of train-ing data to get reliable statistics for the large number of grammatical and lexicalphenomena in a language, and this is provided by the Penn Treebank. The one-million-word Wall Street Journal subcorpus of the PTB is the most frequently used
data set even though Wall Street Journal articles are not particularly representative
of the English language. The other factor for the PTB’s success is its pragmaticapproach. Many key annotation decisions are driven by engineering desideratarather than purely by linguistic considerations. This often means that theoreticallyimportant linguistic distinctions that are hard to make are left unspeciﬁed for thesake of annotation consistency. For example, the argument/adjunct distinctionhas been a key building block for theoretical linguistics, but is avoided in the PennTreebank. In hindsight, this decision cuts both ways. On the one hand, it simpliﬁesthe annotation task and has led to more consistent annotation. On the other hand,it leaves out key information that has to be recovered later in the semantic layerof annotation. For example, the argument/adjunct distinction had to be made atleast superﬁcially in the development of the PropBank (Palmer et al., 2005), whichadds a layer of predicate–argument annotation to the Penn Treebank.2.1.1 Phrase-structure treebanks The success of the Penn Treebank inspired
the development of treebanks in other languages. At Penn, the Chinese (Xue et al.,2005), Korean (Han et al., 2002) and Arabic (Maamouri & Bies, forthcoming) tree-banks have all been developed using a similar annotation scheme. This annotationscheme is characterized by labeled phrase structures, supplemented by functionaltags that represent grammatical relations such as subject (-SBJ), temporal (-TMP)and locational modiﬁers (-LOC), as well as empty categories and co-indices thatlink empty categories to their explicit coreferents, a hallmark of generative gram-mar. Empty categories and co-indices are used to represent left-displacement, butthey are by no means the only way to represent movement phenomena. The

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 242 — #5
242 Martha Palmer and Nianwen Xue
Figure 10.1 An example PTB tree.
different aspects of the representation scheme are illustrated in Figure 10.1: The
other half is an NP with the functional tag -TPC, indicating it is an NP playing the
role of a topic, a grammatical position in English syntax. This topic originates inthe object position of the verb have and then moves to the sentence-initial position.
This information is represented by the empty category *T*in the object position
co-indexed with the topic in the sentence-initial position. The empty category andco-indexation mechanism localize the arguments for a predicate and thus makeit easier to extract the predicate–argument structure. The other half is made adja-
cent to the verb have by positing an empty category *T*that is co-indexed with
it. Although head is a prominent notion in generative grammar, it is not explic-
itly represented in the Penn Treebank. However, the notion of head is implicitlybuilt into the structural conﬁguration of a phrase and in principle can be identi-ﬁed via a ﬁnite set of rules deﬁned for each syntactic category. For example, theverb have is assumed to be the head of the VP have *T*-1 before long by virtue of
being the ﬁrst verb in this VP . This set of rules is generally referred to as a headtable and is widely referenced in statistical parsing literature (Xia & Palmer 2001).In practice, due to annotation errors and underspeciﬁed annotation, the head can-not always be reliably identiﬁed. Therefore, in some phrase-structure annotationschemes, the head is explicitly marked to avoid such pitfalls when extracting thehead. For example, the Tiger corpus for German (Brants et al., 2002) explicitlymarks the head of a phrase.2.1.2 Dependency treebanks The Prague Dependency Treebank (Hajiˇ c 1998)
represents a radically different annotation scheme in the functional generativedescription framework, following the Prague dependency tradition. At the core ofthis annotation scheme is the dependency relation between a head and its depen-dent. While in a phrase-structure representation the explicit markup of the head isoptional, identifying the head is essential in a dependency structure. The depen-dency relation between a head and its dependent is the building block of thedependency structure of a sentence. A dependency structure representation of the

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 243 — #6
Linguistic Annotation 243
Figure 10.2 A labeled dependency structure.
same sentence as in Figure 10.1 is provided in Figure 10.2. While in a PTB-stylephrase structure the dependency between the verb have and the topic the other
half is mediated via a co-indexation mechanism, in the dependency structure this
relation is represented directly. Another key difference is that, while the syntac-tic categories of constituents in a phrase-structure tree represent the distributionalproperties of the constituents, e.g., noun phrases generally occur in subject andobject positions, etc., the focus of a dependency representation is on the relationbetween a head and its dependent. Therefore, while the nodes in a dependencytree are labeled by the head, which is not particularly informative other than say-ing the parent is the head and the child is the dependent, the edges are oftenlabeled with dependency relations such as subject and object. For example, thereis a SBJ relation between have and we,a n da TPC relation between have and the
other half. As Xia & Palmer (2001) showed, since there are no phrasal labels in adependency representation and, more importantly, the subject and complementare all attached to the head at the same level, it is generally not possible to auto-matically convert a dependency tree such as the one in Figure 10.2 to the PTB-stylephrase-structure representation. On the other hand, assuming that the head can bereliably identiﬁed, it is possible to automatically derive this dependency-structurerepresentation from a phrase-structure representation. However, since there is nolimit to the possible labels for dependency relations, it might be possible to labeldependency relations in such a way that a phrase-structure representation can bereconstructed.
There is a growing realization that both dependency- and phrase-structure
treebanks are needed. In fact, both tree adjoining grammar (TAG) and lexicalfunctional grammar (LFG) provide in a sense a combination of phrase-structureand dependency-structure representations, with the LFG c-structure (constituentstructure) corresponding to the phrase-structure layer and the LFG f-structure(functional structure) corresponding to the dependency-structure layer. With TAGthe derivation tree is the constituent structure and the derived tree is closer tothe dependency structure. Although there are no manually annotated large-scaleLFG style or TAG-style treebanks that we are aware of, there have been effortsto convert the phrase-structure annotation of the Penn Treebank into both TAG

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 244 — #7
244 Martha Palmer and Nianwen Xue
structures (Xia et al., 2001) and LFG f-structures (Cahill et al., 2002), and this hasprovided training data for successful statistical TAG and LFG parsers. There is arecent initiative funded by NSF to build a Hindi/Urdu dependency treebank thathas rich enough annotation that it can be readily converted to a phrase structuretreebank (Bhatt et al., 2009).
2.2 Semantic classiﬁcation, e.g., sense tagging
Sense tagging is essentially a semantic classiﬁcation task. Given a set of predeﬁnedsemantic labels for a distinct lexical item, one or more labels are associated witheach occurrence of that item in a sentential context in a corpus. For instance, the call
lemma in (1) is tagged with the OntoNotes (see Section 2.8) Sense 1, correspondingtocommunicate orally, usually in a loud, distinct tone.
(1) “You people here think this is Russian music,” she said with disdain, and called
over to the waitress: “Could you turn it off?”
In contrast, (2) is tagged with Sense 5, to label with a name or quality.
(2) “A spokesman for the state, however, calls the idea ‘not effective or cost
efﬁcient.’ ”
The traditional assumption is that the labels correspond to sense entries from
a pre-existing sense inventory, such as a dictionary, and annotators apply theselabels after reading the sentence containing the lemma.
There are other closely related tasks such as nominal entity tagging which are
also basically semantic classiﬁcation tasks but with different notions of wherethe class labels come from. Nominal entity tagging, as deﬁned by the AutomaticContent Extraction (ACE) project (Strassel et al., 2008), is focused primarily onnouns and consists of choosing a semantic category from a predeﬁned categorylist (PERson, ORGanization, GeoPoliticalEntity, LOCation, FACility, SUBstance,VEHicle, WEApon)
1for each occurrence of the noun in context in a corpus.
Several nouns, especially proper nouns such as the White House, can have multiple
tags, such as PER, GPE, or LOC. In these cases, determining which tag isappropriate, given a speciﬁc sentence as the context, amounts to the equiva-lent of a sense-tagging task. An important difference is that for nominal entitytagging there is one set of sense tags for all nouns, rather than a unique set of sensetags for each lexical item. However, the entity type tags can easily be mappedto standard dictionary sense entries, which for nouns in particular are oftenseparated according to semantic category. For instance, in the following exam-ple the deﬁnition of regulator has two senses, one of which can be mapped to the
SUBstance category and the other of which can be mapped to the ORGanizationor PERson category. If a corpus containing this word had already been annotatedwith nominal entity categories, then those category labels for regulator could be
deterministically mapped to either Sense 1 or Sense 2, providing a sense-taggedcorpus.

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 245 — #8
Linguistic Annotation 245
regulator1: a device to control the rate of some activity,e.g., chemical or mechanical – SUBSTANCE2: an ofﬁcial with responsibility to supervise somedomain of social activity – ORG, PER
2.2.1 Choosing the sense inventory Clearly the most important decision in
creating sense-tagged corpora (or nominal entity tagged corpora) is the choice ofthe sense inventory (or label set) that will be used. For classic sense-tagging tasksthis is a computational lexicon or a machine readable dictionary that partitionsthe meaning of each word into numbered senses, allowing the word plus a num-ber to uniquely refer to an entry. An ideal sense inventory should make clear andconsistent sense distinctions for each word. Unfortunately, sense inventories for alanguage can be discouragingly diverse, with signiﬁcant differences with respectto entries for polysemous words, and different levels of granularity of the sensedistinctions. Corpora tagged with two different English sense inventories will notprovide coherent training data unless a comprehensive mapping can be providedbetween every entry, and the mappings are often not one-to-one (Palmer et al.,2007).
The sense-tagged data itself can be framed as either lexical sample data or as
an all-words corpus. In the all-words corpus, all words (or all content words) in arunning text or discourse are tagged. While superﬁcially similar to part-of-speechtagging,
2all-words tagging via a sense inventory is signiﬁcantly different in that
a different set of sense tags is required for each lemma. If public distribution isdesired, this severely limits the choice of possible sense inventories, because itrequires access to a publicly available, wide-coverage dictionary that is preferablyalso free or at least low-cost. For a lexical sample corpus, a sample of words iscarefully selected from the lexicon, along with a number of corpus instances ofeach word to be tagged. Unlike all-words tagging, dictionary entries are requiredonly for these selected words, so given a small enough sample of words, therecould be more ﬂexibility in dictionary choice.
The methodology for manual annotation depends on the type of tagging. Words
can be annotated more quickly and consistently if all instances of a word (type)are tagged at once (targeted tagging), instead of tagging all words sequentially asthey appear in the text. The advantages of targeted tagging make lexical sampletagging easier to implement than all-words tagging. The largest all-words cor-pus, SemCor, based on the Brown corpus (Francis & Kucera 1982), is tagged withWordNet senses (Miller 1995). Created by George Miller and his team at Prince-ton University, WordNet (Miller et al., 1990b; Miller & Fellbaum 1991; Fellbaumet al., 1998) is a large electronic database organized as a semantic network builton paradigmatic relations including synonymy, hyponymy, antonymy, and entail-ment. WordNet has become the most widely used lexical database today for NLP

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 246 — #9
246 Martha Palmer and Nianwen Xue
research, and its approach has now been ported to several other languages, suchas several European languages in EuroWordNet (Vossen 1998) and in BalkaNet(Stamou et al., 2002), as well a Japanese (Bond et al., 2008) and a Chinese Word-Net (Xu et al., 2008). WordNet lists the different senses for each English open-classword (nouns, verbs, adjectives, and adverbs). Sense tagging is typically done asdouble-blind annotation by two linguistically or lexicographically trained anno-tators, with a third tagger adjudicating between inter-annotator differences tocreate a gold standard. Because of issues that have been raised about low ITArates due to ﬁne-grained sense distinctions in English WordNet, and correspond-ing unacceptably low system performance, manual groupings of WordNet sensesare now being used for tagging in a large DARPA-funded project (see Section 2.8).
2.3 Semantic relation labeling, e.g., semantic
role labeling
A closely related but distinct semantic annotation task involves identifying withina single sentence a relation and its arguments, and then labeling the arguments.The classic example is a verb where the labels of the verb arguments, once theyhave been identiﬁed, correspond to semantic role labels. The semantic role labels are
intended to indicate a speciﬁc semantic relation between a verb and its argumentthat holds consistently even when the argument is in different syntactic positions.This description covers several current semantic role labeling tasks, in which thesemantic roles can come variously from PropBank (Palmer et al., 2005), FrameNet(Baker et al., 1998), VerbNet (Kipper et al., 2006), or the Prague tecto-grammaticalformalism (Hajiˇ c et al., 2000). It can also be extended to similar semantic roles thatare introduced by other parts of speech, such as nominal or adjectival elements.A major difference between sense tagging and semantic role labeling is the inter-dependence between the semantic role labels. If the Agent, or Arg0, of a verb hasalready been labeled, that changes the available choices for the remaining argu-ments. This interdependence of labels is also the case for discourse relations andtemporal relations; however, given the distinctive nature of these annotation tasks,they will be dealt with in separate sections. ACE relations, which are also similar,are discussed at the end of this section.2.3.1 The Proposition Bank The Proposition Bank, originally funded by ACE
(DOD), focuses on the argument structure of verbs, and provides a corpusannotated with semantic roles, including participants traditionally viewed asarguments and adjuncts. Correctly identifying the semantic roles of the sentenceconstituents, or Who did what to whom, and when, where and how? is a crucial part
of interpreting text and, in addition to forming a component of the informationextraction problem, can serve as an intermediate step in machine translation,automatic summarization, or question answering.
At the beginning of the PropBank project, the decision was made to associate
the semantic role labels directly with nodes in the Penn Treebank phrase-structure

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 247 — #10
Linguistic Annotation 247
parses. The boundaries of the constituents corresponding to the nodes werealready deﬁned, so the annotators did not have to add that task to their duties,simplifying their cognitive load. The PTB also had indicated empty arguments,so these could easily be given semantic role labels as well, making the annota-tion more complete. Finally, the assumption was that the syntax and semanticswould be highly correlated, with the semantic roles occurring within the domainof locality of the predicating element. Therefore having access to the syntacticstructure should help in simplifying and focusing the task for the annotators.In contrast, FrameNet (see below) annotation did not initially begin with pre-parsed sentences, and this was found to lower agreement among the annotators,primarily because of different constituent boundary decisions. Another feasiblemethod is to annotate dependency structure parses directly with semantic rolelabels, and the Hindi/Urdu Treebank project will explore this approach in depth.One of the questions to be addressed is the issue of empty arguments; if emptyarguments have not been inserted by the dependency annotation, should theybe added at the PropBank level? Since the goal of this project is to eventuallyconvert the dependency structure + PropBank annotation automatically into aphrase-structure treebank, having the empty arguments in place could simplifythe conversion process (Bhatt et al., 2009).
The one-million word Penn Treebank II Wall Street Journal corpus has been
successfully annotated with semantic argument structures for verbs and is nowavailable via the Penn Linguistic Data Consortium as PropBank I (Palmer et al.,2005). More speciﬁcally, PropBank annotation involves three tasks: argumentlabeling, annotation of modiﬁers, and creating coreference chains for empty argu-ments. The ﬁrst goal is to provide consistent argument labels across differentsyntactic realizations of the same verb, as in
(3) a) “[ARG0 John] [REL broke] [ARG1 the window]”
b) “[ARG1 The window] [REL broke].”
The Arg1 or PATIENT in (3a) is the same window that is annotated as the Arg1
in (3b), even though it is the syntactic subject in one sentence and the syntac-tic object in the other. As this example shows, semantic arguments are taggedwith numbered argument labels, such as Arg0, Arg1, Arg2, where these labelsare deﬁned on a verb-by-verb basis. The second task of the PropBank annotationinvolves assigning functional tags to all modiﬁers of the verb, such as MNR (man-ner), LOC (locative), TMP (temporal), DIS (discourse connectives), PRP (purpose)or DIR (direction), and others, as in (4).
(4) “[ARG0 John] [REL broke] [ARG1 the window] [ARGM:TMP yesterday ].”
Finally, PropBank annotation involves ﬁnding antecedents for empty arguments
of the verbs, as in (5).
(5) “You people here think this is Russian music,” she said [*T*-1] with disdain, and
called over to the waitress: “Could you turn it off?”

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 248 — #11
248 Martha Palmer and Nianwen Xue
The object of the verb sayin this example, You people here think this is Russian
music is represented as an empty category [*T*-1] in the treebank. In PropBank, all
empty categories that could be coreferred with an NP within the same sentenceare linked in coreference chains. So the [*T*-1] is linked to You people here think this
is Russian music. The primary goal of PropBank is to supply consistent, simple,general-purpose labeling of semantic roles for a large quantity of coherent textthat can provide training data for supervised machine learning algorithms, in thesame way the Penn Treebank has supported the training of statistical syntacticparsers. PropBank also provides a lexicon which lists, for each broad meaningof each annotated verb, its Frameset, i.e., the possible arguments in the predicate
and their labels and all possible syntactic realizations. PropBank’s focus is verbs,so NomBank, an annotation of nominalizations and other noun predicates usingPropBank style Framesets, was done at NYU, also funded by ACE (Meyers et al.,2004).2.3.2 FrameNet FrameNet consists of collections of semantic frames, lexical
units that evoke these frames, and annotation reports that demonstrate uses oflexical units. Each semantic frame speciﬁes a set of frame elements, or arguments.Semantic frames are related to one another via a set of possible relations such asis-a and uses. Frame elements are classiﬁed in terms of how central they are to a
particular frame, distinguishing three levels: core, peripheral, and extra-thematic.FrameNet is designed to group lexical items based on frame semantics, and setsof verbs with similar syntactic behavior may appear in multiple frames, while asingle FrameNet frame may contain sets of verbs with related senses but differentsubcategorization properties. FrameNet places a primary emphasis on providingrich, idiosyncratic descriptions of semantic properties of lexical units in context,and making explicit subtle differences in meaning. As such it could provide animportant foundation for reasoning about context-dependent semantic represen-tations. However, the large number of frame elements and the current sparse-ness of available annotations for each one has been an impediment to machinelearning.2.3.3 VerbNet VerbNet is midway between PropBank and FrameNet in terms
of lexical speciﬁcity, and is closer to PropBank in its close ties to syntactic struc-ture. It consists of hierarchically arranged verb classes, inspired by and extendedfrom classes of Levin (1993). Each class and subclass is characterized by its setof verbs, by a list of the arguments of those verbs, and by syntactic and seman-tic information about the verbs. The argument list consists of thematic roles (23in total) and possible selectional restrictions on the arguments expressed usingbinary predicates. Additional semantic predicates describe the participants dur-ing various stages of the event described by the syntactic frame, and provideclass-speciﬁc interpretations of the thematic roles. VerbNet now covers over 6,000senses for 5,319 lexemes. A primary emphasis for VerbNet is the coherent syntacticand semantic characterization of the classes, which will facilitate the acquisition ofnew class members based on observable syntactic and semantic behavior.

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 249 — #12
Linguistic Annotation 249
2.3.4 SemLink Although PropBank, FrameNet, and VerbNet have all been
created independently, with differing goals, they are surprisingly compatible intheir shared focus on labeling verb arguments. PropBank uses very generic labelssuch as Arg0, as in (6).
(6) “[ARG0 President Bush] has [REL approved] [ARG1 duty-free treatment forimports of certain types of watches].”
In addition to providing several alternative syntactic frames and a set of seman-
tic predicates, VerbNet marks the PropBank Arg0 in this sentence as an Agent,and the Arg1 as a Theme. FrameNet labels them as Grantor and Action respec-tively, and puts them in the Grant_Permission class. The additional semanticrichness provided by VerbNet and FrameNet does not contradict PropBank, butcan be seen as complementary. These resources can also be seen as complemen-tary with WordNet, in that they provide explicit descriptions of participants andties to syntactic structure that WordNet does not provide. The PropBank labels,being the most generic, will cover the widest number of WordNet senses fora particular word. A verb in a VerbNet class will also usually correspond toseveral WordNet senses, which are explicitly marked. FrameNet provides theﬁnest sense granularity of these resources, and speciﬁc FrameNet frames aremore likely to map onto individual WordNet senses. There are signiﬁcant dif-ferences in the coverage of lexemes and the structuring of data in each of theseresources, which could be used to bootstrap coverage extensions for each one.The simple labels provided by PropBank are more amenable to machine learning,and have resulted in the training of successful automatic semantic role labelingsystems. A semi-automatic mapping from PropBank to VerbNet has been pro-duced (and hand corrected) which has been used to successfully train systemsthat can produce either PropBank or VerbNet semantic role labels (Yi et al., 2007).Altogether, 3,465 types have been mapped, comprising over 80 percent of thetokens in the PropBank. In parallel a type-to-type mapping table from VerbNetclass(es) to FrameNet frame(s) has been created, as well as a mapping from rolelabel to frame element. This will facilitate the generation of FrameNet represen-tations for every VerbNet version of a PropBank instance that has an entry in thetable.2.3.5 ACE relations This style of annotation also bears a close resemblance
to the ACE relation task, which is aimed at detecting within a sentence a par-ticular type of relation and its arguments (LDC 2008). There is a shift in focuswith ACE, however, from a lexically oriented, linguistically motivated task, suchas semantic role labeling, to a more pragmatic, relation-type task. The relationtypes include: PHYSICALly located, as in LOCATED or NEAR (see (7)); PART-WHOLE, which could be a GEOGRAPHICAL PART-WHOLE relation, such asColorado being PART of the United States, or SUBSIDIARY, as in Umbria beinga SUBSIDIARY or PART of JD Powers; PERSONAL-SOCIAL, as in BUSINESS(co-workers), FAMILY (siblings), or LASTING-PERSONAL (life-long neighbors);

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 250 — #13
250 Martha Palmer and Nianwen Xue
ORG-AFFILIATION, which includes EMPLOYMENT, OWNERSHIP , FOUNDER,etc.; AGENT-ARTIFACT, and others. For example, there is a PHYSICALly Located
relation between ‘Barack Obama’ and ‘Germany’ in (7). An important distinctionis that the same ACE relation type could be introduced by a verb, a noun or apreposition, so the annotators need to focus more on semantic content and less onsyntactic structure.
(7) “Barack Obama traveled to Germany to give a speech at Buchenwald.”
2.4 TimeBank
TimeBank (Pustejovsky et al., 2003) is a corpus annotated with temporal infor-mation based on TimeML (Pustejovsky et al., 2005), a general-purpose temporalmarkup language that has been adopted as an ISO (International Organization forStandardization) semantic annotation standard (ISO/TC 37/SC 4/WG 2, 2007).The basic elements of the TimeML are events, time expressions, and signals, aswell as temporal relations between these temporal entities. For example, in (8),glossed ,warnings ,strikes, do,a n d harm would all be identiﬁed as anchors of events
in the TimeBank annotation; Thursday would be marked up as a temporal expres-
sion; and onwould be marked as a signal for the temporal relation between the
glossing-over event and the temporal expression Thursday. Temporal relations also
hold between events. For example, the strike event would precede the harm event,
which would in turn be annotated as identical to the doevent.
(8) “President Clinton, meantime, glossed over stern warnings from Moscow on
Thursday that US air strikes against Iraq could doserious harm to relations
with the Kremlin.”
TimeML adopts a broad deﬁnition of event. Event for TimeBank is a cover term
for situations that happen oroccur. Events can be punctual or last for a period
of time. The TimeBank events also include states orcircumstances in which some-
thing obtains or holds true. However, TimeBank does not mark up all states ina document. It only annotates states that are relevant to temporal interpretation,for example, states that are identiﬁably changed during the document time (9a), orstates (9b) that are directly related to a temporal expression. States that persistentlyhold true are excluded from annotation. Syntactically, events can be realized asverbs, nominalizations, adjectives, predicative clauses, or prepositional phrases.
(9) a) “All 75 on board the Aeroﬂot Airbus died.”
b)“They lived in U.N.-run refugee camps for 2 1/2 years.”
A time expression belongs to one of four types: Date, Time, Duration or Set. A
Date describes a calendar time, and examples are Friday, October 1 ,1999, yesterday,
last week . Time refers to a time of day, even if it is indeﬁnite, e.g., ten minutes from
three, ﬁve to eight, late last night. Durations are assigned to explicit durations such
as2 months,a n d 48 hours . Finally, a Set describes a set of times, e.g., twice a week or

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 251 — #14
Linguistic Annotation 251
every two days. Time expressions are also annotated with a normalized value. Forexample, twelve o’clock midnight would be normalized to T24:00.
A signal is a textual element that makes explicit the temporal relation between
a temporal expression and an event, or between a temporal expression and a tem-poral expression, or between an event and an event. Signals are generally temporalprepositions such as on,in,at,from, to, or temporal conjunctions such as before ,after,
while, when , as well as special characters like ‘-’ and ‘/’ that indicate ranges of time.
Events, time expressions, and signals are temporal entities that are linked by
temporal relations to form an overall temporal interpretation of a text. The maintemporal relations are represented by Temporal Links (TLINKs), which representthe temporal relation between events, betwen times, or between an event and atime. The TLINK annotation is illustrated in (10), where there is a BEFORE relationbetween the events anchored by invited and come, which means the inviting event
happens before the coming event.
(10) “Fidel Castro invited John Paul to come
for a reason.”
Subordination Links (SLINKs) are another type of temporal link, and they are
used to represent modal, factive, counter-factive, evidential, and negative eviden-tial relations, as well as conditional relations that usually hold between a mainevent and a subordinate event. For example, in (11), an SLINK can be establishedbetween the events anchored by adopt and ensure.
(11) “The Environmental commission must adopt regulations to ensure
people are
not exposed to radioactive waste.”
A third and ﬁnal type of link is the Aspectual Link (ALINK), which represents
the relationship between an aspectual event and its argument event. The relationthat an ALINK represents can be one of ﬁve types: Initiates, Culminates, Termi-nates, Continues, or Reinitiates. The example in (12) represents an Initiates relationbetween the events anchored by began and trading.
(12) “The stock began trading
this summer at $14 apiece.”
Achieving consistency in the TimeBank annotation has proven to be very
difﬁcult, with temporal ordering of events being the most challenging part of theannotation. It is neither feasible nor necessary to temporally order each pair ofevents in a document, but without some clear guidance, different annotators tendto choose different pairs of events to annotate, leading to poor inter-annotatoragreement. In practical temporal annotation, some form of temporal inferencemechanism has to be implemented (Verhagen 2005) so that the temporal order-ing of some pairs of events can be automatically inferred. The ﬁne-grained natureof some temporal relations also makes it difﬁcult to separate one relation fromanother.

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 252 — #15
252 Martha Palmer and Nianwen Xue
2.5 Discourse relation annotation
Treebank and PropBank annotations are all focused on getting linguistic informa-tion from within the sentence. More recently, there have been efforts to annotatelinguistic structures beyond the sentence level. These new efforts make the struc-ture of a whole text their target of annotation. In this subsection we discuss twosuch projects, the RST Corpus and the Penn Discourse Treebank project, whichhave taken very different approaches to discourse-structure annotation.2.5.1 RST Corpus The RST Corpus (Carlson et al., 2003) consists of 385 articles
from the Penn Treebank, representing over 176K words of text. The RST Corpus ishierarchically annotated in the framework of Rhetorical Structure Theory (Mannand Thompson 1988). In rhetorical structure theory, the discourse structure of atext is represented as a tree, and the leaves of the tree are text fragments that rep-resent the minimal units of discourse, called elementary discourse units or EDUs.
Each node in the discourse tree is characterized by a rhetorical relation that holds
between two or more adjacent nodes, and corresponds to contiguous spans of text.The rhetorical relation between the children of a node is characterized by nucle-
arity, with the nucleus being the essential unit of information, while a satelliteindicates a supporting or background unit of information.
The annotation of the RST Corpus starts off by identifying the elementary dis-
course units, or EDUs, which are building blocks of a discourse tree. The EDUsroughly correspond to clauses, although not all clauses are EDUs. For example,a subordinate clause that is an adjunct to the main clause is usually an EDU, butclauses that are subjects, objects, or complements of a main clause are not usuallyEDUs.
The discourse relations between child discourse units of a node in the discourse
tree can be either mononuclear or multinuclear, based on the relative salience ofthe discourse units. A mononuclear relation is between two discourse units whereone is the nucleus and another is the satellite. The nucleus represents the moresalient or essential information while the satellite indicates supporting and back-ground information. A multinuclear relation is between two or more discourseunits that are of equal importance and thus are all nuclei. This in a way paral-lels the endocentric and exocentric structures at the sentence level. A total of 53mononuclear and 25 multinuclear relations are used to annotate the RST Corpus;these 78 relations fall into 16 broad classes. These discourse relations are identiﬁedempirically, based on evidence from the corpus. ‘Elaboration’ is an example of amononuclear discourse relation, while ‘list’ is a multinuclear discourse relation.For the complete set of discourse relations tagged in the RST, the reader is referredto the discourse tagging manual of the RST Corpus.2.5.2 The Penn Discourse Treebank While the RST annotation of discourse
relations is organized around EDUs, the building blocks of the Penn DiscourseTreebank (Miltsakaki et al., 2004) are discourse connectives and their argu-ments. The annotation framework of the Penn Discourse Treebank is based on

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 253 — #16
Linguistic Annotation 253
a theoretical framework developed in Webber and Joshi (1998), where discourseconnectives are considered to be predicates that take abstract objects such asevents, states, and propositions as their arguments. The Penn Discourse Treebankannotates both explicit and implicit discourse connectives and their arguments.
Explicit discourse connectives include subordinating conjunctions and coordi-nating conjunctions, as well as discourse adverbials. While in most cases thearguments of discourse connectives are local and adjacent to the discourse con-nective, they do not have to be. Webber et al. (Webber & Joshi 1998) considerssubordinating and coordinating conjunctions to be structural in the sense that their
arguments are local to the discourse connective, while discourse adverbials areconsidered to be anaphorical, because their ﬁrst arguments can be long-distance.
Where explicit discourse connectives are absent, implicit discourse connectives
are inserted between paragraph-internal sentence pairs, as illustrated in (13). Insome cases, it may not be possible to insert a discourse connective because the dis-course relation is expressed with a non-discourse connective element, or becausediscourse coherence is achieved by an entity chain, or simply because there is norelation of any kind.
(13) “Motorola is ﬁghting back against junk mail. [ARG1 So much of the stuff pouredinto its Austin, Texas, ofﬁces that its mail rooms there simply stopped deliveringit]. Implicit=so
[ARG2 Now, thousands of mailers, catalogs and sales pitches go
straight into the trash].”
The lexically grounded approach of the Penn Discourse Treebank opens the door
for the possibility that one discourse connective might be a lexical realization ofmultiple discourse relations. The Penn Discourse Treebank addresses this by spec-ifying an inventory of discourse relations that serve as senses of the discourseconnectives. In a way, this inventory is similar to the set of discourse relationsadopted by the RST, while the actual discourse relations posited might be dif-ferent. Like the RST Corpus, the discourse relations are hierarchically organized.The top level has four major semantic classes: TEMPORAL, CONTINGENCY,COMPARISON, and EXPANSION. For each class, a second level of type is deﬁned,
and then for each type, there may be a third level of subtype deﬁned. There are 16
types and 23 subtypes. Some types do not have subtypes. The reader is referred tothe PDTB annotation manual for details.2.5.3 A comparison of the two approaches The RST Corpus and the Penn
Discourse Treebank represent very different approaches to the annotation of dis-course relations. The most fundamental difference is that the RST Corpus iscommitted to building a discourse tree representation for the entire text. The leavesof the tree are elementary discourse units or EDUs, which are non-overlappingspans of text. Discourse units built on the EDUs are also non-overlapping spansof text, and discourse relations are always local in the sense that they only holdbetween adjacent discourse units. The Penn Discourse Treebank, on the otherhand, is not committed to a tree representation of the entire text. The building

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 254 — #17
254 Martha Palmer and Nianwen Xue
blocks are discourse connectives, which are treated as predicates, the arguments ofwhich are identiﬁed for each discourse connective instance in a text, whether theyare explicit or implicit. Although the text spans that are identiﬁed as argumentsof a discourse connective never overlap, there is no guarantee that arguments ofdifferent connectives do not overlap. In fact, Lee et al. (2006) show that there isa variety of possible patterns of dependencies between pairs of discourse rela-tions, including nested, crossed, and other non-tree-like conﬁgurations. Part ofthe reason for this complex structure is due to the anaphoric discourse relationsfor discourse adverbials, whose arguments are not necessarily local, as discussedabove.
Another reason why there exist complex dependencies in the Penn Discourse
Treebank is the way attribution is annotated. PDTB adopts the strict view that
discourse relations are between abstract objects such as events, states, and propo-sitions. Since attributions are relations between an agent and a proposition, theattribution annotation is treated as a separate layer of annotation. Depending onthe context, the attribution may be included as part of an argument in some casesbut excluded in others – (14) is an example. The higher verb ‘He said’ is includedas an argument of ALTHOUGH, but excluded as an argument of ALSO. There isonly a partial overlap between the arguments for these two discourse connectives.If the attribution is excluded from the discourse processing, then the discourserelation for ALTHOUGH would be properly contained as an argument for ALSO.RST, on the other hand, includes attribution as one type of discourse relation, notdistinguished from other discourse relations. The RST Corpus also does not con-sider ALSO as a trigger for a discourse relation. Discourse connectives are used ascues to identify EDUs and determine their discourse relations, but they have noformal role in the RST annotation scheme.
(14) “He (Mr. Meek) said the evidence pointed to wrongdoing by Mr. Keating ‘andothers,’ ALTHOUGH
he didn’t allege any speciﬁc violation. Richard Newsom,
a California state ofﬁcial who last year examined Lincoln’s parent, AmericanContinental Corp., said he ALSO
saw evidence that crimes had been committed.”
A ﬁnal difference between the RST annotation and the PDTB annotation is that
PDTB only considers discourse relations between clauses, while RST also consid-ers discourse relations between subclause relations. For example, EDUs in the RSTCorpus can be phrases. A discourse relation can be between the head NP and itspostmodiﬁers.
Measuring annotation consistency for discourse annotation can be very compli-
cated. Carlson et al. (2003) report inter-annotator agreement on four levels of theRST Corpus: elementary discourse units, hierarchical spans, hierarchical nucle-arity, and hierarchical relation assignments. The agreement is the highest for theidentiﬁcation of EDUs (0.97 kappa) and the lowest in discourse relation assign-ment (0.75 kappa), which is not unexpected. Miltsakaki et al. (2004a) reportedinter-annotator agreement on the Penn Discourse Treebank using two differentmeasures. The ﬁrst measure is more lenient and is calculated on a per-argument

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 255 — #18
Linguistic Annotation 255
basis. That is, if two annotators assign the same argument label to the same spansof text, it is counted as a match, regardless of whether the other argument of thesame discourse connective is a match or not. By this measure, the average agree-ment score is 90.2 percent. The second measure is more stringent and is calculatedon a per-discourse relation basis. That is, two annotators have to agree on botharguments of a discourse connective in order for that to be counted as a match.By this measure the agreement is 82.8 percent. Although the consistency measuresare not comparable between the two discourse annotation projects, it appears thatboth projects have achieved reasonably consistent scores, indicating the viabilityof these annotation schemes (see Section 3.5).
2.6 Coreference
References to entities in a document are identiﬁed as mentions, and mentions ofthe same entity are linked as being coreferences, or members of a coreferenceset. These can include pronouns, nominal entities, named entities, elided argu-ments, and events. For example, Barack Hussein Obama II and hein (15) corefer.
Researchers at Essex (UK) were responsible for the coreference markup schemedeveloped in MATE (Poesio et al., 1999; Poesio 2004), partially implemented inthe annotation tool MMAX and now proposed as an ISO standard. They have alsobeen responsible for the creation of two small, but commonly used, anaphoricallyannotated corpora: the Vieira/Poesio subset of the Penn Treebank (Poesio & Vieira1998), and the GNOME corpus (Poesio 2004). Their work also includes extendedguidelines (Mengel et al., 2000), and annotation of Italian. Parallel coreferenceannotation efforts funded ﬁrst by ACE and more recently by DARPA GALE (seeSection 2.8) have resulted in similar guidelines, best exempliﬁed by BBN’s recentefforts to annotate named entities, common nouns and pronouns consistently(Pradhan et al., 2007c). These two approaches provide a suitable springboard foran attempt at achieving a community consensus on coreference.
(15) “Barack Hussein Obama II is the 44th and current President of the United
States. Heis the ﬁrst African American to hold the ofﬁce.”
Techniques for annotating and evaluating entire sets of coreferences are signiﬁ-
cantly more complex than techniques for straightforward class-labeling tasks.
2.7 Opinion annotation
The Pittsburgh Opinion annotation project (Wiebe et al., 2005) funded by IARPA,focuses on the annotation of opinions, evaluations, emotions, sentiments, andother private states in text. A ﬁne-grained annotation scheme has been developedfor annotating text at the word and phrase levels. For every expression of a privatestate, a private state frame is deﬁned that identiﬁes whose private state it is, whatthe private state is about, and various properties involving intensity, signiﬁcance,and type of attitude. For example, in (16) a private state frame is anchored by fear.

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 256 — #19
256 Martha Palmer and Nianwen Xue
The source of the private state is attributed to the US , and the attitude type of the
private state is negative. The intensity of the private state is medium. A corpus of
over 15,000 sentences has been annotated according to the scheme. The corpus isfreely available at: nrrc.mitre.org/NRRC/publications.htm.
(16) “The US fears a spill-over.”
There are several applications for corpora annotated with rich information
about opinions. Government, commercial, and political information analystsare all interested in developing tools that can automatically track attitudes andfeelings in the news and in online forums. They would also be interested intools that would support information extraction systems trying to distinguishbetween factual and non-factual information as well as question answeringsystems that could present multiple answers to non-factual questions basedon opinions derived from different sources. In addition there is an interest inmulti-document summarization systems, which would summarize differingopinions and perspectives.
2.8 Multi-layered annotation projects
The annotation resources we have described so far in this section are mostly one-dimensional tasks that focus on a single language processing goal. Treebanksare used to develop syntactic parsers and propbanks are used to train semanticrole labelers. A recent trend in linguistic annotation is aimed at building multi-layered linguistic resources, fueled by the realization in the natural languageprocessing community that there is great value in annotating the same linguisticsource with multiple levels of linguistic information. A major advantage ofhaving a multi-layered linguistic resource is that information encoded in onelayer of representation can be used to infer that of another. For example, therole of syntactic parsing in semantic role labeling, a form of semantic parsing,is well documented (Gildea & Palmer 2002; Punyakanok et al., 2005). It isperhaps not a coincidence that many semantic annotation projects are built ontop of syntactic annotation projects, as discussed in Section 2.3. For example,PropBank (Palmer et al., 2005) is built on top of the Penn Treebank (Marcus et al.,1993). The Salsa Project (Burchardt et al., 2006), a semantic annotation project forGerman, is built on top of the Tiger treebank (Brants et al., 2002), a syntacticallyannotated corpus. The Prague Dependency Treebank has a syntactic (the analyt-
ical layer ) and semantic (the tectogrammatical layer) annotation layer. Perhaps the
most ambitious multi-layered annotation project is OntoNotes (Pradhan et al.,2007a), funded through GALE, a large-scale DARPA program focused on auto-matic machine translation and summarization of Arabic and Chinese speech andtext. OntoNotes is a ﬁve-year, multi-site collaboration between BBN Technologies,the Information Sciences Institute of the University of Southern California, theUniversity of Colorado, the University of Pennsylvania, and Brandeis University.The goal of the OntoNotes project is to provide linguistic data annotated with

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 257 — #20
Linguistic Annotation 257
Arg0admit 1Arg1
Arg1Arg1Arg0e1:
e2:
e3 e4e6:
e8 e10e9
e11 e12founder 2transfer 2
Arg2–10
Pakistan’snucleardepartment
2AbdulQadearKhanhe Nuclear
technology
1Iran Libya NorthKoreaandPropositions
and Word Sense
nation
large geopolitical entity
geopolitical entity
social object
objectoriginator
causal agent
tangible objectengineering
subject area
cognition
mental object
eventtangible eventjudgeadmit
summum genus Ontology Coreferencee2 = e4e2 = e7NPNP
NPNP
NP NP NP NP NP NPNPPP
PPVP
VPSS
ThefoundsPakistan’snucleardepartmentAbdulQadearKhanadmitted he transferred NucleartechnologyIran, Libya NorthKoreaSyntax
e7
of has to and
Figure 10.3 OntoNotes: a model for multi-layer annotation.
a skeletal representation of the literal meaning of sentences including syntacticparse, predicate–argument structure, coreference, and word senses linked to anontology, allowing a new generation of language understanding technologies tobe developed with new functional capabilities. The OntoNotes annotation coversmultiple genres (newswire, broadcast news, broadcast conversation, and weblogs)in multiple languages (English, Chinese, and Arabic). The guiding principle hasbeen to ﬁnd a ‘sweet spot’ in the space of inter-tagger agreement, productivity,and depth of representation. Figure 10.3 illustrates the inter-connection betweenthe different layers of linguistic annotation in OntoNotes.
Many new challenges come with multi-layered annotation, particularly for
annotation models like OntoNotes. Since the different layers of annotation areperformed in different sites following different guidelines, incompatibilities canarise; (17) is an example with the same sentence annotated with both syntacticparses and semantic roles. By assigning different semantic roles to a letter (Arg1)
and for Mary (Arg2), the PropBank annotator makes the implicit judgment that
for Mary is an argument to the verb wrote and should be attached to this verb
instead of the noun phrase a letter, a judgment that is different from the treebank
annotation where the PP for Mary is treated as a modiﬁer of a letter.I no r d e rt o
achieve coherent annotation, these incompatibilities need to be reconciled. Babko-Malaya et al. (2006) describe the many inconsistencies between syntactic parsesand predicate–argument structure annotation that need to be resolved under theOntoNotes annotation effort.
(17) a) “(S (NP She)(VP wrote (NP (NP a letter)(PP for Mary))))”
b)“[Arg0 She] wrote [Arg1 a letter] [Arg2 for Mary]”
A linguistic source with multiple linguistic annotation also accentuates the data
access problem. The most effective use of such a resource requires simultaneous

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 258 — #21
258 Martha Palmer and Nianwen Xue
access to multiple layers of annotation. OntoNotes addresses this by storing thecorpus as a relational database to accommodate the dense connectedness of thedata and ensure consistency across layers. In order to facilitate ease of under-standing and manipulability, the database has also been supplemented with anobject-oriented Python API (Pradhan et al., 2007a).
3 The Annotation Process
Linguistic annotation is still in its infancy, and only a small portion of possibleannotation schemes have been clearly deﬁned and put into practice. The creationof each new level of annotation involves equal amounts of linguistic knowledge,inspiration, and experimentation: linguistic knowledge of the phenomena thatneed to be identiﬁed and described as a justiﬁable layer of annotation; inspira-tion as to achievable levels of granularity and precision; and experimentation todetermine the gaps and weaknesses in the guidelines. For any particular schemea ﬁnite list of allowable annotations must be speciﬁed, with careful attentionbeing paid to accounting for all possible contexts and to the cognitive load onthe annotators. A clear understanding of linguistic phenomena does not neces-sarily translate directly into the development of annotated data that is suitablefor training machine learning systems. The more precise the description of the lin-guistic phenomena, the greater the likelihood of a sparse data problem. Issues withrespect to consistency, coherence, and clarity of the annotation scheme need to bewell understood; a goal that can only be attained through trial implementations.From the perspective of the annotators, the ideal annotation scheme is clear andunambiguous in all circumstances and can be learned quickly by someone with-out an extensive linguistic background. This absolute goal may be unattainable,but efforts in its direction are rewarded by rapid, consistent annotation.
The development of an annotation scheme requires addressing at a minimum
the following issues, each of which will be discussed in turn below:•target phenomena deﬁnition;
•corpus selection;
•annotation efﬁciency and consistency;
•annotation infrastructure;
•annotation evaluation;
•the use of machine learning for pre-processing and sampling.
3.1 The phenomena to be annotated
The most important decision has to do with the phenomena to be annotated.Annotation can be an extremely expensive and labor-intensive process, so theﬁrst question is: is it absolutely essential that this phenomena be manually anno-tated, or will automatic techniques be almost as good? Having decided that

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 259 — #22
Linguistic Annotation 259
automatic techniques are insufﬁcient, there are several other questions that needto be addressed, such as: what is the scope of this annotation task, what otherresources, including other annotation layers, might the annotators need access to,and what level of training will they require?
For example, with respect to deﬁning the scope of the task, given a corefer-
ence task, are bridging references and event references to be considered as well?Adding these types of coreference might make the annotation task more difﬁ-cult for the annotators but, on the other hand, it might be impossible to deﬁnea coherent coreference task that does not include them.
With respect to other resources, given a semantic or pragmatic target area, the
annotators might need access to prior syntactic or semantic annotation. For exam-ple, if a researcher is interested in relative clauses, is it possible to annotate acorpus for relative clauses in isolation without having to consider the overall syn-tactic structure of the sentence? If not, will it matter whether the syntactic analysisis phrase-structure based or dependency based? Alternatively, if the annotationtask is sense tagging, which sense inventory will be the most appropriate lexicalresource? How ﬁne-grained do the senses need to be? If a parallel corpus is beingannotated with sense tags, does a bilingual sense inventory need to be used?
As an example of determining necessary training, if a researcher is interested
in providing syntactic analyses of sentences from biomedical texts, do all of theannotators need to become experts in biomedical terminology as well as in syntax?
The answers to any of these questions cannot be determined precisely without
considering the task as a whole, the cognitive load on the annotators and the avail-ability of appropriate corpora, resources, and tools. We will return to this topic atthe end of this section, but for now will assume that a researcher starts with atleast a general idea of the phenomena of interest in mind.
3.2 Choosing a target corpus
The criteria for corpus selection depend closely on the objectives for intendeduse. A large amount of data in a single genre, with as little variation in topic aspossible, will yield the best possible performance when tested on similar data. Ifthe characteristics of the test corpus are known in advance, then matching themas closely as possible in the training corpus is effective. However, a signiﬁcantdecrease in performance can be expected when testing on a disparate corpus.The same amount of data selected from a broader, more representative set ofdocuments will yield lower initial performance but more robust results whentested on diverse data. The ﬁeld is only too familiar with the degradation in per-formance that occurs when parsers trained on the one-million-word Wall Street
Journal Treebank are tested on different corpora. This was showcased in the 2005
CoNLL shared task for semantic role labeling (SRL) which included an evalu-ation on the Brown corpus, to “test the robustness of the presented systems”(Carreras & Màrquez 2005). The Charniak POS tagger degrades by 5 percent,and the Charniak parser F-score degrades by 8 percent, from 88.25 percent to

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 260 — #23
260 Martha Palmer and Nianwen Xue
80.84 percent. For the 19 systems participating in the semantic role labelingevaluation, there was in general a 10 percent performance decrease from WSJ to
Brown. The DARPA-GALE funded OntoNotes project (see Section 2.8) is speciﬁ-cally targeting 200,000 and 300,000 word selections of data from broadcast news,broadcast conversation (talk shows), newsgroups, and weblogs for treebanking,propbanking, sense tagging, and coreference annotation. The assumption is that amore balanced, more representative training corpus will improve the portabilityof systems trained on the data. For systems that are intended to be broad coverage,portability issues are of paramount importance.3.2.1 Isolated sentences Alternatively, broader coverage can be achieved by
augmenting a target corpus with a hand selected set of constructions. Standardevaluation against a treebank simply selects a 10 percent or smaller chunk ofthe data for evaluation purposes. This has the beneﬁt of testing against naturallyoccurring sentences, but there is no control over which types of phenomena, suchas wh-movement, gapping, reduced relative clauses, etc., are covered. Unfortu-nately, a particular corpus may provide few instances of rare phenomena, andtherefore this type of testing may not adequately cover performance on thesetypes of constructions, especially if the initial corpus is small. If necessary a set ofselected instances of rare constructions that are poorly represented in the trainingand test corpus can be prepared and added to the treebank corpora. Sentence (18)is an example of a statement that has a participial modiﬁer, sometimes also called areduced relative, that is in the passive voice and coincides with an inﬁnitival con-struction. This may not occur frequently, but a few more similar examples shouldprovide a statistical parser with sufﬁcient training material to correctly analyze itwhen it does occur.
(18) “The student, found to be failing several subjects, was still promoted to the nextgrade.”
Another technique, given a large enough original target corpus, would be
to carefully select a subset for annotation that ensures coverage of particularphenomena. A good illustration of hand selection/construction of instances ofparticular phenomena are the Test Suites for English pioneered by Stephen Oepenand Dan Flickinger (Oepen & Flickinger 1998; Oepen et al., 2002). The followingexamples illustrate the types of phenomena with marked word order for which aparser might require additional examples.
3
•Heavy NP-shift: “We saw on Tuesday a most amazing ﬁlm.”
•Relative clause extraposition: “Someone walked in whom I hadn’t met.”
•Locative inversion: “In the corner stood an antique coatrack.”
The difﬁculty of corpus selection is exacerbated when primarily lexical phe-
nomena are under consideration. Discourse oriented annotation tasks such ascoreference or discourse connectives require coherent chunks of text. Unfortu-nately, even a one-million-word corpus of coherent articles such as the WSJ

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 261 — #24
Linguistic Annotation 261
treebank will not contain sufﬁcient numbers of representative instances of mostverbs to be suitable as a training corpus for semantic role labeling or sense tag-ging. Over half the verbs in WordNet are not present at all, and for those that arepresent, two thirds (2,167 out of 3,100) occur less than 10 times. Accurate auto-matic performance on these verbs can only be achieved by augmenting the WSJ
corpus with additional instances. Yet annotating an additional 10 million words isclearly not feasible, and would also involve extremely tedious, unnecessary anno-tation of the 700 verbs that constitute 80 percent of the token coverage, and alreadyhave sufﬁcient instances. The only solution is to once again augment the originaltarget corpus with selected instances from other sources, with the knowledge thatthe lack of coherence in the augmentation will render it of little use to discourseannotation tasks.
Similar in spirit to the subset selection approach mentioned above, the
OntoNotes sense-tagging group at Colorado is currently experimenting with bothactive learning and language modeling as techniques for ﬁnding more examples ofrare senses to provide a more even distribution of the training data for a particularlemma (Dligach & Palmer 2009). The arithmetic sense of addis perhaps the most
familiar one, but it actually occurs quite infrequently. The saysense, as in “And
I miss you, too.” he added mendaciously occurs more frequently by far. However, in
certain domains it would be critical to correctly detect the arithmetic sense, anda few carefully selected additional training examples can signiﬁcantly improveperformance for this type of lemma.3.2.2 Parallel corpora Perhaps the greatest challenge to corpus selection
involves parallel corpora. Parallel treebanks and PropBanks are of increasinginterest to syntax- and semantics-based statistical machine translation efforts,especially for evaluation purposes.
4However, the ideal parallel corpus is almost
impossible to ﬁnd. It should be equally ﬂuent with respect to both the sourceand the target languages, yet at the same time provide a translation that is asliteral as possible and where each individual sentence can be aligned with acorresponding translated sentence, criteria that are at best at odds with eachother. Parliamentary proceedings, such as the Hansards (the ofﬁcial records of theCanadian Parliament), the documents available through Europa (the EuropeanUnion online), and radio shows that are simultaneously broadcast in multiplelanguages, such as FBIS, offer the most promising sources and are treasured bythe community. Indeed, the availability of the Hansards sparked a major transfor-mation in machine translation. These are kept by law in both French and English,and may be legally reproduced and distributed as long as “it is accurately repro-duced and that it does not offend the dignity of the House of Commons or one ofits Members.” The researchers at IBM were thus provided with a large parallelFrench/English corpus of closely aligned, literal translations that proved idealfor statistical word alignment techniques (Brown et al., 1990). The astonishinglyaccurate translations their system was able to produce revolutionized the machinetranslation ﬁeld, and their approach is the basis for increasingly accurate statisticalmachine translation of Chinese and Arabic to English (funded by DARPA-GALE).

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 262 — #25
262 Martha Palmer and Nianwen Xue
3.3 Annotation efﬁciency and consistency
When considering which phenomena are to be annotated, it is crucial to thinkthrough the decision making process the annotators will be faced with, andwhether the task is best done as a single step or with multiple passes. If it is tobe done in multiple passes, then the boundaries between the different layers ofannotation need to be clearly deﬁned. Sometimes the most time-intensive stepin the annotation process is comprehending the sentence or phrase to be anno-tated. There is always a trade-off between deﬁning simple, modular annotationsteps with coherent decision spaces and multiplying the number of times a sen-tence has to be comprehended. The success of the annotation process will dependalmost entirely on how constrained the choices are and on how clear and com-prehensive the guidelines are. It is also important to consider the efﬁcacy of theresulting annotations with respect to training machine learning systems. Althoughit is no absolute guarantee, human accuracy at the task provides an encouragingindication of at least potential system accuracy. Poor human performance almostcertainly presages poor system performance.3.3.1 Annotation errors A primary goal in achieving efﬁcient, consistent
annotation is reducing unnecessary annotation errors. Apart from expected errorscaused by carelessness or fatigue, which can usually be caught through dou-ble annotation, other annotation disagreements are almost entirely a direct resultof confusion about the guidelines. The annotator may not have read the guide-lines thoroughly enough, but more often the guidelines are themselves vague orambiguous with respect to particular phenomena. It is also sometimes the casethat the instances in the data are also vague and/or ambiguous, and open to mul-tiple interpretations. It is important to give the annotators an escape hatch whenthey do not have a clear intuition about an appropriate label, so that they do notspend too much time agonizing.3.3.2 Alternative guideline styles Each different type of annotation requires
a stable, language-independent methodology based on guidelines and widelyaccessible tools. The guidelines need to explicate the details of each individualannotation type as well as interactions between the different types of annotation.For instance, the guidelines for the Proposition Bank outline a process that beginswith creating a Frameset for each individual verb in the corpus to be annotated.The Framesets provide invaluable additional direction for the annotation of theindividual verbs over and above the general annotation guidelines. The Nom-Bank annotation in turn begins by referencing the verb Framesets for associatednominalizations whenever possible. The same approach has been used success-fully for the Chinese PropBank/NomBank. In contrast, the guidelines for thetreebank constitute over 300 pages of detailed syntactic description which haveto be thoroughly analyzed and memorized before a treebanker can be consideredto be fully trained.

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 263 — #26
Linguistic Annotation 263
Syntactic parsing, or treebanking, is undoubtedly one of the most demanding
annotation tasks. Every single possible syntactic phenomenon has to be accountedfor in the guidelines, with examples and explanations, and clearly distinguishedfrom other, similar phenomena that it could be confused with, such as subject-control and object-control verbs, raising verbs, etc. The general rule of thumbis that the treebanking guidelines should not be ﬁnalized until at least 100,000words have been successfully treebanked, which can easily take a year. It takes sixmonths to fully train a treebanker, and there are no shortcuts. For phrase-structuretreebanking, starting with a solid grounding in generative grammar helps, butsince the guidelines make many departures from an exact theoretical interpre-tation, the treebanker also has to be ﬂexible and open minded, and not adheretoo rigidly to theoretical generative grammar. However, given highly motivatedannotators with a thorough understanding of syntax and the ability to pay closeattention to detail, inter-annotator agreement rates of 95 percent and above havebeen achieved for English, Chinese, and Korean treebanking.
OntoNotes verb sense tagging, on the other hand, requires a very short train-
ing period of approximately 20 to 30 hours, which can take as little as two weeks.The guidelines amount to only 11 pages, 4 of which are very detailed instruc-tions for logging onto unix and running the annotation tool. Sense taggers onlyneed to know enough syntax to distinguish certain parts of speech, such as a mainverb as opposed to a verbal past participle used as a modiﬁer, or to recognize thedifference between verb arguments that are noun phrases rather than sententialcomplements. One of the reasons for this brevity is that all of the information fordistinguishing between the different senses of a verb has to be made explicit in theentry for that verb in the sense inventory. Each verb is different, and there are nooverarching general principles for disambiguation that apply equally to all verbs.The sense inventory itself is the most critical component of the sense-taggingguidelines.3.3.3 The annotation process As an illustration of the contrast between
treebanking and sense tagging, treebanking is done on coherent text, sentenceby sentence. Having the entire discourse context in mind can be helpful to thetreebanker faced with referring pronouns and ambiguous attachment choices. Onthe other hand, sense tagging is most effectively done using a lexical sampleapproach, where all of the sentences from the target corpus containing the lemmato be tagged are extracted and displayed to the tagger in a single task. Whiletagging these instances the tagger only has to have the single lemma and its sensesin mind, and does not have to stop and become familiar with several new entriesfor every sentence to be tagged.
5This approach also facilitates the tagger making
consistent choices for that lemma. Even so, if the lemma has too many senses, thecognitive load on the tagger is simply too high. All of the senses cannot be kept inmind, and the tagger has to repeatedly read through the sense entry looking for anappropriate sense label for each instance. This slows down the process and leads toinconsistencies, since the relevant sense might easily be missed. A useful number tokeep in mind is Miller’s 7, plus or minus 2. Taggers can manage up to 9 or 10 senses,

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 264 — #27
264 Martha Palmer and Nianwen Xue
or even in some cases as many as 12. More than that leads to inconsistency anddelay. For the GALE OntoNotes annotation, senses for verb particle constructionsfor highly polysemous verbs (often accounting for close to half the instances) weresplit off from the entry for the main verb, and the instances for the verb weretagged in two passes. The ﬁrst pass allowed for selecting one of 10 to 12 entriesfor the verb or an additional entry for multi-word expressions including verb particle
constructions (MWE). The instances given the MWE tag were tagged in a second
pass using a separate entry which split the verb particle constructions as well asidioms and metaphors into several senses. Overall, OntoNotes sense tagging hasan ITA of 89 percent and is three to four times as fast as WordNet sense tagging.3.3.4 Determining tagging candidates Determining what constitutes an item
to be tagged is just as important as knowing the set of possible labels. In treebank-ing the span is the entire sentence, so there is no room for confusion. For sensetagging, it is a single lexical item (or a multi-word expression based on a spe-ciﬁc item) which has been predetermined and appears highlighted in the sentence.However, the choice of tagging candidates is not always so clear cut. For the ACEevent tagging, the annotators had to select from a sentence the string of words thatcorresponded to each event participant, a major source of inter-annotator disagree-ment. For the initial TimeML annotation, the annotators could pick and choosewhich events in a paragraph were supposed to have temporal relations, again, amajor source of disagreement. A major simplifying assumption for the PropBankannotation was the decision to base it on the existing treebank parses. The events
corresponded to clausal verbs, and the event participants that received semanticrole labels were all arguments of the verb. The annotators simply had to choose anode in the tree that corresponded to a verb argument and assign a label to it; thespan of the node was already predetermined.3.3.5 The necessity of pilots Finally, no matter how much thought has gone
into deﬁning the guidelines and constraining annotator choices, a pilot annotationphase is still essential. Annotators may not interpret the guidelines as intended,the data may present much more variance than was predicted, there could be(and almost certainly are) bugs in the annotation tools. Guidelines should neverbe ﬁnalized until they have been thoroughly tested on a substantial representativedata set with several annotators. Any annotation proposal should include time fora pilot test, revision of the guidelines, and then another pilot test with additionalrevisions. The goal of guidelines development is to deﬁne a clear, unambiguoustask which can be decided by rapid application of human intuition. If annotationagreement is lower than expected, it is much more likely the fault of the guidelinesor the task deﬁnition than the fault of the annotators.
3.4 Annotation infrastructure and tools
Because linguistic annotation is generally considered to be a ‘data’ project, anno-tation infrastructure and tools, which involve programming support, are an

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 265 — #28
Linguistic Annotation 265
often overlooked area. It is not uncommon for a linguistic annotation project tohave insufﬁcient funds to hire programmers to provide necessary programmingsupport. This results in annotators having to make do with suboptimal annota-tion tools. Poor tools have a negative impact on both the quality and the quantityof the annotated data. This section will outline the infrastructure and tool needsbefore, during, and after annotation. For any large-scale, production-level annota-tion project, having the proper annotation infrastructure in place at the beginning,as well as equipping the annotator with user-friendly annotation tools, is essentialfor the success of the project.3.4.1 Task assignment For the infrastructure of large-scale annotation projects,
before human annotation can take place, it is essential to think through two keyissues: annotator management and data ﬂow. Large-scale annotation projects can-not be done by one person and typically involve multiple annotators. Having aclear idea of who can access what data is crucial. For sense tagging and Prop-Banking, where the standard practice is to perform double-blind annotation wheretwo (and only two) annotators are asked to annotate the same data, it is vir-tually impossible to ask the annotators themselves to keep track of which datathey should or should not annotate. Sense taggers and propbankers are typicallypart-time annotators, and they tend to work on an annotation project for variablelengths of time. It is often a luxury to have the same two annotators ﬁnish anno-tating the same data during the lifetime of the project. As a result, it is necessaryto build into the annotation infrastructure a mechanism to distribute annotationassignments to annotators so that the annotator can simply take the next avail-able assignment and get on with it. Such a mechanism ensures that a given chunkof data is always double annotated if that is the goal and that an annotator doesnot accidentally re-annotate data that already has double annotations or skip datathat needs annotations; mistakes that inevitably happen if the annotators are leftto their own devices.3.4.2 Data ﬂow management In addition to annotator management, it is also
important to build into the infrastructure functionalities for data ﬂow manage-ment. Annotation is expensive and time-intensive, and it is frustrating to loseannotated data. Annotators are usually linguistic experts who are familiar withthe linguistic phenomena they are asked to annotate, but are not necessarily savvycomputer users who could be expected to maintain data security. It is necessaryto think through how the annotated data should be saved periodically while theannotator is annotating. It is also good practice to maintain version control of theannotated data using a version control facility such as CVS or SVN. Automatic ver-sion control systems allows multiple copies of data to be checked in, keeping trackof any differences in the most recently checked in versions and of who checkedthem in and when.
6
3.4.3 User-friendly annotation tools After an annotation task is set up and the
annotator starts tagging, the annotation tool becomes central to the annotation

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 266 — #29
266 Martha Palmer and Nianwen Xue
process. The quality of the annotation tool can have a great impact on annotationefﬁciency and consistency. Although it seems counterintuitive, since mouse-basedannotation tools have a shorter learning curve and are often preferred by novelannotators, veteran annotators generally prefer keyboard-based annotation inter-faces, since they are faster and are less likely to result in issues such as tendonitisand carpal tunnel syndrome. Long hours spent at annotation have emphasizedthe ergonomic advantages and greater annotation efﬁciency of the keyboard. Theannotation interface used for the syntactic annotation of the Chinese Treebank isan Emacs-based tool that makes heavy use of keyboard strokes and uses veryfew mouse clicks. Quite a few mouse-based treebanking tools such as WordFreakand Tred have subsequently been built, and LDC now uses one for English, butthe Emacs-based tool is still the preferred treebanking tool for many veterantreebankers, including Chinese and Korean treebankers. It is important to giveannotators a choice of keyboard strokes versus mouse clicks.
Annotation tools also have a role in maintaining annotation consistency. If
designed properly, an annotation tool can prevent an annotator from entering alabel that is not in the tagset associated with the task, or from accidentally deletingor changing the source data, which is usually forbidden. Maintainability, cus-tomizability, and portability are also important considerations when designingan annotation tool. An annotation tool is often still used long after its originaldevelopers have moved on, so someone else needs to maintain it. It is thus advis-able that the tool be well documented and written in a widely used programminglanguage. Multi-lingual annotation is increasingly gaining in popularity, so porta-bility to other natural languages is also an important consideration when choosinga programming language.3.4.4 Postprocessing The annotation process does not stop when the human
annotator ﬁnishes manual annotation. The output of the annotation tool is oftennot in the ﬁnal format that the annotation data users expect for processing. Also,for quality-control purposes, there is often a data validation process after thehuman annotation is done. For syntactic parsing, this validation can check if theparse is in a valid format, for example, if the left and right brackets match upand if the syntactic labels in the annotated data are all legitimate labels. For sensetagging, the validation process can check if the sense numbers correspond to thesense inventory entry choices. Certain predictable annotation errors can also beautomatically detected and corrected. Just as there is user-friendly software, thereis also user-friendly data. Not all users of linguistically annotated data are wellversed in the linguistic concepts and their linguistic justiﬁcations as encoded inthe annotations, and some will lose interest if the representation is too complicatedfor them to understand. Putting the annotations in an easily understood formatcan maximize the usability of the data that has taken so much effort to produce.
3.5 Annotation evaluation
There are two main aspects to evaluating the annotation itself. One has to do withextrinsic measures of the annotation validity and consistency. The other focuses on

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 267 — #30
Linguistic Annotation 267
the agreement between the annotators (ITA), which, since common wisdom treatshuman annotator agreement ﬁgures as an upper bound for system performance,is of central importance (see Chapter 11,
EVALUATION OF NLP SYSTEMS , for a more
detailed discussion).
The question then arises as to what to do with annotator disagreements. Are
they just mistakes on the part of one annotator that need to be corrected viaan adjudication mechanism? This is in fact often the case, but disagreementscan also indicate especially vague or ambiguous linguistic phenomena whichmight need special handling. The OntoNotes sense-tagging project uses ITA asa measure of the clarity of the sense inventory. If ITA is below 90 percent thelexicographers are asked to re-examine the groupings of the WordNet senses forthat lemma. They cannot examine the actual tagged instances, but they can lookat a confusion matrix that shows which senses the annotators disagree on. Theconfusion matrix sometimes reveals a striking mix-up between two particularsenses, pointing the lexicographer exactly to the senses that need clariﬁcation.On the other hand, even after a new lexicographer has examined the confusionmatrix and the groupings carefully, it may not be at all clear what could or shouldbe changed. Even when all disagreements have been adjudicated to producethe gold standard data, system builders often want information about whichlemmas, or senses, have been especially difﬁcult to tag, and the original anno-tator disagreements can be a useful source of this type of information. Evaluationtechniques can be weighted to penalize systems less for missing the more difﬁcultcases.
The most straightforward measurement of ITA is simple percentage agreement,
and this is also the ﬁgure that correlates the most highly with system performance(Chen & Palmer 2009). However, it has often been pointed out that there is a largediscrepancy between 90 percent ITA when a lemma has a most frequent sensebaseline of 85 percent, and 90 percent ITA when the most frequent sense base-line is 60 percent. Chance agreement, and therefore expected agreement, wouldbe much higher with the former than with the latter. The kappa, k, coefﬁcient
of agreement can take this into account, and is generally considered as provid-ing a more accurate assessment of how much value the annotation is actuallyadding. The basic technique subtracts expected agreement, E, from observedagreement, A, and then divides it by 1 minus the expected agreement, as givenbelow.k=A−E
1−E
There are several subtleties in how the expected agreement can be calculated,
depending on whether or not there are more than two annotators, what theexpected distribution of choices would be, and whether or not annotator biasneeds to be taken into account. Artstein and Poesio do an excellent job of sur-veying the state of the art with respect to these various options (Artstein & Poesio2008). In general a kappa score of 0.8 or higher is considered desirable.

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 268 — #31
268 Martha Palmer and Nianwen Xue
An equally important consideration in measuring ITA is to decide exactly what
is being taken into consideration. If treebankers are being evaluated, a simpleParseval score (Black et al., 1991) which matches the sequences of words thathave been bracketed together is usually deemed sufﬁcient. This technique pro-duces precision (of the total number of bracketed constituents produced by anannotator, what percentage are correct) and recall (what percentage of the cor-rect possible bracketed constituents did the annotator produce) ﬁgures as wellas the number of crossing brackets. The F-score is a weighted harmonic meanof precision and recall. These scores may or may not take the labels of thosebracketed phrases into account. For the Chinese Treebank, based on a randomlyselected 20 percent portion of the corpus, the F-score for the average ITA is 93.8percent. After discrepancies between the annotators are reconciled and a goldstandard produced, annotator gold-standard comparisons, or accuracy compar-isons, can be made. For the Chinese Treebank, the F-score for average accuracy was96.7 percent.
However, many researchers feel the need for parsing evaluations that are more
stringent than Parseval (Carroll et al., 2002, 2003; Hajiˇ c et al., 2009), which wouldtranslate into more detailed annotator comparisons as well.
With respect to sense tagging, the determination of ITA is from one perspective
more straightforward. Senseval (Palmer et al., 2001) and Semeval (Pradhan et al.,2007b) evaluations typically provide pointers to the words to be tagged, so recall isalways 100 percent and there is no need to calculate an F-score. Precision is there-fore equivalent to accuracy. However, as discussed in Chapter 11,
EVALUATION
OF NLP SYSTEMS , if hierarchical sense entries are provided, the correctness of an
answer tag may be weighted depending on its closeness to the correct tag in thehierarchy, making things more complex. In the same way, annotator agreementsand disagreements on sense tags can be weighted based on the relatedness of thesense tags chosen.
Coreference annotation presents yet another set of challenges, since coreferences
typically consist of sets of mentions. The question is how to score two coreferencesets which are almost, but not quite, identical. Artstein and Poesio (2008) offeruseful insights on this issue.
3.6 Pre-processing
There are several questions to be addressed when considering pre-processing:•What types of pre-processing might facilitate the annotation, and can this bedone automatically?
•Does the corpus need to be stripped of headers and extraneous markup? Willthey need to be replaced later?
•Is there a pre-existing automatic annotation tool that can be applied withoutimposing undue bias?

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 269 — #32
Linguistic Annotation 269
There is considerable evidence that the productivity of manual annotation can
be sped up by pre-processing the data with sufﬁciently accurate automatic taggers(Chiou et al., 2001). This method has been particularly successful with treebank-ing, where automatic parsers are ﬁrst run and then the output is hand corrected.Note that this is only useful if the automatic parsers already have high accuracy.Poor parsers simply slow down the process, and the treebankers would pre-fer to start from scratch.
7However, in spite of demonstrated productivity gains
from automatic pre-processing, current annotation practices frequently fail to takeadvantage of this approach, possibly because of the difﬁculty of integrating thesesystems into new annotation tasks.
Even more beneﬁt could be derived from using sophisticated machine learning
techniques to aid in the selection of instances to be tagged, in order to maxi-mize their utility and minimize the total annotation effort. For simple classiﬁcationtasks like word-sense disambiguation, there are accepted practices which utilizeautomatic WSD systems, such as active learning techniques (Chen et al., 2006a).However, for more complex annotations such as syntactic structure, pinpointingnovel or unfamiliar items in the data remains a more challenging problem (Hwa2004). Fundamental research is needed to develop informed sampling techniquesfor complex annotation that can be integrated into the annotation effort.
4 Conclusion
This chapter has brieﬂy surveyed several speciﬁc annotation layers and reviewedgeneral principles for developing annotation projects. Annotation schemes arelikely to be as varied as natural languages, and there are a host of reasons forchoosing one annotation tool or evaluation technique over another. However, theprinciples stated by Krippendorf (2004) for a content analysis annotation schemeare equally applicable to linguistic annotation schemes for natural languageprocessing systems:•it must employ an exhaustively formulated, clear, and usable coding schemetogether with step-by-step instructions on how to use it;
•it must use clearly speciﬁed criteria concerning the choice of coders (so thatothers may use such criteria to reproduce the data);
•it must ensure that the coders that generate the data used to measure repro-ducibility work independently of each other. (See Chapter 11,
EVALUATION OF
NLP SYSTEMS )
We all long for the day when unsupervised and semi-supervised techniques
will automatically induce the grammars and sense clusters that drive our nat-ural language processing systems. It may be the case that the more effectivelyand coherently we annotate the linguistic layers that we currently understand, thesooner that day will come.

“9781405155816_4_010” — 2010/5/8 — 11:55 — page 270 — #33
270 Martha Palmer and Nianwen Xue
ACKNOWLEDGMENT
The authors would like to express their gratitude to Stephanie Strassel, Randi Tangee,Christiane Fellbaum and Eduard Hovy, whose contributions as co-authors with MarthaPalmer for the MINDS report on “Historical development and future directions in dataresource development” informed this article in many ways.
NOTES
1 The ACE guidelines include complex subtypes for each of these categories and multi-
tudinous examples.
2 Named entity (or nominal entity) tagging is very similar to part-of-speech tagging, in
that one set of tags is used for all entities.
3 Dan Flickinger provided these examples during an oral presentation at an invitation-
only treebank workshop held in conjunction with NAACL07, http://faculty.washington.edu/fxia/treebank/workshop07/formalisms/hpsg.pdf
4 The OntoNotes group, at the insistent request of the BBN, IBM, and SRI teams, is
currently treebanking and PropBanking all of the evaluation data from the ﬁrst threeyears of the program.
5 This beneﬁt has to be weighed against how many times the sentence will be read. If all
the content words are to be tagged, and if several of them are polysemous (not likely),the sentence may be re-read several times.
6 See www.nongnu.org/cvs/ for CVS and http://en.wikipedia.org/wiki/Subversion_
(software) for SVN.
7 This is similar to the dismay with which human translators face the task of hand
correcting the output of machine translation systems.

