“9781405155816_4_014” — 2010/5/8 — 12:04 — page 364 — #1
14 Segmentation
and Morphology
JOHN A. GOLDSMITH
1 Introduction
1.1 General remarks
The ﬁeld of morphology has as its domain the study of what a word is in naturallanguages. In practice, this has meant the study of four relatively autonomousaspects of natural language: (1) the identiﬁcation of the lexicon of a language,(2) morphophonology, (3) morphosyntax, and (4) morphological decomposition,or the study of word-internal structure.
At ﬁrst blush, identifying the lexicon of a language – what the words are – may
seem simple, especially in languages which are conventionally written with spacesbetween words, but, as we shall see below, the task is more complicated at morepoints than one would expect, and in some scientiﬁc contexts we may be inter-ested in knowing under what conditions the spaces that mark separation betweenwords can be predicted. To explain what points (2) through (4) above cover, weintroduce the notion of morph – a natural, but not entirely uncontroversial notion.
If we consider the written English words jump, jumps ,jumped ,a n d jumping,w e
note that they all begin with the string jump, and three of them are formed by fol-
lowing jump bys,ed,o ring. When words can be decomposed directly into such
pieces, and when the pieces recur in a functionally regular way, we call thosepieces morphs. With the concept of morph in hand, we may consider the following
deﬁnitions:•Morphophonology . It is often the case that two or more morphs are similar
in form, play a nearly identical role in the language, and can each be analyt-ically understood as the realization of a single abstract element – ‘abstract’ inthe sense that it characterizes a particular grammatical function, and abstractsaway from one or more changes in spelling or pronunciation. For example,the regular way in which nouns form a plural in English is with a sufﬁxal-s, but words ending in s, sh ,a n d chform their plurals with a sufﬁxal -es.
Both -sand -esare thus morphs in English, and we may consider them as

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 365 — #2
Segmentation and Morphology 365
forming a class which we call a morpheme: the pair of morphs { s, -es},
whose grammatical function is to mark plural nouns. The principles thatare involved in determining which morph is used as the correct realizationof a morpheme in any given case is the responsibility of morphophonology.Morphophonology is the shared responsibility of the disciplines of phonologyand morphology.
•Morphosyntax. Syntax is the domain of language analysis responsible for theanalysis of sentence formation, given an account of the words of a language.
1
In the very simplest cases, the syntactic structure of well-formed sentences ina language can be described in terms of atomic and unanalyzed words, butgrammar is never really that simple. In reality, the morphs that appear insideone word may also specify information about other words in the sentence –for example, the verbal sufﬁx -sinSincerity frightens John speciﬁes that the
subject of the verb is grammatically singular. Thus statements about syn-tax inevitably include some that peer into the internal structure of at leastsome words in a language, and in many languages this is the rule ratherthan the exception. Morphosyntax deals with the relationship between themorphemes found inside one word and the other words that surround it in thelarger sentence; it is the shared responsibility of the disciplines of syntax andmorphology.
•Morphological decomposition. While English has many words which containonly a single morpheme (e.g., while, class, change ), it also has many words that
are decomposable into morphs, with one or more sufﬁxes ( help-ful ,thought-less-
ness), one or more preﬁxes (out-last), or combinations (un-help-ful ). But English
is rather on the tame side as natural languages go; many languages regularlyhave several afﬁxes in their nouns, adjectives, and, even more often, their verbs(e.g., Spanish bon-it-a-s, which consists of a root meaning ‘good,’ a diminutive
sufﬁx -it, a feminine sufﬁx - a, and a plural sufﬁx -s).
In the remainder of this introductory section, we will give a brief overview
of the kinds of questions that have traditionally been the focus of the study ofmorphology in general linguistics. This will serve as background to the dis-cussion of the following three questions which are speciﬁcally computational incharacter.(1) Can we develop – and if so, how – a language-independent algorithm that
takes as input a large sequence of symbols representing letters or phonemesand provides as output that same sequence with an indication of how thesequence is divided into words? This question puts into algorithmic form thequestion of how we divide a string of symbols into words.
(2) How can we develop a language-independent algorithm that takes as input
a list of words and provides as output a segmentation of the words intomorphemes, appropriately labeled as preﬁx, stem, or sufﬁx – in sum, a basicmorphology of the language that produced the word list?
(3) How can we implement our knowledge of morphology in computational
systems in order to improve performance in natural language processing?

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 366 — #3
366 John A. Goldsmith
1.2 Morphology
Users of natural languages (which is to say, all of us) need no persuasion thatwords are naturally occurring units. We may quibble as to whether expressionslike of course should be treated as one word or two, but there is no disagreement
about the notion that sentences can be analytically broken down into componentwords. Linguists and others who think deeply about such questions as ‘what isaword ?’ generally focus on the idea that there is evidence in each language from
phonology, morphology, syntax, and semantics which points in the direction of anatural chunk corresponding to the traditional notion of word. From a phonologi-
cal point of view, phenomena that occur inside a word are often quite distinct fromphenomena that occur at word boundary – the conditions under which a tis a ﬂap
in American English differ considerably in this way, for example. In a similar way,we ﬁnd that at the point in an utterance between two words, we can expand theutterance by adding material. For example, we can convince ourselves that their is
a separate word, and not a preﬁx to the word dream, because we can say: John and
his wife will follow their – or at least his – dream next year.
There are some difﬁcult intermediate cases which linguists call clitics –
morphemes whose status as a full-ﬂedged word is dubious; the possessive suf-ﬁx ’s in English is such a case, because although in many respects it seems like a
sufﬁx to the word that precedes it, it may nonetheless be syntactically and seman-tically associated with a preceding phrase, as in an example like a friend of mine’s
ﬁrst husband (contrast this with a friend of my ﬁrst husband).
In all languages, or virtually all, it is appropriate to analytically break words
down into component pieces, called morphs, and then to bundle morphs back into
the functional units we call morphemes; such an analysis is part of the functional-ity of a morphology, and is the central subject of this chapter (when a morphemecorresponds to only a single morph, as is often the case, we generally ignorethe difference between a morph and a morpheme). In addition, we expect of acomplete morphology that it will associate the appropriate set of morphosyntacticfeatures with a word, to the extent that the word’s morphological decompositioncan serve as a basis of specifying those features. Thus books should be analyzed as
book plus a sufﬁx - s,a n dt h es u f ﬁ x- sshould be marked as indicating plurality for
nouns in English.
Morphologies are motivated by four considerations: (1) the discovery of reg-
ularities and redundancies in the lexicon of a language (such as the pattern inwalk:walks:walking :: jump:jumps:jumping); (2) the need to make explicit the relation-ship between grammatical features (such as nominal
NUMBER or verbal TENSE )
and the afﬁxes whose function it is to express these features; (3) the need to predictthe occurrences of words not found in a training corpus; and (4) the usefulness ofbreaking words into parts in order to achieve better models for statistical trans-lation, information retrieval, and other tasks that are sensitive to the meaningof a text.
Thus morphological models offer a level of segmentation that is typically larger
than the individual letter,
2and smaller than the word . For example, the English

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 367 — #4
Segmentation and Morphology 367
word unhelpful can be analyzed as a single word, as a sequence of nine letters, or
from a morphological point of view as a sequence of the preﬁx un-, the stem help,
and the sufﬁx - ful.
The distinction between inﬂectional morphology and derivational morphology is
one drawn by most accounts of morphology, but it remains a controversial ques-tion for some as to whether a clear line can be drawn between the two. Theintuition that lies behind the distinction is reasonable enough; to illustrate this,let us consider an example from English. We may wish to say that jump, jumps ,
and jumped are three words, but they are all different versions (in some sense)
of a single verb stem. The verb stem (jump) is coherent in three ways: it has arecognizable phonological form ( jump), it shares a coherent semantic content, and
it is inﬂected in ways that it shares with many other stems: in particular, it takes a
sufﬁxal -sin the third person singular present tense, and an -edin the past tense. In
addition to the characteristics just mentioned, inﬂectional afﬁxes also are usuallyperipheral – if they are sufﬁxes, the inﬂectional sufﬁxes are at the very end of the
word, and if preﬁxes, at the very beginning, and while they contribute grammat-ical information to the word they contain, they do not shift the part of speech oftheir word. The sufﬁxes -sand - edare taken to be inﬂectional sufﬁxes, and they
differ from derivational sufﬁxes such as -ity (as in sanity )o r-ness (as in goodness,
truthiness)
3or-ize(as in radicalize, winterize ). Derivational afﬁxes more often than
not play the role of indicating a change of part of speech, in the sense that sane is an
adjective, and san-ity is a noun, just as radicalize and winterize are verbs, but contain
within them stems of a different category (adjective and noun, respectively). Inaddition, the semantic relationship between pairs of words related by derivationalafﬁxes is often far less regular than that found between pairs of words related byinﬂectional afﬁxes. Thus, while the relationship between jump and jumped ,walk
and walked , and so on, is semantically regular, the same cannot be said of the rela-
tionship between words such as woman and womanize, author and authorize,a n d
winter and winterize .
4
For all of these reasons, most accounts of morphology distinguish between the
analysis of a word’s inﬂectional morphology, which isolates a stem (an inﬂectional
stem ) from its inﬂectional afﬁxes, and the word’s derivational morphology, which
further breaks the (inﬂectional) stem into component pieces. Thus winterized is
analyzed into a stem winterize plus an inﬂectional sufﬁx -ed , and the stem win-
terize is divided into a stem winter plus a derivational sufﬁx -ize. The term root
is often used to refer to a stem that cannot be morphologically decomposed. Aninﬂectional stem is associated with a single lexical category, such as noun, verb,adjective, etc. Just as importantly, the inﬂectional stem is the item in a lexiconwhich can be (and usually is) associated with a particular meaning, one that isgenerally not strictly predictable from the meanings of its components.
It is not unusual to ﬁnd situations in which (by what I have just said) we ﬁnd two
stems that are spelled and pronounced identically: walk is both a noun and a verb,
for example. The term conversion is often used to refer to this situation, in which
a stem is (so to speak) converted from one part of speech to another without anyovert afﬁxation, though such analysis generally assumes that one can determine

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 368 — #5
368 John A. Goldsmith
which of the two (noun or verb, in this case) is the more fundamental category ofthe two, on a stem-by-stem basis.
Inﬂectional stems can also be created by a process of compounding, as in watch-
dogoreyepatch . Such compound stems may include inﬂectional afﬁxes, though
many languages impose rather severe restrictions on the inﬂectional morphologypermitted inside a compound (this is distinct from the case in which inﬂec-tional afﬁxes ‘attach’ to compounds, as in watchdog-s, and exceptions exist, such
asmonths-long, which is quite different in meaning from month-long). In some
languages, a compound is formed by concatenating two stems; in others, a shortlinking element appears between them. The linking element of Greek compounds,-o-, appears in many compounds borrowed into English, such as in hipp-o-potamus .
All of the broad generalizations that I have suggested to this point, like most
such broad generalizations, only go so far, and there are always phenomena in anatural language which demand a more complex view. I will sketch here some ofthe ways in which complexities arise most often.
First of all, in inﬂectional systems, there are typically a set of anywhere from two
to a dozen relatively independent grammatical features which may be relevant toa particular word class, such as noun or verb. For example, a verb may be speci-ﬁed for the
PERSON and NUMBER of its subject, of its object, and for its TENSE ,a n d
for other characteristics as well. Only rarely – indeed, vanishingly rarely – is eachsuch feature realized separately as its own morph. In most cases, it is a small tupleof features that is linked to a particular afﬁx, as in the case of the English verbalsufﬁx -s, which marks third person & singular & present tense .O n
the other hand, it is often the case that a single afﬁx is used to mark more thanone tuple of features; in written French, the sufﬁx - ismarks the present-tense
singular subject agreement marker for a certain class of verbs; for exam-
ple, ﬁnis is either ‘(I) ﬁnish’o r‘ (you (sg.)) ﬁnish,’ in either the first orsecond
person, but not in the third person (which is spelled ﬁnit).
For this reason, linguists often think of inﬂectional systems as being hyper-
rectangles in a large space, where each dimension corresponds to a grammaticalfeature, and where the edge of a hyperrectangle is divided into intervals corre-sponding to the feature values that the feature may take on (that is, person is a
feature, and it may take on the values first, second, orthird;
NUMBER
is a feature, and it may take on the values singular andplural, in some
languages). Each afﬁx will be associated with one or, quite often, several smallsub-hyper-rectangles in such a system.
The complexities do not stop there. It is often the case that there are two or more
forms of the stem used, depending on which subpart of the inﬂectional hyper-rectangle we are interested in. An extreme case is that of the stem went in English,
used as the stem in the past, when gois used otherwise. This case is a bit special,
since the form of goand went is so different (when the stems are this different,
linguists refer to this as suppletion ), but it is often found that several related (but
distinct) stems will be used for different parts of the system. In French, for exam-ple, the present tense stem for ‘ write’ is spelled ‘écri-’ in the singular, but ‘ écriv-’i n
the plural. This is often referred to as stem alternation .

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 369 — #6
Segmentation and Morphology 369
In addition, a language may employ a whole arsenal of different inﬂectional
hyper-rectangles, even within a single lexical category. The Romance languagesare perfectly typical in having between three and six so-called ‘verb classes,’ whichemploy quite different sets of sufﬁxal patterns for marking precisely the same setof grammatical features. It is the verb stem that decides which inﬂectional set willbe used for its morphology (see Goldsmith & O’Brien 2006).
Finally, we must acknowledge that not all morphology is properly thought of
as the concatenation of morphs. In English, and many of the other Indo-Europeanlanguages, we ﬁnd inﬂectional patterns on verbs which consist of sets of stems(these are called strong verbs) that differ primarily with regard to the vowel: the
past of stand isstood, the past of sing issang,a n dt h ep a s to f catch iscaught .W e
will focus on those aspects of morphology which are strictly concatenative – inwhich words can be analyzed as sequences of morphs – but we will return to thetreatment of the more general case below as well.
1.3 Static and dynamic metaphors
Inﬂectional morphology is complex in most natural languages. It is common fornouns to be marked morphologically for
NUMBER and CASE , and for verbs to be
marked morphologically for TENSE ,PERSON ,NUMBER ,MOOD (whether the verb
is in the indicative or the subjunctive), and syntactic position (whether the verbis in a subordinate clause of the sentence or not), for example. In fact, the hall-mark of inﬂectional morphology – how we recognize it when we see it – is theappearance of several features that are logically orthogonal to one another, all ofwhich are relevant for the realization of all, or most, of the words in a given partof speech (noun, verb, adjective). To put that a bit more concretely: to know Latinmorphology is to know that a given verb is speciﬁed for the features of person,number, tense, and mood. The verb cant¯ois in the first-person ,singular,
present-tense indicative form, and the same is true of a very large, and
potentially unbounded, set of verbs ending in -¯o.
Lying behind that very speciﬁc knowledge is the understanding that first
person is a value of the feature
PERSON ,t h a t singular is a value of the feature
NUMBER ,t h a t present is a value of the feature TENSE ,a n dt h a t indicative
is a value of the feature MOOD . There are some dependencies among these fea-
tures: there are more tenses when the mood is indicative and fewer when it issubjunctive, but these dependencies are the exception rather than the rule amonginﬂectional features. There is no logical ordering of the features, for the most part:there is no logical or grammatical reason for
PERSON to precede NUMBER ,o rt o
follow it (there may well be linear ordering of the morphemes that realize thesemorphemes, though). For all of these reasons, inﬂectional systems can encodemany different combinations of feature speciﬁcations – quite unlike what we ﬁndwith derivational morphology.
Some regularities in the morphology of a language are best expressed in terms
that refer only to these inﬂectional features: for example, while the feature
TENSE
in Spanish may take on four values in the indicative mood ( present, future,

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 370 — #7
370 John A. Goldsmith
aorist,a n d imperfective), it takes on only two values in the subjunctive
mood ( present andpast); in German, the forms of the nominative and
accusative are the same for all neuter nouns. On the other hand, other general-
izations address characteristics of the phonological realization of these features asmorphs: in Finnish nouns, plural
NUMBER is marked (temporally, graphically)
before CASE .
Much of contemporary linguistic theory is dominated in a curious way by the
belief that there is a correct order in which various aspects of the representation ofa word or sentence are constructed ; derivational theories are the clearest exam-
ple of this. Reﬂecting on this, Stump (2001) distinguishes between incremental
approaches, in which the process of adding an afﬁx also adds morphosyntacticfeatures, and realizational approaches, in which the process of adding an afﬁx has
access to a representation in which morphosyntactic features are present (or already
present, as a derivationalist would have it). However, it is frequently the case thatthe distinction between these two approaches vanishes in a computational imple-mentation, either because the analysis is conceptually static rather than dynamic(that is, it places well-formedness conditions on representations rather than offer-ing a step-by-step method of producing representations), or because the dynamicthat the computational implementation embodies is a different one (for a detaileddiscussion of this point in the context of ﬁnite state transducers, see Roark & Sproat2006, chapter 3).
All of the material presented in this section is the result of the work of genera-
tions of linguists reﬂecting on many languages. In the next two sections, we willconsider how the problem of learning about words and morphological structure
can be reconstructed as a computational question of learning.
2 Unsupervised Learning of Words
In this section, we will discuss the computational problem of discovering wordsfrom a large sequence of symbols that bear no explicit indication of where oneword ends and the next begins. We will start by distinguishing two formulationsof this problem: one relatively easy, and the other quite difﬁcult.
2.1 The two problems of word segmentation
Let us consider strings of symbols chosen from an alphabet Σ, which the reader
may think of as the letters of a written language or the sounds of a spoken lan-guage. There are two broad families of ways in which we analyze the structure ofstrings of symbols. One uses probabilistic models, which tell us about the probabil-ities of selection of elements from Σin the future, given the past, typically the very
local, recent past. In such models, the structure that we impose lies in the depar-ture of the system from a uniform distribution, and the probability of a symbol istypically conditioned by a small number of immediately preceding symbols. Theother uses segmentation models, whose purpose is to allow for the restructuring

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 371 — #8
Segmentation and Morphology 371
of a string of elements from a ﬁne-grained alphabet (such as Σ) to a coarser set L
which we call a lexicon , and which should be thought of intuitively as a set of sub-
strings generated from Σ, that is, as a subset of Σ∗. For now, we may simply think
of the members of Las our words . We will focus primarily on the second family of
models, those employing chunking, but I do not wish to even suggest that there is
any sort of incompatibility between the two approaches, because there is not.
Each word w∈Lis associated with an element of Σ∗,i t sspell-out – I write ‘asso-
ciated with’ rather than ‘is,’ because wmay be decorated with other information,
including meaning, syntactic category, and so on; but for simplicity of exposition,we may assume that no two elements in a lexicon are associated with the samespell-out. L
∗is any concatenation of words, and any member sofL∗has a natural
way of being thought of as a member of Σ∗: any sequence of words is naturally
thought of as a sequence of letters, too. So far, no delimiters, like space or otherpunctuation, have come into the picture.
We will always assume that each member of Σis also a member of L(roughly
speaking, each member of the alphabet is a word), and so we can be sure that anystring in Σ
∗corresponds to at least one member of L∗, but in most cases that we
care about, a string in Σ∗will correspond to more than one string in L∗, which is
just a way of saying that breaking a string into words is not trivial. Each memberofL
∗which corresponds to a given string of letters we call a parse of that string.
The string atone has three natural non-trivial parses: atone, at one ,a n d a tone,b u ti t
has others as well.
The ﬁrstproblem of word segmentation, then, is to ﬁnd a method to take a string
that in fact consists of strings of words, but which is presented as a string of letterswith no indication of where one word ends and the next begins, and then from thisstring to reconstruct where the word breaks are. It is easy to go from such a corpusC
1in which words are separated by spaces to a corpus C2in which all spaces have
been removed, but can we reconstruct C1from C2with no information beyond
word frequency? Given a corpus C1which indicates word boundaries separating
words, we can easily construct a lexicon L, and a new corpus C2in which the word
boundaries have been eliminated. Can we ﬁnd a language-independent algorithmS
1(L,C2)that can reconstruct C1? Put another way, can we deﬁne a method (either
foolproof or just very good) that is able to put spaces back into a text with nomore than a knowledge of the lexicon of the language from which the text wasdrawn?
There is no guarantee that such an algorithm exists for a speciﬁc language or
for languages in general, nor is there a guarantee that, if it exists (for a language,or for languages), we can ﬁnd it. Two families of natural approaches exist: thegreedy and the probabilistic. The greedy approach scans through the string S:a t
position i, it looks for the longest substring s
∗inSbeginning at point ithat appears
in the lexicon; it then decides that s∗appears there in S, and it then skips to position
i+|s∗|and repeats the operation. The probabilistic model assumes a Markov prob-
abilistic model over L∗(typically a zero-order or ﬁrst-order Markov model), and
ﬁnds the string in L∗with the highest probability among all such strings whose
spell-out is S.

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 372 — #9
372 John A. Goldsmith
Device 1
Stripped corpusLexicon
Original corpus
Stripped corpus Device 2 Lexicon
Figure 14.1 The two problems of word segmentation.
In general, we may wish to develop an algorithm that assigns a probability dis-
tribution over possible analyses, allowing for ranking of analyses: given a stringanicecream, we may develop an algorithm that prefers an ice cream toan i c ec r e a m by
assigning a higher probability to an ice cream. Linguists working on Chinese and
Japanese have contributed signiﬁcantly to improvements in our understanding ofthis problem (see, e.g., Sproat et al., 1996; Ando & Lee 2003; Teahan et al., 2000, andthe series of Chinese word segmentation bakeoffs easily found on the internet).
The second problem of word segmentation is one large step harder than the ﬁrst:
given a long string of symbols with no breaks indicated, can we infer what thewords are? See Figure 14.1. This problem asks whether it is possible to ﬁnd ageneral algorithm S
2which takes as input a corpus C2, which was created by strip-
ping boundaries from a corpus C1, and which gives as output a lexicon Lwhich
will satisfy the conditions for the lexicon Lneeded for S1, the solution to the ﬁrst
problem.
Since, for any large corpus which has been stripped of its word boundaries,
there are an astronomical number of different lexicons that are logically consis-tent with that stripped corpus, it should go without saying that if we can solvethe second problem for naturally occurring corpora in real languages, we do notexpect it to be extendable to just anyrandomly generable corpus: to put it another
way, to the extent that we can solve this problem, it will be by inferring some-thing about the nature of the device that generated the data in the ﬁrst place –something about the nature of human language, if it is natural language that weare exploring.
The problem of word segmentation may seem artiﬁcial from the point of view
of someone familiar with reading Western languages: it is the problem of locatingthe breaks between words in a corpus. In written English, as in many other writtenlanguages, the problem is trivial, since we effortlessly mark those breaks withwhite space. But the problem is not at all trivial in the case of a number of Asianlanguages, including Chinese and Japanese, where the white space conventionis not followed, and the problem is not at all trivial from the point of view ofcontinuous speech recognition, or that of the scientiﬁc problem of understandinghow infants, still incapable of reading, are able to infer the existence of words inthe speech they hear around them.

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 373 — #10
Segmentation and Morphology 373
Another computational perspective from which the problem of word breaking
is interesting is this: to what extent do methods of analysis that have worked wellin non-linguistic domains work well to solve this particular problem? This ques-tion is of general interest to the computer scientist, who is interested in a generalway of regarding the range of problems for which an approach is suitable, andof considerable interest to the linguist, for the following reason. The most impor-tant contribution to linguistics of the work of Noam Chomsky since the mid-1950shas been his insistence that some aspects of the structure of natural language areunlearnable, or at the very least unlearned, and that therefore the speciﬁcationof a human’s knowledge of language prior to any exposure to linguistic data is
a valid and an important task for linguistics. But knowledge of the lexicon of agiven language, or the analysis of the words of the lexicon into morphemes, is amost unlikely candidate for any kind of innate knowledge. Few would seriouslyentertain the idea that our knowledge of the words of this book are matters ofinnate knowledge or linguistic theory; at best – and this is plausible – the linguistmust attempt to shed light on the process by which the language learner infers
the lexicon, given sufﬁcient data. To say that the ability to derive the lexicon from
the data is innate is something with which few would disagree, and to the extentthat a careful study of what it takes to infer a lexicon or a morphology from dataprovides evidence of an effective statistically based method of language learning,such work sheds important light on quite general questions of linguistic theory.
The idea of segmenting a long string S∈Σ
∗into words is based on a simple
intuition: that between two extreme analyses, there must be a happy medium thatis optimal. The two extremes I refer to are the two ‘trivial’ ways to slice Sinto
pieces: the ﬁrst is to not slice it at all, and to treat it as composed of exactly onepiece, identical to the original S, while the second is to slice it into many, many
pieces, each of which is one symbol in length. The ﬁrst is too coarse, and the secondis too ﬁne, for any long string that comes from natural languages (in fact, for mostsystems generating strings that are symbolic in any sense). The ﬁrst explains toomuch, in a sense, and overﬁts the data; the second explains too little. The chal-lenge is to ﬁnd an intermediate level of chunking at which interesting structureemerges, and at which the average length of the chunks is greater than one, butnot enormously greater than one. The goal is to ﬁnd the right intermediate level –and to understand what ‘right’ means in such a context. We will have succeededwhen we can show that the string Sis the concatenation of a sequence of members
of a lexicon.
2.2 Trawling for chunks
There is a considerable literature on the task of discovering the words of an unla-beled stream of symbols, and we shall look at the basic ideas behind four majorapproaches.2.2.1 Olivier The ﬁrst explicit computational model of word learning is found
in Olivier (1968) (my description of this unpublished work is based primarily on

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 374 — #11
374 John A. Goldsmith
Kit (2000)). The algorithm begins with a division of the corpus, in some arbitraryfashion, into successive chapters which will be analyzed, one at a time and succes-sively. Letting ibe 1, we establish a ‘provisional’ lexicon from chapter i; it could
be simply the lexicon consisting of all the individual letters of the alphabet thatis used, and in some provisional fashion a probability distribution is establishedover the lexicon. Given that lexicon at pass i, we can relatively easily ﬁnd the divi-
sion of the chapter into words that maximizes the probability of the string, on theassumption that the probability of a string is simply the product of the (unigram)probabilities of its words. This maximum likelihood parse provides us with a newset of counts of the items in the lexicon and, normalizing, we take these as form-
ing a new probability distribution over the lexicon. We now modify the lexicon byadding and removing some of its members. If we ﬁnd that there is a sequence oftwo lexicon members that occurs more frequently than we would expect (by virtueof its frequency being greater than the product of the frequencies of its individualmembers), then we add that word to the lexicon. On the other hand, if a lexiconmember occurs only once, we remove it from the lexicon. Having done this, weincrement i, and reapply this process to the next chapter in the corpus.
This algorithm contains several elements that would be retained in later
approaches to the problem but, in retrospect, we can see that its primary weak-ness is that it does not offer a principled answer to the question as to how large(i.e., how long) the words should be. It avoids the question in a sense by not reap-plying recursively on a single text (chapter), but does not address the questionhead-on.2.2.2 MK10 Another early lexicon-building approach was MK10, proposed by
Wolff (1975; 1977). The initial state of the device is a lexicon consisting of all of theletters of the corpus. The iterative step is a continuous scanning through the text,parsing the text Cat point i(asigoes from 1 to |C|) by ﬁnding the longest string
sin the lexicon which matches the text from point ito point i+|s−1|, and then
proceeding from the next point, point i+|s|; when ireaches the end of the corpus,
we begin the scan again. The algorithm keeps track of each pairof adjacent ‘lexical
items’ that occur, and when the count of a particular pair exceeds a threshold (suchas 10), the pair is added to the lexicon, all counts are reset to zero, and the processbegins again. In sum, this is a greedy system that infers that any sequence whichoccurs at least 10 times is a single lexical item, or a part of a larger one.
Wolff’s paper includes a brief discussion in which the relevance of his analy-
sis is explicitly made to a number of important elements, including associationistpsychology, the elimination of redundancy, natural selection, economy in stor-age and retrieval, induction, analysis by synthesis, probability matching, and thepossibility of extending the algorithm to the discovery of lexical classes based onneighborhood-based distribution.2.2.3 Sequitur Craig Nevill-Manning, along with Ian Witten (see Nevill-
Manning 1996; Nevill-Manning & Witten 1997) developed an intriguing non-probabilistic approach to the discovery of hierarchical structure, dubbed Sequitur.

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 375 — #12
Segmentation and Morphology 375
They propose a style of analysis for a string S, employing context-free phrase-structure rules {R
i}that are subject to two restrictions demanding a strong form of
non-redundancy: (1) no pair of symbols S,T, in a given order, may appear twice in
the set of rules, and (2) every rule must be used more than once. Violation of eitherprinciple gives rise immediately either to the creation of a new rule (if the ﬁrst isviolated) or to the elimination of a rule (if the second is violated). Such sets of rulescan be viewed as compressions of the original data which reveal redundancies inthe data. An example will make clear how this is done.
Suppose the data is thecatinthehatisnocat. The algorithm will begin with a single
rule expanding the root symbol Sas the ﬁrst symbol, here t:S→t. As we scan
the next letter, we extend the rule to S→th, and then to S→the,a n ds oo n ,
eventually to S→thecatinth . Now a violation of the ﬁrst principle has occurred,
because thoccurs twice, and the repair strategy invoked is the creating of a non-
terminal symbol (we choose to label it ‘A’) which expands to the twice-used string:A→th, which allows us to rewrite our top rule as S→AecatinA, which no longer
violates the principles. We continue scanning and extend the top rule now to: S→
AecatinAe, still maintaining the second rule, A→th. Since Aeappears twice, we
create a new rule, B→Ae, and our top rule becomes: S→BcatinB. But now rule A
only appears once, in the rule expanding B, so we must dispense with it, and bulk
upBso that it becomes: B→the. We now see successful word recognition for the
ﬁrst time. We continue three more iterations, till we have S→BcatinBhat . Since at
is repeated, we create a rule for it C→at, and the top rule becomes S→BcCinBhC.
Scanning six more times, we arrive at S→BcCinBhCisnocC, after the ﬁnal atis
replaced by C. We then create a new rule D→cC, leading to the top rule S→
BDinBhCisnoD. Here we stop, and we have the top-level parse corresponding tothe−cat−in−the−h−at−isno−cat, with groups corresponding to theand to c−at.A s
this example illustrates, the very strict conditions set on the relationship betweenthe rule set and the compression representation of the data lead to a powerfulmethod of extracting local string regularities in the data.
Sequitur is an instance of what has come to be known as grammar-based
compression (Kieffer & hui Yang 2000), whose goal is to develop a formal gram-mar that generates exactly one string: the text being compressed, and the grammaritself serves as a lossless compression of the text. That there should be a logicalconnection between an optimal compression of a string of symbols and the struc-ture that inheres in the system that generated the string lies at the heart of the nextperspective we discuss, minimum description length analysis.2.2.4 MDL approaches Some of the most interesting of the probabilistic
approaches to word segmentation employ probabilistic models that are inﬂuencedby Kolmogorov’s notions of complexity, such as Rissanen’s notion of minimumdescription length analysis.
5These approaches provide explicit formulations of
the idea mentioned above that word segmentation is a problem of ﬁnding a happymedium, somewhere between the two extremal analyses of a text string: oneextreme in which the string is treated as a single, unanalyzed chunk, and the other

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 376 — #13
376 John A. Goldsmith
in which it is treated as a concatenation of single symbols, each a ‘chunk’ separatefrom the previous one and the next one.
In a sense, the problem of discovering what the words of a text are means giving
up any interest in what the speciﬁc message is that is encoded there, at least forthe time being. If that sounds paradoxical, just think about it: in asking what thewords are in an utterance and nothing else, we care about the building blocks ofthe message, not the message itself. So a hypothesis about the correct segmenta-tion of a text is, in part, a hypothesis about what information is in the message
being encoded, and what information is part of the larger system being used toperform the encoding – which is to say, the language; it is a hypothesis about thefactorization of linear information into system and message. If we say that theentire text (A Tale of Two Cities, by Charles Dickens, for example) is a single lexicalitem, then we have truly missed the generalization that the work actually sharesa lexicon with any other text in English! If we say that the lexicon of the text issimply the 26 or so letters needed to write it out, then we have also missed thegeneralization that there are many often repeated strings of symbols, like it,times,
and Paris.
The heart of the MDL approach is the realization that each of those two extremes
results in an overloading of one of two encodings. The ﬁrst approach mentionedabove, treating the text as a single lexical item, leads to the overloading of thelexicon; although it contains only one item, that single item is very, very long.The second approach leads to a single lexicon, with no more than a few dozensymbols in it, but specifying what makes A Tale of Two Cities different from any
text which is not AT a l eo fT w oC i t i e s requires specifying every single successive
letter in the text. That is simply too many speciﬁcations: there are far better waystoencode the content of the text than by specifying each successive letter. The better
ways are ones in which there is a lexicon with the real words of the language,and then a spelling out of the text by means of that lexicon. Because the averagelength of the words in the lexicon is much greater than one, the description of thetext by specifying each word , one after the other, will take up much less space (or
technically, far fewer bits of information) than specifying each letter, one after theother. The happy medium, then, is the analysis which minimizes the sum of thesetwo complexities: the length of the lexicon and the length of the description of thetext on the basis of that lexicon.
It turns out (though this is by no means obvious) that, if we make our lexi-
con probabilistic, it is easy to measure the number of bits it takes to describe aspeciﬁc text S, given a particular lexicon; that number is the negative base two
logarithm of the probability of the text, as assigned by the lexicon (rounded upto the nearest integer); we write this ⌈−log
2pr(S)⌉. Probability plays a role here
that is based entirely on encoding, and not on randomness (i.e., the presumption of
the lack of structure) in the everyday sense. Making the lexicon probabilistic heremeans imposing the requirement that it assign a probability distribution over thewords that comprise it, and that the probability of a word string Sbe the product
of the probability of its component words (times a probability that the string is ofthe length that it actually happens to be). ⌈−log
2pr(S)⌉ speciﬁes exactly how many

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 377 — #14
Segmentation and Morphology 377
bits it would take to encode that particular message, using the lexicon in question.This is not obvious, but it is true.
How do we measure the number of bits in the description of the lexicon? A
reasonable ﬁrst approximation would be to calculate how much information ittakes to specify a list of words, written in the usual way from an alphabet of aparticular size. Ignoring a number of niceties, the length of a list of Nwords, each
of length |w
i|, written in an alphabet consisting of mletters, is log2N+∑Ni=1|wi|
log2m, which is very close to the length of the lexicon (times a small constant),
that is to say, the sum of the lengths of the words that make up the lexicon. Thisquantity, c
L, is naturally referred to as the complexity, or information content, of
the lexicon. We will come back to this, and consider some alternatives which makethe description shorter; what I have just mentioned is a simple baseline for length,a length that we know we can easily calculate. In a manner entirely parallel towhat I alluded to in the preceding paragraph, there is a natural way to assign awell-formed probability to a lexicon as well, based on its complexity c
L:i ti s2−cL.6
Minimum description length analysis proposes that, if we choose to analyze
as t r i n gS into words (chunks that do not overlap, but cover the entire string),
then the optimal analysis of Sis by means of the lexicon Lfor which the sum of
the two quantities we have just discussed forms a minimum: the ﬁrst quantity is−log
2pr(S)computed using given L, and the second quantity is cL, which is the
number of bits in the description of lexicon L.
We can now turn all of this discussion of description length into an algorithm
for the discovery of the words of a corpus ifwe can ﬁnd a method for actually
ﬁnding the lexicon that minimizes the combined description length.7A number
of methods have been explored exploiting the observation that, as we build upa
lexicon from small pieces (starting with the individual letters [Line 1]) to largerpieces, the only candidates we ever need to consider are pairs of items that occurnext to each other somewhere in the string (and, most likely, a number of times inthe string). In short, we batch process the text: we analyze the whole text severaltimes [Line 2]. We begin with the ‘trivial’ lexicon consisting of just the letters ofthe alphabet, but we build the lexicon up rapidly by iteration. On each iteration,we ﬁnd the parse which maximizes the probability of the data, given the currenthypothesized lexicon [Lines 3,4]. We consider as a tentative new candidate for thelexicon any pair of ‘words’ that occur next to each other in the string [Line 5]. Wecompute the description length of the entire string with and without the additionof the new lexical item, and we retain the candidate, and the new lexicon that itcreates, if its retention leads to a reduction in the total description length [Line 8].We set a reasonable stopping condition, such as having considered all adjacentpairs of words and ﬁnding none that satisfy the condition in Line 8.
MDL-based approaches work quite well in practice and, as a selling point,
they have the advantage that they offer a principled answer to the question ofhow and why natural language should be broken up into chunks. Many variantson the approach sketched here can be explored. For example, we could explorethe advantages of a lexicon that has some internal structure, allowing words tobe speciﬁed in the lexicon as concatenation of two or more other lexical entries;

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 378 — #15
378 John A. Goldsmith
1: Start condition: L⇐Σ
2:repeat
3: π∗⇐arg max π∈{parses of D }pr(π),g i v e nL
4: Assign a probability distribution over Lbased on the counts of words in π∗
5: Choose at random two adjacent words, wikwik+1
6: w∗⇐w⌢i
kwik+1
7:L∗⇐L∪w∗
8: If DL(C,L∗)<DL(C,L), then L⇐L∗
9:until Stopping Condition is satisﬁed
10:return arg maxπ∈{parses of D }pr(π),g i v e nL
Figure 14.2 Word discovery from an MDL point of view.
de Marcken’s model permits this, thus encouraging the discovery of a lexiconwhose entries are composed of something like morphs. We return to the generalquestion shortly, in connection with the discovery of true linguistic morphology.2.2.5 Hierarchical Bayesian models In a series of recent papers, Goldwater,
Johnson, and Grifﬁths (henceforth, GJG) have explored a different approachinvolving hierarchical Bayesian models, and they have applied this to the prob-lem of inducing words, among other things (see Goldwater 2006; Goldwater et al.,2006; Johnson et al., 2006; Teh et al., 2006; Johnson 2008). Like MDL models, thesegrammars are non-parametric models, which is to say, in the study of different sets
of data of the same kind, they consider models with different numbers of param-
eters – or to put it more crudely, the model complexity increases as we give thesystem more data. GJG describe the models by means of the process that generatesthem – where each model is a distribution, or a set of distributions over differentbases – and what distinguishes this approach is that the history of choices made iscached, that is, made available to the process in a fashion that inﬂuences its behav-ior at a given moment. The process leans towards reproducing decisions that it hasoften made in the past, based upon a scalar concentration parameter α> 0. After
having already generated nwords, the probability that we generate a novel word
from an internal base word-generating distribution is
α
α+n, a value that diminishes
rapidly as the process continues. Conversely, the probability of generating wordw
kwhich has already occurred [wk]times is[wk]
α+[w k].
Such processes share with MDL models (though for quite different reasons)
what sociologists call a Matthew effect (also called a rich get richer effect), whereby
choices that have been selected over the past of the process are more likely to beselected in the future.
GJG use this process to model the generation of a lexicon, and Gibbs sampling to
ﬁnd the appropriate lexicon parameters.
8We describe the simplest of their models,
the unigram model, here. As we have noted, the process has three parameters: aconcentration parameter α; a ﬁnite lexicon, i.e., a ﬁnite set of elements of Σ
∗, each
of which is associated with a parameter corresponding to how often that wordhas been seen in the data at this point; and a base distribution ΦoverΣ
∗used
eventually to create new words for the lexicon.

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 379 — #16
Segmentation and Morphology 379
We thus have two abstract objects to consider in connection with any long
unbroken string S: one is a string bof 1s (yes) and 0s (no), as to whether a word
break occurs after the nthsymbol of S; and the other is the ﬁnite lexicon (which is
what we are trying to ﬁgure out, after all). Given Sand a particular b, a speciﬁc
lexicon follows directly. The Gibbs sampling that is used provides a path from anyinitial set of assumptions about what bis (that is, any initial set of assumptions
as to where the word breaks are) to essentially the same steady-state analysis ofSinto words. Here is how it does it, and it does not matter whether we begin
with the assumption that there is a break between every symbol, between nosymbols, or that breaks are initially assigned at random. We will iterate thefollowing procedure until we reach equilibrium: we select an integer between 1and|S|− 1, and calculate anew whether there should be a break there, i.e., we make
a new decision as to whether bh a sa0o ra1a tp o s i t i o n n, conditioned on the loca-
tions of the other breaks speciﬁed in b, which implicitly determines the words in
the lexicon and their frequency. We do this by looking just at the righmost chunkto the left of position nand the leftmost chunk to the right of position n. For exam-
ple, if the string is ...isawa−ca−t−inth−ewind ...(where the hyphen indicates
a break, and no hyphen indicates no break), and we choose to sample the positionbetween caand t, then we calculate the probability of two different strings cum
breaks: the one just indicated, and this one: ...isawa −cat−inth−ewind ....I f
we assume words are generated independently of their neighbors (the unigramassumption), then we need simply compare the probability of ca−tand that of
cat, and that decision will be made on the basis of the probability assigned bythe process we have described. That probability, in turn, will not weigh heav-
ily in favor of catover caand tearly on, but if the corpus is in fact drawn from
English, the counts of catwill begin to build up over those of caand t,a n dt h e
local decision made at position nwill reﬂect the counts for all of the words that
occur, given the analysis so far, in the string. GJG show that if we drop the unigramassumption about words, and assume essentially that the probability of each wordis conditioned by the preceding word (more accurately, that the parameters of theDirichlet process selecting a word are conditioned by the preceding word) and ifwe let the distribution Φthat proposes new words itself adapt to the language’s
phonotactics (which can be learned from the lexicon), then results are considerablyimproved.
2.3 Word boundary detectors
The very ﬁrst work on explicit development of boundary detectors was due toZellig Harris, but his primary application of the notion was to morpheme detec-tion, which we will return to shortly. Nonetheless, his ideas have inspired manysubsequent workers, who have looked to see if there were local characteristics,detectable within a small window of ﬁve or six letters, which would give astrong indication of where a word boundary falls in a text. We will look at onerecent example, that of Cohen et al. (2002), which makes an effort to detect word

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 380 — #17
380 John A. Goldsmith
boundaries both by ﬁnding likely boundary points and by ﬁnding likely wordsequences. Cohen et al. (see also Cohen et al., 2007) let their hybrid model voteon the best spot to hypothesize a break to be, and so they call this a votingexpert model. Their expert uses the log-frequency of a conjectured word chunkwas its measure of goodness as a word, and their measure of whether a point
iis a good break point is what they call the boundary entropy, deﬁned roughly
(but only roughly) as the entropy of the frequency distribution of individual let-ters that follow the hypothesized word that ends at point i. Thus, if a string
thusifastringisanalyzedat ...is analyzed at point 13 as containing the word string
stretching from point 7 to point 13, then we compute the frequency of all of the let-ters that in fact follow the string string somewhere in the corpus. The greater the
entropy of that multiset is, the likelier it is that the ending point of string is a word
boundary (on the grounds that words are relatively poor as regards their abilityto impose a decision on what letter should come next). This is the entropic ver-sion, employed by Hafer and Weiss (1974), of Harris’s successor frequency notion,to which we return in the next section. Cohen et al. take into account phonologi-cal frequency effects by not using observed frequencies, but rather correspondingz-scores. For any sequence sof letters of length |s|and frequency f(s), they know
both the average frequency μ(|s|)of distinct observed strings of length |s|in the
text, and the standard deviation from this mean of all of the strings of length |s|,
so everywhere where one would expect to put a probability, they use a z-score
instead (i.e.,
freq(s)−μ( |s|)
σ): the measure of goodness of a chunk is the logarithm of
that value, and the familiar notion of conditional entropy is modiﬁed to use thissort of z-score instead of a frequency.
The algorithm makes one pass over the string, shifting a window of limited
size (the authors give an example of width 3, but in actual applications theyuse a window of 6, or more); at each stop of the window, the two measures(goodness of chunk, goodness of break point) each independently select the pointwithin the window which maximizes their own measure, but looking only at thesmall substring within the window (and not, for example, what had been decidedearlier on in the pass to segments ‘to the left,’ so to speak, except insofar as thatinformation is implicit in the accrued voting). When the string has been scanned,a (non-linear) counting process decides how the votes which have been assignedto each point between the letters by the two measures should be transformed intohypotheses regarding word breaks.
2.4 Successes and failures in word segmentation
The largest part of the failures of all approaches to word segmentation are failuresof level rather than failures of displacement: that is, failures are typically eitherof ﬁnding chunks that are too large, consisting of common pairs of words ( ofthe,
NewYork ) or of not-so-common words composed of common pieces ( commit ment,
artiﬁcial ly ), rather than errors like c hunks , though those certainly do appear as
well. The most interesting result of all of the work in this area is this: there is no

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 381 — #18
Segmentation and Morphology 381
way to solve the word segmentation problem without also making major progresswith the problem of automatic learning of morphology and syntax. Knowledge ofthe statistical properties of strings can be used to infer words only to the extent thatthe device that generated the strings in the ﬁrst place used knowledge of words,
and only knowledge of words, to generate the string in the ﬁrst place; and, inactual fact, the systems that generate our natural language strings employ systemsat several levels: it is not words, but morphemes that consist of relatively arbitrary
sequences of letters, and words are the result of a system responsible for the linearplacement of morphemes. In addition, there is a system responsible for the sequen-tial placement of words – we call it syntax – and it too has a great impact on the
statistics of letter placement. A system that tries to learn the structure of languageon the basis of a model that is far poorer than the real structure of language willnecessarily fail – we may be impressed by how well it does at ﬁrst, but failure isinevitable, unless and until we endow the learning algorithm with the freedom ofthought to consider models that take into consideration the structure that indeedlies behind and within language. In the next section, we turn to the task of learningmorphological structure.
3 Unsupervised Learning of Morphology
In this section, we will discuss the automatic learning of morphology. Most of theattention in this part of the literature has gone to the problem of segmentation,which is to say, the identiﬁcation of the morphs and morphemes of a language,based entirely on naturalistic corpora. The identiﬁcation and treatment of mor-phosyntactic features is an additional problem, which we shall touch on only inpassing (though it is a real and important challenge). When we look at a sampleof English, we certainly want to discover that jumps and jumping consist of a stem
jump followed by the two sufﬁxes sand ing, and a solution to the problem of iden-
tifying morphs gives us that information; but at the same time, we would like toknow that - smarks the third-person singular present form of the verb.
Such information goes well beyond the problem of segmentation, and brings us tothe domain of morphosyntactic information, an area in which relatively little hasbeen done in the domain of unsupervised learning of morphology.
3.1 Zellig Harris
All discussions of the problem of automatic segmentation aiming at discoveringlinguistically relevant units start with the work of Zellig Harris. In the mid 1950s,he noticed that one could deﬁne a function that, informally speaking, speciﬁeshow many alternative symbols may appear at any point in the string, given whathas preceded. In light of both the method and the date, it is impossible not tosense an inspiration from Shannon’s work, which had just appeared (Shannon& Weaver 1949). Harris himself published two papers addressing this approach

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 382 — #19
382 John A. Goldsmith
(1955; 1967), and returned to it brieﬂy in other works till the end of his life (Harris1991). At least as interesting for our purposes was the computational implemen-tation of Harris’s idea in Hafer and Weiss (1974). The presentation in this chapterrelies primarily on Harris (1967) and Hafer and Weiss. We consider a family ofHarrisian algorithms for segmenting words into morphemes, given a sample ofwordsW={w
i}from a language, where each wi∈Σ∗, for some alphabet Σ.W e
wish to associate a real value with the position that lies between each symbol ineach word and, while we can imagine several slightly different ways to do this, theways all attempt to capture the idea of measuring how many different ways thestring to the left can be continued, in view of what we know about the entire set ofwordsW. The core notion of successorfrequency is deﬁned as follows: The successor
frequency SF (p,W)of a string pin a set of words Wis 0 if no words in Wbegin
with p(i.e., there is no winWwhich can be expressed as p
⌢αwhere α∈Σ∗), and,
more interestingly, it is equal to the number of distinct symbols {l1,...,lk}all of
which can follow the preﬁx pinW: that is, our successor frequency is the size of
the set {l1,...,lk}such that pliis a preﬁx of a word in W. A similar deﬁnition can be
constructed to deﬁne the predecessor frequency in a mirror-image fashion, spec-ifying how many different letters can immediately precede any given word-ﬁnalsubstring.
Harris’s intuition was that the successor frequency was high, or relatively high,
at points in a string corresponding to morpheme boundaries, and the same istrue for the predecessor frequency. But if the method works well in many cases,it fails in many others as well, due either to data sparsity or to other effects.One such effect arises when the set of sufﬁxes to a given stem all sharing acommon letter (for example, the words construction and constructive have a peak
of successor frequency after constructi , and a peak of predecessor frequency after
construct).
Hafer and Weiss (1974) tested 15 different interpretations of Harris’s algorithm,
and found a wide variety in precision and recall of the interpretations, rangingfrom ‘completely unacceptable’ when cuts were made at thresholds of successorfrequency, to as high as 91 percent precision and recall of 61 percent; this wasthe result of making a morpheme cut when either of two conditions was met: (a)the word up to that point was also a free-standing word, and the predecessorfrequency there was 5 or greater; or (b) the successor frequency at the point wasgreater than 1, and the predecessor frequency was greater than 16. One can seethat some effort was expended to tune the parameters to suit the data.
3.2 Using description length
Anybody’s list of words in a language, no matter how it is obtained, contains agreat deal of redundancy, for all of the reasons that we discussed in the ﬁrst sectionof this paper: morphological roots appear with a variety of preﬁxes and sufﬁxes,but that variety is limited to a relatively small number of patterns. The discoveryof the morphological structure of a language is essentially the discovery of this

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 383 — #20
Segmentation and Morphology 383
AB Cwalk
jump∅
s
ed
ing
Figure 14.3 A signature for two verbs in English.
1: Start condition: M⇐M0(C)
2: Some approaches: bootstrap operation B:M⇐B(M 0(C))
3:repeat
4:M′=F(M,M0)
5: If condition C(M,M′)is satisﬁed, M⇐M′
6:until Stopping Condition
Figure 14.4 Morphology discovery as local descent.
kind of redundancy in the lexicon; removing the redundancy will both shortenthe description of the lexicon and take us closer to an accurate characterization ofthe morphology. For example, if a language (in this case, English) contains a set ofwords walk, walks, walked ,walking, jump, jumps ,jumped ,jumping, then rather than
expressing all eight of the words separately, we can achieve greater simplicity byextracting the redundancy inherent in the data by identifying two stems, walk and
jump, and four sufﬁxes, ∅,s,ed,ing. See Figure 14.3.
But how do we turn that intuition into a computationally implemented
approach? We employ a hill-climbing strategy, and focus on deciding what deter-mines the shape of the landscape that we explore, and how to avoid discoveringlocal maxima that are not global maxima.
We specify ﬁrst what the formal nature is of our morphology – typically, a
ﬁnite state automaton (for a deﬁnition of an FSA, see Section 4.3 of Chapter 1,
FORMAL LANGUAGE THEORY , and Section 4.1 below). Given the nature of a mor-
phology, it is always possible to treat a word as an unanalyzed morpheme, and an
algorithm will generally begin by assuming that, in the initial state, the morphol-ogy treats all words of the corpus Cas unanalyzable single morphemes (Step 1)
(see Figure 14.4). We will call that state of a morphology M
0(C). Some approaches
take a one-time initial step of analysis (Step 2), with the aim of avoiding localoptima that are not global optima. A loop is entered (Steps 3–5) during whichhypotheses about morphological structure are entertained; any such hypothe-sis may either be dependent on what the current hypothesized morphology M
is, or simply be based on information that is independent of the current Mby
using information that was already available in M
0, such as how often a string
occurs in the corpus C(Step 4). A simplicity-based approach deﬁnes some notion

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 384 — #21
384 John A. Goldsmith
of simplicity – the function C(M,M′)in line 5 – to decide whether the modiﬁed
morphology is preferable to the current hypothesis.
A method that employs minimum description length analysis,9such as
Goldsmith (2001) and Goldsmith (2006), will use description length of the dataas the condition in Step 5: a new morphology is preferred to the current one ifthe description length of the data is decreased with the new morphology, wherethe description length of the data is the sum of two quantities: (1) the inverse logprobability of the data, and (2) the length of the morphology as measured in bits.How are these quantities calculated?
In order to calculate the probability of a word that the FST generates, we must
associate a probability distribution with each set of edges that leaves a given state.Because a word, considered as a string of letters, typically is associated with onlyone path through the FST, it is easy to assign probabilities based on observedfrequencies; we count the number of times each edge (n
i,nj)is traversed when all
of the data is traversed, and then assign the probability of going to node nj,g i v e n
that we are at node ni,a sCount(n i,nj)∑
kCount(n i,nk), and the probability of any path through
the FST (hence, of any word that it generates) is the product of the probabilitiesassociated with each edge on the path. The number of bits required to encode sucha word is, then, the negative log probability of the word, as just calculated.
How should we compute the length of a morphology in bits? In the discussion of
MDL above concerning word segmentation, we saw one natural way to computethe length of a list of strings, and nothing would be simpler than to count thenumber of bits that were required in order to spell out the list of labels on eachof the edges. The result would be log
2N+∑Ni=1|wi|log2m, where Nis the total
number of morphemes, mthe number of symbols in the alphabet, and the set of
labels is the set {wi}.
In our discussion of MDL in connection with word discovery, we suggested
that discovering the right lexicon is a matter of ﬁnding the right balance betweena lexicon that was not overburdened and an encoding of the data that was not toointricate. Now we are saying something more: we are recognizing that we were notambitious enough when we took it for granted that a lexicon was a list of words,because any lexicon that simply lists all its members is highly redundant in muchthe same way that a text in a particular language that is not analyzed into words ishighly redundant. We therefore turn to the discovery of morphology as the meansto reduce the redundancy in the lexicon.
This suggests a broader understanding of the cost of designing a particular
morphology for a set of data: use information theory in order to make explicitwhat the cost of every single piece of the morphology is – which is to say, themorphology is a labeled, probabilistic FSA. It consists of a set of states {N}(includ-
ing a start state, and a set of ﬁnal, or accepting, states), a set of edges E ⊂N×N,a
list of morphemes, and a label on each edge consisting of a pointer to an itemon the morpheme list. The cost of the list of morphemes is calculated just aswe discussed in Section 2.2.4, in the context of a lexicon of words. More inter-esting is the complexity, or cost, in bits associated with the FSA itself, which

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 385 — #22
Segmentation and Morphology 385
is the sum of the cost of each edge (ni,nj)and its associated label lm. The cost
of the edge is −(log pr n(ni)+log pr n(nj)), where prn()is a probability distribu-
tion over nodes, or states, based on how often each node is traversed in parsingthe data, as discussed four paragraphs above. The cost in bits of the pointer toan item on the morpheme list is equal to −log pr
μ(m), where prμ()is a prob-
ability distribution over the items in the morpheme list, based on how manywords in the corpus are generated by a path which includes an edge pointing tomorpheme m.
10
3.3 Work in the ﬁeld
There has been considerable work in the area of automatic learning of mor-phology since the 1950s, and quite a bit of it since the mid-1990s. Subsequentto Zellig Harris’s work mentioned earlier, there was work by Nikolaj Andreev(Andreev 1965; 1967, described in Cromm 1997) in the 1960s, and later by de Kockand Bossaert (1969; 1974). An interesting paper by Radhakrishnan (1978) in thetext compression literature foreshadows some of the work that was yet to come.Several years later there was a series of papers by Klenk and others (Klenk &Langer 1989; Wothke & Schmidt 1992; Flenner 1995) which focused on discoveryof local segment-based sequences which would be telltale indicators of morphemeboundaries. More recently, there has been a series of papers by Medina Urrea(2000; 2006) and Medina Urrea and Hlaváˇ cová (2005). Approaches employingMDL have been discussed in van den Bosch et al. (1996), Kit and Wilks (1999),Goldsmith (2001; 2006), Baroni (2003), and Argamon et al. (2004). Important con-tributions have been made by Yarowsky and Wicentowski (2000), by Schone andJurafsky (2001), and Creutz and colleagues (see Creutz & Lagus 2002; Creutz 2003;Creutz & Lagus 2004; 2005a; 2005b).
Clark (2001b; 2001c; 2002) explored the use of stochastic ﬁnite state automata
in learning the concatenative sufﬁxal morphology of English and Arabic, butalso the more challenging case of strong verbs in English and broken plurals inArabic, using expectation-maximization (Dempster et al., 1977c) to ﬁnd an opti-mal account, given the training data.
11It is interesting to note that this work takes
advantage of the active work in bioinformatics based on extensions of hiddenMarkov models, which itself came largely from the speech recognition commu-nity. Memory-based approaches such as van den Bosch and Daelemans (1999) (onDutch) employ rich training data to infer morphological generalizations extendingwell past the training data.
Algorithms that learn morphology in a strictly unsupervised way are never
certain about what pairs of words really are morphologically related; they canonly make educated guesses, based in part on generalizations that they observe inthe data. Some researchers have explored what morphological learners might beable to do if they were told what pairs of words were morphologically related,and the systems would have to induce the structure or principles by whichthey were related. This work is not strictly speaking unsupervised learning; the

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 386 — #23
386 John A. Goldsmith
learner is helped along considerably by knowledge that pairs of words are indeedmorphologically related.
Let us suppose, for purposes of discussion, that we have determined in some
fashion that the pairs of words that we are given are largely distinguished by mate-rial on the right-hand side of the word: that is, that the system is largely sufﬁxal.Then, given any pair of related words w
1and w2, where w1is not longer than
w2, then there are at most |w1+1|ways to account for the pairing, based solely
on treating the relationship as based on morphs. Given the pair jump/jumped ,
there are ﬁve generalizations we might consider, each specifying a pair of sufﬁxesin that pair of words: ∅/ed ,p/ped ,mp/mped ,ump/umped,a n d jump/jumped .A sw e
consider a large set of pairs of words, it is not hard to see that the correct gener-alization will generally be the one which occurs most frequently among a largenumber of word pairs. This approach has the advantage that it can say some-thing useful even about generalizations that involve a very small number of pairs(e.g., say/said ,pay/paid); this is more difﬁcult for a purely unsupervised approach,
because it is difﬁcult for a purely unsupervised approach to become aware thatthose pairs of words should be related, so to speak. An early effort along these
lines was Zhang and Kim (1990). Research employing inductive logic program-ming to deal with this problem by automatically creating decisions lists or treeshas included Mooney and Califf (1996) (on English strong verbs), Manandharet al. (1998), Kazakov and Manandhar (1998) (an approach that also employsan algorithmic preference for simpler morphological analyses), Kazakov (2000) –which presents a very useful survey of work done in the 1990s on computationalmorphology – Erjavec and Džeroski (2004), which discusses the case of Slovenein some detail, and Shalonova and Flach (2007) (English and Russian). Baroniet al. (2002) took an interesting step of using mutual information between pairsof nearby words in a corpus as a crude measure of semantic relatedness. It hadbeen noticed (Brown et al., 1992) that words that are semantically related have ahigher probability than chance to occur within a window of 3 to 500 words of eachother in a running text, and they explored the consequences for analyzing pairs ofwords (looking at English, and at German) that are both formally similar and withrelatively large point-wise mutual information. This work therefore looks a lot likethe work described in the preceding paragraph, but it resembles it in a rigorouslyunsupervised way.
4 Implementing Computational Morphologies
Sophisticated computational accounts of natural language morphology go backmore than 40 years in the literature; we can still proﬁtably read early articles suchas that by P . H. Matthews (1966).
There have been several excellent book-length studies of computational
morphology in recent years, with considerable concern for actual, real-worldimplementation, notably by Beesley and Karttunen (2003) and Roark and Sproat

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 387 — #24
Segmentation and Morphology 387
(2006), as well as Ritchie et al. (1992) and Sproat (1992). Current work in thisarea focuses to a very large extent on the use of ﬁnite state transducers as ameans to carry out the functions of morphology. This work was stimulated by thework of Douglas Johnson (1972), Ronald Kaplan and Martin Kay (1981), KimmoKoskenniemi (1983), and developed in a number of places by Lauri Karttunen andcolleagues (1993). Beesley and Karttunen’s recent book is especially detailed andlucid, and contains Xerox software that can be used by the reader.
A well-functioning computational morphology for a language can be vital for
many practical applications. Spell-checking is a humble but honest function ofmany products appreciated by a wide range of end users, and in a language witha rich inﬂectional morphology, as we ﬁnd in languages such as Finnish, Hungar-ian, Turkish, the Bantu languages, and many others, the total number of possibleforms that a user might reasonably generate is far greater than the capacity of acomputer to hold in its memory, unless the entire family of forms is compressedto a manageable size by virtue of the redundancies inherent in a computationalmorphology. It is typically the inﬂectional morphology which gives rise to thevery large number of possible forms for nouns and verbs, and it is typically inﬂec-tional morphology which can most usefully be stripped off when one wishesto build a document-retrieval system based not on actual words, but on themost useful part of the words. Syntactic parsing in most languages requiresa knowledge of the morphosyntactic features carried by each word, and thatknowledge is generally understood as being wrapped up in the morphology (pri-marily the inﬂectional morphology) of the language.
12A number of researchers
have explored the effect on the quality of information and document retrievalthat is produced by incorporating knowledge of inﬂectional and derivationalmorphology, including Harman (1991), Krovetz (2000), Hull (1996), Kraaij andPohlmann (1996), Xu and Croft (1998), Goldsmith et al. (2001), Larkey (2002), andSavoy (2006).
Let us consider brieﬂy how a practical system can be overwhelmed by the size
of natural language word sets if morphology is not addressed in a systematicway. In Swahili, a typical Bantu language in this regard, a verb is composed of asequence of morphemes. Without pretending to be exhaustive, we would includean optional preﬁx marking negation, a subject marker, a tense marker, an optionalobject marker, a verb root, a choice of zero or more derivational sufﬁxes mark-ing such functions as causative, benefactive, and reﬂexive, and ended by a vowelmarking mood. Subject and object markers are chosen from a set of approximately20 options, and tenses from a set of about 12. Thus each verb root is a part ofperhaps 100,000 verbs. Similar considerations hold in many languages – in fact,almost certainly in the great majority of the world’s languages.
4.1 Finite state transducers
A large part of the work on computational morphology has involved the useof ﬁnite state devices, including the development of computational tools and

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 388 — #25
388 John A. Goldsmith
infrastructure. Finite state methods have been used to handle both the strictlymorphological and morphotactic, on the one hand, and the morphophonologyand graphotactics on the other. We have already encountered the way in whichstrictly morphological information can be implemented with a ﬁnite state automa-ton, as in Figure 14.3. By extending the notion of ﬁnite state automaton to that ofﬁnite state transducer , we can use much the same notions in order to not only
generate the correct surface morphemes but also create a device that can mapsurface sequences of letters (or phones) to abstract morphosyntactic features suchas
NUMBER and TENSE .
Computational morphology has also applied the notion of ﬁnite state transducer
(the precise details of which we return to shortly) to deal with the problem ofaccounting for regularities of various sorts concerning alternative ways of realiz-ing morphemes. For example, both the English nominal sufﬁx marking
PLURAL
and the English verbal sufﬁx marking third person singular is normally
realized as - s, but both are regularly realized as - esafter a range of stems which
end in - s,-sh,-ch,a n d- z.
We refer to these two aspects of the problem as morphotactics and phonology
respectively. Two methods have been developed in considerable detail for theimplementation of these two aspects within the context of ﬁnite state devices.One, often called ‘two-level morphology,’ is based on an architecture in whicha set of constraints is expressed as ﬁnite state transducers that apply in parallel toan underlying and a surface representation. Informally speaking, each such trans-ducer acts like a constraint on possible differences that are permitted between theunderlying and the surface labels, and, as such, any paired underlying/surfacestring must satisfy all transducers. The other approach involves not a parallel set of
ﬁnite state transducers, but rather a cascaded set of ﬁnite state transducers, which
can be compiled into a single transducer. A lucid history of this work, with anaccount of the relationship between these approaches, can be found in Karttunenand Beesley (2005); a more technical, but accessible, account is given by Karttunen(1993).
The term ‘two-level morphology,’ due to Koskenniemi (1983), deserves some
explanation: by its very name, it suggests that it is possible to deal with thecomplexities of natural language morphology (including morphophonology)without recourse to derivations or intermediate levels. That is, formal accountswhich are inﬂuenced by generative linguistics have tended uniformly to analyzelanguage by breaking up phenomena into pieces that could be thought of as apply-ing successively to generate an output from an input with several intermediatestages. It would take us too far aﬁeld to go through an example in detail, but onecould well imagine that the formation of the plural form of shelf could be broken
up into successive stages: shelf → shelf+s→ shelv+s→ shelves. Here,
we see the sufﬁxation of the plural ‘s’ happening (in some sense!) ﬁrst, followedby the change of ftov, followed in turn by the insertion of e. In contrast, ﬁnite
state automata offer a way of dealing with the central phenomena of morphol-ogy without recourse to such a step-by-step derivation: hence the term ‘two-levelmorphology,’ which employs only two levels: one in which morphosyntactic

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 389 — #26
Segmentation and Morphology 389
features and lexical roots are speciﬁed, and one which matches the spelled (orpronounced) form of the word. We return to this in Section 4.2.
The notion of ﬁnite state automaton (often abbreviated as FSA) was ﬁrst pre-
sented in Kleene (1956), itself inspired by the work of McCulloch and Pitts (1943)some ten years earlier. An FSA is a kind of directed graph: a directed graph is bydeﬁnition a ﬁnite set of nodes N, along with a set of edges E, where an edge is an
ordered pair of nodes. Nodes in an FSA are often called states. For a directed graph
to be an FSA, it must be endowed with three additional properties: it must havea distinguished node identiﬁed as its start state; it must have a set of one or more
stopping (oraccepting) states; and it must have a set of labels, L, with each edge
associated with exactly one label in L. While Lcannot in general be null, it may
contain the null string as one of its members. In purely mathematical contexts, itis convenient to assume that each label is an atomic element, indivisible, but in thecontext of computational linguistics, we rather think of Las a subset of Σ
∗,f o r
some appropriately chosen Σ. In that way, the morphs of a given language (e.g.,
jump, dog, ing) will be members of L, as will be descriptions of grammatical feature
speciﬁcations, such as first person orpast tense.
When we explore an FSA, we are typically interested in the set of paths through
the graph, and the strings associated with each such path – we say that a pathgenerates the string. A path in a given FSA is deﬁned as a sequence of nodes
selected from N, in which the ﬁrst node in the sequence is the starting state of
the FSA, the last node in the sequence is one of the stopping states of the FSA, andeach pair of successive nodes (n
i,ni+1)in the sequence corresponds to an edge ej
of the FSA. We associate a string Swith a path psimply by concatenating all of the
labels of the edges corresponding to the successive pairs of nodes comprising p.I f
we take a grammar of a language to be a formal device which identiﬁes a set ofgrammatical strings of symbols, then an FSA is a grammar, because it can be usedto identify the set of strings that correspond to all paths through it. Given a stringSinL
∗, we can identify all paths through the FSA that generate S.
Finite state morphologies employ a generalization of the ﬁnite-state automaton
called a ﬁnite state transducer ,o rFST, following work by Johnson (1972). An FST
differs from an FSA in that an FST has two sets of labels (or in principle evenmore, though we restrict the discussion here to the more common case), one calledunderlying labels,L
U, and one called surface labels,LS, and each edge is associated
with a pair of labels (l U,lS), the ﬁrst chosen from the underlying labels, and the
second from the surface labels – it is traditional, however, to mark the pair notwith parentheses, but with a simple colon between the two: l
U:lS. The FSA thus
serves as a sort of translation system between L∗
UandL∗
S. In fact, an FST can be
thought of as two(or even more) FSAs which share the same nodes, edges, starting
states, and stopping states, but which differ with regard to the labels associatedwith each edge, and we only care about looking at pairs of identical paths throughthese two FSAs. The beauty of the notion of FST lies in the fact that it allows us tothink about pairs of parallel paths through otherwise identical FSAs as if they werejust a single path through a single directed graph. For this reason, we can say thatFSAs are bidirectional , in the sense that they have no preference for the underlying

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 390 — #27
390 John A. Goldsmith
AB Cwalk
jump∅
s
ed
ing
AB CWALK
JUMPPRES
PRES3RDSG.
PAST
CONTINUOUS
AB Cwalk :WALK
jump:JUMP∅:PRES
s:PRES3RDSG.
ed:PAST
ing:C ONTINUOUS
Figure 14.5 Building an FST from two FSAs.
labels or the surface labels: the same FST can translate a string from L∗
StoL∗
U,
and also from L∗
UtoL∗
S. If we construct an FST whose second set of labels is not
underlying forms but rather category lables, then the same formalism gives us aparser: tracing the path of a string through the FST associates the string with asequence of categories. Finite state automata are relatively simple to implement,and very rapid in their functioning once implemented. See Figure 14.5.
4.2 Morphophonology
Rare is the language which does not contain rules of its spelling system whoseeffect is to vary the spelling of a morpheme depending on the characteristics ofthe neighboring morphemes. English has many such cases: as we noted, someEnglish words, like wife and knife, change their ftovin the plural; the plural sufﬁx

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 391 — #28
Segmentation and Morphology 391
itself is sometimes - sand sometimes -es, depending on what precedes it. Since
these patterns tend to recur within a given language, it is traditional to analyzethis by saying that there is a single underlying label for the morpheme, but two ormore surface labels that the transducer relates to the single underlying label.
Koskenniemi (1983) developed a system of notation widely used in ﬁnite state
morphophonology to deal with this. The challenge of the task is to make explicitwhen any departure from simple identity is required, or permitted, between theunderlying label uand the surface label s. Koskenniemi’s idea was that for any
given pairing like f:v, we can deﬁne a context that either permits or requires that
correspondence, where by a context we mean a speciﬁcation of the symbols that
appear to the left and to the right, on both the underlying labels and the surfacelabels. For example, if every occurrence of underlying tcorresponds to a surface s
when and only when an ifollows on both the underlying and surface labels, then
we can specify this thus: t:s⇔_ _i:i. If we wanted to express the generaliza-
tion that when two vowels were adjacent on the string of underlying labels, onlythe second of them appears among the surface labels, then we would representit this way: V:∅⇐ _ _V :, where Vis a cover symbol standing for any vowel.
The⇐is taken to mean that in the context described to the right of the arrow, any
occurrence of the underlying label in the pair on the left must be realized as the
surface label of the pair on the left (in this case, as the null symbol). If the arrowpointed to the right, as in V:∅⇒ _ _V:, the rule would be saying that the corre-
spondence of underlying V to surface ∅can only occur when an underlying vowel
follows.
In the case of wife/wives, we must account for the pair (f : v). Since this same
pairing is found in a good number of English words, an appealing way to formal-ize this is to specify that the morpheme underlying wife contains a special symbol,
which we indicate with a capital F: wiFe . An underlying Fcorresponds to a surface
v, when the plural sufﬁx follows, or in all other cases to a surface f. If the underly-
ing form of the plural form of wife iswiFe+NounPlural , then we can express this
as:F:v⇐e+NounPlural :, and the associated surface label will be wives.
5 Conclusions
The computational study of morphology is of interest because of its importance inpractical applications, both present and future, and because of its theoretical inter-est. We have seen that the words of a language are not simply a ﬁxed set of stringschosen from a language’s inventory of letters, or phonemes; the vocabulary is infact built out of a set of morphemes in ways that are potentially quite complex, andin ways that may give rise to complex modiﬁcations of one or more morphemes ina word, each modiﬁcation of each morpheme potentially sensitive to the choicesof each of the other morphemes in the word.
While the number of distinct words in a language does not grow as rapidly with
length as the number of sentences in a language does, it is nonetheless true that

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 392 — #29
392 John A. Goldsmith
the size of the total lexicon of a language is vastly larger than the size of the set ofmorphemes used to generate those words. In order to ensure that a system handlesthe entire lexicon, it is both practically and theoretically necessary to generate thelexicon computationally in a way that reﬂects the true structure of the morphol-ogy of the language. In addition, the meaning and function of a word is in manyrespects decomposable into the meaning and function of its inﬂectional stem andinﬂectional afﬁxes, and so morphological analysis is an important step in statis-tical and data-driven methods of machine translation, at least in languages withrich morphologies.
At the same time, the formal structure of the morphological grammar of a
language may be quite a bit simpler than the syntactic grammar, and allow forgreater success at this point in the task of automatically inferring the morphologyfrom data with relatively little hand tagging of the data or contribution on the partof a human linguist. At present, the work on unsupervised learning in this areahas focused on the problem of segmentation, but work is certain to procede inthe direction of choosing the correct structure among alternative candidate FSAs,given a training corpus. Advances in this area will shed light on the more gen-eral problem of induction of regular languages, which in turn may be helpful inthe goal of induction of more comprehensive grammars from natural languagecorpora.
ACKNOWLEDGMENT
I am endebted to many friends and colleagues, both for conversations on the topicsdiscussed in this chapter and for comments on an earlier draft, including Carl de Marcken,Paul Cohen, Walter Daelemans, Tomaž Erjavec, Antonio Galves, Sharon Goldwater, Yu Hu,Mark Johnson, Chunyu Kit, Ursula Klenk, Kimmo Koskenniemi, Colin Sprague, RichardSproat, Antal van den Bosch, J. G. Wolff, Aris Xanthos, and the editors of this volume; Ihope that I have succeeded in correcting the errors they pointed out to me. Reader, bear inmind that all the ideas presented here have been simpliﬁed to improve comprehensibility.As always, if you are interested, read the original.
NOTES
1 See Chapter 4, THEORY OF PARSING , Chapter 8, UNSUPERVISED LEARNING AND
GRAMMAR INDUCTION , and Chapter 13, STATISTICAL PARSING .
2 Since computational linguists have traditionally interested themselves more with writ-
ten language than spoken language, I write here of letters rather than phonemes, but the
reader who is interested in spoken language should substitute phoneme forletter in the
text.
3 http://en.wikipedia.org/wiki/Truthiness

“9781405155816_4_014” — 2010/5/8 — 12:04 — page 393 — #30
Segmentation and Morphology 393
4 There are two sufﬁxes -ing in English; the one that appears in sentences in the progres-
sive ( John is running) is inﬂectional; the one that creates nominals is derivational ( No
singing of songs will be tolerated).
5 For general discussion of this approach, see Rissanen (2007), Li and Vitányi (1993).
The ﬁrst general exploration of these ideas with application to linguistic questions wasundertaken in Ellison (1994), and several developments along similar lines appearedin the early to mid-1990s, notably Rissanen and Ristad (1994), Cartwright and Brent(1994), Brent et al. (1995), Brent and Cartwright (1996), de Marcken (1996), Cairns et al.(1997), Brent (1999), and Kit and Wilks (1999), as well as Kit (2000). My discussion herepresents a simpliﬁed version of the approach that is common to those accounts.
6 The reason for this is that the quantity c
Lessentially counts the number of 0s and 1s that
would be required to efﬁciently express the lexicon in a purely binary format. If we alsoplace the so-called preﬁx property condition on the encoding we use, which means that
no such binary encoding may be identical to the beginning of another such encoding,then it is relatively straightforward to show that each such binary expression can beassociated with a subinterval of [0,1], and that these subintervals do not overlap. Theﬁnal step of deriving a well-formed probability involves determining whether there areany subintervals of [0,1] which have not been put into correspondence with a lexicon,and dealing with the total length of such subintervals.
7 See pseudo-code in Figure 14.2.8 On Gibbs sampling, see Mackay (2002), for example.9 An open source project that implements such an approach can be found at
http://linguistica.uchicago.edu
10 That is, if there are a total of Vwords in the corpus, and the number of occurrences of all
of the morphemes in the corpus is M(and M, unlike V, depends on the morphology we
assume), and if K(m)is the number of words that contain the morpheme m(we make
the realistic assumption that no word contains two instances of the same morpheme),then the cost of a pointer to mis equal to pr
μ(m)=log2M
K(m).
11 A number of studies have dealt with Arabic, such as Klenk (1994); see also van den
Bosch et al. (2007a) and other chapters in that book.
12 Even for a language like English, in which the morphology is relatively simple, and
one could in principle not do toobadly in an effort to list all of the inﬂected forms
of the known words of English, the fact remains that it is rarely feasible in practice toconstruct a list of all such words simply by data-scraping – that is, by ﬁnding the wordsin nature. To be sure that one had obtained all of the inﬂected forms of each stem, onewould have to build a morphology to generate the forms, thereby bringing us back tothe problem of building a morphology for the language.

