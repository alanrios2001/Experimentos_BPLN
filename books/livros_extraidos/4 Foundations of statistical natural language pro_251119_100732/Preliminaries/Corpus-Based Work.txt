p/CX /CX4 Corpus-Based Work
This chapter begins with some brief advice on getting set up to do
corpus-based work. The main requirements for Statistical NLPwork are
computers, corpora, and software. Many of the details of computers andcorpora are subject to rapid change, and so it does not make sense todwell on these. Moreover, in many cases, one will have to make do withthe computers and corpora at one’s local establishment, even if they arenot in all respects ideal. Regarding software, this book does not attempt
to teach programming skills as it goes, but assumes that a reader inter-
ested in implementing any of the algorithms described herein can alreadyprogram in some programming language. Nevertheless, we provide in
this section a few pointers to languages and tools that may be generallyuseful.
After that the chapter covers a number of interesting issues concerning
the formats and problems one encounters when dealing with ‘raw data’ –
plain text in some electronic form. A very important, if often neglected,
issue is the low-level processing which is done to the text before the realwork of the research project begins. As we will see, there are a number ofdiﬃcult issues in determining what is a word and what is a sentence. Inpractice these decisions are generally made by imperfect heuristic meth-ods, and it is thus important to remember that the inaccuracies of thesemethods aﬀect all subsequent results.
Finally the chapter turns to marked up data, where some process –
often a human being – has added explicit markup to the text to indicatesomething of the structure and semantics of the document. This is oftenhelpful, but raises its own questions about the kind and content of themarkup used. We introduce the rudiments of
SGML markup (and thus

p/CX /CX118 4 Corpus-Based Work
also XML) and then turn to substantive issues such as the choice of tag
sets used in corpora marked up for part of speech.
4.1 Getting Set Up
4.1.1 Computers
Text corpora are usually big. It takes quite a lot of computational re-sources to deal with large amounts of text. In the early days of comput-ing, this was the major limitation on the use of corpora. For example in
the earliest years of work on constructing the Brown corpus (the 1960s),
just sorting all the words in the corpus to produce a word list would take17 hours of (dedicated) processing time. This was because the computer(an
IBM7070) had the equivalent of only about 40 kilobytes of memory,
and so the sort algorithm had to store the data being sorted on tapedrives. Today one can sort this amount of data within minutes on even amodest computer.
As well as needing plenty of space to store corpora, Statistical
NLP
methods often consist of a step of collecting a large number of counts
from corpora, which one would like to access speedily. This means thatone wants a computer with lots of hard disk space, and lots of memory.In a rapidly changing world, it does not make much sense to be more pre-cise than this about the hardware one needs. Fortunately, all the changeis in a good direction, and often all that one will need is a decent personal
computer with its
RAM cheaply expanded (whereas even a few years ago,
a substantial sum of money was needed to get a suitably fast computerwith suﬃcient memory and hard disk space).
4.1.2 Corpora
A selection of some of the main organizations that distribute text cor-pora for linguistic purposes are shown in table 4.1. Most of these orga-
nizations charge moderate sums of money for corpora.
1If your budget
does not extend to this, there are now numerous sources of free text,ranging from email and web pages, to the many books and (maga)zines
1. Prices vary enormously, but are normally in the range of US$100–2000 per CD for
academic and nonproﬁt organizations, and reﬂect the considerable cost of collecting andprocessing material.

p/CX /CX4.1 Getting Set Up 119
Linguistic Data Consortium ( LDC) http://www.ldc.upenn.edu
European Language Resources Association ( ELRA ) http://www.icp.grenet.fr/ELRA/
International Computer Archive of Modern English ( ICAME ) http://nora.hd.uib.no/icame.html
Oxford Text Archive ( OTA) http://ota.ahds.ac.uk/
Child Language Data Exchange System ( CHILDES ) http://childes.psy.cmu.edu/
Table 4.1 Major suppliers of electronic corpora with contact URLs.
that are available free on the web. Such free sources will not bring you
linguistically-marked-up corpora, but often there are tools that can dothe task of adding markup automatically reasonably well, and at any rate,
working out how to deal with raw text brings its own challenges. Further
resources for online text can be found on the website.
When working with a corpus, we have to be careful about the valid-
ity of estimates or other results of statistical analysis that we produce.A corpus is a special collection of textual material collected according toa certain set of criteria. For example, the Brown corpus was designedas a representative sample of written American English as used in 1961
(Francis and Ku ˇcera 1982: 5–6). Some of the criteria employed in its
construction were to include particular texts in amounts proportionalto actual publication and to exclude verse because “it presents speciallinguistic problems” (p. 5).
As a result, estimates obtained from the Brown corpus do not neces-
sarily hold for British English or spoken American English. For example,the estimates of the entropy of English in section 2.2.7 depend heavily on
the corpus that is used for estimation. One would expect the entropy of
poetry to be higher than that of other written text since poetry can ﬂauntsemantic expectations and even grammar. So the entropy of the Browncorpus will not help much in assessing the entropy of poetry. A moremundane example is text categorization (see chapter 16) where the per-formance of a system can deteriorate signiﬁcantly over time because asample drawn for training at one point can lose its representativeness
after a year or two.
The general issue is whether the corpus is a representative sample of
representative
sample the population of interest. A sample is representative if what we ﬁnd
for the sample also holds for the general population. We will not dis-cuss methods for determining representativeness here since this issueis dealt with at length in the corpus linguistics literature. We also refer

p/CX /CX120 4 Corpus-Based Work
the reader to this literature for creating balanced corpora , which are put balanced corpus
together so as to give each subtype of text a share of the corpus that is
proportional to some predetermined criterion of importance. In Statis-tical
NLP, one commonly receives as a corpus a certain amount of data
from a certain domain of interest, without having any say in how it is
constructed. In such cases, having more training text is normally more
useful than any concerns of balance, and one should simply use all thetext that is available.
In summary, there is no easy way of determining whether a corpus is
representative, but it is an important issue to keep in mind when doingStatistical
NLPwork. The minimal questions we should attempt to answer
when we select a corpus or report results are what type of text the corpus
is representative of and whether the results obtained will transfer to the
domain of interest.
The eﬀect of corpus variability on the accuracy of part-of-speech tag-
ging is discussed in section 10.3.2.
4.1.3 Software
There are many programs available for looking at text corpora and ana-lyzing the data that you see. In general, however, we assume that readerswill be writing their own software, and so all the software that is reallyneeded is a plain text editor, and a compiler or interpreter for a lan-guage of choice. However, certain other tools, such as ones for searchingthrough text corpora can often be of use. We brieﬂy describe some suchtools later.
Text editors
You will want a plain text editor that shows fairly literally what is actually
in the ﬁle. Fairly standard and cheap choices are Emacs for Unix (or
Windows), TextPad for Windows, and BBEdit for Macintosh.
Regular expressions
In many places and in many programs, editors, etc., one wishes to ﬁnd
certain patterns in text, that are often more complex than a simple matchagainst a sequence of characters. The most general widespread notationfor such matches are regular expressions which can describe patterns
regular expressions

p/CX /CX4.1 Getting Set Up 121
that are a regular language , the kind that can be recognized by a ﬁnite regular language
state machine. If you are not already familiar with regular expressions,
you will want to become familiar with them. Regular expressions can beused in many plain text editors ( Emacs ,TextPad ,Nisus ,BBEdit , . . . ), with
many tools (such as grep and sed), and as built-ins or libraries in many
programming languages (such as Perl,C, . . . ). Introductions to regular
expressions can be found in (Hopcroft and Ullman 1979; Sipser 1996;Friedl 1997).
Programming languages
Most Statistical
NLPwork is currently done in C/C++. The need to deal
with large amounts of data collection and processing from large texts
means that the eﬃciency gains of coding in a language like C/C++ are
generally worth it. But for a lot of the ancillary processing of text, thereare many other languages which may be more economical with humanlabor. Many people use Perlfor general text preparation and reformat-
ting. Its integration of regular expressions into the language syntax isparticularly powerful. In general, interpreted languages are faster for
these kinds of tasks than writing everything in C. Old timers might still
use awkrather than Perl– even though what you can do with it is rather
more limited. Another choice, better liked by programming purists isPython , but using regular expressions in Python just is not as easy as
Perl. One of the authors still makes considerable use of Prolog . The built-
in database facilities and easy handling of complicated data structuresmakes Prolog excel for some tasks, but again, it lacks the easy access to
regular expressions available in Perl. There are other languages such as
SNOBOL/SPITBOL orIcon developed for text computing, and which are
liked by some in the humanities computing world, but their use doesnot seem to have permeated into the Statistical
NLPcommunity. In the
last few years there has been increasing uptake of Java. While not as
fast as C,Java has many other appealing features, such as being object-
oriented, providing automatic memory management, and having many
useful libraries.
Programming techniques
This section is not meant as a substitute for a general knowledge of com-
puter algorithms, but we brieﬂy mention a couple of useful tips.

p/CX /CX122 4 Corpus-Based Work
Coding words. Normally Statistical NLPsystems deal with a large num-
ber of words, and programming languages like C(++) provide only quite
limited facilities for dealing with words. A method that is commonly usedin Statistical
NLPand Information Retrieval is to map words to numbers
on input (and only back to words when needed for output). This gives a
lot of advantages because things like equality can be checked more easily
and quickly on numbers. It also maps all tokens of a word to its type,which has a single number. There are various ways to do this. One goodway is to maintain a large hash table (a hash function maps a set of ob-jects into a speciﬁced range of integers, for example, 0;:::; 127). A hash
table allows one to see eﬃciently whether a word has been seen before,and if so return its number, or else add it and assign a new number. The
numbers used might be indices into an array of words (especially eﬀec-
tive if one limits the application to 65,000 or fewer words, so they canbe stored as 16 bit numbers) or they might just be the address of thecanonical form of the string as stored in the hashtable. This is especiallyconvenient on output, as then no conversion back to a word has to bedone: the string can just be printed.
There are other useful data structures such as various kinds of trees.
See a book on algorithms such as (Cormen et al. 1990) or (Frakes and
Baeza-Yates 1992).
Collecting count data. For a lot of Statistical
NLPwork, there is a ﬁrst
step of collecting counts of various observations, as a basis for estimating
probabilities. The seemingly obvious way to do that is to build a big data
structure (arrays or whatever) in which one counts each event of interest.But this can often work badly in practice since this model requires a hugememory address space which is being roughly randomly accessed. Unlessyour computer has enough memory for all those tables, the program willend up swapping a lot and will run very slowly. Often a better approach isfor the data collecting program to simply emit a token representing each
observation, and then for a follow on program to sort and then count
these tokens. Indeed, these latter steps can often be done by existingsystem utilities (such as sort and uniq on Unix systems). Among other
places, such a strategy is very successfully used in the
CMU-Cambridge
Statistical Language Modeling toolkit which can be obtained from the web(see website).

p/CX /CX4.2 Looking at Text 123
4.2 Looking at Text
Text will usually come in either a raw format, or marked up in some
way. Markup is a term that is used for putting codes of some sort into a markup
computer ﬁle, that are not actually part of the text in the ﬁle, but explain
something of the structure or formatting of that text. Nearly all computer
systems for dealing with text use mark-up of some sort. Commercial
word processing software uses markup, but hides it from the user byemploying
WYSIWYG (What You See Is What You Get) display. Normally,
when dealing with corpora in Statistical NLP, we will want explicit markup
that we can see. This is part of why the ﬁrst tool in a corpus linguist’stoolbox is a plain text editor.
There are a number of features of text in human languages that can
make them diﬃcult to process automatically, even at a low level. Here
we discuss some of the basic problems that one should be aware of. Thediscussion is dominated by, but not exclusively concerned with, the mostfundamental problems in English text.
4.2.1 Low-level formatting issues
Junk formatting/content
Depending on the source of the corpus, there may be various formatting
and content that one cannot deal with, and is just junk that needs to be
ﬁltered out. This may include: document headers and separators, type-
setter codes, tables and diagrams, garbled data in the computer ﬁle, etc.If the data comes from OCR (Optical Character Recognition), the
OCR pro- OCR
cess may have introduced problems such as headers, footers and ﬂoating
material (tables, ﬁgures, and footnotes) breaking up the paragraphs ofthe text. There will also usually be
OCR errors where words have been
misrecognized. If your program is meant to deal with only connected En-
glish text, then other kinds of content such as tables and pictures need
to be regarded as junk. Often one needs a ﬁlter to remove junk contentbefore any further processing begins.
Uppercase and lowercase
The original Brown corpus was all capitals (a before a letter was used to
indicate a capital letter in the original source text). All uppercase text is

p/CX /CX124 4 Corpus-Based Work
rarely seen these days, but even with modern texts, there are questions of
how to treat capitalization. In particular, if we have two tokens that areidentical except that one has certain letters in uppercase, should we treatthem as the same? For many purposes we would like to treat the,The,
andTHE as the same, for example if we just want to do a study of the
usage of deﬁnite articles, or noun phrase structure. This is easily done
by converting all words to upper- or lowercase, but the problem is thatat the same time we would normally like to keep the two types of Brown
inRichard Brown andbrown paint distinct. In many circumstances it is
easy to distinguish proper names and hence to keep this distinction, but
proper names
sometimes it is not. A simple heuristic is to change to lowercase letters
capital letters at the start of a sentence (where English regularly capi-
talizes all words) and in things like headings and titles when there is a
series of words that are all in capitals, while other words with capital let-ters are assumed to be names and their uppercase letters are preserved.This heuristic works quite well, but naturally, there are problems. Theﬁrst problem is that one has to be able to correctly identify the ends ofsentences, which is not always easy, as we discuss later. In certain gen-res (such as Winnie the Pooh ), words may be capitalized just to stress that
they are making a Very Important Point, without them indicating a proper
name. At any rate, the heuristic will wrongly lowercase names that ap-pear sentence initially or in all uppercase sequences. Often this source oferror can be tolerated (because regular words are usually more commonthan proper names), but sometimes this would badly bias estimates. Onecan attempt to do better by keeping lists of proper names (perhaps withfurther information on whether they name a person, place, or company),
but in general there is not an easy solution to the problem of accurate
proper name detection.
4.2.2 Tokenization: What is a word?
Normally, an early step of processing is to divide the input text into unitscalled tokens where each is either a word or something else like a number
tokens
wordor a punctuation mark. This process is referred to as tokenization .T h e
tokenizationtreatment of punctuation varies. While normally people want to keep sen-
tence boundaries (see section 4.2.4 below), often sentence-internal punc-tuation has just been stripped out. This is probably unwise. Recent workhas emphasized the information contained in all punctuation. No mat-ter how imperfect a representation, punctuation marks like commas and

p/CX /CX4.2 Looking at Text 125
dashes give some clues about the macro structure of the text and what is
likely to modify what.
The question of what counts as a word is a vexed one in linguistics,
and often linguists end up suggesting that there are words at various lev-els, such as phonological words versus syntactic words, which need not
all be the same. What is a humble computational linguist meant to do?
Kuˇcera and Francis (1967) suggested the practical notion of a graphic
graphic word
word which they deﬁne as “a string of contiguous alphanumeric charac-
ters with space on either side; may include hyphens and apostrophes, butno other punctuation marks.” But, unfortunately, life is not that simple,even if one is just looking for a practical, workable deﬁnition. Ku ˇcera
and Francis seem in practice to use intuition, since they regard as words
numbers and monetary amounts like $22.50 which do not strictly seem to
obey the deﬁnition above. And things get considerably worse. Especiallyif using online material such as newsgroups and web pages for data, buteven if sticking to newswires, one ﬁnds all sorts of oddities that shouldpresumably be counted as words, such as references to Micro$oft or the
web company Cjnet, or the various forms of smilies made out of punctu-
ation marks, such as :-). Even putting aside such creatures, working out
word tokens is a quite diﬃcult aﬀair. The main clue used in English is
the occurrence of whitespace – a space or tab or the beginning of a new
whitespace
line between words – but even this signal is not necessarily reliable. What
are the main problems?
Periods
Words are not always surrounded by white space. Often punctuation
marks attach to words, such as commas, semicolons, and periods (fullstops). It at ﬁrst seems easy to remove punctuation marks from wordtokens, but this is problematic for the case of periods. While most peri-ods are end of sentence punctuation marks, others mark an abbreviationsuch as in etc.orCalif. These abbreviation periods presumably should
remain as part of the word, and in some cases keeping them might be im-
p o r t a n ts ot h a tw ec a nd i s t i n g u i s h Wash. , an abbreviation for the state of
Washington, from the capitalized form of the verb wash . Note especially
that when an abbreviation like etc.appears at the end of the sentence,
then only one period occurs, but it serves both functions of the period,simultaneously! An example occurred with Calif. earlier in this para-
graph. Within morphology, this phenomenon is referred to as haplology .
haplology

p/CX /CX126 4 Corpus-Based Work
The issue of working out which punctuation marks do indicate the end
of a sentence is discussed further in section 4.2.4.
Single apostrophes
It is a diﬃcult question to know how to regard English contractions such
asI’llorisn’t. These count as one graphic word according to the deﬁnition
above, but many people have a strong intuition that we really have twowords here as these are contractions for I will andis not . Thus some
processors (and some corpora, such as the Penn Treebank) split such
contractions into two words, while others do not. Note the impact that
not splitting them has. The traditional ﬁrst syntax rule:
S-!NP VP
stops being obviously true of sentences involving contractions such as
I’m right . On the other hand, if one does split, there are then funny
words like ’sandn’tin your data.
Phrases such as the dog’s andthe child’s , when not abbreviations for
the dog is orthe dog has , are commonly seen as containing dog’s as the
genitive or possessive case of dog. But as we mentioned in section 3.1.1,
this is not actually correct for English where ’sis aclitic which can at-
clitic
tach to other elements in a noun phrase, such as in The house I rented
yesterday’s garden is really big . Thus it is again unclear whether to re-
gard dog’s as one word or two, and again the Penn Treebank opts for the
latter. Orthographic-word-ﬁnal single quotations are an especially trickycase. Normally they represent the end of a quotation – and so should not
be part of a word, but when following an s, they may represent an (unpro-
nounced) indicator of a plural possessive, as in the boys’ toys – and then
should be treated as part of the word, if other possessives are being sotreated. There is no easy way for a tokenizer to determine which functionis intended in many such cases.
Hyphenation: Diﬀerent forms representing the same word
Perhaps one of the most diﬃcult areas is dealing with hyphens in the
input. Do sequences of letters with a hyphen in between count as oneword or two? Again, the intuitive answer seems to be sometimes one,sometimes two. This reﬂects the many sources of hyphens in texts.

p/CX /CX4.2 Looking at Text 127
One source is typographical. Words have traditionally been broken and
hyphens inserted to improve justiﬁcation of text. These line-breaking hy-phens may be present in data if it comes from what was actually typeset.It would seem to be an easy problem to just look for hyphens at the endof a line, remove them and join the part words at the end of one line and
the beginning of the next. But again, there is the problem of haplology.
If there is a hyphen from some other source, then after that hyphen isregarded as a legitimate place to break the text, and only one hyphenappears not two. So it is not always correct to delete hyphens at theend of a line, and it is diﬃcult in general to detect which hyphens wereline-breaking hyphens and which were not.
Even if such line-breaking hyphens are not present (and they usually
are not in truly electronic texts), diﬃcult problems remain. Some things
with hyphens are clearly best treated as a single word, such as e-mail
orco-operate orA-1-plus (as in A-1-plus commercial paper , a ﬁnancial
rating). Other cases are much more arguable, although we usually wantto regard them as a single word, for example, non-lawyer ,pro-Arab ,a n d
so-called . The hyphens here might be termed lexical hyphens. They are
commonly inserted before or after small word formatives, sometimes for
the purpose of splitting up vowel sequences.
The third class of hyphens is ones inserted to help indicate the cor-
rect grouping of words. A common copy-editing practice is to hyphenatecompound pre-modiﬁers, as in the example earlier in this sentence or inexamples like these:
(4.1) a. the once-quiet study of superconductivity
b. a tough regime of business-conduct rulesc. the aluminum-export ban
d. a text-based medium
And hyphens occur in other places, where a phrase is seen as in some
sense quotative or as expressing a quantity or rate:
(4.2) a. the idea of a child-as-required-yuppie-possession must be motivating
them
b. a ﬁnal “take-it-or-leave-it” oﬀer
c. the 90-cent-an-hour raise

p/CX /CX128 4 Corpus-Based Work
d. the 26-year-old
In these cases, we would probably want to treat the things joined by hy-
phens as separate words. In many corpora this type of hyphenation isvery common, and it would greatly increase the size of the word vocab-ulary (mainly with items outside a dictionary) and obscure the syntacticstructure of the text if such things were not split apart into separatewords.
2
A particular problem in this area is that the use of hyphens in many
such cases is extremely inconsistent. Some texts and authorities usecooperate , while others use co-operate . As another example, in the Dow
Jones newswire, one can ﬁnd all of database ,data-base anddata base
(the ﬁrst and third are commonest, with the former appearing to domi-nate in software contexts, and the third in discussions of company assets,but without there being any clear semantic distinction in usage). Closer
to home, look back at the beginning of this section. When we initially
drafted this chapter, we (quite accidentally) used all of markup ,mark-up
andmark(ed) up . A careful copy editor would catch this and demand con-
sistency, but a lot of the text we use has never been past a careful copyeditor, and at any rate, we will commonly use texts from diﬀerent sourceswhich often adopt diﬀerent conventions in just such matters. Note thatthis means that we will often have multiple forms, perhaps some treated
as one word and others as two, for what is best thought of as a single
lexeme (a single dictionary entry with a single meaning).
lexeme
Finally, while British typographic conventions put spaces between
dashes and surrounding words, American typographic conventions nor-mally have a long dash butting straight up against the words—like this.While sometimes this dash will be rendered as a special character or asmultiple dashes in a computer ﬁle, the limitations of traditional com-
puter character sets means that it can sometimes be rendered just as a
hyphen, which just further compounds the diﬃculties noted above.
The same form representing multiple ‘words’
In the main we have been collapsing distinctions and suggesting that
one may wish to regard variant sequences of characters as really the
2. One possibility is to split things apart, but to add markup, as discussed later in this
chapter, which records that the original was hyphenated. In this way no information islost.

p/CX /CX4.2 Looking at Text 129
same word. It is important to also observe the opposite problem, where
one might wish to treat the identical sequence of characters as diﬀerentwords. This happens with homographs , where two lexemes have overlap-
homographs
ping forms, such as sawas a noun for a tool, or as the past tense of the
verb see. In such cases we might wish to assign occurrences of saw to
two diﬀerent lexemes.
Methods of doing this automatically are discussed in chapter 7.
Word segmentation in other languages
Many languages do not put spaces in between words at all, and so the ba-
sic word division algorithm of breaking on whitespace is of no use at all.
Such languages include the major East-Asian languages/scripts such as
Chinese, Japanese, Korean, and Thai. Ancient Greek was also written byAncient Greeks without word spaces. Spaces were introduced (togetherwith accent marks, etc.) by those who came afterwards. In such lan-guages, word segmentation is a much more major and challenging task.
word segmentation
While maintaining most word spaces, in German compound nouns are
written as a single word, for example Lebensversicherungsgesellschafts-
angestellter ‘life insurance company employee.’ In many ways this makes
linguistic sense, as compounds area single word, at least phonologically.
But for processing purposes one may wish to divide such a compound,or at least to be aware of the internal structure of the word, and thisbecomes a limited word segmentation task. While not the rule, joiningof compounds sometimes also happens in English, especially when theyare common and have a specialized meaning. We noted above that one
ﬁnds both data base anddatabase . As another example, while hard disk
is more common, one sometimes ﬁnds harddisk in the computer press.
Whitespace not indicating a word break
Until now, the problems we have dealt with have mainly involved splitting
apart sequences of characters where the word divisions are not shown by
whitespace. But the opposite problem of wanting to lump things together
also occurs. Here, things are separated by whitespace but we may wishto regard them as a single word. One possible case is the reverse ofthe German compound problem. If one decides to treat database as one
w o r d ,o n em a yw i s ht ot r e a ti ta so n ew o r de v e nw h e ni ti sw r i t t e na s data
base. More common cases are things such as phone numbers, where we

p/CX /CX130 4 Corpus-Based Work
m a yw i s ht or e g a r d 9365 1873 as a single ‘word,’ or in the cases of multi-
part names such as New York orSan Francisco . An especially diﬃcult
case is when this problem interacts with hyphenation as in a phrase likethis one:
(4.3) the New York-New Haven railroad
Here the hyphen does not express grouping of just the immediately ad-
jacent graphic words – treating York-New as a semantic unit would be a
big mistake.
Other cases are of more linguistic interest. For many purposes, one
would want to regard phrasal verbs ( make up ,work out ) as a single lex-
eme (section 3.1.4), but this case is especially tricky since in many casesthe particle is separable from the verb ( I couldn’t work the answer out),
and so in general identiﬁcation of possible phrasal verbs will have to be
left to subsequent processing. One might also want to treat as a single
lexeme certain other ﬁxed phrases, such as in spite of, in order to ,a n d be-
cause of , but typically a tokenizer will regard them as separate words. A
partial implementation of this approach occurs in the
LOBcorpus where
certain pairs of words such as because of are tagged with a single part of
speech, here preposition, by means of using so-called ditto tags . ditto tags
Variant coding of information of a certain semantic type
Many readers may have felt that the example of a phone number in the
previous section was not very recognizable or convincing because their
phone numbers are written as 812-4374, or whatever. However, even if
one is not dealing with multilingual text, any application dealing with
text from diﬀerent countries or written according to diﬀerent stylisticconventions has to be prepared to deal with typographical diﬀerences. Inparticular, some items such as phone numbers are clearly of one seman-tic sort, but can appear in many formats. A selection of formats for phonenumbers with their countries, all culled from advertisements in one issueof the magazine The Economist , is shown in table 4.2. Phone numbers var-
iously use spaces, periods, hyphens, brackets, and even slashes to group
digits in various ways, often not consistently even within one country.Additionally, phone numbers may include international or national longdistance codes, or attempt to show both (as in the ﬁrst three UK entriesin the table), or just show a local number, and there may or may notbe explicit indication of this via other marks such as brackets and plus

p/CX /CX4.2 Looking at Text 131
Phone number Country Phone number Country
0171 378 0647 UK +45 43 48 60 60 Denmark
(44.171) 830 1007 UK 95-51-279648 Pakistan
+44 (0) 1225 753678 UK +411/284 3797 Switzerland01256 468551 UK (94-1) 866854 Sri Lanka(202) 522-2230 USA +49 69 136-2 98 05 Germany1-925-225-3000 USA 33 1 34 43 32 26 France212. 995.5402 USA ++31-20-5200161 The Netherlands
Table 4.2 Diﬀerent formats for telephone numbers appearing in an issue of
The Economist .
signs. Trying to deal with myriad formats like this is a standard prob-
lem in information extraction . It has most commonly been dealt with by information
extraction building carefully handcrafted regular expressions to match formats, but
given the brittleness of such an approach, there is considerable interestin automatic means for learning the formatting of semantic types.
We do not cover information extraction extensively in this book, but
there is a little further discussion in section 10.6.2.
Speech corpora
Our discussion has concentrated on written text, but the transcripts of
speech corpora provide their own additional challenges. Speech corpora
normally have more contractions, various sorts of more phonetic rep-
resentations, show pronunciation variants, contain many sentence frag-ments, and include ﬁllers like erandum. Example (4.4) – from the Switch-
board corpus available from the
LDC – shows a typical extract from a
speech transcript:
(4.4) Also I [cough] not convinced that the, at least the kind of people that I
work with, I’m not convinced that that’s really, uh, doing much for theprogr-, for the, uh, drug problem.
4.2.3 Morphology
Another question is whether one wants to keep word forms like sit,sits
andsatseparate or to collapse them. The issues here are similar to those
in the discussion of capitalization, but have traditionally been regarded

p/CX /CX132 4 Corpus-Based Work
as more linguistically interesting. At ﬁrst, grouping such forms together
and working in terms of lexemes feels as if it is the right thing to do. Do-ing this is usually referred to in the literature as stemming in reference to
stemming
a process that strips oﬀ aﬃxes and leaves you with a stem. Alternatively,
the process may be referred to as lemmatization where one is attempting lemmatization
to ﬁnd the lemma orlexeme of which one is looking at an inﬂected form. lemma
These latter terms imply disambiguation at the level of lexemes, such as
whether a use of lying represents the verb lie-lay ‘to prostrate oneself’ or
lie-lied ‘to ﬁb.’
Extensive empirical research within the Information Retrieval (IR) com-
munity has shown that doing stemming does not help the performanceof classic IR systems when performance is measured as an average over
queries (Salton 1989; Hull 1996). There are always some queries for which
stemming helps a lot. But there are others where performance goes down.This is a somewhat surprising result, especially from the viewpoint of lin-guistic intuition, and so it is important to understand why that is. Thereare three main reasons for this.
One is that while grouping the various forms of a stem seems a good
thing to do, it often costs you a lot of information. For instance, while
operating can be used in a periphrastic tense form as in Bill is operating a
tractor (section 3.1.3), it is usually used in noun- and adjective-like uses
such as operating systems oroperating costs . It is not hard to see why a
search for operating systems will perform better if it is done on inﬂected
words than if one instead searches for all paragraphs that contain operat-
andsystem . Or to consider another example, if someone enters business
and the stemmer then causes retrieval of documents with busy in them,
the results are unlikely to be beneﬁcial.
Secondly, morphological analysis splits one token into several. How-
ever, often it is worthwhile to group closely related information intochunks, notwithstanding the blowout in the vocabulary that this causes.Indeed, in various Statistical
NLPdomains, people have been able to im-
prove system performance by regarding frequent multiword units as asingle distinctive token. Often inﬂected words are a useful and eﬀective
chunk size.
Thirdly, most information retrieval studies have been done on English
– although recently there has been increasing multilingual work. Englishhas very little morphology, and so the need for dealing intelligently withmorphology is not acute. Many other languages have far richer systemsof inﬂection and derivation, and then there is a pressing need for mor-

p/CX /CX4.2 Looking at Text 133
phological analysis. A full-form lexicon for such languages, one that sepa- full-form lexicon
rately lists all inﬂected forms of all words, would simply be too large. For
instance, Bantu languages (spoken in central and southern Africa) displayrich verbal morphology. Here is a form from KiHaya (Tanzania). Note thepreﬁxes for subject and object agreement, and tense:
(4.5) akabimúha
a-ka-bi-mú-ha1sg-past-3pl-3sg -give
‘I gave them to him.’
For historical reasons, some Bantu language orthographies write many
of these morphemes with whitespace in between them, but in the lan-
guages with ‘conjunctive’ orthographies, morphological analysis is badly
needed. There is an extensive system of pronoun and tense markers ap-pearing before the verb root, and quite a few other morphemes that canappear after the root, yielding a large system of combinatoric possibili-ties. Finnish is another language famous for millions of inﬂected formsfor each verb.
One might be tempted to conclude from the paragraphs above that,
in languages with rich morphology, one would gain by stripping inﬂec-
tional morphology but not derivational morphology. But this hypothesisremains to be carefully tested in languages where there is suﬃcient in-ﬂectional morphology for the question to be interesting.
It is important to realize that this result from IR need not apply to any
or all Statistical
NLPapplications. It need not even apply to all of IR.
Morphological analysis might be much more useful in other applications.
Stemming does not help in the non-interactive evaluation of IR systems,
where a query is presented and processed without further input, and theresults are evaluated in terms of the appropriateness of the set of docu-ments returned. However, principled morphological analysis is valuablein IR in an interactive context, the context in which IR should really beevaluated. A computer does not care about weird stems like busy from
business , but people do. They do not understand what is going on when
business is stemmed to busy and a document with busy in it is returned.
It is also the case that nobody has systematically studied the possi-
bility of letting people interactively inﬂuence the stemming. We believethat this could be very proﬁtable, for cases like saw(where you want to
stem for the sense ‘see,’ but not for the sense ‘cutting implement’), orderivational cases where in some cases you want the stems ( arbitrary

p/CX /CX134 4 Corpus-Based Work
from arbitrariness ) ,b u ti ns o m ey o ud on o t( busy from business ). But the
suggestion that human input may be needed does show the diﬃculties ofdoing automatic stemming in a knowledge-poor environment of the sortthat has often been assumed in Statistical
NLPwork (for both ideological
and practical reasons).
Stemming and IR in general are further discussed in chapter 15.
4.2.4 Sentences
What is a sentence?
The ﬁrst answer to what is a sentence is “something ending with a ‘.’, ‘?’
or ‘!’ .” We have already mentioned the problem that only some periodsmark the end of a sentence: others are used to show an abbreviation, orfor both these functions at once. Nevertheless, this basic heuristic getsone a long way: in general about 90% of periods are sentence boundary
indicators (Riley 1989). There are a few other pitfalls to be aware of.
Sometimes other punctuation marks split up what one might want toregard as a sentence. Often what is on one or the other or even bothsides of the punctuation marks colon, semicolon, and dash (‘:’, ‘;’, and‘—’) might best be thought of as a sentence by itself, as ‘:’ in this example:
(4.6) The scene is written with a combination of unbridled passion and sure-
handed control: In the exchanges of the three characters and the rise and
fall of emotions, Mr. Weller has captured the heartbreaking inexorabilityof separation.
Related to this is the fact that sometimes sentences do not nicely follow
in sequence, but seem to nest in awkward ways. While normally nested
things are not seen as sentences by themselves, but clauses, this classi-
ﬁcation can be strained for cases such as the quoting of direct speech,where we get subsentences:
(4.7) “You remind me,” she remarked, “of your mother.”
A second problem with such indirect speech is that it is standard type-
setting practice (particularly in North America) to place quotation marks
after sentence ﬁnal punctuation. Therefore, the end of the sentence isnot after the period in the example above, but after the close quotationmark that follows the period.
The above remarks suggest that the essence of a heuristic sentence
division algorithm is roughly as in ﬁgure 4.1. In practice most systems

p/CX /CX4.2 Looking at Text 135
Place putative sentence boundaries after all occurrences of . ? ! (and
maybe ; : —)
Move the boundary after following quotation marks, if any.
Disqualify a period boundary in the following circumstances:
–If it is preceded by a known abbreviation of a sort that does not nor-
mally occur word ﬁnally, but is commonly followed by a capitalized
proper name, such as Prof. orvs.
–If it is preceded by a known abbreviation and not followed by an
uppercase word. This will deal correctly with most usages of ab-
breviations like etc. orJr.which can occur sentence medially or
ﬁnally.
Disqualify a boundary with a ? or ! if:
–It is followed by a lowercase letter (or a known name).
Regard other putative sentence boundaries as sentence boundaries.
Figure 4.1 Heuristic sentence boundary detection algorithm.
have used heuristic algorithms of this sort. With enough eﬀort in their
development, they can work very well, at least within the textual domainfor which they were built. But any such solution suﬀers from the sameproblems of heuristic processes in other parts of the tokenization pro-
cess. They require a lot of hand-coding and domain knowledge on the
part of the person constructing the tokenizer, and tend to be brittle anddomain-speciﬁc.
There has been increasing research recently on more principled meth-
ods of sentence boundary detection. Riley (1989) used statistical clas-siﬁcation trees to determine sentence boundaries. The features for theclassiﬁcation trees include the case and length of the words preceding
and following a period, and the a priori probability of diﬀerent words to
occur before and after a sentence boundary (the computation of whichrequires a large quantity of labeled training data). Palmer and Hearst(1994; 1997) avoid the need for acquiring such data by simply using thepart of speech distribution of the preceding and following words, andusing a neural network to predict sentence boundaries. This yields a

p/CX /CX136 4 Corpus-Based Work
robust, largely language independent boundary detection algorithm with
high performance (about 98–99% correct). Reynar and Ratnaparkhi (1997)and Mikheev (1998) develop Maximum Entropy approaches to the prob-lem, the latter achieving an accuracy rate of 99.25% on sentence boundaryprediction.
3
Sentence boundary detection can be viewed as a classiﬁcation problem.
We discuss classiﬁcation, and methods such as classiﬁcation trees andmaximum entropy models in chapter 16.
What are sentences like?
In linguistics classes, and when doing traditional computational linguis-
tics exercises, sentences are generally short. This is at least in part be-cause many of the parsing tools that have traditionally been used havea runtime exponential in the sentence length, and therefore become im-
practical for sentences over twelve or so words. It is therefore important
to realize that typical sentences in many text genres are rather long. Innewswire, the modal (most common) length is normally around 23 words.A chart of sentence lengths in a sample of newswire text is shown in ta-ble 4.3.
4.3 Marked-up Data
While much can be done from plain text corpora, by inducing the struc-
ture present in the text, people have often made use of corpora where
some of the structure is shown, since it is then easier to learn more.This markup may be done by hand, automatically, or by a mixture ofthese two methods. Automatic means of learning structure are coveredin the remainder of this book. Here we discuss the basics of markup.Some texts mark up just a little basic structure such as sentence andparagraph boundaries, while others mark up a lot, such as the full syntac-
tic structure in corpora like the Penn Treebank and the Susanne corpus.
However, the most common grammatical markup that one ﬁnds is a cod-ing of words for part of speech, and so we devote particular attention tothat.
3.Accuracy as a technical term is deﬁned and discussed in section 8.1. However, the
deﬁnition corresponds to one’s intuitive understanding: it is the percent of the time thatone is correctly classifying items.

p/CX /CX4.3 Marked-up Data 137
Length Number Percent Cum. %
1–5 1317 3.13 3.13
6–10 3215 7.64 10.77
11–15 5906 14.03 24.8016–20 7206 17.12 41.9221–25 7350 17.46 59.3826–30 6281 14.92 74.3031–35 4740 11.26 85.5636–40 2826 6.71 92.26
41–45 1606 3.82 96.10
46–50 858 2.04 98.1451–100 780 1.85 99.99101+ 6 0.01 100.00
Table 4.3 Sentence lengths in newswire text. Column “Percent” shows the per-
centage in each range, column “Cum. %” shows the cumulative percentage belowa certain length.
4.3.1 Markup schemes
Various schemes have been used to mark up the structure of text. In
the early days, these were developed on an ad hoc basis, as needs arose.One of the more important early examples was the
COCOA format, which
was used for including header information in texts (giving author, date,title, etc.). This information was enclosed within angle brackets with the
ﬁrst letter indicating the broad semantics of the ﬁeld. Some other ad hoc
systems of this sort are still in quite common use. The most commonform of grammatical markup, which we discuss in great detail below, isindicating the part of speech of words by adding a part of speech tagto each word. These tags are commonly indicated by devices such asfollowing each word by a slash or underline and then a short code namingthe part of speech. The Penn Treebank uses a form of Lisp-like bracketing
to mark up a tree structure over texts.
However, currently by far the most common and supported form of
markup is to use
SGML (the Standard Generalized Markup Language ). Standard
Generalized Markup
LanguageSGML is a general language that lets one deﬁne a grammar for texts, in
particular for the type of markup they contain. The now-ubiquitous HTML
is an instance of an SGML encoding. The Text Encoding Initiative ( TEI)w a s

p/CX /CX138 4 Corpus-Based Work
a major attempt to deﬁne SGML encoding schemes suitable for marking
up various kinds of humanities text resources ranging from poems andnovels to linguistic resources like dictionaries. Another acronym to beaware of is
XML.XMLdeﬁnes a simpliﬁed subset of SGML that was partic- XML
ularly designed for web applications. However, the weight of commercial
support behind XMLand the fact that it avoids some of the rather arcane,
and perhaps also archaic, complexities in the original SGML speciﬁcation
means that the XML subset is likely to be widely adopted for all other
purposes as well.
This book does not delve deeply into SGML . We will give just the rudi-
mentary knowledge needed to get going. SGML speciﬁes that each doc-
ument type should have a Document Type Deﬁnition (DTD), which is a Document Type
Deﬁnition
DTDgrammar for legal structures for the document. For example, it can state
rules that a paragraph must consist of one or more sentences and nothingelse. An
SGML parser veriﬁes that a document is in accordance with this
DTD, but within Statistical NLPtheDTD is normally ignored and people
just process whatever text is found. An SGML document consists of one
or more elements, which may be recursively nested. Elements normallybegin with a begin tag and end with an end tag, and have document con-
tent in between. Tags are contained within angle brackets, and end tags
begin with a forward slash character. As well as the tag name, the begintag may contain additional attribute and value information. A couple ofexamples of
SGML elements are shown below:
(4.8) a. <p><s>And then he left.</s>
<s>He did not say another word.</s></p>
b.<utt speak="Fred" date="10-Feb-1998">That is an ugly
couch.</utt>
The structure tagging shown in (4.8a), where the tag sis used for sen-
tences and pfor paragraphs, is particularly widespread. Example (4.8b)
shows a tag with attributes and values. An element may also consist ofjust a single tag (without any matching end tag). In
XML, such empty ele-
ments must be specially marked by ending the tag name with a forward
slash character.
In general, when making use of SGML -encoded text in a casual way,
one will wish to interpret some tags within angle brackets, and to simplyignore others. The other
SGML syntax that one must be aware of is char-
acter and entity references. These begin with an ampersand and end with

p/CX /CX4.3 Marked-up Data 139
a semicolon. Character references are a way of specifying characters not
available in the standard ASCII character set (minus the reserved SGML
markup characters) via their numeric code. Entity references have sym-bolic names which were deﬁned in the
DTD (or are one of a few predeﬁned
entities). Entity references may expand to any text, but are commonly
used just to encode a special character via a symbolic name. A few exam-
ples of character and entity references are shown in (4.9). They might berendered in a browser or when printed as shown in (4.10).
(4.9) a. &#x43; is the less than symbol
b.r&eacute;sum&eacute;
c.This chapter was written on &docdate;.
(4.10) a.<is the less than symbol
b. résuméc. This chapter was written on January 21, 1998.
There is much more to know about
SGML , and some references appear in
the Further Reading below, but this is generally enough for what the XML
community normally terms the ‘Desperate Perl Hacker’ to get by.
4.3.2 Grammatical tagging
A common ﬁrst step of analysis is to perform automatic grammaticaltagging for categories roughly akin to conventional parts of speech, butoften considerably more detailed (for instance, distinguishing compara-
tive and superlative forms of adjectives, or singular from plural nouns).
This section examines the nature of tag sets. What tag sets have beenused? Why do people use diﬀerent ones? Which one should you choose?
How tagging is done automatically is the subject of chapter 10.
Tag sets
Historically, the most inﬂuential tag sets have been the one used for tag-
ging the American Brown corpus (the Brown tag set ) and the series of
Brown tag set
tag sets developed at the University of Lancaster, and used for tagging
the Lancaster-Oslo-Bergen corpus and more recently the British National

p/CX /CX140 4 Corpus-Based Work
Sentence CLAWS c5 Brown Penn Treebank ICE
she PNP PPS PRP PRON(pers,sing)
was VBD BEDZ VBD AUX(pass,past)
told VVN VBN VBN V(ditr,edp)that CJT CS IN CONJUNC(subord)the AT0 AT DT ART(def)journey NN1 NN NN N(com,sing)might VM0 MD MD AUX(modal,past)kill VVI VB VB V(montr,inﬁn)
her PNP PPO PRP PRON(poss,sing)
. PUN . . PUNC(per)
Figure 4.2 A sentence as tagged according to several diﬀerent tag sets.
Tag set Basic size Total tags
Brown 87 179
Penn 45
CLAWS 1 132
CLAWS 2 166
CLAW Sc 5 6 2
London-Lund 197
Table 4.4 Sizes of various tag sets.
Corpus ( CLAWS 1 through CLAWS 5;CLAWS 5 is also referred to as the c5 c5 tag set
tag set ). Recently, the Penn Treebank tag set has been the one most Penn Treebank tag
set widely used in computational work. It is a simpliﬁed version of the Brown
tag set. A brief summary of tag set sizes is shown in table 4.4. An ex-ample sentence shown tagged via several diﬀerent tag sets is shown inﬁgure 4.2. These tag sets are all for English. In general, tag sets incorpo-rate morphological distinctions of a particular language, and so are notdirectly applicable to other languages (though often some of the design
ideas can be transferred). Many tag sets for other languages have also
been developed.
An attempt to align some tag sets, roughly organized by traditional
parts of speech appears in tables 4.5 and 4.6, although we cannot guar-antee that they are accurate in every detail. They are mostly alphabetical,but we have deviated from alphabetical order a little so as to group cat-

p/CX /CX4.3 Marked-up Data 141
Category Examples Claws c5 Brown Penn
Adjective happy, bad AJ0 JJ JJ
Adjective, ordinal number sixth, 72nd, last ORD OD JJ
Adjective, comparative happier, worse AJC JJR JJRAdjective, superlative happiest, worst AJS JJT JJS
Adjective, superlative, semantically chief, top AJ0 JJS JJ
Adjective, cardinal number 3, ﬁfteen CRD CD CDAdjective, cardinal number, one one PNI CD CDAdverb often, particularly AV0 RB RB
Adverb, negative not, n’t XX0 * RB
Adverb, comparative faster AV0 RBR RBRAdverb, superlative fastest AV0 RBT RBSAdverb, particle up, oﬀ, out AVP RP RP
Adverb, question when, how, why AVQ WRB WRB
Adverb, degree & question how, however AVQ WQL WRBAdverb, degree very, so, too AV0 QL RBAdverb, degree, postposed enough, indeed AV0 QLP RB
Adverb, nominal here, there, now AV0 RN RB
Conjunction, coordination and, or CJC CC CCConjunction, subordinating although, when CJS CS INConjunction, complementizer that that CJT CS IN
Determiner this, each, another DT0 DT DT
Determiner, pronoun any, some DT0 DTI DTDeterminer, pronoun, plural these, those DT0 DTS DTDeterminer, prequaliﬁer quite DT0 ABL PDT
Determiner, prequantiﬁer all, half DT0 ABN PDT
Determiner, pronoun or double conj. both DT0 ABX DT (CC)Determiner, pronoun or double conj. either, neither DT0 DTX DT (CC)Determiner, article the, a, an AT0 AT DT
Determiner, postdeterminer many, same DT0 AP JJ
Determiner, possessive their, your DPS PP$ PRP$Determiner, possessive, second mine, yours DPS PP$$ PRPDeterminer, question which, whatever DTQ WDT WDT
Determiner, possessive & question whose DTQ WP$ WP$
Noun aircraft, data NN0 NN NNNoun, singular woman, book NN1 NN NN
Noun, plural women, books NN2 NNS NNS
Noun, proper, singular London, Michael NP0 NP NNPNoun, proper, plural Australians, Methodists NP0 NPS NNPSNoun, adverbial tomorrow, home NN0 NR NN
Noun, adverbial, plural Sundays, weekdays NN2 NRS NNS
Pronoun, nominal (indeﬁnite) none, everything, one PNI PN NNPronoun, personal, subject you, we PNP PPSS PRPPronoun, personal, subject, 3SG she, he, it PNP PPS PRP
Pronoun, personal, object you, them, me PNP PPO PRP
Pronoun, reﬂexive herself, myself PNX PPL PRPPronoun, reﬂexive, plural themselves, ourselves PNX PPLS PRPPronoun, question, subject who, whoever PNQ WPS WP
Pronoun, question, object who, whoever PNQ WPO WP
Pronoun, existential there there EX0 EX EX
Table 4.5 Comparison of diﬀerent tag sets: adjective, adverb, conjunction, de-
terminer, noun, and pronoun tags.

p/CX /CX142 4 Corpus-Based Work
Category Examples Claws c5 Brown Penn
Verb, base present form (not inﬁnitive) take, live VVB VB VBP
Verb, inﬁnitive take, live VVI VB VB
Verb, past tense took, lived VVD VBD VBD
Verb, present participle taking, living VVG VBG VBGVerb, past/passive participle taken, lived VVN VBN VBNVerb, present 3SG -sform takes, lives VVZ VBZ VBZ
Verb, auxiliary do, base do VDB DO VBP
Verb, auxiliary do, inﬁnitive do VDB DO VB
Verb, auxiliary do, past did VDD DOD VBD
Verb, auxiliary do, present part. doing VDG VBG VBG
Verb, auxiliary do, past part. done VDN VBN VBN
Verb, auxiliary do, present 3SG does VDZ DOZ VBZ
Verb, auxiliary have , base have VHB HV VBP
Verb, auxiliary have , inﬁnitive have VHI HV VB
Verb, auxiliary have , past had VHD HVD VBD
Verb, auxiliary have , present part. having VHG HVG VBG
Verb, auxiliary have , past part. had VHN HVN VBN
Verb, auxiliary have , present 3SG has VHZ HVZ VBZ
Verb, auxiliary be, inﬁnitive be VBI BE VB
Verb, auxiliary be, past were VBD BED VBD
Verb, auxiliary be, past, 3SG was VBD BEDZ VBD
Verb, auxiliary be, present part. being VBG BEG VBG
Verb, auxiliary be, past part. been VBN BEN VBN
Verb, auxiliary be, present, 3SG is, ’s VBZ BEZ VBZ
Verb, auxiliary be, present, 1SG am, ’m VBB BEM VBP
Verb, auxiliary be, present are, ’re VBB BER VBP
Verb, modal can, could, ’ll VM0 MD MDInﬁnitive marker to TO0 TO TOPreposition, to to PRP IN TO
Preposition for, above PRP IN IN
Preposition, of of PRF IN INPossessive ’s, ’ POS $ POSInterjection (or other isolate) oh, yes, mmm ITJ UH UH
Punctuation, sentence ender . ! ? PUN . .
Punctuation, semicolon ; PUN . :Punctuation, colon or ellipsis : . . . PUN : :Punctuation, comma , PUN , ,
Punctuation, dash – PUN – –
Punctuation, dollar sign $ PUN not $Punctuation, left bracket ( [ { PUL ( (Punctuation, right bracket ) ] } PUR ) )
Punctuation, quotation mark, left ‘ “ PUQ not “
Punctuation, quotation mark, right ’ ” PUQ not ”Foreign words (not in English lexicon) UNC (FW-) FW
Symbol [fj] * not SYM
Symbol, alphabetical A, B, c, d ZZ0Symbol, list item A A. First LS
Table 4.6 Comparison of diﬀerent tag sets: Verb, preposition, punctuation and
symbol tags. An entry of ‘not’ means an item was ignored in tagging, or was notseparated oﬀ as a separate token.

p/CX /CX4.3 Marked-up Data 143
egories that are sometimes collapsed. In this categorization, we use an
elsewhere convention where the least marked category is used in all caseswhere a word cannot be placed within one of the more precise subclassi-ﬁcations. For instance, the plain Adjective category is used for adjectivesthat aren’t comparatives, superlatives, numbers, etc. The complete Brown
tag set was made larger by two decisions to augment the tag set. Normal
tags could be followed by a hyphen and an attribute like TL (for a ti-tle word), or in the case of foreign words, the FW foreign word tag wasfollowed by a hyphen and a part of speech assignment. Secondly, theBrown tag scheme makes use of ‘combined tags’ for graphic words thatone might want to think of as multiple lexemes, such as you’ll .
4Normally
such items were tagged with two tags joined with a plus sign, but for
negation one just adds * to a tag. So isn’t is tagged BEZ* and she’ll is
tagged PPS+MD. Additionally, possessive forms like children’s are tagged
with a tag ending in ‘$’. Normally, these tags are transparently derivedfrom a base non-possessive tag, for instance, NNS$ in this case. Thesetechniques of expanding the tag set are ignored in the comparison.
Even a cursory glance will show that the tag sets are very diﬀerent. Part
of this can be attributed to the overall size of the tag set. A larger tag
set will obviously make more ﬁne-grained distinctions. But this is not the
only diﬀerence. The tag sets may choose to make distinctions in diﬀerentareas. For example, the c5 tag set is larger overall than the Penn Treebanktag set, and it makes many more distinctions in some areas, but in otherareas it has chosen to make many fewer. For instance, the Penn tag setdistinguishes 9 punctuation tags, while c5 makes do with only 4. Pre-sumably this indicates some diﬀerence of opinion on what is considered
important. Tag sets also disagree more fundamentally in how to classify
certain word classes. For example, while the Penn tag set simply regardssubordinating conjunctions as prepositions (consonant with work in gen-erative linguistics), the c5 tag set keeps them separate, and moreoverimplicitly groups them with other types of conjunctions. The notion ofimplicit grouping referred to here is that all the tag sets informally showrelationships between certain sets of tags by having them begin with the
same letter or pair of letters. This grouping is implicit in that although
it is obvious to the human eye, they are formally just distinct symbolic
4. Compare the discussion above. This is also done in some other corpora, such as the
London-Lund corpus, but the recent trend seems to have been towards dividing suchgraphic words into two for the purposes of tagging.

p/CX /CX144 4 Corpus-Based Work
tags, and programs normally make no use of these families. However,
in some other tag sets, such as the one for the International Corpus ofEnglish (Greenbaum 1993), an explicit system of high level tags with at-tributes for the expression of features has been adopted. There has alsobeen some apparent development in people’s ideas of what to encode.
The early tag sets made very ﬁne distinctions in a number of areas such
as the treatment of certain sorts of qualiﬁers and determiners that wererelevant to only a few words, albeit common ones. More recent tag setshave generally made fewer distinctions in such areas.
The design of a tag set
What features should guide the design of a tag set? Standardly, a tag set
encodes both the target feature of classiﬁcation, telling the user the use-ful information about the grammatical class of a word, and the predictivefeatures, encoding features that will be useful in predicting the behaviorof other words in the context. These two tasks should overlap, but theyare not necessarily identical.
The notion of part of speech is actually complex, since parts of speech
part of speech
can be motivated on various grounds, such as semantic (commonly called
notional) grounds, syntactic distributional grounds, or morphologicalgrounds. Often these notions of part of speech are in conﬂict. For thepurposes of prediction, one would want to use the deﬁnition of part ofspeech that best predicts the behavior of nearby words, and this is pre-sumably strictly distributional tags. But in practice people have often
used tags that reﬂect notional or morphological criteria. For example one
of the uses of English present participles ending in -ingis as a gerund
where they behave as a noun. But in the Brown corpus they are quiteregularly tagged with the VBG tag, which is perhaps better reserved forverbal uses of participles. This happens even within clear noun com-pounds such as this one:
(4.11) Fulton/ np-tl County/ nn-tl Purchasing/ vbgDepartment/ nn
Ideally, we would want to give distinctive tags to words that have dis-
tinctive distributions, so that we can use that information to help pro-cessing elsewhere. This would suggest that some of the tags in, for ex-ample, the Penn Treebank tag set are too coarse to be good predictors.For instance, the complementizer that has a very distinct distribution
from regular prepositions, and degree adverbs and the negative nothave

p/CX /CX4.4 Further Reading 145
very diﬀerent distributions from regular adverbs, but neither of these
distinctions show up in the tag set. People have frequently made changesto add or remove distinctions according to their intuitions – for exam-ple, Charniak (1996) questions the decision of the Penn Treebank to tagauxiliaries with the same tags as other verbs, given that auxiliary verbs
have a very distinctive distribution, and proceeds to retag them with an
aux tag. In general, the predictive value of making such changes in the
set of distinctions in part of speech systems has not been very system-atically evaluated. So long as the same tag set is used for predictionand classiﬁcation, making such changes tends to be a two-edged sword:splitting tags to capture useful distinctions gives improved informationfor prediction, but makes the classiﬁcation task harder.
5For this reason,
there is not necessarily a simple relationship between tag set size and the
performance of automatic taggers.
4.4 Further Reading
The Brown corpus (the Brown University Standard Corpus of Present-DayAmerican English) consists of just over a million words of written Amer-ican English from 1961. It was compiled and documented by W. NelsonFrancis and Henry Ku ˇcera (Francis and Ku ˇcera 1964; Ku ˇcera and Fran-
cis 1967; Francis and Ku ˇcera 1982). The details on early processing of
the Brown corpus are from an email from Henry Ku ˇcera (posted to the
corpora mailing list by Katsuhide Sonoda on 26 Sep 1996). The
LOB
(Lancaster-Oslo-Bergen) corpus was built as a British-English replication
of the Brown Corpus during the 1970s (Johansson et al. 1978; Garsideet al. 1987).
Identifying proper names is a major issue in Information Extraction.
See (Cardie 1997) for an introduction.
A carefully designed and experimentally tested set of tokenization
rules is the set used for the Susanne corpus (Sampson 1995: 52–59).
Nunberg (1990) provides a linguistic perspective on the importance of
punctuation . An introductory discussion of what counts as a word in
punctuation
linguistics can be found in (Crowley et al. 1995: 7–9). Lyons (1968: 194–
206) provides a more thorough discussion. The examples in the sectionon hyphenation are mainly real examples from the Dow Jones newswire.
5. This is unless one category groups two very separate distributional clusters, in which
case splitting the category can actually sometimes make classiﬁcation easier.

p/CX /CX146 4 Corpus-Based Work
Others are from e-mail messages to the corpora list by Robert Amsler and
Mitch Marcus, 1996, and are used with thanks.
There are many existing systems for morphological analysis available,
and some are listed on the website. An eﬀective method of doing stem-ming in a knowledge-poor way can be found in Kay and Röscheisen
(1993). Sproat (1992) contains a good discussion of the problems mor-
phology presents for
NLPand is the source of our German compound
example.
The COCOA (COunt and COncordance on Atlas) format was used in
corpora from ICAME and in related software such as LEXA (Hickey 1993).
SGML and XML are described in various books (Herwijnen 1994; Mc-
Grath 1997; St. Laurent 1998), and a lot of information, including some
short readable introductions, is available on the web (see website).
The guidelines of the Text Encoding initiative (1994 P3 version) are
published as McQueen and Burnard (1994), and include a very readableintroduction to
SGML in chapter 2. In general, though, rather than read
the actual guidelines, one wants to look at tutorials such as Ide and Véro-nis (1995), or on the web, perhaps starting at the sites listed on the web-site. The full complexity of the
TEIoverwhelmed all but the most dedi-
cated standard bearers. Recent developments include TEILite, which tries
to pare the original standard down to a human-usable version, and theCorpus Encoding Standard, a
TEI-conformant SGML instance especially
designed for language engineering corpora.
Early work on CLAWS (Constituent-Likelihood Automatic Word-tagging
System) and its tag set is described in (Garside et al. 1987). The more re-cent c5 tag set presented above is taken from (Garside 1995). The Brown
tag set is described in (Francis and Ku ˇcera 1982) while the Penn tag set is
described in (Marcus et al. 1993), and in more detail in (Santorini 1990).
This book is not an introduction to how corpora are used in linguistic
studies (even though it contains a lot of methods and algorithms usefulfor such studies). However, recently there has been a ﬂurry of new textsoncorpus linguistics (McEnery and Wilson 1996; Stubbs 1996; Biber et al.
corpus linguistics
1998; Kennedy 1998; Barnbrook 1996). These books also contain much
more discussion of corpus design issues such as sampling and balance
than we have provided here. For an article speciﬁcally addressing theproblem of designing a representative corpus, see (Biber 1993).
More details about diﬀerent tag sets are collected in Appendix B of
(Garside et al. 1987) and in the web pages of the
AMALGAM project (see
website). The AMALGAM website also has a description of the tokenizing

p/CX /CX4.5 Exercises 147
rules that they use, which can act as an example of a heuristic sentence di-
vider and tokenizer. Grefenstette and Tapanainen (1994) provide anotherdiscussion of tokenization, showing the results of experiments employ-ing simple knowledge-poor heuristics.
4.5 Exercises
Exercise 4.1 [««]
As discussed in the text, it seems that for most purposes, we’d want to treat
some hyphenated things as words (for instance, co-worker ,Asian-American ),
but not others (for instance, ain’t-it-great-to-be-a-Texan ,child-as-required-yuppie-
possession ). Find hyphenated forms in a corpus and suggest some basis for which
forms we would want to treat as words and which we would not. What are thereasons for your decision? (Diﬀerent choices may be appropriate for diﬀerentneeds.) Suggest some methods to identify hyphenated sequences that should
be broken up – e.g., ones that only appear as non-ﬁnal elements of compound
nouns:
[n[child-as-required-yuppie-possession] syndrome]
Exercise 4.2 [«« For linguists]
Take some linguistic problem that you are interested in (non-constituent coordi-
nation, ellipsis, idioms, heavy NP shift, pied-piping, verb class alternations, etc.).Could one hope to ﬁnd useful data pertaining to this problem in a general cor-pus? Why or why not? If you think it might be possible, is there a reasonableway to search for examples of the phenomenon in either a raw corpus or onethat shows syntactic structures? If the answer to both these questions is yes,then look for examples in a corpus and report on anything interesting that youﬁnd.
Exercise 4.3 [««]
Develop a sentence boundary detection algorithm. Evaluate how successful it is.
(In the construction of the Wall Street Journal section of the
ACL-DCI CD -ROM
(Church and Liberman 1991), a rather simplistic sentence boundary detection
algorithm was used, and the results were not hand corrected, so many errorsremain. If this corpus is available to you, you may want to compare your resultswith the sentence boundaries marked in the corpus. With luck, you should beable to write a system that performs considerably better!)



