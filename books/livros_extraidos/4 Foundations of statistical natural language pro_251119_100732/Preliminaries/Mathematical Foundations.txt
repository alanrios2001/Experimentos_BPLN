p/CX /CX2 Mathematical Foundations
This chapter presents some introductory material on probability and
information theory, while the next chapter presents some essential know-ledge of linguistics. A thorough knowledge of one or more of the ï¬elds of
probability and statistics, information theory, and linguistics is desirable,
and perhaps even necessary, for doing original research in the ï¬eld of Sta-tistical
NLP. We cannot provide a thorough well-motivated introduction to
each of these three ï¬elds within this book, but nevertheless, we attemptto summarize enough material to allow understanding of everything thatfollows in the book. We do however assume knowledge of parsing, ei-ther from a computer science or computational linguistics perspective.
We also assume a reasonable knowledge of mathematical symbols and
techniques, perhaps roughly to the level of a ï¬rst year undergraduatecourse, including the basics of such topics as: set theory, functions andrelations, summations, polynomials, calculus, vectors and matrices, andlogarithms. Mathematical notations that we use are summarized in theTable of Notations.
If you are familiar with one of the areas covered in these two chap-
ters, then you should probably just skim the corresponding section. If
youâ€™re not familiar with a topic, we think it is probably best to try toread through each section, but you will probably need to reread sectionswhen the techniques in them are put to use. These chapters donâ€™t saymuch about applications â€“ they present the preparatory theory for whatfollows.

p/CX /CX40 2 Mathematical Foundations
2.1 Elementary Probability Theory
This section sketches the essentials of probability theory necessary to
understand the rest of this book.
2.1.1 Probability spaces
Probability theory deals with predicting how likely it is that something probability theory
will happen. For example, if one tosses three coins, how likely is it that
they will all come up heads? Although our eventual aim is to look atlanguage, we begin with some examples with coins and dice, since theirbehavior is simpler and more straightforward.
The notion of the likelihood of something is formalized through the
concept of an experiment (ortrial) â€“ the process by which an observation
experiment
trialis made. In this technical sense, tossing three coins is an experiment.
All that is crucial is that the experimental protocol is well deï¬ned. Weassume a collection of basic outcomes (orsample points ) for our experi-
basic outcomes
ment, the sample space â„¦. Sample spaces may either be discrete ,h a v i n ga t sample space
discretemost a countably inï¬nite number of basic outcomes, or continuous ,h a v -
continuousing an uncountable number of basic outcomes (for example, measuring a
personâ€™s height). For language applications and in this introduction, wewill mainly deal with discrete sample spaces which only contain a ï¬nitenumber of basic outcomes. Let an eventAbe a subset of â„¦. For example,
event
in the coin experiment, the ï¬rst coin being a head, and the second and
third coming down tails is one basic outcome, while any result of onehead and two tails is an example of an event. Note also that â„¦represents
the certain event, the space of all possible experimental outcomes, and
;represents the impossible event. We say that an experimental outcome
must be an event. The foundations of probability theory depend on theset of events Fforming a-ï¬eld â€“ a set with a maximal element â„¦and
-ï¬eld
arbitrary complements and unions. These requirements are trivially sat-
isï¬ed by making the set of events, the event space , the power set of the event space
sample space (that is, the set of all subsets of the sample space, often
written 2F).
Probabilities are numbers between 0 and 1, where 0 indicates impos-
sibility and 1 certainty. A probability function (also known as a prob- probability
function
probability
distributionability distribution ) distributes a probability mass of 1 throughout the
sample space â„¦. Formally, a discrete probability function is any function
P:F!Â†0;1Â‡such that:

p/CX /CX2.1 Elementary Probability Theory 41
PÂ„â„¦Â…Âƒ1
Countable additivity: For disjoint setsAj2F (i.e.,Aj\AkÂƒ; for disjoint
j6Âƒk)
P1[
jÂƒ1Aj
Âƒ1X
jÂƒ1PÂ„AjÂ… (2.1)
We callPÂ„AÂ… the probability of the event A. These axioms say that an
event that encompasses, say, three distinct possibilities must have aprobability that is the sum of the probabilities of each possibility, andthat since an experiment must have some basic outcome as its result,the probability of that is 1. Using basic set theory, we can derive from
these axioms a set of further properties of probability functions; see ex-
ercise 2.1.
A well-founded probability space consists of a sample space â„¦,a-ï¬eld
probability space
of eventsF, and a probability function P. In Statistical NLPapplications,
we always seek to properly deï¬ne such a probability space for our mod-els. Otherwise, the numbers we use are merely ad hoc scaling factors, andthere is no mathematical theory to help us. In practice, though, corners
often have been, and continue to be, cut.
Example 1: A fair coin is tossed 3 times. What is the chance of 2 heads?
Solution: The experimental protocol is clear. The sample space is:
â„¦ÂƒfHHH;HHT;HTH;HTT;THH;THT;TTH;TTT g
Each of the basic outcomes in â„¦is equally likely, and thus has probability
1=8. A situation where each basic outcome is equally likely is called a
uniform distribution . In a ï¬nite sample space with equiprobable basic
uniform
distribution outcomes,PÂ„AÂ…ÂƒjAj
jâ„¦j(wherejAjis the number of elements in a set A).
The event of interest is:
AÂƒfHHT;HTH;THH g
So:
PÂ„AÂ…ÂƒjAj
jâ„¦jÂƒ3
8

p/CX /CX42 2 Mathematical Foundations
A\Bâ„¦
A B
Figure 2.1 A diagram illustrating the calculation of conditional probabil-
ityPÂ„AjBÂ…. Once we know that the outcome is in B, the probability of Abecomes
PÂ„A\BÂ…=PÂ„BÂ… .
2.1.2 Conditional probability and independence
Sometimes we have partial knowledge about the outcome of an experi-
ment and that naturally inï¬‚uences what experimental outcomes are pos-
sible. We capture this knowledge through the notion of conditional proba- conditional
probability bility . This is the updated probability of an event given some knowledge.
The probability of an event before we consider our additional knowledgeis called the prior probability of the event, while the new probability that
prior probability
results from using our additional knowledge is referred to as the pos- posterior
probability terior probability of the event. Returning to example 1 (the chance of
getting 2 heads when tossing 3 coins), if the ï¬rst coin has been tossed
and is a head, then of the 4 remaining possible basic outcomes, 2 result
in 2 heads, and so the probability of getting 2 heads now becomes1
2.T h e
conditional probability of an event Agiven that an event Bhas occurred
(PÂ„BÂ…> 0) is:
PÂ„AjBÂ…ÂƒPÂ„A\BÂ…
PÂ„BÂ…(2.2)
Even ifPÂ„BÂ…Âƒ0 we have that:
PÂ„A\BÂ…ÂƒPÂ„BÂ…PÂ„AjBÂ…ÂƒPÂ„AÂ…PÂ„BjAÂ… [The multiplication rule] (2.3)
We can do the conditionalization either way because set intersection is
symmetric (A\BÂƒB\A). One can easily visualize this result by looking
at the diagram in ï¬gure 2.1.

p/CX /CX2.1 Elementary Probability Theory 43
The generalization of this rule to multiple events is a central result that
will be used throughout this book, the chain rule : chain rule
PÂ„A 1\:::\AnÂ…ÂƒPÂ„A 1Â…PÂ„A 2jA1Â…PÂ„A 3jA1\A2Â…PÂ„Anj\nâˆ’1
iÂƒ1AiÂ… (2.4)
The chain rule is used in many places in Statistical NLP, such as working
out the properties of Markov models in chapter 9.
Two eventsA,Bareindependent of each other if PÂ„A\BÂ…ÂƒPÂ„AÂ…PÂ„BÂ… . independence
UnlessPÂ„BÂ…Âƒ0 this is equivalent to saying that PÂ„AÂ…ÂƒPÂ„AjBÂ…(i.e.,
knowing that Bis the case does not aï¬€ect the probability of A). This
equivalence follows trivially from the chain rule. Otherwise events aredependent . We can also say that AandBareconditionally independent
dependence
conditional
independencegivenCwhenPÂ„A\BjCÂ…ÂƒPÂ„AjCÂ…PÂ„BjCÂ….
2.1.3 Bayesâ€™ theorem
Bayesâ€™ theorem lets us swap the order of dependence between events. Bayesâ€™ theorem
That is, it lets us calculate PÂ„BjAÂ…in terms ofPÂ„AjBÂ…. This is useful when
the former quantity is diï¬ƒcult to determine. It is a central tool that wewill use again and again, but it is a trivial consequence of the deï¬nition ofconditional probability and the chain rule introduced in equations (2.2)and (2.3):
PÂ„BjAÂ…ÂƒPÂ„B\AÂ…
PÂ„AÂ…ÂƒPÂ„AjBÂ…PÂ„BÂ…
PÂ„AÂ…(2.5)
The righthand side denominator PÂ„AÂ… can be viewed as a normalizing normalizing
constant constant , something that ensures that we have a probability function. If
we are simply interested in which event out of some set is most likelygivenA, we can ignore it. Since the denominator is the same in all cases,
we have that:
arg max
BPÂ„AjBÂ…PÂ„BÂ…
PÂ„AÂ…Âƒarg max
BPÂ„AjBÂ…PÂ„BÂ… (2.6)
However, we can also evaluate the denominator by recalling that:
PÂ„A\BÂ…ÂƒPÂ„AjBÂ…PÂ„BÂ…
PÂ„A\BÂ…ÂƒPÂ„AjBÂ…PÂ„BÂ…
So we have:
PÂ„AÂ…ÂƒPÂ„A\BÂ…Â‚PÂ„A\BÂ… [additivity]
ÂƒPÂ„AjBÂ…PÂ„BÂ…Â‚PÂ„AjBÂ…PÂ„BÂ…

p/CX /CX44 2 Mathematical Foundations
BandBserve to split the set A into two disjoint parts (one possibly
empty), and so we can evaluate the conditional probability on each, andthen sum, using additivity. More generally, if we have some group of setsB
ithatpartitionA, that is, ifA[iBiand theBiare disjoint, then: partition
PÂ„AÂ…ÂƒX
iPÂ„AjBiÂ…PÂ„BiÂ… (2.7)
This gives us the following equivalent but more elaborated version of
Bayesâ€™ theorem:
Bayesâ€™ theorem: IfA[n
iÂƒ1Bi,PÂ„AÂ…> 0, andBi\BjÂƒ; fori6Âƒjthen:
PÂ„BjjAÂ…ÂƒPÂ„AjBjÂ…PÂ„BjÂ…
PÂ„AÂ…ÂƒPÂ„AjBjÂ…PÂ„BjÂ…Pn
iÂƒ1PÂ„AjBiÂ…PÂ„BiÂ…(2.8)
Example 2: Suppose one is interested in a rare syntactic construction,
perhaps parasitic gaps, which occurs on average once in 100,000 sen-tences. Joe Linguist has developed a complicated pattern matcher thatattempts to identify sentences with parasitic gaps. Itâ€™s pretty good, butitâ€™s not perfect: if a sentence has a parasitic gap, it will say so with proba-
bility 0.95, if it doesnâ€™t, it will wrongly say it does with probability 0.005.
Suppose the test says that a sentence contains a parasitic gap. What isthe probability that this is true?
Solution: LetGbe the event of the sentence having a parasitic gap, and
letTbe the event of the test being positive. We want to determine:
PÂ„GjTÂ…ÂƒPÂ„TjGÂ…PÂ„GÂ…
PÂ„TjGÂ…PÂ„GÂ…Â‚PÂ„TjGÂ…PÂ„GÂ…
Âƒ0:950:00001
0:950:00001Â‚0:0050:999990:002
Here we use having the construction or not as the partition in the de-
nominator. Although Joeâ€™s test seems quite reliable, we ï¬nd that using it
wonâ€™t help as much as one might have hoped. On average, only 1 in every
500 sentences that the test identiï¬es will actually contain a parasitic gap.This poor result comes about because the prior probability of a sentencecontaining a parasitic gap is so low.
Bayesâ€™ theorem is central to the noisy channel model described in sec-
tion 2.2.4.

p/CX /CX2.1 Elementary Probability Theory 45
First Second die
die 123456
6 7 8 9 10 11 12
5 6789 1 0 1 1
4 56789 1 0
3 456789
2 345678
1 234567
x 23456 7 891 01 11 2
pÂ„XÂƒxÂ…1
361
181
121
95
361
65
361
91
121
181
36
Figure 2.2 A random variable Xfor the sum of two dice. Entries in the body
of the table show the value of Xgiven the underlying basic outcomes, while the
bottom two rows show the pmf p Â„xÂ….
2.1.4 Random variables
Arandom variable is simply a function X:â„¦!Rn(commonly with nÂƒ1), random variable
whereRis the set of real numbers. Rather than having to work with some
irregular event space which diï¬€ers with every problem we look at, a ran-dom variable allows us to talk about the probabilities of numerical values
that are related to the event space. We think of an abstract stochastic pro-
stochastic process
cessthat generates numbers with a certain probability distribution. (The
word stochastic simply means â€˜probabilisticâ€™ or â€˜randomly generated,â€™ but
is especially commonly used when referring to a sequence of results as-sumed to be generated by some underlying probability distribution.)
A discrete random variable is a function X:â„¦!SwhereSis a count-
able subset of R.I fX:â„¦!f0;1g, thenXis called an indicator random
indicator random
variable variable or aBernoulli trial .
Bernoulli trial
Example 3: Suppose the events are those that result from tossing two
dice. Then we could deï¬ne a discrete random variable Xthat is the sum
of their faces: SÂƒf2;:::;12g, as indicated in ï¬gure 2.2.
Because a random variable has a numeric range, we can often do math-
ematics more easily by working with the values of a random variable,rather than directly with events. In particular we can deï¬ne the probabil-
probability mass
function ity mass function (pmf) for a random variable X, which gives the proba-

p/CX /CX46 2 Mathematical Foundations
bility that the random variable has diï¬€erent numeric values:
pmf pÂ„xÂ…ÂƒpÂ„XÂƒxÂ…ÂƒPÂ„AxÂ…whereAxÂƒf!2â„¦:XÂ„!Â…Âƒxg (2.9)
We will write pmfs with a lowercase roman letter (even when they are vari-
ables). If a random variable Xis distributed according to the pmf p Â„xÂ…,
then we will write XpÂ„xÂ….
Note that pÂ„xÂ…> 0 at only a countable number of points (to satisfy the
stochastic constraint on probabilities), say fxi:i2Ng,w h i l epÂ„xÂ…Âƒ0
elsewhere. For a discrete random variable, we have that:
X
ipÂ„xiÂ…ÂƒX
iPÂ„AxiÂ…ÂƒPÂ„â„¦Â…Âƒ1
Conversely, any function satisfying these constraints can be regarded as
a mass function.
Random variables are used throughout the introduction to information
theory in section 2.2.
2.1.5 Expectation and variance
Theexpectation is the mean or average of a random variable. expectation
meanIfXis a random variable with a pmf p Â„xÂ…such thatP
xjxjpÂ„xÂ…<1
then the expectation is:
EÂ„XÂ…ÂƒX
xxpÂ„xÂ… (2.10)
Example 4: If rolling one die and Yis the value on its face, then:
EÂ„YÂ…Âƒ6X
yÂƒ1xpÂ„yÂ…Âƒ1
66X
yÂƒ1yÂƒ21
6Âƒ31
2
This is the expected average found by totaling up a large number of
throws of the die, and dividing by the number of throws.
IfYpÂ„yÂ…is a random variable, any function gÂ„YÂ… deï¬nes a new
random variable. If EÂ„gÂ„YÂ…Â… is deï¬ned, then:
EÂ„gÂ„YÂ…Â…ÂƒX
ygÂ„yÂ… pÂ„yÂ… (2.11)
For instance, by letting gbe a linear function gÂ„YÂ…ÂƒaYÂ‚b, we see that
EÂ„gÂ„YÂ…Â…ÂƒaEÂ„YÂ…Â‚b.W ea l s oh a v et h a t EÂ„XÂ‚YÂ…ÂƒEÂ„XÂ…Â‚EÂ„YÂ… and ifX
andYare independent, then EÂ„XYÂ…ÂƒEÂ„XÂ…EÂ„YÂ… .

p/CX /CX2.1 Elementary Probability Theory 47
Thevariance of a random variable is a measure of whether the values variance
of the random variable tend to be consistent over trials or to vary a lot.
One measures it by ï¬nding out how much on average the variableâ€™s valuesdeviate from the variableâ€™s expectation:
VarÂ„XÂ…ÂƒE(
Â„Xâˆ’EÂ„XÂ…Â…
2
(2.12)
ÂƒEÂ„X2Â…âˆ’E2Â„XÂ…
The commonly used standard deviation of a variable is the square root of standard deviation
the variance. When talking about a particular distribution or set of data,
the mean is commonly denoted as , the variance as 2, and the standard
deviation is hence written as .
Example 5: What is the expectation and variance for the random vari-
able introduced in example 3, the sum of the numbers on two dice?
Solution: For the expectation, we can use the result in example 4, and
the formula for combining expectations in (or below) equation (2.11):
EÂ„XÂ…ÂƒEÂ„YÂ‚YÂ…ÂƒEÂ„YÂ…Â‚EÂ„YÂ…Âƒ31
2Â‚31
2Âƒ7
The variance is given by:
VarÂ„XÂ…ÂƒE(
Â„Xâˆ’EÂ„XÂ…Â…2
ÂƒX
xpÂ„xÂ…(
xâˆ’EÂ„XÂ…2Âƒ55
6
Because the results for rolling two dice are concentrated around 7, the
variance of this distribution is less than for an â€˜11-sided die,â€™ which re-
turns a uniform distribution over the numbers 2â€“12. For such a uniformlydistributed random variable U, we ï¬nd that Var Â„UÂ…Âƒ10.
Calculating expectations is central to Information Theory, as we will
see in section 2.2. Variances are used in section 5.2.
2.1.6 Notation
In these sections, we have distinguished between Pas a probability func-
tion and p as the probability mass function of a random variable. How-ever, the notations PÂ„Â…and pÂ„Â…do not always refer to the same function.
Any time that we are talking about a diï¬€erent probability space, then weare talking about a diï¬€erent function. Sometimes we will denote these

p/CX /CX48 2 Mathematical Foundations
diï¬€erent functions with subscripts on the function to make it clear what
we are talking about, but in general people just write Pand rely on con-
text and the names of the variables that are arguments to the function todisambiguate. It is important to realize that one equation is often refer-ring to several diï¬€erent probability functions, all ambiguously referred
to asP.
2.1.7 Joint and conditional distributions
Often we deï¬ne many random variables over a sample space giving us a
joint (or multivariate) probability distribution. The joint probability mass
function for two discrete random variables X,Yis:
pÂ„x;yÂ…ÂƒPÂ„XÂƒx;YÂƒyÂ…
Related to a joint pmf are marginal pmfs , which total up the probability marginal
distribution masses for the values of each variable separately:
pXÂ„xÂ…ÂƒX
ypÂ„x;yÂ… pYÂ„yÂ…ÂƒX
xpÂ„x;yÂ…
In general the marginal mass functions do not determine the joint mass
function. But if XandYare independent, then p Â„x;yÂ…ÂƒpXÂ„xÂ…pYÂ„yÂ….
For example, for the probability of getting two sixes from rolling twodice, since these events are independent, we can compute that:
pÂ„YÂƒ6;ZÂƒ6Â…ÂƒpÂ„YÂƒ6Â…pÂ„ZÂƒ6Â…Âƒ1
61
6Âƒ1
36
There are analogous results for joint distributions and probabilities for
the intersection of events. So we can deï¬ne a conditional pmf in terms of
the joint distribution:
pXjYÂ„xjyÂ…ÂƒpÂ„x;yÂ…
pYÂ„yÂ…forysuch that pYÂ„yÂ…> 0
and deduce a chain rule in terms of random variables, for instance:
pÂ„w;x;y;zÂ…ÂƒpÂ„wÂ…pÂ„xjwÂ…pÂ„yjw;xÂ… pÂ„zjw;x;yÂ…
2.1.8 Determining P
So far we have just been assuming a probability function Pand giving it
the obvious deï¬nition for simple examples with coins and dice. But what

p/CX /CX2.1 Elementary Probability Theory 49
do we do when dealing with language? What do we say about the proba-
bility of a sentence like The cow chewed its cud ? In general, for language
events, unlike dice, Pis unknown. This means we have to estimateP.W e estimation
do this by looking at evidence about what Pmust be like based on a sam-
ple of data. The proportion of times a certain outcome occurs is called
therelative frequency of the outcome. If CÂ„uÂ… is the number of times relative frequency
an outcomeuoccurs inNtrials thenCÂ„uÂ…
Nis the relative frequency of u.
The relative frequency is often denoted fu. Empirically, if one performs
a large number of trials, the relative frequency tends to stabilize aroundsome number. That this number exists provides a basis for letting uscalculate probability estimates.
Techniques for how this can be done are a major topic of this book, par-
ticularly covered in chapter 6. Common to most of these techniques is
to estimatePby assuming that some phenomenon in language is accept-
ably modeled by one of the well-known families of distributions (such asthe binomial or normal distribution), which have been widely studied instatistics. In particular a binomial distribution can sometimes be usedas an acceptable model of linguistic events. We introduce a couple offamilies of distributions in the next subsection. This is referred to as a
parametric approach and has a couple of advantages. It means we have
parametric
an explicit probabilistic model of the process by which the data was gen-
erated, and determining a particular probability distribution within thefamily only requires the speciï¬cation of a few parameters, since most ofthe nature of the curve is ï¬xed in advance. Since only a few parametersneed to be determined, the amount of training data required is not great,and one can calculate how much training data is suï¬ƒcient to make good
probability estimates.
But, some parts of language (such as the distributions of words in
newspaper articles in a particular topic category) are irregular enoughthat this approach can run into problems. For example, if we assumeour data is binomially distributed, but in fact the data looks nothing likea binomial distribution, then our probability estimates might be wildlywrong.
For such cases, one can use methods that make no assumptions about
the underlying distribution of the data, or will work reasonably well fora wide variety of diï¬€erent distributions. This is referred to as a non-
non-parametric
parametric ordistribution-free approach. If we simply empirically esti- distribution-free
matePby counting a large number of random events (giving us a discrete
distribution, though we might produce a continuous distribution from

p/CX /CX50 2 Mathematical Foundations
such data by interpolation, assuming only that the estimated probability
density function should be a fairly smooth curve), then this is a non-parametric method. However, empirical counts often need to be modiï¬edor smoothed to deal with the deï¬ciencies of our limited training data, atopic discussed in chapter 6. Such smoothing techniques usually assume
a certain underlying distribution, and so we are then back in the world of
parametric methods. The disadvantage of nonparametric methods is thatwe give our system less prior information about how the data are gener-ated, so a great deal of training data is usually needed to compensate forthis.
Non-parametric methods are used in automatic classiï¬cation when the
underlying distribution of the data is unknown. One such method, near-
est neighbor classiï¬cation, is introduced in section 16.4 for text catego-
rization.
2.1.9 Standard distributions
Certain probability mass functions crop up commonly in practice. Inparticular, one commonly ï¬nds the same basic form of a function, but
just with diï¬€erent constants employed. Statisticians have long studied
these families of functions. They refer to the family of functions as adistribution and to the numbers that deï¬ne the diï¬€erent members of the
distribution
family as parameters . Parameters are constants when one is talking about parameters
a particular pmf, but variables when one is looking at the family. When
writing out the arguments of a distribution, it is usual to separate therandom variable arguments from the parameters with a semicolon (;). In
this section, we just brieï¬‚y introduce the idea of distributions with one
example each of a discrete distribution (the binomial distribution), and acontinuous distribution (the normal distribution).
Discrete distributions: The binomial distribution
Abinomial distribution results when one has a series of trials with only
binomial
distribution two outcomes (i.e., Bernoulli trials), each trial being independent from all
the others. Repeatedly tossing a (possibly unfair) coin is the prototypicalexample of something with a binomial distribution. Now when looking atlinguistic corpora, it is never the case that the next sentence is truly inde-pendent of the previous one, so use of a binomial distribution is alwaysan approximation. Nevertheless, for many purposes, the dependency be-

p/CX /CX2.1 Elementary Probability Theory 51
tween words falls oï¬€ fairly quickly and we can assume independence. In
any situation where one is counting whether something is present or ab-sent, or has a certain property or not, and one is ignoring the possibilityof dependencies between one trial and the next, one is at least implic-itly using a binomial distribution, so this distribution actually crops up
quite commonly in Statistical
NLPapplications. Examples include: look-
ing through a corpus to ï¬nd an estimate of the percent of sentences inEnglish that have the word thein them or ï¬nding out how commonly
a verb is used transitively by looking through a corpus for instances of acertain verb and noting whether each use is transitive or not.
The family of binomial distributions gives the number rof successes
out ofntrials given that the probability of success in any trial is p:
bÂ„r;n;pÂ…Âƒ 
n
r!
p
rÂ„1âˆ’pÂ…nâˆ’rwhere 
n
r!
Âƒn!
Â„nâˆ’rÂ…!r!0rn (2.13)
The term
n
r
counts the number of diï¬€erent possibilities for choosing
robjects out of n, not considering the order in which they are chosen.
Examples of some binomial distributions are shown in ï¬gure 2.3. The bi-nomial distribution has an expectation of npand a variance of npÂ„1âˆ’pÂ….
Example 6: LetRhave as value the number of heads in ntosses of a
(possibly weighted) coin, where the probability of a head is p.
Then we have the binomial distribution:
pÂ„RÂƒrÂ…ÂƒbÂ„r;n;pÂ…
(The proof of this is by counting: each basic outcome with rheads and
nâˆ’rtails has probability h
rÂ„1âˆ’hÂ…nâˆ’r, and there are
n
r
of them.)
The binomial distribution turns up in various places in the book, such
as when counting n-grams in chapter 6, and for hypothesis testing in
section 8.2.
The generalization of a binomial trial to the case where each of the tri-
als has more than two basic outcomes is called a multinomial experiment,and is modeled by the multinomial distribution . A zeroth order n-gram
multinomial
distribution model of the type we discuss in chapter 6 is a straightforward example
of a multinomial distribution.
Another discrete distribution that we discuss and use in this book is the
Poisson distribution (section 15.3.1). Section 5.3 discusses the Bernoullidistribution, which is simply the special case of the binomial distributionwhere there is only one trial. That is, we calculate b Â„r;1;pÂ….

p/CX /CX52 2 Mathematical Foundations
Â·Â· Â·Â·Â·Â·Â·Â·
Â·
Â·
Â·
countprobability
024681 00.0 0.1 0.2 0.3 0.4 0.5Â·Â·
Â·
Â·
Â·Â·Â·Â· Â·Â·Â·
024681 00.0 0.1 0.2 0.3 0.4 0.5p = 0.7
p = 0.1
Figure 2.3 Two examples of binomial distributions: b Â„r;1 0;0:7Â…and
bÂ„r;1 0;0:1Â….
Continuous distributions: The normal distribution
So far we have looked only at discrete probability distributions and
discrete random variables, but many things, such as measurements ofheights and lengths, are best understood as having a continuous domain,
over the real numbers R. In this book, we do not outline the mathematics
of continuous distributions. Suï¬ƒce it to say that there are generally anal-ogous results, except with points becoming intervals, and sums becomingintegrals. However, we will occasionally have need to refer to continuousprobability distributions, so we will give one example here: the normaldistribution, which is central to all work in probability and statistics.
For many things in the world, such as the heights or IQs of people,
one gets a distribution that is known in the media as a bell curve ,b u t
bell curve
w h i c hi sr e f e r r e dt oi ns t a t i s t i c sa sa normal distribution . Some normal normal
distribution distribution curves are shown in ï¬gure 2.4. The values of the graphed
functions, probability density functions (pdf), do not directly give theprobabilities of the points along the x-axis (indeed, the probability of a
point is always 0 for a continuous distribution). Rather the probability

p/CX /CX2.1 Elementary Probability Theory 53
valuedensity
-4 -2 0 2 40.0 0.1 0.2 0.3 0.4
-4 -2 0 2 40.0 0.1 0.2 0.3 0.4N(0,1)
N(1.5,2)
Figure 2.4 Example normal distribution curves: n Â„x;0;1Â…and nÂ„x;1:5;2Â….
of a result within a certain interval on the x-axis is given by the area
delimited by that region, the x-axis and the function curve.
The normal distribution has two parameters for the mean , and the
standard deviation , and the curve is given by:
nÂ„x;;Â…Âƒ1p
2eâˆ’Â„xâˆ’Â…2=Â„22Â…(2.14)
The curve where Âƒ0a n dÂƒ1 is referred to as the standard normal standard normal
distribution distribution . A few ï¬gures for areas under this curve are given in the
appendix.
While it is much better to refer to such a curve as a â€˜normal distributionâ€™
than as a â€˜bell curve,â€™ if you really want to ï¬t into the Statistical NLPor
pattern recognition communities, you should instead learn to refer to
these functions as Gaussians , and to remark things like, â€˜Maybe we could Gaussians
model that using 3 Gaussiansâ€™ at appropriate moments.1
1. Carl Friedrich Gauss was the ï¬rst to use normal curves to model experimental data,
using them to model the errors made by astronomers and surveyors in repeated measure-ments of the same quantity, but the normal curve was discovered by Abraham de Moivre.

p/CX /CX54 2 Mathematical Foundations
In much of statistics, the discrete binomial distribution is approxi-
mated by the continuous normal distribution â€“ one can see the basicsimilarity in the shapes of the curves by comparing ï¬gures 2.3 and 2.4.Such an approximation is acceptable when both basic outcomes have areasonable probability of occurring or the amount of data is very large
(roughly, when npÂ„1âˆ’pÂ…> 5). But, in natural language, events like oc-
currences of the phrase shade tree mechanics are so rare, that even if you
have a huge amount of text, there will be a signiï¬cant diï¬€erence betweenthe appropriate binomial curve and the approximating normal curve, andso use of normal approximations can be unwise.
Gaussians are often used in clustering, as discussed in chapter 14. In
particular, here we have only discussed the one-dimensional or univariate
normal distribution, while we present there the generalization to many
dimensions (the multivariate normal distribution).
Other continuous distributions discussed in this book are the hyper-
bolic distributions discussed in section 1.4.3, and the tdistribution used
for hypothesis testing in section 5.3.
2.1.10 Bayesian statistics
So far, we have presented a brief introduction to orthodox frequentist frequentist
statistics statistics . Not everyone is agreed on the right philosophical foundations
for statistics, and the main rival is a Bayesian approach to statistics. Ac- Bayesian statistics
tually, the Bayesians even argue among themselves, but we are not going
to dwell on the philosophical issues here. We want to just brieï¬‚y intro-duce the Bayesian approach because Bayesian methods are very useful in
Statistical
NLP, and we will come across them in later chapters.
Bayesian updating
Suppose one takes a coin and tosses it 10 times, and gets 8 heads. Then
from a frequentist point of view, the result is that this coin comes downheads 8 times out of 10. This is what is called the maximum likelihood es-
maximum likelihood
estimate timate , as discussed further in section 6.2.1. However, if one has looked
the coin over, and there doesnâ€™t seem anything wrong with it, one wouldbe very reluctant to accept this estimate. Rather, one would tend to thinkthat the coin would come down equally head and tails over the long run,and getting 8 heads out of 10 is just the kind of thing that happens some-times given a small sample. In other words one has a prior belief that
prior belief

p/CX /CX2.1 Elementary Probability Theory 55
inï¬‚uences oneâ€™s beliefs even in the face of apparent evidence against it.
Bayesian statistics measure degrees of belief, and are calculated by start-ing with prior beliefs and updating them in the face of evidence, by useof Bayesâ€™ theorem.
For example, let 
mbe the model2that assertsPÂ„headÂ…Âƒm.L e tsbe
a particular sequence of observations yielding iheads andjtails. Then,
for anym,0m1:
PÂ„sjmÂ…ÂƒmiÂ„1âˆ’mÂ…j(2.15)
From a frequentist point of view, we wish to ï¬nd the MLE:
arg max
mPÂ„sjmÂ…
To do this, we can diï¬€erentiate the above polynomial, and ï¬nd its max-
imum, which fortunately gives the intuitive answer ofi
iÂ‚j,o r0:8 for the
case of 8 heads and 2 tails.
But now suppose that one wants to quantify oneâ€™s belief that the coin
is probably a regular, fair one. One can do that by assuming a prior
probability distribution over how likely it is that diï¬€erent models mare
true. Since one would want most of the probability mass close to1
2,o n e
might use something like a Gaussian distribution centered on1
2, but since
polynomials are the only things we can remember how to diï¬€erentiate, let
us instead assume that oneâ€™s prior belief is modeled by the distribution:
PÂ„mÂ…Âƒ6mÂ„1âˆ’mÂ… (2.16)
This polynomial was chosen because its distribution is centered on1
2,
and, conveniently, the area under the curve between 0 and 1 is 1.
When one sees an observation sequence sone wants to know oneâ€™s new
belief in the fairness of the coin. One can calculate this from (2.15) and(2.16) by Bayesâ€™ theorem:
PÂ„
mjsÂ…ÂƒPÂ„sjmÂ…PÂ„mÂ…
PÂ„sÂ…(2.17)
ÂƒmiÂ„1âˆ’mÂ…j6mÂ„1âˆ’mÂ…
PÂ„sÂ…
2. By a model we mean whatever theoretical ediï¬ces we construct to explain something
in the world. A probabilistic model might comprise the speciï¬cation of a distributionand certain parameter values. Thus, we are introducing some notational sloppiness inequation (2.15), since previously we were conditioning on an event, that is, a subset of theevent space, and now we are conditioning on a model, but we will allow ourselves thatfreedom.

p/CX /CX56 2 Mathematical Foundations
Âƒ6miÂ‚1Â„1âˆ’mÂ…jÂ‚1
PÂ„sÂ…
NowPÂ„sÂ… is the prior probability of s. Let us assume for the moment
that it does not depend on m, and therefore that we can ignore it while
ï¬nding themthat maximizes this equation. If we then diï¬€erentiate the
numerator so as ï¬nd its maximum, we can determine that for the case of
8 heads and 2 tails:
arg max
mPÂ„mjsÂ…Âƒ3
4
Because our prior was weak (the polynomial is a quite ï¬‚at curve centered
over1
2), we have moved a long way in the direction of believing that the
coin is biased, but the important point is that we havenâ€™t moved all theway to 0:8. If we had assumed a stronger prior, we would have moved a
smaller distance from
1
2. (See exercise 2.8.)
But what do we make of the denominator PÂ„sÂ…? W e l l ,s i n c ew eh a v e
just seens, one might conclude that this is 1, but that is not what it
means. Rather, it is the marginal probability which is obtained by adding marginal
probability up all thePÂ„sjmÂ…weighted by the probability of m, as we saw earlier in
equation (2.8). For the continuous case, we have the integral:
PÂ„sÂ…ÂƒZ1
0PÂ„sjmÂ…PÂ„mÂ…dm (2.18)
ÂƒZ1
06miÂ‚1Â„1âˆ’mÂ…jÂ‚1dm
This just happens to be an instance of the beta integral, another contin-
uous distribution well-studied by statisticians, and so we can look up a
book to ï¬nd out that:
PÂ„sÂ…Âƒ6Â„iÂ‚1Â…!Â„jÂ‚1Â…!
Â„iÂ‚jÂ‚3Â…!(2.19)
But the important point is that the denominator is just a normalization normalization
factor factor , which ensures that what we calculate for PÂ„mjsÂ…in (2.17) is ac-
tually a probability function.
In the general case where data come in sequentially and we can reason-
ably assume independence between them, we start oï¬€ with an a priori
probability distribution, and when a new datum comes in, we can updateour beliefs by calculating the maximum of the a posteriori distribution,what is sometimes referred to as the
MAP probability. This then becomes
the new prior, and the process repeats on each new datum. This processis referred to as Bayesian updating .
Bayesian updating

p/CX /CX2.1 Elementary Probability Theory 57
Bayesian decision theory
But there is another thing that we can do with this new approach: use it
to evaluate which model or family of models better explains some data.Suppose that we did not actually see the sequence of coin tosses but justheard the results shouted out over the fence. Now it may be the case, aswe have assumed so far, that the results reported truly reï¬‚ect the resultsof tossing a single, possibly weighted coin. This is the theory ,w h i c hi s
a family of models, with a parameter representing the weighting of the
coin. But an alternative theory is that at each step someone is tossing
two fair coins, and calling out â€œtailsâ€ if both of them come down tails,
and heads otherwise. Let us call this new theory . According to ,i fsis
a particular observed sequence of iheads andjtails, then:
PÂ„sjÂ…Âƒ3
4i1
4j
(2.20)
Note that one of these theories has a free parameter (the weighting
of the coinm), while the other has no parameters. Let us assume that,
a priori, both of these theories are equally likely, for instance:
PÂ„Â…ÂƒPÂ„Â…Âƒ1
2(2.21)
We can now attempt to work out which theory is more likely given the
data we have seen. We use Bayesâ€™ theorem again, and write down:
PÂ„jsÂ…ÂƒPÂ„sjÂ…PÂ„Â…
PÂ„sÂ…PÂ„jsÂ…ÂƒPÂ„sjÂ…PÂ„Â…
PÂ„sÂ…
The potentially confusing point here is that we have made a quick
change in our notation. The quantity we are now describing as PÂ„sjÂ…
is the quantity that we wrote as just PÂ„sÂ… in (2.19) â€“ since at that time we
were assuming that theory mwas true and we were just trying to deter-
minem, whereas what we are now writing as PÂ„sÂ… is the prior probability
ofs, not knowing whether is true or not. With that gotten straight,
we can calculate the likelihood ratio between these two models. The PÂ„sÂ… likelihood ratio
terms in the denominators cancel, and we can work out the rest using
equations (2.19), (2.20), and (2.21):
PÂ„jsÂ…
PÂ„jsÂ…ÂƒPÂ„sjÂ…PÂ„Â…
PÂ„sjÂ…PÂ„Â…(2.22)
Âƒ6Â„iÂ‚1Â…!Â„jÂ‚1Â…!
Â„iÂ‚jÂ‚3Â…!(3
4i(1
4j

pa/CX /CX58 2 Mathematical Foundations
10 Results Reported 20 Results Reported
Heads Tails Likelihood ratio Heads Tails Likelihood ratio
01 0 4 :0310402 0 1 :301010
1 9 2444.23 2 18 2 :07107
2 8 244.42 4 16 1 :34105
3 7 36.21 6 14 2307.06
4 6 7.54 8 12 87.895 5 2.16 10 10 6.896 4 0.84 12 8 1.09
7 3 0.45 14 6 0.35
8 2 0.36 16 4 0.259 1 0.37 18 2 0.48
10 0 0.68 20 0 3.74
Table 2.1 Likelihood ratios between two theories. The left three columns are
for a sequence sof 10 pieces of data, and the right three columns for a sequence
of 20 pieces of data.
If this ratio is greater than 1, we should prefer , and otherwise we should
prefer(or commonly people take the log of this ratio and see if that
value is greater than or less than zero).
We can calculate this ratio for diï¬€erent combinations of heads and
tails. Table 2.1 shows likelihood values for sequences of 10 and 20 re-sults. If there are few heads, then the likelihood ratio is greater than one,and the possibly weighted coin theory wins, since it is never strongly in-
compatible with any data (because of its free parameter). On the other
hand, if the distribution is roughly what weâ€™d expect according to the twofair coins theory (a lot more heads than tails) then the likelihood ratio issmaller than one, and the simpler two fair coins theory wins. As thequantity of data available becomes greater, the ratio of heads needs tobe nearer
3
4in order for the two fair coins model to win. If these are the
only two theories under consideration, and we choose the one that wins
in such a likelihood ratio, then we have made what is called the Bayes Bayes optimal
decision optimal decision .
If there are more theories, we can compare them all and decide on the
most likely one in the same general manner. An example of this andmore general discussion of Bayesian decision theory can be found in ourdiscussion of word sense disambiguation in section 7.2.1.

pa/CX /CX2.1 Elementary Probability Theory 59
2.1.11 Exercises
Exercise 2.1 [Â«]
This exercise indicates the kind of facility with set theory needed for this book,
and summarizes a few useful results in probability theory. Use set theory andthe axioms deï¬ning a probability function to show that:
a.PÂ„A[BÂ…ÂƒPÂ„AÂ…Â‚PÂ„BÂ…âˆ’PÂ„A\BÂ…[the addition rule]
b.PÂ„;Â…Âƒ0
c.PÂ„
AÂ…Âƒ1âˆ’PÂ„AÂ…
d.AB)PÂ„AÂ…PÂ„BÂ…
e.PÂ„Bâˆ’AÂ…ÂƒPÂ„BÂ…âˆ’PÂ„A\BÂ…
Exercise 2.2 [Â«]
Assume the following sample space:
â„¦Âƒfis-noun;has-plural-s;is-adjective;is-verbg (2.23)
and the function f:2â„¦!Â†0;1Â‡with the following values:
x fÂ„xÂ…
{ is-noun } 0.45
{ has-plural-s } 0.2{ is-adjective } 0.25{ is-verb } 0.3
Canfbe extended to all of 2
â„¦such that it is a well-formed probability distribu-
tion? If not, how would you model these data probabilistically?
Exercise 2.3 [Â«]
Compute the probability of the event â€˜A period occurs after a three-letter word
and this period indicates an abbreviation (not an end-of-sentence marker),â€™ as-suming the following probabilities.
PÂ„is-abbreviation jthree-letter-word Â…Âƒ0:8 (2.24)
PÂ„three-letter-word Â…Âƒ0:0003 (2.25)
Exercise 2.4 [Â«]
AreXandYas deï¬ned in the following table independently distributed?
x 0011
y 0101
pÂ„XÂƒx;YÂƒyÂ… 0.32 0.08 0.48 0.12

p/CX /CX60 2 Mathematical Foundations
Exercise 2.5 [Â«]
In example 5, we worked out the expectation of the sum of two dice in terms
of the expectation of rolling one die. Show that one gets the same result if onecalculates the expectation for two dice directly.
Exercise 2.6 [Â«Â«]
Consider the set of grades you have received for courses taken in the last two
years. Convert them to an appropriate numerical scale. What is the appropriatedistribution for modeling them?
Exercise 2.7 [Â«Â«]
Find a linguistic phenomenon that the binomial distribution is a good model for.
What is your best estimate for the parameter p?
Exercise 2.8 [Â«Â«]
ForiÂƒ8 andjÂƒ2, conï¬rm that the maximum of equation (2.15) is at 0.8,
and that the maximum of equation (2.17) is 0.75. Suppose our prior belief hadinstead been captured by the equation:
PÂ„
mÂ…Âƒ30m2Â„1âˆ’mÂ…2
What then would the MAP probability be after seeing a particular sequence of 8
heads and 2 tails? (Assume the theory mand a prior belief that the coin is fair.)
2.2 Essential Information Theory
The ï¬eld of information theory was developed in the 1940s by Claude
Shannon, with the initial exposition reported in (Shannon 1948). Shannonwas interested in the problem of maximizing the amount of information
that you can transmit over an imperfect communication channel such as
a noisy phone line (though actually many of his concerns stemmed fromcodebreaking in World War II). For any source of â€˜informationâ€™ and anyâ€˜communication channel,â€™ Shannon wanted to be able to determine theo-retical maxima for (i) data compression â€“ which turns out to be given bythe EntropyH(or more fundamentally, by the Kolmogorov complexity K),
and (ii) the transmission rate â€“ which is given by the Channel Capac-
ityC. Until Shannon, people had assumed that necessarily, if you send
your message at a higher speed, then more errors must occur during thetransmission. But Shannon showed that providing that you transmit theinformation in your message at a slower rate than the Channel Capacity,then you can make the probability of errors in the transmission of yourmessage as small as you would like.

p/CX /CX2.2 Essential Information Theory 61
2.2.1 Entropy
Let pÂ„xÂ…be the probability mass function of a random variable X, over a
discrete set of symbols (or alphabet )X: alphabet
pÂ„xÂ…ÂƒPÂ„XÂƒxÂ…; x2X
For example, if we toss two coins and count the number of heads, we
have a random variable: p Â„0Â…Âƒ1=4, pÂ„1Â…Âƒ1=2, pÂ„2Â…Âƒ1=4.
Theentropy (orself-information ) is the average uncertainty of a single entropy
self-informationrandom variable:
EntropyHÂ„pÂ…ÂƒHÂ„XÂ…Âƒâˆ’X
x2XpÂ„xÂ…log2pÂ„xÂ… (2.26)
Entropy measures the amount of information in a random variable. It is
normally measured in bits (hence the log to the base 2), but using anyother base yields only a linear scaling of results. For the rest of this
book, an unadorned log should be read as log to the base 2. Also, for this
deï¬nition to make sense, we deï¬ne 0 log 0 Âƒ0.
Example 7: Suppose you are reporting the result of rolling an 8-sided
die. Then the entropy is:
HÂ„XÂ…Âƒâˆ’ 8X
iÂƒ1pÂ„iÂ…log pÂ„iÂ…Âƒâˆ’8X
iÂƒ11
8log1
8Âƒâˆ’log1
8Âƒlog 8Âƒ3b i t s
This result is what we would expect. Entropy, the amount of information
in a random variable, can be thought of as the average length of themessage needed to transmit an outcome of that variable. If we wish tosend the result of rolling an eight-sided die, the most eï¬ƒcient way is tosimply encode the result as a 3 digit binary message:
12345678
001 010 011 100 101 110 111 000
The transmission cost of each result is 3 bits, and there is no cleverer way
of encoding the results with a lower average transmission cost. In gen-eral, an optimal code sends a message of probability p Â„iÂ…indâˆ’log pÂ„iÂ…e
bits.
The minus sign at the start of the formula for entropy can be moved
inside the logarithm, where it becomes a reciprocal:
HÂ„XÂ…ÂƒX
x2XpÂ„xÂ…log1
pÂ„xÂ…(2.27)

p/CX /CX62 2 Mathematical Foundations
People without any statistics background often think about a formula
like this as a sum of the quantity p Â„xÂ…logÂ„1=pÂ„xÂ…Â… for eachx. While this
is mathematically impeccable, it is the wrong way to think about suchequations. Rather you should think ofP
x2XpÂ„xÂ…::: as an idiom. It says
to take a weighted average of the rest of the formula (which will be a
function ofx), where the weighting depends on the probability of each x.
Technically, this idiom deï¬nes an expectation , as we saw earlier. Indeed,
HÂ„XÂ…ÂƒE 
log1
pÂ„XÂ…!
(2.28)
Example 8: Simpliï¬ed Polynesian Simpliï¬ed Polynesian3appears to be
just a random sequence of letters, with the letter frequencies as shown:
pt kai u
1/8 1/4 1/8 1/4 1/8 1/8
Then the per-letter entropy is:
HÂ„PÂ…Âƒâˆ’X
i2fp;t;k;a;i;ugPÂ„iÂ…logPÂ„iÂ…
Âƒâˆ’
41
8log1
8Â‚21
4log1
4
Âƒ21
2bits
This is supported by the fact that we can design a code that on average
takes 21
2bits to transmit a letter:
pt ka i u
100 00 101 01 110 111
Note that this code has been designed so that fewer bits are used to send
more frequent letters, but still so that it can be unambiguously decoded
â€“ if a code starts with a 0 then it is of length two, and if it starts with a 1it is of length 3. There is much work in information theory on the designof such codes, but we will not further discuss them here.
One can also think of entropy in terms of the Twenty Questions game.
Twenty Questions
If you can ask yes/no questions like â€˜Is it a tor an a?â€™ or â€˜Is it a conso-
nant?â€™ then on average you will need to ask 21
2questions to identify each
letter with total certainty (assuming that you ask good questions!). In
3. Polynesian languages, such as Hawaiâ€™ian, are well known for their small alphabets.

p/CX /CX2.2 Essential Information Theory 63
00.20.40.60.81
00.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1H(p)
Figure 2.5 The entropy of a weighted coin. The horizontal axis shows the prob-
ability of a weighted coin to come up heads. The vertical axis shows the entropyof tossing the corresponding coin once.
other words, entropy can be interpreted as a measure of the size of the
â€˜search spaceâ€™ consisting of the possible values of a random variable andits associated probabilities.
Note that: (i) HÂ„XÂ…0, (ii)HÂ„XÂ…Âƒ0 only when the value of Xis
determinate, hence providing no new information, and that (iii) entropy
increases with the message length. The information needed to transmit
the results of tossing a possibly weighted coin depends on the probabilitypthat it comes up heads, and on the number of tosses made. The entropy
for a single toss is shown in ï¬gure 2.5. For multiple tosses, since eachis independent, we would just multiply the number in the graph by thenumber of tosses.
2.2.2 Joint entropy and conditional entropy
The joint entropy of a pair of discrete random variables X;YpÂ„x;yÂ…
is the amount of information needed on average to specify both theirvalues. It is deï¬ned as:
HÂ„X;YÂ…Âƒâˆ’X
x2XX
y2YpÂ„x;yÂ… log pÂ„X;YÂ… (2.29)

p/CX /CX64 2 Mathematical Foundations
The conditional entropy of a discrete random variable Ygiven an-
otherX,f o rX;YpÂ„x;yÂ… , expresses how much extra information you
still need to supply on average to communicate Ygiven that the other
party knowsX:
HÂ„YjXÂ…ÂƒX
x2XpÂ„xÂ…HÂ„YjXÂƒxÂ… (2.30)
ÂƒX
x2XpÂ„xÂ…2
4âˆ’X
y2YpÂ„yjxÂ…log pÂ„yjxÂ…3
5
Âƒâˆ’X
x2XX
y2YpÂ„x;yÂ… log pÂ„yjxÂ…
There is also a Chain rule for entropy:
HÂ„X;YÂ…ÂƒHÂ„XÂ…Â‚HÂ„YjXÂ… (2.31)
HÂ„X 1;:::;XnÂ…ÂƒHÂ„X 1Â…Â‚HÂ„X 2jX1Â…Â‚:::Â‚HÂ„XnjX1;:::;Xnâˆ’1Â…
The products in the chain rules for probabilities here become sums be-
cause of the log:
HÂ„X;YÂ…Âƒâˆ’EpÂ„x;yÂ…(
log pÂ„x;yÂ…
Âƒâˆ’EpÂ„x;yÂ…(
logÂ„pÂ„xÂ…pÂ„yjxÂ…Â…
Âƒâˆ’EpÂ„x;yÂ…(
log pÂ„xÂ…Â‚log pÂ„yjxÂ…
Âƒâˆ’EpÂ„xÂ…(
log pÂ„xÂ…
âˆ’EpÂ„x;yÂ…(
log pÂ„yjxÂ…
ÂƒHÂ„XÂ…Â‚HÂ„YjXÂ…
Example 9: Simpliï¬ed Polynesian revisited An important scientiï¬c
idea is the distinction between a model and reality. Simpliï¬ed Polyne-sian isnâ€™t a random variable, but we approximated it (or modeled it) asone. But now letâ€™s learn a bit more about the language. Further ï¬eldwork
has revealed that Simpliï¬ed Polynesian has syllable structure. Indeed, it
turns out that all words consist of sequences of CV (consonant-vowel)syllables. This suggests a better model in terms of two random variablesCfor the consonant of a syllable, and Vfor the vowel, whose joint dis-
tributionPÂ„C;VÂ… and marginal distributions PÂ„C;Â…andPÂ„;VÂ…are as
follows:

p/CX /CX2.2 Essential Information Theory 65
(2.32) ptk
a1
163
81
161
2
i1
163
1601
4
u03
161
161
4
1
83
41
8
Note that here the marginal probabilities are on a per-syllable basis,
and are therefore double the probabilities of the letters on a per-letterbasis, which would be:
(2.33) p t k a i u
1/16 3/8 1/16 1/4 1/8 1/8
We can work out the entropy of the joint distribution, in more than one
way. Let us use the chain rule:
4
HÂ„CÂ…Âƒ21
83Â‚3
4(
2âˆ’log 3
Âƒ9
4âˆ’3
4log 3 bits1:061 bits
HÂ„VjCÂ…ÂƒX
cÂƒp;t;kpÂ„CÂƒcÂ…HÂ„VjCÂƒcÂ…
Âƒ1
8H1
2;1
2;0
Â‚3
4H1
2;1
4;1
4
Â‚1
8H1
2;0;1
2
Âƒ21
81Â‚3
4h1
21Â‚21
42i
Âƒ1
4Â‚3
43
2
Âƒ11
8bitsÂƒ1:375 bits
HÂ„C;VÂ…ÂƒHÂ„CÂ…Â‚HÂ„VjCÂ…
Âƒ9
4âˆ’3
4log 3Â‚11
8
Âƒ29
8âˆ’3
4log 32:44 bits
4. Within the calculation, we use an informal, but convenient, notation of expressing
a ï¬nite-valued distribution as a sequence of probabilities, which we can calculate theentropy of.

p/CX /CX66 2 Mathematical Foundations
Note that those 2 :44 bits are now the entropy for a whole syllable (which
was 221
2Âƒ5 for the original Simpliï¬ed Polynesian example). Our better
understanding of the language means that we are now much less uncer-tain, and hence less surprised by what we see on average than before.
Because the amount of information contained in a message depends on
the length of the message, we normally want to talk in terms of the per-
letter or per-word entropy. For a message of length n, the per-letter/word
entropy, also known as the entropy rate ,i s :
5entropy rate
HrateÂƒ1
nHÂ„X 1nÂ…Âƒâˆ’1
nX
x1npÂ„x1nÂ…log pÂ„x1nÂ… (2.34)
If we then assume that a language is a stochastic process consisting of
a sequence of tokens LÂƒÂ„XiÂ…, for example a transcription of every word
you utter in your life, or a corpus comprising everything that is sentdown the newswire to your local paper, then we can deï¬ne the entropyof a human language Las the entropy rate for that stochastic process:
H
rateÂ„LÂ…Âƒlim
n!11
nHÂ„X 1;X2;:::;XnÂ… (2.35)
We take the entropy rate of a language to be the limit of the entropy rate
of a sample of the language as the sample gets longer and longer.
2.2.3 Mutual information
By the chain rule for entropy,
HÂ„X;YÂ…ÂƒHÂ„XÂ…Â‚HÂ„YjXÂ…ÂƒHÂ„YÂ…Â‚HÂ„XjYÂ…
Therefore,
HÂ„XÂ…âˆ’HÂ„XjYÂ…ÂƒHÂ„YÂ…âˆ’HÂ„YjXÂ…
This diï¬€erence is called the mutual information between X and Y. It is the mutual
information reduction in uncertainty of one random variable due to knowing about
another, or in other words, the amount of information one random vari-able contains about another. A diagram illustrating the deï¬nition of mu-tual information and its relationship to entropy is shown in ï¬gure 2.6(adapted from Cover and Thomas (1991: 20)).
5. Commonly throughout this book we use two subscripts on something to indicate a sub-
sequence. So, here, we use Xijto represent the sequence of random variables Â„Xi;:::;XjÂ…
and similarly xijÂƒÂ„xi;:::;xjÂ…. This notation is slightly unusual, but very convenient
when sequences are a major part of the domain of discourse. So the reader should re-member this convention and be on the lookout for it.

p/CX /CX2.2 Essential Information Theory 67
IÂ„X;YÂ…HÂ„XjYÂ… HÂ„Y jXÂ…
HÂ„XÂ… HÂ„YÂ…HÂ„X;YÂ…
Figure 2.6 The relationship between mutual information Iand entropyH.
Mutual information is a symmetric, non-negative measure of the com-
mon information in the two variables. People thus often think of mutual
information as a measure of dependence between variables. However, itis actually better to think of it as a measure of independence because:
It is 0 only when two variables are independent, but
For two dependent variables, mutual information grows not only with
the degree of dependence, but also according to the entropy of thevariables.
Simple arithmetic gives us the following formulas for mutual informa-
tionIÂ„X;YÂ…:
6
IÂ„X;YÂ…ÂƒHÂ„XÂ…âˆ’HÂ„XjYÂ… (2.36)
ÂƒHÂ„XÂ…Â‚HÂ„YÂ…âˆ’HÂ„X;YÂ…
ÂƒX
xpÂ„xÂ…log1
pÂ„xÂ…Â‚X
ypÂ„yÂ…log1
pÂ„yÂ…Â‚X
x;ypÂ„x;yÂ… log pÂ„x;yÂ…
ÂƒX
x;ypÂ„x;yÂ… logpÂ„x;yÂ…
pÂ„xÂ…pÂ„yÂ…
SinceHÂ„XjXÂ…Âƒ0, note that:
HÂ„XÂ…ÂƒHÂ„XÂ…âˆ’HÂ„XjXÂ…ÂƒIÂ„X;XÂ…
6. Mutual information is conventionally written with a semi-colon separating the two ar-
guments. We are unsure why.

p/CX /CX68 2 Mathematical Foundations
This illustrates both why entropy is also called self-information, and how
the mutual information between two totally dependent variables is notconstant but depends on their entropy.
We can also derive conditional mutual information and a chain rule:
IÂ„X;YjZÂ…ÂƒIÂ„Â„X;YÂ…jZÂ…ÂƒHÂ„XjZÂ…âˆ’HÂ„XjY;ZÂ… (2.37)
IÂ„X
1n;YÂ…ÂƒIÂ„X 1;YÂ…Â‚:::Â‚IÂ„Xn;YjX1;:::;Xnâˆ’1Â… (2.38)
ÂƒnX
iÂƒ1IÂ„Xi;YjX1;:::;Xiâˆ’1Â…
In this section we have deï¬ned the mutual information between two
random variables. Sometimes people talk about the pointwise mutual pointwise mutual
information information between two particular points in those distributions:
IÂ„x;yÂ…ÂƒlogpÂ„x;yÂ…
pÂ„xÂ…pÂ„yÂ…
This has sometimes been used as a measure of association between ele-
ments, but there are problems with using this measure, as we will discussin section 5.4.
Mutual information has been used many times in Statistical
NLP,s u c h
as for clustering words (section 14.1.3). It also turns up in word sense
disambiguation (section 7.2.2).
2.2.4 The noisy channel model
Using information theory, Shannon modeled the goal of communicating
down a telephone line â€“ or in general across any channel â€“ in the follow-ing way: The aim is to optimize in terms of throughput and accuracy thecommunication of messages in the presence of noise in the channel. Itis assumed that the output of the channel depends probabilistically onthe input. In general, there is a duality between compression ,w h i c hi s
compression
achieved by removing all redundancy, and transmission accuracy, which
is achieved by adding controlled redundancy so that the input can be redundancy
recovered even in the presence of noise. The goal is to encode the mes-
sage in such a way that it occupies minimal space while still containingenough redundancy to be able to detect and correct errors. On receipt,the message is then decoded to give what was most likely the originalmessage. This process is shown in ï¬gure 2.7.

p/CX /CX2.2 Essential Information Theory 69
EncoderChannel
pÂ„yjxÂ…DecoderW
Message
from a ï¬nite
alphabetX
Input to
channelY
Output from
channelË†W
Attempt to
reconstruct message
based on output
Figure 2.7 The noisy channel model.
00
111âˆ’p
p
1âˆ’pp
Figure 2.8 A binary symmetric channel. A 1 or a 0 in the input gets ï¬‚ipped on
transmission with probability p.
The central concept that characterizes a channel in information theory
is its capacity . The channel capacity describes the rate at which one can capacity
transmit information through the channel with an arbitrarily low proba-
bility of being unable to recover the input from the output. For a memory-less channel, Shannonâ€™s second theorem states that the channel capacitycan be determined in terms of mutual information as follows:
CÂƒmax
pÂ„XÂ…IÂ„X;YÂ… (2.39)
According to this deï¬nition, we reach a channelâ€™s capacity if we man-
age to design an input code Xwhose distribution maximizes the mutual
information between the input and the output over all possible inputdistributions p Â„XÂ….
As an example, consider the binary symmetric channel in ï¬gure 2.8.
Each input symbol is either a 1 or a 0, and noise in the channel causeseach symbol to be ï¬‚ipped in the output with probability p. We ï¬nd that:
IÂ„X;YÂ…ÂƒHÂ„YÂ…âˆ’HÂ„YjXÂ…
ÂƒHÂ„YÂ…âˆ’HÂ„pÂ…
Therefore,max
pÂ„XÂ…IÂ„X;YÂ…Âƒ1âˆ’HÂ„pÂ…

p/CX /CX70 2 Mathematical Foundations
Noisy Channel
pÂ„ojiÂ…DecoderI O Ë†I
Figure 2.9 The noisy channel model in linguistics.
This last line follows because the mutual information is maximized by
maximizing the entropy in the codes, which is done by making the inputand hence the output distribution uniform, so their entropy is 1 bit. Sinceentropy is non-negative, C1. The channel capacity is 1 bit only if the
entropy is zero, that is if pÂƒ0 and the channel reliably transmits a 0 as
0 and a 1 as 1, or if pÂƒ1 and it always ï¬‚ips bits. A completely noisy
binary channel which transmits both 0s and 1s with equal probability as0s and 1s (i.e., pÂƒ
1
2) has capacity CÂƒ0, since in this case there is
no mutual information between XandY. Such a channel is useless for
communication.
It was one of the early triumphs of information theory that Shannon
was able to show two important properties of channels. First, channel
capacity is a well-deï¬ned notion. In other words, for each channel there
is a smallest upper bound of IÂ„X;YÂ…over possible distributions p Â„XÂ….
Second, in many practical applications it is easy to get close to the opti-mal channel capacity. We can design a code appropriate for the channelthat will transmit information at a rate that is optimal or very close to op-timal. The concept of capacity eliminates a good part of the guessworkthat was involved in designing communications systems before Shannon.
One can precisely evaluate how good a code is for a communication line
and design systems with optimal or near-optimal performance.
The noisy channel model is important in Statistical
NLPbecause a sim-
pliï¬ed version of it was at the heart of the renaissance of quantitativenatural language processing in the 1970s. In the ï¬rst large quantitativeproject after the early quantitative
NLPwork in the 1950s and 60s, re-
searchers at IBMâ€™s T. J. Watson research center cast both speech recogni-
tion and machine translation as a noisy channel problem.
Doing linguistics via the noisy channel model, we do not get to con-
trol the encoding phase. We simply want to decode the output to givethe most likely input, and so we work with the channel shown in ï¬g-ure 2.9. Many problems in
NLPcan be construed as an attempt to de-
termine the most likely input given a certain output. We can determine

p/CX /CX2.2 Essential Information Theory 71
Application Input Output p Â„iÂ… pÂ„ojiÂ…
Machine L1wordL2word pÂ„L1Â…in a translation
Translation sequences sequences language model model
Optical Character actual text text with prob of model of
Recognition ( OCR) mistakes language text OCR errors
Part Of Speech POStag English prob of p Â„wjtÂ…
(POS) tagging sequences words POSsequences
Speech word speech prob of word acoustic
recognition sequences signal sequences model
Table 2.2 Statistical NLPproblems as decoding problems.
this as follows, by using Bayesâ€™ theorem, and then noting that the output
probability is a constant:
Ë†IÂƒarg max
ipÂ„ijoÂ…Âƒarg max
ipÂ„iÂ…pÂ„ojiÂ…
pÂ„oÂ…Âƒarg max
ipÂ„iÂ…pÂ„ojiÂ… (2.40)
Here we have two probability distributions to consider: p Â„iÂ…is the lan- language model
guage model , the distribution of sequences of â€˜wordsâ€™ in the input lan-
guage, and pÂ„ojiÂ…is the channel probability . channel
probability As an example, suppose we want to translate a text from English to
French. The noisy channel model for translation assumes that the true
text is in French, but that, unfortunately, when it was transmitted to us,it went through a noisy communication channel and came out as English.So the word cowwe see in the text was really vache , garbled by the noisy
channel to cow. All we need to do in order to translate is to recover the
original French â€“ or to decode the English to get the French.
7decode
The validity of the noisy channel model for translation is still giving
rise to many a heated debate among NLP researchers, but there is no
doubt that it is an elegant mathematical framework that has inspired asigniï¬cant amount of important research. We will discuss the model inmore detail in chapter 13. Other problems in Statistical
NLPcan also be
seen as instantiations of the decoding problem. A selection is shown intable 2.2.
7. The French reader may be sympathetic with the view that English is really a form of
garbled French that makes the language of clartÃ© unnecessarily ambiguous!

p/CX /CX72 2 Mathematical Foundations
2.2.5 Relative entropy or Kullback-Leibler divergence
For two probability mass functions, p Â„xÂ…,qÂ„xÂ…their relative entropy is relative entropy
given by:
DÂ„pkqÂ…ÂƒX
x2XpÂ„xÂ…logpÂ„xÂ…
qÂ„xÂ…(2.41)
where again we deï¬ne 0 log0
qÂƒ0 and otherwise p logp
0Âƒ1. The relative
entropy, also known as the Kullback-Leibler divergence ,i sam e a s u r eo f Kullback-Leibler
divergence how diï¬€erent two probability distributions (over the same event space)
are. Expressed as an expectation, we have:
DÂ„pkqÂ…ÂƒEp 
logpÂ„XÂ…
qÂ„XÂ…!
(2.42)
Thus, the KL divergence between p and q is the average number of bits
that are wasted by encoding events from a distribution p with a code
based on a not-quite-right distribution q.
This quantity is always non-negative, and DÂ„pkqÂ…Âƒ0i ï¬€pÂƒq. For
these reasons, some authors use the name â€˜KL distance,â€™ but note thatrelative entropy is not a metric (in the sense in which the term is usedin mathematics): it is not symmetric in p and q (see exercise 2.12), andit does not satisfy the triangle inequality .
8Hence we will use the name triangle inequality
â€˜KL divergence,â€™ but nevertheless, informally, people often think about
the relative entropy as the â€˜distanceâ€™ between two probability distribu-tions: it gives us a measure of how close two pmfs are.
Mutual information is actually just a measure of how far a joint distri-
bution is from independence:
IÂ„X;YÂ…ÂƒDÂ„pÂ„x;yÂ…kpÂ„xÂ…pÂ„yÂ…Â… (2.43)
We can also derive conditional relative entropy and a chain rule for
relative entropy (Cover and Thomas 1991: 23):
D(
pÂ„yjxÂ…kqÂ„yjxÂ…
ÂƒX
xpÂ„xÂ…X
ypÂ„yjxÂ…logpÂ„yjxÂ…
qÂ„yjxÂ…(2.44)
D(
pÂ„x;yÂ…kqÂ„x;yÂ…
ÂƒD(
pÂ„xÂ…kqÂ„xÂ…
Â‚D(
pÂ„yjxÂ…kqÂ„yjxÂ…
(2.45)
8. The triangle inequality is that for any three points x;y;z :
dÂ„x;yÂ…dÂ„x;zÂ…Â‚dÂ„z;yÂ…

p/CX /CX2.2 Essential Information Theory 73
KL divergence is used for measuring selectional preferences in sec-
tion 8.4.
2.2.6 The relation to language: Cross entropy
So far we have examined the notion of entropy, and seen roughly how itis a guide to determining eï¬ƒcient codes for sending messages, but howdoes this relate to understanding language? The secret to this is to returnto the idea that entropy is a measure of our uncertainty. The more weknow about something, the lower the entropy will be because we are lesssurprised by the outcome of a trial.
We can illustrate this with the examples used above. Consider again
Simpliï¬ed Polynesian from examples 8 and 9. This language has 6 let-
ters. The simplest code is to use 3 bits for each letter of the language.This is equivalent to assuming that a good model of the language (whereour â€˜modelâ€™ is simply a probability distribution) is a uniform model. How-ever, we noticed that not all the letters occurred equally often, and, notingthese frequencies, produced a zeroth order model of the language. Thishad a lower entropy of 2 :5 bits per letter (and we showed how this obser-
vation could be used to produce a more eï¬ƒcient code for transmitting the
language). Thereafter, we noticed the syllable structure of the language,and developed an even better model that incorporated that syllable struc-ture into it. The resulting model had an even lower entropy of 1.22 bitsper letter. The essential point here is that if a model captures more of thestructure of a language, then the entropy of the model should be lower.In other words, we can use entropy as a measure of the quality of our
models.
Alternately, we can think of entropy as a matter of how surprised we
will be. Suppose that we are trying to predict the next word in a Sim-pliï¬ed Polynesian text. That is, we are examining PÂ„wjhÂ…,w h e r ewis
the next word and his the history of words seen so far. A measure of
oursurprise on seeing the next word can be derived in terms of the con-
surprise
ditional probability assigned to wby our model m of the distribution of
Simpliï¬ed Polynesian words. Surprise can be measured by what we might
term the pointwise entropy HÂ„wjhÂ…Âƒâˆ’log2mÂ„wjhÂ…. If the predictor is pointwise entropy
certain that word wfollows a given history hand it is correct, then the in-
formation supplied to the predictor on seeing wisâˆ’log21Âƒ0. In other
words, the predictor does not experience any surprise at all. On the otherhand, if the model thinks that wcannot follow h, then mÂ„wjhÂ…Âƒ0a n d

p/CX /CX74 2 Mathematical Foundations
so the information supplied to the predictor is inï¬nite ( âˆ’log20Âƒ1). In
this case our model is inï¬nitely surprised, which is normally a very badthing. Usually our models will predict a probability between these twoextremes for each event and so the model will gain some information, oralternatively, be somewhat surprised, when it sees the next word, and the
goal is to keep that level of surprise as low as possible. Summing over the
surprise of the predictor at each word gives an expression for our totalsurprise:
H
totalÂƒâˆ’nX
jÂƒ1log2mÂ„wjjw1;w2;:::;wjâˆ’1Â…
Âƒâˆ’ log2mÂ„w1;w2;:::;wnÂ…
The second line above follows from the chain rule. Normally, we would
want to normalize this measure by the length of the text so our notionof surprise is not dependent on the size of the text. This normalizedmeasure gives the average surprise of the predictor per word.
So far this discussion has been rather informal, but we can formalize
it through the notion of relative entropy. Suppose that we have some
empirical phenomenon, in Statistical
NLPusually utterances in a certain
language. Assuming some mapping to numbers, we can represent it viaa random variable X. Then we assume that there is some probability
distribution over the utterances â€“ for instance, you hear Thank you much
more often than On you .S ow et a k eXpÂ„xÂ….
Now, unfortunately we do not know what p Â„Â…is for empirical phenom-
ena. But by looking at instances, for example by looking at a corpus of
utterances, we can estimate roughly what p seems to be like. In other
words, we can produce a model m of the real distribution, based on ourbest estimates. In making this model, what we want to do is to mini-mizeDÂ„pkmÂ…â€“ to have as accurate a probabilistic model as possible.
Unfortunately, we normally cannot calculate this relative entropy â€“ again,because we do not know what p is. However, there is a related quantity,the cross entropy, which we fortunately can get a handle on.
Thecross entropy between a random variable Xwith true probability
cross entropy
distribution p Â„xÂ…and another pmf q (normally a model of p) is given by:
HÂ„X; qÂ…ÂƒHÂ„XÂ…Â‚DÂ„pkqÂ… (2.46)
Âƒâˆ’X
xpÂ„xÂ…log qÂ„xÂ…

p/CX /CX2.2 Essential Information Theory 75
ÂƒEp
log1
qÂ„xÂ…
(2.47)
(Proof of this is left to the reader as exercise 2.13.)
Just as we deï¬ned the entropy of a language in section 2.2.2, we can
deï¬ne the cross entropy of a language LÂƒÂ„XiÂ…pÂ„xÂ…according to a
model m by:
HÂ„L; mÂ…Âƒâˆ’lim
n!11
nX
x1npÂ„x1nÂ…log mÂ„x1nÂ… (2.48)
We do not seem to be making much progress, because it still seems that
we cannot calculate this quantity without knowing p. But if we makecertain assumptions that the language is â€˜nice,â€™ then the cross entropyfor the language can be calculated as:
HÂ„L; mÂ…Âƒâˆ’lim
n!11
nlog mÂ„x1nÂ… (2.49)
Using this second form, we can calculate the cross entropy based only
on knowing our probability model and having a large body of utterancesavailable. That is, we do not actually attempt to calculate the limit, butapproximate it by calculating for a suï¬ƒciently large n:
HÂ„L; mÂ…âˆ’1
nlog mÂ„x1nÂ… (2.50)
This measure is just the ï¬gure for our average surprise. Our goal will
be to try to minimize this number. Because HÂ„XÂ… is ï¬xed (if unknown),
this is equivalent to minimizing the relative entropy, which is a measure
of how much our probability distribution departs from actual language
use. The only additional requirement is that the text that we use to testthe model must be an independent test set, and not part of the trainingcorpus that we used to estimate the parameters of the model. Crossentropy is inversely related to the average probability a model assigns towords in test data. Lower model cross entropy normally leads to betterperformance in applications, but it need not do so if it is just a matter of
improving the magnitude of probability estimates, but not their relative
ordering. (See section 6.2.3 for more practical details on calculating thecross entropy of models.)
But what justiï¬es going from equation (2.48) to equation (2.49)? The
formula for language cross entropy has an expectation embedded withinit:
HÂ„L; mÂ…Âƒlim
n!11
nE
log1
mÂ„X1nÂ…
(2.51)

p/CX /CX76 2 Mathematical Foundations
Recall that the expectation is a weighted average over all possible se-
quences. But in the above formula we are using a limit and looking atlonger and longer sequences of language use. Intuitively, the idea is thenthat if we have seen a huge amount of the language, what we have seenis â€˜typical.â€™ We no longer need to average over all samples of the lan-
guage; the value for the entropy rate given by this particular sample will
be roughly right.
The formal version of this is to say that if we assume that LÂƒÂ„X
iÂ…is
a stationary ergodic process, then we can prove the above result. This isa consequence of the Shannon-McMillan-Breiman theorem, also known asthe Asymptotic Equipartition Property:
Theorem: IfH
rateis the entropy rate of a ï¬nite-valued stationary er-
godic process Â„XnÂ…, then:
âˆ’1
nlog pÂ„X1;:::;XnÂ…!H; with probability 1
We will not prove this theorem; see Cover and Thomas (1991: ch. 3, 15).
Anergodic process is one that, roughly, cannot get into diï¬€erent sub- ergodic
states that it will not escape from. An example of a non-ergodic process
is one that in the beginning chooses one of two states: one in which it
generates 0 forever, one in which it generates 1 forever. If a process is
not ergodic, then even looking at one very long sequence will not neces-sarily tell us what its typical behavior is (for example, what is likely tohappen when it gets restarted).
Astationary process is one that does not change over time. This is
stationary
clearly wrong for language: new expressions regularly enter the language
while others die out. And so, it is not exactly correct to use this result
to allow the calculation of a value for cross entropy for language applica-
tions. Nevertheless, for a snapshot of text from a certain period (such asone yearâ€™s newswire), we can assume that the language is near enough tounchanging, and so this is an acceptable approximation to truth. At anyrate, this is the method regularly used.
2.2.7 The entropy of English
As noted above, English in general is not a stationary ergodic process. Butwe can nevertheless model it with various stochastic approximations. Inparticular, we can model English with what are known as n-gram models
n-gram models

p/CX /CX2.2 Essential Information Theory 77
orMarkov chains . These models, which we discuss in detail in chapters 6 Markov chains
and 9, are ones where we assume a limited memory. We assume that the
probability of the next word depends only on the previous kwords in the
input. This gives a kthorder Markov approximation : Markov assumption
PÂ„XnÂƒxnjXnâˆ’1Âƒxnâˆ’1;:::;X 1Âƒx1Â…Âƒ
PÂ„XnÂƒxnjXnâˆ’1Âƒxnâˆ’1;:::;Xnâˆ’kÂƒxnâˆ’kÂ…
If we are working on a character basis, for example, we are trying to guess
what the next character in a text will be given the preceding kcharacters.
Because of the redundancy of English, this is normally fairly easy. Forinstance, a generation of students have proved this by being able to makedo with photocopies of articles that are missing the last character or twoof every line.
By adding up counts of letters, letter digraphs (that is, sequences of two
letters), and so on in English, one can produce upper bounds for the en-
tropy of English.
9We assume some such simpliï¬ed model of English and
compute its cross entropy against a text and this gives us an upper boundfor the true entropy of English â€“ since DÂ„pkmÂ…0,HÂ„X; mÂ…HÂ„XÂ… .
Shannon did this, assuming that English consisted of just 27 symbols(the 26 letters of the alphabet and space â€“ he ignored case distinctions
and punctuation). The estimates he derived were:
(2.52) Model Cross entropy (bits)
zeroth order 4.76 (uniform model, so log 27)
ï¬rst order 4.03second order 2.8Shannonâ€™s experiment 1.3 (1.34) (Cover and Thomas 1991: 140)
The ï¬rst three lines show that as the order of the model increases, that is,
as information about the frequencies of letters (ï¬rst order) and digraphs(second order) is used, our model of English improves and the calculatedcross entropy drops. Shannon wanted a tighter upper bound on the en-tropy of English, and derived one by human experiments â€“ ï¬nding out
how good at guessing the next letter in a text a human being was. This
gave a much lower entropy bound for English. (A later experiment with
9. More strictly, one produces an estimate for the text on which the counts are based, and
these counts are good for â€˜Englishâ€™ only to the extent that the text used is representativeof English as a whole. Working at the character level, this is not too severe a problem, butit becomes quite important when working at the word level, as discussed in chapter 4.

pa/CX /CX78 2 Mathematical Foundations
more subjects on the same text that Shannon used produced the ï¬gure
in parentheses, 1.34.)
Of course, the real entropy of English must be lower still: there are
doubtless patterns in peopleâ€™s speech that humans do not pick up on(although maybe not that many!). But at present, the statistical language
models that we can construct are much worse than human beings, and
so the current goal is to produce models that are as good as Englishspeakers at knowing which English utterances sound normal or commonand which sound abnormal or marked.
We return ton-gram models in chapter 6.
2.2.8 Perplexity
In the speech recognition community, people tend to refer to perplexity perplexity
rather than cross entropy. The relationship between the two is simple:
perplexityÂ„x1n;mÂ…Âƒ2HÂ„x 1n;mÂ…(2.53)
ÂƒmÂ„x1nÂ…âˆ’1
n (2.54)
We suspect that speech recognition people prefer to report the larger
non-logarithmic numbers given by perplexity mainly because it is mucheasier to impress funding bodies by saying that â€œweâ€™ve managed to re-duce perplexity from 950 to only 540â€ than by saying that â€œweâ€™ve reduced
cross entropy from 9.9 to 9.1 bits.â€ However, perplexity does also have
an intuitive reading: a perplexity of kmeans that you are as surprised
on average as you would have been if you had had to guess betweenkequiprobable choices at each step.
2.2.9 Exercises
Exercise 2.9 [Â«]
Take a (short) piece of text and compute the relative frequencies of the letters
in the text. Assume these are the true probabilities. What is the entropy of thisdistribution?
Exercise 2.10 [Â«]
Take another piece of text and compute a second probability distribution over
letters by the same method. What is the KL divergence between the two distribu-tions? (You will need to â€˜smoothâ€™ the second distribution and replace any zerowith a small quantity .)

p/CX /CX2.3 Further Reading 79
Exercise 2.11 [Â«]
Cast the problem of word sense disambiguation as a noisy channel model, in
analogy to the examples in table 2.2. Word sense disambiguation is the problemof determining which sense of an ambiguous word is used (e.g., â€˜industrial plantâ€™vs. â€˜living plantâ€™ for plant ) and will be covered in chapter 7.
Exercise 2.12 [Â«]
Show that the KL divergence is not symmetric by ï¬nding an example of two
distributions p and q for which DÂ„pkqÂ…Â”DÂ„qkpÂ….
Exercise 2.13 [Â«]
Prove the equality shown in the ï¬rst two lines of (2.46).
Exercise 2.14 [Â«]
We arrived at the simpliï¬ed way of computing cross entropy in equation (2.49)
under the premise that the process we are dealing with is ergodic and station-ary. List some characteristics of natural languages that show that these twoproperties are only approximately true of English.
Exercise 2.15 [Â«Â«]
Reproduce Shannonâ€™s experiment. Write a program that shows you a text one
letter at a time. Run it on a text you have not seen. Can you conï¬rm Shannonâ€™sestimate of the entropy of English?
Exercise 2.16 [Â«Â«]
Repeat the last exercise for one text that is â€˜easyâ€™ (e.g., a newsgroup posting) and
one text that is â€˜hardâ€™ (e.g., a scientiï¬c article from a ï¬eld you donâ€™t know well).Do you get diï¬€erent estimates? If the estimates are diï¬€erent, what diï¬ƒcultiesdoes the experiment raise for interpreting the diï¬€erent estimates of the entropyof English?
2.3 Further Reading
Aho et al. (1986: ch. 4) cover parsing in computer science, and Allen
(1995: ch. 3) covers parsing in computational linguistics. Most of themathematics we use is covered in Part I of (Cormen et al. 1990), but notvector spaces and matrices, for which one should consult an introduction
to linear algebra such as (Strang 1988).
Many books give good introductions to basic probability theory. A few
good ones, listed in approximate order of increasing diï¬ƒculty are (Mooreand McCabe 1989; Freedman et al. 1998; Siegel and Castellan 1988; De-Groot 1975). Krenn and Samuelsson (1997) is particularly recommendedas a much more thorough introduction to statistics aimed at a Statistical

p/CX /CX80 2 Mathematical Foundations
NLPaudience. Unfortunately most introduction to statistics textbooks
follow a very ï¬xed syllabus which is dominated by hypothesis testing asapplied in experimental sciences such as biology and psychology. Of-ten these concerns are rather distant from the issues of most relevanceto Statistical
NLP, and it can be helpful to also look at books covering
quantitative methods for machine learning, such as (Mitchell 1997).
The coverage of information theory here barely scratches the surface
of that ï¬eld. Cover and Thomas (1991) provide a thorough introduction.
Brown et al. (1992b) present an estimate of 1.75 bits per character for
the entropy of English based on predicting the next word, trained on anenormous corpus of English text.

