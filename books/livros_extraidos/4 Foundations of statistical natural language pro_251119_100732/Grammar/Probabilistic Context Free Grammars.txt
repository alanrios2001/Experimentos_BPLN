p/CX /CX11Probabilistic Context Free
Grammars
People write and say lots of diï¬€erent things, but the way people say
things â€“ even in drunken casual conversation â€“ has some structure andregularity. The goal of syntax within linguistics is to try to isolate that
structure. Until now, the only form of syntax we have allowed ourselves
is methods for describing the ordering and arrangement of words, eitherdirectly in terms of the words, or in terms of word categories. In thischapter, we wish to escape the linear tyranny of these n-gram models
and
HMM tagging models, and to start to explore more complex notions
of grammar.
Even in the most traditional forms of grammar, syntax is meant to
show something more that just linear order. It shows how words group
together and relate to each other as heads and dependents. The dominantmethod used for doing this within the last 50 or so years has been toplace tree structures over sentences, as we saw in chapter 3. Languagehas a complex recursive structure, and such tree-based models â€“ unlikeMarkov models â€“ allow us to capture this. For instance Kupiec (1992b)notes that his
HMM -based tagger has problems with constructions like:
(11.1) The velocity of the seismic waves rises to . . .
because a singular verb (here, rises) is unexpected after a plural noun.
Kupiecâ€™s solution is to augment the HMM model so it can recognize a
rudimentary amount of NP structure, which in his model is encoded asa higher-order context extension glued on to his basic ï¬rst order
HMM .
Leaving aside the technical details of the solution, the essential observa-tion about verb agreement is that it is reï¬‚ecting the hierarchical structureof the sentence, as shown in (11.2), and not the linear order of words.

p/CX /CX382 11 Probabilistic Context Free Grammars
(11.2) S
NPsg
DT
TheNN
velocityPP
IN
ofNPpl
the seismic wavesVPsg
rises to . . .
The verb agrees in number with the noun velocity which is the head of the
preceding noun phrase, and not with the noun that linearly precedes it.
The simplest probabilistic model for recursive embedding is a PCFG , PCFG
aProbabilistic (sometimes also called Stochastic )Context Free Grammar Probabilistic
Context Free
Grammarâ€“ which is simply a CFGwith probabilities added to the rules, indicating
how likely diï¬€erent rewritings are. We provide a detailed discussion of
PCFG s in this chapter for a number of reasons: PCFG s are the simplest
and most natural probabilistic model for tree structures, the mathemat-ics behind them is well understood, the algorithms for them are a naturaldevelopment of the algorithms employed with
HMM s, and PCFG sp r o -
vide a suï¬ƒciently general computational device that they can simulate
various other forms of probabilistic conditioning (as we describe in sec-tion 12.1.9). Nevertheless, it is important to realize that
PCFG s are only
one of many ways of building probabilistic models of syntactic structure,and in the next chapter we study the domain of probabilistic parsingmore generally.
A
PCFGGconsists of:
A set of terminals, fwkg;kÂƒ1;:::;V
A set of nonterminals, fNig;iÂƒ1;:::;n
A designated start symbol, N1
A set of rules, fNi!jg,( w h e r ejis a sequence of terminals and
nonterminals)
A corresponding set of probabilities on rules such that:
8iX
jPÂ„Ni!jÂ…Âƒ1 (11.3)

p/CX /CX383
Notation Meaning
G Grammar ( PCFG )
L Language (generated or accepted by a grammar)
t Parse tree
fN1;:::;NngNonterminal vocabulary ( N1is start symbol)
fw1;:::;wVgTerminal vocabulary
w1wm Sentence to be parsed
Nj
pq NonterminalNjspans positions pthroughqin string
jÂ„p;qÂ… Outside probabilities (11.15)
jÂ„p;qÂ… Inside probabilities (11.14)
Table 11.1 Notation for the PCFG chapter.
Note that when we write PÂ„Ni!jÂ…in this chapter, we always mean
PÂ„Ni!jjNiÂ…. That is, we are giving the probability distribution of the
daughters for a certain head. Such a grammar can be used either to parseor generate sentences of the language, and we will switch between theseterminologies quite freely.
Before parsing sentences with a
PCFG , we need to establish some no-
tation. We will represent the sentence to be parsed as a sequence of
wordsw1wm, and usewabto denote the subsequence wawb.W e
denote a single rewriting operation of the grammar by a single arrow !.
If as a result of one or more rewriting operations we are able to rewritea nonterminal N
jas a sequence of words wawb, then we will say
thatNjdominates the wordswawb, and write either Nj=)wawb domination
or yieldÂ„NjÂ…Âƒwawb. This situation is illustrated in (11.4): a sub-
tree with root nonterminal Njdominating all and only the words from
wawbin the string:
(11.4)Nj
wawb
To say that a nonterminal Njspans positions athroughbin the string,
but not to specify what words are actually contained in this subsequence,we will writeN
j
ab. This notation is summarized in table 11.1.
The probability of a sentence (according to a grammar G)i sg i v e nb y :
PÂ„w 1mÂ…ÂƒX
tPÂ„w 1m;tÂ… wheretis a parse tree of the sentence (11.5)

p/CX /CX384 11 Probabilistic Context Free Grammars
S!NP VP 1.0 NP !NP PP 0.4
PP!P NP 1.0 NP !astronomers 0.1
VP!V NP 0.7 NP !ears 0.18
VP!VP PP 0.3 NP !saw 0.04
P!with 1.0 NP !stars 0.18
V!saw 1.0 NP !telescopes 0.1
Table 11.2 A simple Probabilistic Context Free Grammar ( PCFG ). The nontermi-
nals are S, NP, PP, VP, P, V. We adopt the common convention whereby the startsymbolN
1is denoted by S. The terminals are the words in italics. The table
shows the grammar rules and their probabilities. The slightly unusual NP ruleshave been chosen so that this grammar is in Chomsky Normal Form, for use asan example later in the section.
ÂƒX
ft:yieldÂ„tÂ…Âƒw1mgPÂ„tÂ…
Moreover, it is easy to ï¬nd the probability of a tree in a PCFG model. One
just multiplies the probabilities of the rules that built its local subtrees.
Example 1: Assuming the grammar in table 11.2, the sentence as-
tronomers saw stars with ears has two parses with probabilities as shown
in ï¬gure 11.1.
What are the assumptions of this model? The conditions that we need
are:
Place invariance. The probability of a subtree does not depend onwhere in the string the words it dominates are (this is like time invari-ance in
HMM s):
8kP Â„ Nj
kÂ„kÂ‚cÂ…!Â…is the same (11.6)
Context-free. The probability of a subtree does not depend on words
not dominated by the subtree.
PÂ„Nj
kl!janything outside kthroughlÂ…ÂƒPÂ„Nj
kl!Â… (11.7)
Ancestor-free. The probability of a subtree does not depend on nodes
in the derivation outside the subtree.
PÂ„Nj
kl!jany ancestor nodes outside Nj
klÂ…ÂƒPÂ„Nj
kl!Â… (11.8)

p/CX /CX385
t1:S 1:0
NP0:1
astronomersVP0:7
V1:0
sawNP0:4
NP0:18
starsPP1:0
P1:0
withNP0:18
ears
t2:S 1:0
NP0:1
astronomersVP0:3
VP0:7
V1:0
sawNP0:18
starsPP1:0
P1:0
withNP0:18
ears
PÂ„t1Â…Âƒ1:00:10:71:00:40:181:01:00:18
Âƒ0:0009072
PÂ„t2Â…Âƒ1:00:10:30:71:00:181:01:00:18
Âƒ0:0006804
PÂ„w 15Â…ÂƒPÂ„t1Â…Â‚PÂ„t2Â…Âƒ0:0015876
Figure 11.1 The two parse trees, their probabilities, and the sentence probabil-
ity. This is for the sentence astronomers saw stars with ears , according to the
grammar in table 11.2. Nonterminal nodes in the trees have been subscriptedwith the probability of the local tree that they head.

p/CX /CX386 11 Probabilistic Context Free Grammars
Using these conditions we can justify the calculation of the probability of
a tree in terms of just multiplying probabilities attached to rules. But toshow an example, we need to be able to distinguish tokens of a nontermi-nal. Therefore, let the upper left index in
iNjbe an arbitrary identifying
index for a particular token of a nonterminal. Then,
P0
BBBB@1S
2NP
the man3VP
snores1
CCCCA
ÂƒPÂ„
1S13!2NP123VP33;2NP12!the1man 2;3VP33!snores 3Â…
ÂƒPÂ„1S13!2NP123VP33Â…PÂ„2NP12!the1man 2j1S13!2NP123VP33Â…
PÂ„3VP33!snores 3j1S13!2NP123VP33;2NP12!the1man 2Â…
ÂƒPÂ„1S13!2NP123VP33Â…PÂ„2NP12!the1man 2Â…PÂ„3VP33!snores 3Â…
ÂƒPÂ„S!NP VPÂ…PÂ„NP!the manÂ…PÂ„VP!snoresÂ…
where, after expanding the probability by the chain rule, we impose
ï¬rst the context-freeness assumption, and then the position-invariant as-
sumption.
11.1 Some Features of PCFG s
H e r ew eg i v es o m er e a s o n st ou s ea PCFG , and also some idea of their
limitations:
As grammars expand to give coverage of a large and diverse corpus oftext, the grammars become increasingly ambiguous. There start to bemany structurally diï¬€erent parses for most word sequences. A
PCFG
gives some idea of the plausibility of diï¬€erent parses.
APCFG does not give a very good idea of the plausibility of diï¬€erent
parses, since its probability estimates are based purely on structural
factors, and do not factor in lexical co-occurrence.
PCFG s are good for grammar induction . Gold (1967) showed that CFGs grammar induction
cannot be learned (in the sense of identiï¬cation in the limit â€“ that is, identiï¬cation in
the limit whether one can identify a grammar if one is allowed to see as much
data produced by the grammar as one wants) without the use of neg- negative evidence
ative evidence (the provision of ungrammatical examples). But PCFG s

p/CX /CX11.1 Some Features of PCFG s 387
can be learned from positive data alone (Horning 1969). (However, do-
ing grammar induction from scratch is still a diï¬ƒcult, largely unsolvedproblem, and hence much emphasis has been placed on learning frombracketed corpora, as we will see in chapter 12.)
Robustness. Real text tends to have grammatical mistakes, disï¬‚uen-cies, and errors. This problem can be avoided to some extent with a
PCFG by ruling out nothing in the grammar, but by just giving implau-
sible sentences a low probability.
PCFG s give a probabilistic language model for English (whereas a CFG
does not).
The predictive power of a PCFG as measured by entropy tends to be
greater than that for a ï¬nite state grammar (i.e., an HMM ) with the
same number of parameters. (For such comparisons, we compute thenumber of parameters as follows. A Vterminal,nnonterminal
PCFG
hasn3Â‚nVparameters, while a KstateMoutput HMM hasK2Â‚
MKparameters. While the exponent is higher in the PCFG case, the
number of nonterminals used is normally quite small. See Lari and
Young (1990) for a discussion of this with respect to certain artiï¬cial
grammars.)
In practice, a PCFG is a worse language model for English than an n-
gram model (for n> 1). Ann-gram model takes some local lexical
context into account, while a PCFG uses none.
PCFG s are not good models by themselves, but we could hope to com-
bine the strengths of a PCFG and a trigram model. An early experiment
that conditions the rules of a PCFG by word trigrams (and some addi-
tional context sensitive knowledge of the tree) is presented in Mager-man and Marcus (1991) and Magerman and Weir (1992). Better solu-tions are discussed in chapter 12.
PCFG s have certain biases, which may not be appropriate. All else be-
ing equal, in a PCFG , the probability of a smaller tree is greater than
a larger tree. This is not totally wild â€“ it is consonant with Frazierâ€™s(1978) Minimal Attachment heuristic â€“ but it does not give a sensiblemodel of actual sentences, which peak in frequency at some interme-diate length. For instance, table 4.3 showed that the most frequentlength for Wall Street Journal sentences is around 23 words. A
PCFG

p/CX /CX388 11 Probabilistic Context Free Grammars
gives too much of the probability mass to very short sentences. Sim-
ilarly, all else being equal, nonterminals with a small number of ex-pansions will be favored over nonterminals with many expansions in
PCFG parsing, since the individual rewritings will have much higher
probability (see exercise 12.3).
The one item here that deserves further comment is the claim that PCFG s
deï¬ne a language model. Initially, one might suspect that providing thatthe rules all obey equation (11.3), thenP
!2LPÂ„!Â…ÂƒP
tPÂ„tÂ…Âƒ1. But
actually this is only true if the probability mass of rules is accumulating probability mass of
rules in ï¬nite derivations. For instance, consider the grammar:
(11.9) S!rhubarbPÂƒ1
3
S!SSPÂƒ2
3
This grammar will generate all strings rhubarb . . . rhubarb . However, we
ï¬nd that the probability of those strings is:
(11.10) rhubarb1
3
rhubarb rhubarb2
31
31
3Âƒ2
27
rhubarb rhubarb rhubarb
2
32

1
33
2Âƒ8
243
...
The probability of the language is the sum of this inï¬nite series1
3Â‚2
27Â‚
8
243Â‚:::, which turns out to be1
2. Thus half the probability mass has
disappeared into inï¬nite trees which do not generate strings of the lan-guage! Such a distribution is often termed inconsistent in the probability
inconsistent
literature, but since this word has a rather diï¬€erent meaning in other
ï¬elds related to NLP, we will term such a distribution improper . In prac- improper
tice, improper distributions are not much of a problem. Often, it doesnâ€™t
really matter if probability distributions are improper, especially if we aremainly only comparing the magnitude of diï¬€erent probability estimates.Moreover, providing we estimate our
PCFG parameters from parsed train-
ing corpora (see chapter 12), Chi and Geman (1998) show that one always
gets a proper probability distribution.
11.2 Questions for PCFG s
Just as for HMM s, there are three basic questions we wish to answer:

p/CX /CX11.2 Questions for PCFG s 389
What is the probability of a sentence w1maccording to a grammar G:
PÂ„w 1mjGÂ…?
What is the most likely parse for a sentence: arg maxtPÂ„tjw1m;GÂ…?
How can we choose rule probabilities for the grammar Gthat maxi-
mize the probability of a sentence, arg maxGPÂ„w 1mjGÂ…?
In this chapter, we will only consider the case of Chomsky Normal Form Chomsky Normal
Form grammars, which only have unary and binary rules of the form:
Ni!NjNk
Ni!wj
The parameters of a PCFG in Chomsky Normal Form are:
(11.11)PÂ„Nj!NrNsjGÂ… Ifnnonterminals, an n3matrix of parameters
PÂ„Nj!wkjGÂ… IfVterminals,nVparameters
ForjÂƒ1;:::;n ,
X
r;sPÂ„Nj!NrNsÂ…Â‚X
kPÂ„Nj!wkÂ…Âƒ1 (11.12)
This constraint is seen to be satisï¬ed for the grammar in table 11.2 (un-
der the convention whereby all probabilities not shown are zero). Any
CFGcan be represented by a weakly equivalent CFGin Chomsky Normal
Form.1
To see how we might eï¬ƒciently compute probabilities for PCFG s, let
us work from HMM st o probabilistic regular grammars , and then from probabilistic
regular grammars there to PCFG s. Consider a probabilistic regular grammar ( PRG), which
has rules of the form:
Ni!wjNkorNi!wjand start state N1
This is similar to what we had for an HMM . The diï¬€erence is that in an
HMM there is a probability distribution over strings of a certain length:
8nX
w1nPÂ„w 1nÂ…Âƒ1
1. Two grammars G1andG2areweakly equivalent if they both generate the same lan-
guageL(with the same probabilities on sentences for stochastic equivalence). Two gram-
mars are strongly equivalent if they additionally assign sentences the same tree structures
(with the same probabilities, for the stochastic case).

p/CX /CX390 11 Probabilistic Context Free Grammars
Start HMM Finish
Figure 11.2 A Probabilistic Regular Grammar ( PRG).
whereas in a PCFG or a PRG, there is a probability distribution over the
set of all strings that are in the language Lgenerated by the grammar:
X
!2LPÂ„!Â…Âƒ1
To see the diï¬€erence, consider:
PÂ„John decided to bake a Â…
This would have a high probability in an HMM , since this is a quite likely
beginning to a sentence, but a very low probability in a PRG or a PCFG ,
because it isnâ€™t a complete utterance.
We can think of a PRGas related to an HMM roughly as in ï¬gure 11.2.
We add a start state and the transitions from it to the states of the HMM
mirror the initial probabilities . To represent ending the string, we ad-
join to the HMM a ï¬nish state, often called a sink state , which one never sink state
leaves once one has entered it. From each HMM state one can continue in
the basic HMM or shift to the sink state, which we interpret as the end of
string in the PRG.
This gives the basic idea of how PRGs are related to HMM s. We can
implement the PRGas an HMM where the states are nonterminals and the
terminals are the output symbols, as follows:
States: NP -! N0-! N0-! N0-!sink state
jj j j
Outputs: the big brown box
Recall how for an HMM we were able to eï¬ƒciently do calculations in terms
of forward and backward probabilities:
Forward probability iÂ„tÂ…ÂƒPÂ„w 1Â„tâˆ’1Â…;XtÂƒiÂ…
Backward probability iÂ„tÂ…ÂƒPÂ„wtTjXtÂƒiÂ…

p/CX /CX11.2 Questions for PCFG s 391
w1 wm wpâˆ’1wpwqwqÂ‚1N1
Nj
  

Figure 11.3 Inside and outside probabilities in PCFG s.
But now consider the PRGparse again, drawn as a tree:
(11.13) NP
the N0
big N0
brown N0
box
In the tree, the forward probability corresponds to the probability of ev-
erything above and including a certain node, while the backward proba-bility corresponds to the probability of everything below a certain node(given the node). This suggests an approach to dealing with the moregeneral case of
PCFG s. We introduce Inside and Outside probabilities, as
indicated in ï¬gure 11.3, and deï¬ned as follows:

p/CX /CX392 11 Probabilistic Context Free Grammars
Outside probability jÂ„p;qÂ…ÂƒPÂ„w 1Â„pâˆ’1Â…;Nj
pq;wÂ„qÂ‚1Â…mjGÂ… (11.14)
Inside probability jÂ„p;qÂ…ÂƒPÂ„wpqjNj
pq;GÂ… (11.15)
The inside probability jÂ„p;qÂ… is the total probability of generating words
wpwqgiven that one is starting oï¬€ with the nonterminal Nj.T h e
outside probability jÂ„p;qÂ… is the total probability of beginning with the
start symbolN1and generating the nonterminal Nj
pqand all the words
outsidewpwq.
11.3 The Probability of a String
11.3.1 Using inside probabilities
In general, we cannot eï¬ƒciently calculate the probability of a string by
simply summing the probabilities of all possible parse trees for the string,as there will be exponentially many of them. An eï¬ƒcient way to calcu-
late the total probability of a string is by the inside algorithm , a dynamic
inside algorithm
programming algorithm based on the inside probabilities:
PÂ„w 1mjGÂ…ÂƒPÂ„N1=)w1mjGÂ… (11.16)
ÂƒPÂ„w 1mjN1
1m;GÂ…Âƒ1Â„1;mÂ… (11.17)
The inside probability of a substring is calculated by induction on the
length of the string subsequence:
Base case: We want to ï¬nd jÂ„k;kÂ… (the probability of a rule Nj!wk):
jÂ„k;kÂ…ÂƒPÂ„wkjNj
kk;GÂ…
ÂƒPÂ„Nj!wkjGÂ…
Induction: We want to ï¬nd jÂ„p;qÂ… ,f o rp<q . As this is the inductive
step using a Chomsky Normal Form grammar, the ï¬rst rule must be ofthe formN
j!NrNs, so we can proceed by induction, dividing the string
in two in various places and summing the result:
Nj
Nr
wpwdNs
wdÂ‚1wq

p/CX /CX11.3 The Probability of a String 393
Then,8j;1p<qm,
jÂ„p;qÂ…ÂƒPÂ„wpqjNj
pq;GÂ…
ÂƒX
r;sqâˆ’1X
dÂƒpPÂ„wpd;Nr
pd;wÂ„dÂ‚1Â…q;Ns
Â„dÂ‚1Â…qjNj
pq;GÂ…
ÂƒX
r;sqâˆ’1X
dÂƒpPÂ„Nr
pd;Ns
Â„dÂ‚1Â…qjNj
pq;GÂ…PÂ„wpdjNj
pq;Nr
pd;Ns
Â„dÂ‚1Â…q;GÂ…
PÂ„wÂ„dÂ‚1Â…qjNj
pq;Nr
pd;Ns
Â„dÂ‚1Â…q;wpd;GÂ…
ÂƒX
r;sqâˆ’1X
dÂƒpPÂ„Nr
pd;Ns
Â„dÂ‚1Â…qjNj
pq;GÂ…PÂ„wpdjNr
pd;GÂ…
PÂ„wÂ„dÂ‚1Â…qjNs
Â„dÂ‚1Â…q;GÂ…
ÂƒX
r;sqâˆ’1X
dÂƒpPÂ„Nj!NrNsÂ…rÂ„p;dÂ…sÂ„dÂ‚1;qÂ…
Above, we ï¬rst divided things up using the chain rule, then we made
use of the context-free assumptions of PCFG s, and then rewrote the re-
sult using the deï¬nition of the inside probabilities. Using this recurrence
relation, inside probabilities can be eï¬ƒciently calculated bottom up.
Example 2: The above equation looks scary, but the calculation of in-
side probabilities is actually relatively straightforward. Weâ€™re just tryingto ï¬nd all ways that a certain constituent can be built out of two smallerconstituents by varying what the labels of the two smaller constituentsare and which words each spans. In table 11.3, we show the computations
of inside probabilities using the grammar of table 11.2 and the sentence
explored in ï¬gure 11.1. The computations are shown using a parse tri-
parse triangle
angle where each box records nodes that span from the row index to the
column index.
Further calculations using this example grammar and sentence are left
to the reader in the Exercises.

p/CX /CX394 11 Probabilistic Context Free Grammars
1 2 3 4 5
1NP=0 . 1 S= 0.0126 S= 0.0015876
2 NP= 0.04
V=1 . 0VP= 0.126 VP= 0.015876
3 NP= 0.18 NP= 0.01296
4 P=1 . 0PP= 0.18
5 NP= 0.18
astronomers saw stars with ears
Table 11.3 Calculation of inside probabilities. Table cell Â„p;qÂ… shows non-zero
probabilitiesiÂ„p;qÂ… calculated via the inside algorithm. The recursive com-
putation of inside probabilities is done starting along the diagonal, and thenmoving in diagonal rows towards the top right corner. For the simple gram-
mar of table 11.2, the only non-trivial case is cell Â„2;5Â…, which we calculate as:
PÂ„VP!VN PÂ…
VÂ„2;2Â…NPÂ„3;5Â…Â‚PÂ„VP!VP PPÂ…VPÂ„2;3Â…PPÂ„4;5Â…
11.3.2 Using outside probabilities
We can also calculate the probability of a string via the use of the outside
probabilities. For any k,1km,
PÂ„w 1mjGÂ…ÂƒX
jPÂ„w 1Â„kâˆ’1Â…;wk;wÂ„kÂ‚1Â…m;Nj
kkjGÂ… (11.18)
ÂƒX
jPÂ„w 1Â„kâˆ’1Â…;Nj
kk;wÂ„kÂ‚1Â…mjGÂ…
PÂ„wkjw1Â„kâˆ’1Â…;Nj
kk;wÂ„kÂ‚1Â…n;GÂ…
ÂƒX
jjÂ„k;kÂ…PÂ„Nj!wkÂ… (11.19)
The outside probabilities are calculated top down. As we shall see,
the inductive calculation of outside probabilities requires reference toinside probabilities, so we calculate outside probabilities second, usingtheoutside algorithm .
outside algorithm
Base Case: The base case is the probability of the root of the tree being
nonterminalNiwith nothing outside it:
1Â„1;mÂ…Âƒ1
jÂ„1;mÂ…Âƒ0f o rj6Âƒ1

p/CX /CX11.3 The Probability of a String 395
Inductive case: In terms of the previous step of the derivation, a node
Nj
pqwith which we are concerned might be on the left:
(11.20) N1
Nf
pe
Nj
pq
w1wpâˆ’1wpwqNg
Â„qÂ‚1Â…e
wqÂ‚1weweÂ‚1wm
or right branch of the parent node:
(11.21) N1
Nf
eq
Ng
eÂ„pâˆ’1Â…
w1weâˆ’1wewpâˆ’1Nj
pq
wpwqwqÂ‚1wm
We sum over both possibilities, but restrict the ï¬rst sum to g6Âƒjso as
not to double count in the case of rules of the form X!NjNj:
jÂ„p;qÂ…ÂƒhX
f;g6ÂƒjmX
eÂƒqÂ‚1P(
w1Â„pâˆ’1Â…;wÂ„qÂ‚1Â…m;Nf
pe;Nj
pq;Ng
Â„qÂ‚1Â…ei
Â‚hX
f;gpâˆ’1X
eÂƒ1P(
w1Â„pâˆ’1Â…;wÂ„qÂ‚1Â…m;Nf
eq;Ng
eÂ„pâˆ’1Â…;Nj
pqi
ÂƒhX
f;g6ÂƒjmX
eÂƒqÂ‚1P(
w1Â„pâˆ’1Â…;wÂ„eÂ‚1Â…m;Nf
pe
P(
Nj
pq;Ng
Â„qÂ‚1Â…ejNf
pe
P(
wÂ„qÂ‚1Â…ejNg
Â„qÂ‚1Â…ei
Â‚hX
f;gpâˆ’1X
eÂƒ1P(
w1Â„eâˆ’1Â…;wÂ„qÂ‚1Â…m;Nf
eq
P(
Ng
eÂ„pâˆ’1Â…;Nj
pqjNf
eq
P(
weÂ„pâˆ’1Â…jNg
eÂ„pâˆ’1i

p/CX /CX396 11 Probabilistic Context Free Grammars
ÂƒhX
f;g6ÂƒjmX
eÂƒqÂ‚1fÂ„p;eÂ…PÂ„Nf!NjNgÂ…gÂ„qÂ‚1;eÂ…i
Â‚hX
f;gpâˆ’1X
eÂƒ1fÂ„e;qÂ…PÂ„Nf!NgNjÂ…gÂ„e;pâˆ’1Â…i
As with an HMM , we can form a product of the inside and the outside
probabilities:
jÂ„p;qÂ…jÂ„p;qÂ…ÂƒPÂ„w 1Â„pâˆ’1Â…;Nj
pq;wÂ„qÂ‚1Â…mjGÂ…PÂ„wpqjNj
pq;GÂ…
ÂƒPÂ„w 1m;Nj
pqjGÂ…
But this time, the fact that we postulate a nonterminal node is important
(whereas in the HMM case it is trivial that we are in some state at each
point in time). Therefore, the probability of the sentence andthat there
is some constituent spanning from word ptoqis given by:
PÂ„w 1m;NpqjGÂ…ÂƒX
jjÂ„p;qÂ…jÂ„p;qÂ… (11.22)
However, we know that there will always be some nonterminal spanning
the whole tree and each individual terminal (in a Chomsky Normal Formgrammar). The nonterminal nodes whose only daughter is a single ter-minal node are referred to as preterminal nodes. Just in these cases
preterminal
this gives us the equations in (11.17) and (11.19) for the total prob-
ability of a string. Equation (11.17) is equivalent to 1Â„1;mÂ… 1Â„1;mÂ…
and makes use of the root node, while equation (11.19) is equivalent toP
jjÂ„k;kÂ…jÂ„k;kÂ… and makes use of the fact that there must be some
preterminalNjabove each word wk.
11.3.3 Finding the most likely parse for a sentence
A Viterbi-style algorithm for ï¬nding the most likely parse for a sentence
can be constructed by adapting the inside algorithm to ï¬nd the elementof the sum that is maximum, and to record which rule gave this maxi-
mum. This works as in the
HMM case because of the independence as-
sumptions of PCFG s. The result is an OÂ„m3n3Â…PCFG parsing algorithm.
The secret to the Viterbi algorithm for HMM s is to deï¬ne accumulators
jÂ„tÂ…which record the highest probability of a path through the trellis
that leaves us at state jat timet. Recalling the link between HMM sa n d
PCFG s through looking at PRGs, this time we wish to ï¬nd the highest

p/CX /CX11.3 The Probability of a String 397
probability partial parse tree spanning a certain substring that is rooted
with a certain nonterminal. We will retain the name and use accumula-
tors:
iÂ„p;qÂ…Âƒthe highest inside probability parse of a subtree Ni
pq
Using dynamic programming, we can then calculate the most probable
parse for a sentence as follows. The initialization step assigns to eachunary production at a leaf node its probability. For the inductive step, we
again know that the ï¬rst rule applying must be a binary rule, but this time
we ï¬nd the most probable one instead of summing over all such rules,and record that most probable one in the  variables, whose values are
a list of three integers recording the form of the rule application whichhad the highest probability.
1. Initialization

iÂ„p;pÂ…ÂƒPÂ„Ni!wpÂ…
2. Induction
iÂ„p;qÂ…Âƒmax
1j;kn
pr<qPÂ„Ni!NjNkÂ…jÂ„p;rÂ…kÂ„rÂ‚1;qÂ…
Store backtrace
 iÂ„p;qÂ…Âƒarg max
Â„j;k;rÂ…PÂ„Ni!NjNkÂ…jÂ„p;rÂ…kÂ„rÂ‚1;qÂ…
3. Termination and path readout (by backtracking). Since our grammar
has a start symbol N1, then by construction, the probability of the
most likely parse rooted in the start symbol is:2
PÂ„Ë†tÂ…Âƒ1Â„1;mÂ… (11.23)
We want to reconstruct this maximum probability tree Ë†t.W ed ot h i sb y
regarding Ë†tas a set of nodes fË†Xxgand showing how to construct this
2. We could alternatively ï¬nd the highest probability node of any category that dominates
the entire sentence as:
PÂ„Ë†tÂ…Âƒmax
1iniÂ„1;mÂ…

p/CX /CX398 11 Probabilistic Context Free Grammars
set. Since the grammar has a start symbol, the root node of the tree
must beN1
1m. We then show in general how to construct the left and
right daughter nodes of a nonterminal node, and applying this processrecursively will allow us to reconstruct the entire tree. If X
xÂƒNi
pqis
in the Viterbi parse, and  iÂ„p;qÂ…ÂƒÂ„j;k;rÂ… , then:
leftÂ„Ë†XxÂ…ÂƒNj
pr
rightÂ„Ë†XxÂ…ÂƒNk
Â„rÂ‚1Â…q
Note that where we have written â€˜arg maxâ€™ above, it is possible for there
not to be a unique maximum. We assume that in such cases the parserjust chooses one maximal parse at random. It actually makes things con-siderably more complex to preserve all ties.
11.3.4 Training a PCFG
The idea of training a PCFG is grammar learning or grammar induction,
but only in a certain limited sense. We assume that the structure of thegrammar in terms of the number of terminals and nonterminals, and thename of the start symbol is given in advance. We also assume the set ofrules is given in advance. Often one assumes that all possible rewritingrules exist, but one can alternatively assume some pre-given structurein the grammar, such as making some of the nonterminals dedicated
preterminals that may only be rewritten as a terminal node. Training
the grammar comprises simply a process that tries to ï¬nd the optimalprobabilities to assign to diï¬€erent grammar rules within this architecture.
As in the case of
HMM s, we construct an EM training algorithm, the
Inside-Outside algorithm , which allows us to train the parameters of a Inside-Outside
algorithmPCFG on unannotated sentences of the language. The basic assumption
is that a good grammar is one that makes the sentences in the training
corpus likely to occur, and hence we seek the grammar that maximizes
the likelihood of the training data. We will present training ï¬rst on thebasis of a single sentence, and then show how it is extended to the morerealistic situation of a large training corpus of many sentences, by assum-ing independence between sentences.
To determine the probability of rules, what we would like to calculate
is:
Ë†PÂ„N
j!Â…ÂƒCÂ„Nj!Â…P
Î³CÂ„Nj!Î³Â…

p/CX /CX11.3 The Probability of a String 399
whereCÂ„Â…is the count of the number of times that a particular rule is
used. If parsed corpora are available, we can calculate these probabilitiesdirectly (as discussed in chapter 12). If, as is more common, a parsedtraining corpus is not available, then we have a hidden data problem: wewish to determine probability functions on rules, but can only directly see
the probabilities of sentences. As we donâ€™t know the rule probabilities,
we cannot compute relative frequencies, so we instead use an iterativealgorithm to determine improving estimates. We begin with a certaingrammar topology, which speciï¬es how many terminals and nontermi-nals there are, and some initial probability estimates for rules (perhapsjust randomly chosen). We use the probability of each parse of a trainingsentence according to this grammar as our conï¬dence in it, and then sum
the probabilities of each rule being used in each place to give an expecta-
tion of how often each rule was used. These expectations are then usedto reï¬ne our probability estimates on rules, so that the likelihood of thetraining corpus given the grammar is increased.
Consider:

jÂ„p;qÂ…jÂ„p;qÂ…ÂƒPÂ„N1=)w1m;Nj=)wpqjGÂ…
ÂƒPÂ„N1=)w1mjGÂ…PÂ„Nj=)wpqjN1=)w1m;GÂ…
We have already solved how to calculate PÂ„N1=)w1mÂ…; let us call this
probability. Then:
PÂ„Nj=)wpqjN1=)w1m;GÂ…ÂƒjÂ„p;qÂ…jÂ„p;qÂ…

and the estimate for how many times the nonterminal Njis used in the
derivation is:
EÂ„Njis used in the derivation Â…ÂƒmX
pÂƒ1mX
qÂƒpjÂ„p;qÂ…jÂ„p;qÂ…
(11.24)
In the case where we are not dealing with a preterminal, we substitute
the inductive deï¬nition of into the above probability and then 8r;s;p<
q:
PÂ„Nj!NrNs=)wpqjN1=)w1m;GÂ…
ÂƒPqâˆ’1
dÂƒpjÂ„p;qÂ…PÂ„Nj!NrNsÂ…rÂ„p;dÂ…sÂ„dÂ‚1;qÂ…

Therefore, the estimate for how many times this particular rule is used
in the derivation can be found by summing over all ranges of words that

p/CX /CX400 11 Probabilistic Context Free Grammars
the node could dominate:
EÂ„Nj!NrNs;NjusedÂ… (11.25)
ÂƒPmâˆ’1
pÂƒ1Pm
qÂƒpÂ‚1Pqâˆ’1
dÂƒpjÂ„p;qÂ…PÂ„Nj!NrNsÂ…rÂ„p;dÂ…sÂ„dÂ‚1;qÂ…

Now for the maximization step, we want:
PÂ„Nj!NrNsÂ…ÂƒEÂ„Nj!NrNs;NjusedÂ…
EÂ„NjusedÂ…
So, the reestimation formula is:
Ë†PÂ„Nj!NrNsÂ…ÂƒÂ„11:25Â…=Â„11:24Â… (11.26)
ÂƒPmâˆ’1
pÂƒ1Pm
qÂƒpÂ‚1Pqâˆ’1
dÂƒpjÂ„p;qÂ…PÂ„Nj!NrNsÂ…rÂ„p;dÂ…sÂ„dÂ‚1;qÂ…
Pm
pÂƒ1Pm
qÂƒpjÂ„p;qÂ…jÂ„p;qÂ…
Similarly for preterminals,
PÂ„Nj!wkjN1=)w1m;GÂ…ÂƒPm
hÂƒ1jÂ„h;hÂ…PÂ„Nj!wh;whÂƒwkÂ…

ÂƒPm
hÂƒ1jÂ„h;hÂ…PÂ„whÂƒwkÂ…jÂ„h;hÂ…

ThePÂ„whÂƒwkÂ…above is, of course, either 0 or 1, but we express things
in the second form to show maximal similarity with the preceding case.Therefore,
Ë†PÂ„N
j!wkÂ…ÂƒPm
hÂƒ1jÂ„h;hÂ…PÂ„whÂƒwkÂ…jÂ„h;hÂ…Pm
pÂƒ1Pm
qÂƒpjÂ„p;qÂ…jÂ„p;qÂ…(11.27)
Unlike the case of HMM s, this time we cannot possibly avoid the prob-
lem of dealing with multiple training instances â€“ one cannot use con-
catenation as in the HMM case. Let us assume that we have a set of
training sentences WÂƒÂ„W1;:::;W!Â…,w i t hWiÂƒwi;1wi;mi.L e tfi,gi,
andhibe the common subterms from before for use of a nonterminal at
a branching node, at a preterminal node, and anywhere respectively, nowcalculated from sentence W
i:
fiÂ„p;q;j;r;sÂ… ÂƒPqâˆ’1
dÂƒpjÂ„p;qÂ…PÂ„Nj!NrNsÂ…rÂ„p;dÂ…sÂ„dÂ‚1;qÂ…
PÂ„N1=)WijGÂ…(11.28)
giÂ„h;j;kÂ…ÂƒjÂ„h;hÂ…PÂ„whÂƒwkÂ…jÂ„h;hÂ…
PÂ„N1=)WijGÂ…
hiÂ„p;q;jÂ…ÂƒjÂ„p;qÂ…jÂ„p;qÂ…
PÂ„N1=)WijGÂ…

p/CX /CX11.4 Problems with the Inside-Outside Algorithm 401
If we assume that the sentences in the training corpus are independent,
then the likelihood of the training corpus is just the product of the prob-abilities of the sentences in it according to the grammar. Therefore, inthe reestimation process, we can sum the contributions from multiplesentences to give the following reestimation formulas. Note that the de-
nominators consider all expansions of the nonterminal, as terminals or
nonterminals, to satisfy the stochastic constraint in equation (11.3) thata nonterminalâ€™s expansions sum to 1.
Ë†PÂ„N
j!NrNsÂ…ÂƒP!
iÂƒ1Pmiâˆ’1
pÂƒ1Pmi
qÂƒpÂ‚1fiÂ„p;q;j;r;sÂ…
P!
iÂƒ1Pmi
pÂƒ1Pmi
qÂƒphiÂ„p;q;jÂ…(11.29)
and
Ë†PÂ„Nj!wkÂ…ÂƒP!
iÂƒ1Pmi
hÂƒ1giÂ„h;j;kÂ…P!
iÂƒ1Pmi
pÂƒ1Pmi
qÂƒphiÂ„p;q;jÂ…(11.30)
The Inside-Outside algorithm is to repeat this process of parameter
reestimation until the change in the estimated probability of the trainingcorpus is small. If G
iis the grammar (including rule probabilities) in the
ithiteration of training, then we are guaranteed that the probability of
the corpus according to the model will improve or at least get no worse:
PÂ„WjGiÂ‚1Â…PÂ„WjGiÂ…:
11.4 Problems with the Inside-Outside Algorithm
However, the PCFG learning algorithm is not without problems:
1. Compared with linear models like HMM s, it is slow. For each sentence,
each iteration of training is OÂ„m3n3Â…,w h e r emis the length of the
sentence, and nis the number of nonterminals in the grammar.
2.Local maxima are much more of a problem. Charniak (1993) reports local maxima
that on each of 300 trials of PCFG induction (from randomly initialized
parameters, using artiï¬cial data generated from a simple English-like
PCFG ) a diï¬€erent local maximum was found. Or in other words, the
algorithm is very sensitive to the initialization of the parameters. Thismight perhaps be a good place to try another learning method. (Forinstance, the process of simulated annealing has been used with somesuccess with neural nets to avoid problems of getting stuck in local

p/CX /CX402 11 Probabilistic Context Free Grammars
maxima (Kirkpatrick et al. 1983; Ackley et al. 1985), but it is still per-
haps too compute expensive for large-scale PCFG s.) Other partial so-
lutions are restricting rules by initializing some parameters to zero orperforming grammar minimization, or reallocating nonterminals awayfrom â€œgreedyâ€ terminals. Such approaches are discussed in Lari and
Young (1990).
3. Based on experiments on artiï¬cial languages, Lari and Young (1990)
suggest that satisfactory grammar learning requires many more non-
terminals than are theoretically needed to describe the language at
hand. In their experiments one typically needed about 3 nnontermi-
nals to satisfactorily learn a grammar from a training text generatedby a grammar with nnonterminals. This compounds the ï¬rst problem.
4. While the algorithm is guaranteed to increase the probability of the
training corpus, there is no guarantee that the nonterminals that thealgorithm learns will have any satisfactory resemblance to the kinds ofnonterminals normally motivated in linguistic analysis (NP, VP, etc.).Even if one initializes training with a grammar of the sort familiar to
linguists, the training regime may completely change the meaning of
nonterminal categories as it thinks best. As we have set things up, theonly hard constraint is that N
1must remain the start symbol. One
option is to impose further constraints on the nature of the grammar.For instance, one could specialize the nonterminals so that they eachonly generate terminals ornonterminals. Using this form of grammar
would actually also simplify the reestimation equations we presented
above.
Thus, while grammar induction from unannotated corpora is possible in
principle with
PCFG s, in practice, it is extremely diï¬ƒcult. In diï¬€erent
ways, many of the approaches of the next chapter address various of thelimitations of using vanilla
PCFG s.
11.5 Further Reading
A comprehensive discussion of topics like weak and strong equivalence,Chomsky Normal Form, and algorithms for changing arbitrary
CFGs into
various normal forms can be found in (Hopcroft and Ullman 1979). Stan-dard techniques for parsing with
CFGsi n NLPcan be found in most AI
and NLPtextbooks, such as (Allen 1995).

p/CX /CX11.5 Further Reading 403
Probabilistic CFGs were ï¬rst studied in the late 1960s and early 1970s,
and initially there was an outpouring of work. Booth and Thomson(1973), following on from Booth (1969), deï¬ne a
PCFG as in this chap-
ter (modulo notation). Among other results, they show that there areprobability distributions on the strings of context free languages which
cannot be generated by a
PCFG , and derive necessary and suï¬ƒcient con-
ditions for a PCFG to deï¬ne a proper probability distribution. Other work
from this period includes: (Grenander 1967), (Suppes 1970), (Huang andFu 1971), and several PhD theses (Horning 1969; Ellis 1969; Hutchins1970). Tree structures in probability theory are normally referred to asbranching processes , and are discussed in such work as (Harris 1963) and
branching
processes (Sankoï¬€ 1971).
During the 1970s, work on stochastic formal languages largely died
out, and PCFG s were really only kept alive by the speech community, as an
occasionally tried variant model. The Inside-Outside algorithm was intro-duced, and its convergence properties formally proved by Baker (1979).Our presentation essentially follows (Lari and Young 1990). This paperincludes a proof of the algorithmic complexity of the Inside-Outside al-gorithm. Their work is further developed in (Lari and Young 1991).
For the extension of the algorithms presented here to arbitrary
PCFG s,
see (Charniak 1993) or (Kupiec 1991, 1992a).3Jelinek et al. (1990) and
Jelinek et al. (1992a) provide a thorough introduction to PCFG s. In par-
ticular, these reports, and also Jelinek and Laï¬€erty (1991) and Stolcke(1995), present incremental left-to-right versions of the Inside and Viterbialgorithms, which are very useful in contexts such as language models forspeech recognition.
In the section on training a
PCFG , we assumed a ï¬xed grammar archi-
tecture. This naturally raises the question of how one should determinethis architecture, and how one would learn it automatically. There hasbeen a little work on automatically determining a suitable architectureusing Bayesian model merging ,aMinimum Description Length approach
Bayesian model
merging
Minimum
Description Length(Stolcke and Omohundro 1994b; Chen 1995), but at present this task is
still normally carried out by using the intuitions of a linguist.
3. For anyone familiar with chart parsing, the extension is fairly straightforward: in a
chart we always build maximally binary â€˜traversalsâ€™ as we move the dot through rules. Wecan use this virtual grammar, with appropriate probabilities to parse arbitrary
PCFG s( t h e
rule that completes a constituent can have the same probability as the original rule, whileall others have probability 1).

pa/CX /CX404 11 Probabilistic Context Free Grammars
PCFG s have also been used in bioinformatics (e.g., Sakakibara et al.
1994), but not nearly as much as HMM s.
11.6 Exercises
Exercise 11.1 [Â«Â«]
Consider the probability of a (partial) syntactic parse tree giving part of the struc-
ture of a sentence:
P0
BBBBBBB@NP
Det N
0
Adj N1
CCCCCCCA
In general, as the (sub)tree gets large, we cannot accurately estimate the probabil-
ity of such trees from any existing training corpus (a data sparseness problem).
As we saw,
PCFG s approach this problem by estimating the probability of a tree
like the one above from the joint probabilities of local subtrees:
P0
BB@NP
Det N0;N0
Adj N1
CCA
However, how reasonable is it to assume independence between the probability
distributions of these local subtrees (which is the assumption that licenses us toestimate the probability of a subtree as the product of the probability of each
local tree it contains)?
Use a parsed corpus (e.g., the Penn Treebank) and ï¬nd for some common sub-
trees whether the independence assumption seems justiï¬ed or not. If it is not,see if you can ï¬nd a method of combining the probabilities of local subtrees insuch a way that it results in an empirically better estimate of the probability ofa larger subtree.
Exercise 11.2 [Â«]
Using a parse triangle as in ï¬gure 11.3, calculate the outside probabilities for the
sentence astronomers saw stars with ears according to the grammar in table 11.2.
Start at the top righthand corner and work towards the diagonal.
Exercise 11.3 [Â«]
Using the inside and outside probabilities for the sentence astronomers saw stars
with ears worked out in ï¬gure 11.3 and exercise 11.2, reestimate the proba-
bilities of the grammar in table 11.2 by working through one iteration of theInside-Outside algorithm. It is helpful to ï¬rst link up the inside probabilitiesshown in ï¬gure 11.3 with the particular rules and subtrees used to obtain them.

p/CX /CX11.6 Exercises 405
What would the rule probabilities converge to with continued iterations of the
Inside-Outside algorithm? Why?
Exercise 11.4 [Â«Â«Â« ]
Recording possible spans of nodes in a parse triangle such as the one in ï¬g-
ure 11.3 is the essence of the Cocke-Kasami-Younger ( CKY) algorithm for pars-
ingCFGs (Younger 1967; Hopcroft and Ullman 1979). Writing a CKY PCFG parser
is quite straightforward, and a good exercise. One might then want to extendthe parser from Chomsky Normal Form grammars to the more general case ofcontext-free grammars. One way is to work out the general case oneself, or toconsult the appropriate papers in the Further Reading. Another way is to writea grammar transformation that will take a
CFG and convert it into a Chomsky
Normal Form CFGby introducing specially-marked additional nodes where nec-
essary, which can then be removed on output to display parse trees as given bythe original grammar. This task is quite easy if one restricts the input
CFG to
one that does not contain any empty nodes (nonterminals that expand to givenothing).
Exercise 11.5 [Â«Â«Â« ]
Rather than simply parsing a sequence of words, if interfacing a parser to a
speech recognizer, one often wants to be able to parse a word lattice, of thesort shown in ï¬gure 12.1. Extend a
PCFG parser so it works with word lattices.
(Because the runtime of a PCFG parser is dependent on the number of words in
the word lattice, a PCFG parser can be impractical when dealing with large speech
lattices, but our CPUs keep getting faster every year!)



