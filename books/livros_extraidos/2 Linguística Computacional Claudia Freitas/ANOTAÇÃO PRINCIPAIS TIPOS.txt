55 
  
6. Anotação : Principais tipos  
Anotação linguística é a adição de informação (linguística) a um corpus.  Tecnicamente, 
anotar consiste em delimitar um segmento de texto  (palavra, sintagma, frase, parágrafo...)  
e atribuir -lhe uma etiqueta, definida conforme o objetivo da anotação. No exemplo (1) 
abaixo, t emos uma anotação atribuída a cada palavra, no exemplo (2) temos uma anotação 
que relaciona palavras e no exemplo (3) uma anotação atribuída a palavras e segmentos 
de texto.  
 
 
Eu/PRON  adoro /VERBO  sorvete /SUBST  
 
(1)  
(2) 
  
 
Conferação dos Tamoios é o nome  dado à revolta liderada pela nação 
Tupinambá, que ocupava o litoral do Brasil entre Bertioga e Cabo Frio, que  
 
aconteceu entre 1554 e  1567  
 (3)  
 
A lista dos tipos de anotação é potencialmente infinita , e praticamente não há limites 
quanto à informação linguística que se pode adicionar a um corpus . O conjunto das 
etiquetas utilizadas na anotação de um corpus é chamado de tagset  (porque uma etiqueta, 
em inglês, é uma tag). De um ponto de vista formal, existem algumas maneiras de se 
representar um texto anotado. No capítulo 2  usei uma representação bastante comum , 
reproduzida na tabela a seguir : 
 cada palavra fica em uma linha  
 cada palavra recebe um identificador (id)  (coluna 1)  
 cada tipo de informação linguística fica em uma coluna di stinta  (colunas 3, 4 e 5, 
que representam etiquetas relativas a classe de palavras, tipo de relação sintática, 
e dependência sintática, respectivamente , e as colunas e as colunas 6 e 7 codificam 
informações relacionadas ao sentido ) 
Id palavra  POS  Tipo de relação  
sintática  Dependência 
entre as relações 
sintáticas  Classe 
semântica  Polaridade  
1 Eu  PRON  Sujeito  2 Pessoa  -- 
2 adoro  VERBO  Núcleo da Oração  0 Verbo de 
opinião  Positiva  
3 sorvete  SUBST  Objeto direto  2 Alimento  -- 
4 de PREP  ? 5  -- 
5 laranja  SUBST  Adj. adnominal  3 Alimento  -- 
6 com PREP  ? 7 -- -- 
7 gengibre  SUBST  Adj. adnominal  5 alimento  -- 
8 da PREP  ? 9 -- -- 
9 sorveteria  SUBST  ? 3 ou 10 (?)  Organização  -- 
10 Sabores  PROP ou  
SUBST  Adj. adnominal  9 ou 10  (?)  Organização  -- 
EVENTO  
DATA  DATA  LOCAL  LOCAL  LOCAL  POVO  

56 
 11 do  PREP (ou 
PROP?)  ? 12 Organização  -- 
12 Campo  PROP ou  
SUBST  ? 10 Organização  -- 
Ainda que codifiquem informações que podem se relacionar, as colunas são 
independentes, e um corpus anotado pode ter apenas uma (qualquer uma) ou várias 
colunas (camadas) de anotação. A coluna 5 deve ser lida da seguinte maneira:   
- a 1ª linha indica que a palavra (ou token) 1, “eu” é dependente do token 2 (adoro), e essa 
relação é de “sujeito” (informação que está na coluna 3).  
- a 2ª linha indica que a palavra (ou token) 2, “adoro”, não depende de ninguém, é o 
núcleo  da oração principal (ou raiz), e por  isso ela depende  do token 0 . Por convenção, 0 
sempre é usado para indicar o núcleo da frase. Assim, em Cuidado! Cão bravo  , a palavra 
cuidado será 0, porque é o único elemento dessa fr ase, e a frase cão bravo  terá o 0 
atribuído a cão.   
- a 3ª linha indica que a palavra (ou token) 3, “sorvete” é dependente do token 2 (adoro), 
e essa relação é de “objeto direto” (informação que está na coluna 3).  
- a 4ª linha indica que o token 4 ( de) é dependente  do token 5 (laranja ), e essa relação é 
de.... ? Não estamos habituados, na chamada Gramática Tradicional ( GT), a atribuir uma 
função sintática es pecífica às preposições (ou conjunções ). Justamente para mostrar 
diferentes maneiras de lidar com a gramática , quis trazer um exemplo de um a outra 
abordagem – e formalismo – , chamada gramática de dependências , comum no PLN. 
Nesta abordagem , as relações não se estabelecem entre sintagmas ou grupos de 
constituintes, mas entre cada palavr a individualmente  (retomaremos esse ponto mais à 
frente) . Existem algumas formas  de fazer análise  sintática, e no exemplo do quadro  o “de” 
é dependente de “laranja”, e não de “sorvete ”.  Essa é uma escolha deco rrente de uma 
abordagem  gramatical, e h á abordagens em que o “de” é dependente de “sorvete”.   
- a 5ª linha indica que o token  5, “laranja ” é dependente do token 4 (sorvete ), e essa 
relação é de “ adjunto adnominal ”. 
- a 6ª linha indica que o token 6, “com” é dependente do token 7 (gengibre), e essa relação 
é de ... Toda a explicação sobre o token 4 (“de”) se aplica aqui.  
- a 7ª linha indica que o token 7, “gengibre” é dependente do token 4 (laranja), e essa 
relação é de “adjunto adnominal”.  
É a coluna 5, portanto, das dependências, que indica que “de gengibr e” se relaciona  a (lê-
se como “é  depen dente de ”) laranja , e não a sorvete . Isto é, o sabor do sorvete é “laranja 
com gengibre”.  
Antes de avançar, é possível que você esteja se perguntando: o que um programa faz com 
essa tabela?  
A tabela ilustra uma porção  de um corpus anotado . E um corpus anotado  por pessoas – 
um corpus padrão ouro – é um elemento crucial para o desenvolvimento da área (como 
já vimos na seção 2.1.2). Retomamos aqui dois usos muito comuns: um corpus anotado 
serve p ara fornecer exemplos do q ue um sistema precisa aprender (para treino) e para 
fornecer exemplos para avaliar a qualidade da análise automática (para avaliação). No 

57 
 primeiro caso, e considerando apenas a coluna 3, por exemplo, ele “verá” que adoro  é um 
verbo e, ao lado de muitos outros exemplos, irá aprender – se tudo correr bem –  como 
classificar algo como  um verbo , inclusive palavras que nunca viu . Mas nem sempre o 
procedimento será bem-sucedido , e já vi a palavra dióxido  ser classificada como a  forma 
participial do verbo (inexistente) dioxer . Na página eletrônica associada ao livro , na seção 
@Exercícios , você verá como utilizar uma ferramenta de anotação gramatical criada com 
a abordagem de aprendizado de máquina, ou seja, sem qualquer informaçã o linguística 
explicitamente codificada relativa à gramática da língua portuguesa.  
A figura 1 3 traz mais algumas provocações  gramaticais : 
 - no trecho “sorveteria Sabores do Campo ”, quem é o núcleo  e quem é o modificador? 
Algumas gramáticas trazem a classe do  aposto especificativo  restritivo , mas este é um 
ponto controverso. “Sabores do Campo” especifica “sorveteria” (especifica sobre qual 
sorveteria estamos falando) ou “sorveteria” especifica “Sabores do Campo” (especifica 
que “Sabores do Campo” é um a sorveteria , e não um restaurante, por exemplo )? Estamos 
habituados a pensar que a noção  de dependência relacionada à importância, à hierarquia: 
se um elemento A depende  de um outro, B,  este outro, B, é  um elemento mais importante 
do que A. No nosso exemplo, q uem é mais importante , “sorveteria ” ou “Sabores do 
Campo ”? Eu, sinceramente, não tenho uma opinião definida, e aceito de bom grado 
qualquer uma das leituras. Na anotação, o importante é que sejamos consistentes , isto é, 
sempre que passarmos por f enômenos como esse, estejam analisados (anotados) da 
mesma maneira.  E, se decidimos que “Sabores do Campo” depende de “sorveteria”, então 
“sorveteria” será adjunto adnominal de “sorvete”.  
- Por fim, o  que fazer com “Sabores do Campo”?  São uma  única  “palav ra”,  
“Sabores_do_Campo”? Ou 3 palavras (Sabores; do; Campo)?   Ou 4? (Sabores; de; o; 
Campo?)  No caso de serem 3  (ou 4) , qual é a classe de cada uma delas? Sabores  será 
substantivo próprio ou substantivo comum , cujo lema é “sabor” ? E “do”, será nome 
próprio? E “Campo ”?  
A anotação nos obriga a pensar (e a formalizar) questões linguísticas em seus mínimos 
detalhes, e por isso acaba sendo uma fonte inesgotável de estudo para os linguistas 
(computacionais ou não) .  Por isso, também, a  anotação é o espaço mais evidente de 
relação entre a linguística e o PLN.  
Mais do que sugerir “vejam como  a anotação é cheia de problemas difíceis !”, interessa  
aqui enfatizar a anotação como uma atividade de interpretação : não está dado de 
antemão, e precisa remos decidir, dentre outras coisas, o que é uma palavra (Sabores do 
Campo é 1 palavra ou 4?) e como atribuir as classes gramaticais (o “do” de “Sabores do 
Brasil” é parte de um nome próprio  e, portanto, deve ser anotado como tal ? E “Sabores”? ) 
e funções sintáticas (aposto ou adjunto adnominal ?). São problemas talvez pouco 
interessantes para teorias linguísticas, mas é esta análise que será reproduzida por 
sistemas, e que consequentemente nos será “devolvida” por estes mesmos sistemas 
quando quiser mos analisar um texto qualquer.  
Ainda  é pouco comum no PLN a discussão sobre as análises linguísticas 
subjacentes à anotação . Mas vemos como ela é necessária quando mesmo distinções 
consideradas óbvias, como entre o numeral “um” e o artigo “um”, podem não ser tão  
óbvias assim. No trecho abaixo  estamos diante de um artigo ou de um numeral?  

58 
  
A Media Capital, que gere a TVI, apresenta assim um argumento para a 
assembleia de credores que se realiza no próximo dia 14.  
E para demonstrar que a diferença entre artigos e numerais não é tão óbvia assim, trago 
as palavras d a Gramática de Usos do Português : 
 
Do ponto de vista da quantidade, isso significa que, no caso do artigo 
indefinido, fala -se de “pelo menos um ”, enquanto,  no caso do numeral, fala -
se de “exatamente um ”. (...). Apesar  disso, em muitos enunciados tal 
diferença é neutralizada, pois fica difícil concluir -se se o que está no 
primeiro plano é um ou outro valor. (Neves, 2000:518, grifo meu)  
 
Por isso, não é exatamente correto afirmar que um corpus anotado é uma fonte objetiva 
de informações linguísticas que poderão ser usadas para o aprendizado ou para a avaliação 
de sistemas, mas sim que um corpus anotado  reflete a intepretação  dos anotadores , isto 
é, reflete a interpretação de seres humanos , com relação a um dado fenômeno , sendo fruto 
de um processo de análise  linguística . O que acontece é que alguns fenômenos, como as 
classes de palavras  e as funções sintáticas, já estão tão internalizados devido aos anos de 
escolarização  que ficamos com a falsa impressão de que são classificações mais 
“objetivas” , enquanto outras são consideradas mais subjetivas.   
O fato de não termos hábito de discutir as análises  gramaticais – desde a escola são quase 
sempre ensinadas em termos de acerto  e erro, e nunca em termos de classes  que são 
construtos humanos, situados historicamente e vinculados a uma certa teoria ou visão de 
mundo ou interesse  – contribui para a ideia de que estamos diante de classificações 
objetivas. Some -se a isso o fato de que, dentre todas as disciplinas científicas, a gramática 
é aquela que possui o vocabulário teórico próprio mais estável e mais antigo de todas: as  
classes de palavras.  
A seguir estão listados os tipos mais frequentes de anotação. Para cada tipo, é apresentada 
uma descrição geral da anotação e da tarefa a ela associada, desafios linguísticos e corpora 
padrão ouro em português. A lista dos corpora não é exaust iva, e pode facilmente ficar 
desatualizada, mas serve como um incentivo para explorações futuras nos materiais.  
6.1 POS – as classes de palavras  
A sigla POS vem d o inglês part of speech  e é o equivalente ao nosso “partes do discurso” 
ou “classes  de palavras ”. As classes de palavras são a classificação linguística mais 
antiga, aliás, a classificação científica  mais antiga de que se tem notícia, e surgem da 
necessidade de fornecer um  vocabulário t eórico  que pudesse ser usado na análise lógica 
das proposições  (falamos delas  e de sua relação estreita com os primeiros estudos da 
linguagem  em 3.2) . É esse vocabul ário cristalizado pela tradiç ão que comparece até hoje 
nos estudos linguísticos e na anotação de POS, ainda que com pequenas variações.  E assim 
como não há, nos estudos gramaticais, consenso absoluto sobre quais sejam as classes de 
palavras , pode haver diferentes  conjuntos de  POS (chamados tagsets  de POS)  disponíveis 
para uma mesma língua.  Para a língua portuguesa, temos  atualmente  os conjuntos abaixo, 
e a página eletrônica d este livro  (@Ponteiros ) remete a cada  um dos projetos, que por sua 
vez contém  a explicação das etiquetas : 

59 
  Tagset do projeto Lácio -Web/ Mac-Morpho  (Aluisio et al., 2003)  – 23 etiquetas  
na primeira versão   
 Tagset do sistema PALAVRAS (Bick, 2000)  – 14 etiquetas  (são as mesmas 
utilizadas pelo projeto AC/DC)  
 Tagset do projeto / corpus  Tycho Brahe (Galves et al., 2017)  – cerca de 28 
etiquetas principais  
Além disso, existem  os projetos multilíngues, que propõem tagsets  independente s de 
língua, e, portanto, aplicáveis também ao português. O projeto Eagles, de 1996,  e a 
proposta mais recente  do projeto Universal Dependencies  são exemplos de projetos 
multilíngues .  
Projetos de ta gsets compartilháveis e independentes de língua pretendem oferecer uma 
metalinguagem e uma análise linguística comuns para todas as línguas. Se tais projetos 
são/serão bem -sucedidos, no sentido de permitirem uma análise igualmente satisfatória 
das várias l ínguas, é uma questão em aberto . 
Desafios linguísticos  
Em uma tarefa de anotação de POS é preciso atribuir , para cada palavra, a classe 
gramatical adequada – e o que deve contar como palavra, como já vimos, está longe de 
ser óbvio . Do mesmo modo , o que conta como “ classe adequada ” vai depender da teoria 
subjacente, ou das  decisões linguísticas subjacentes à anotação. Por exemplo, como deve 
ser classificada a palavra proibido s na frase abaixo: como verbo ou como adjetivo?  
  1. Empresas usam liminares para manter painéis proibidos  
  1a. Empresas usam liminares para manter painéis ( que foram ) proibidos (verbo)  
  1b. Empresas usam liminares para manter painéis proibidos (=ilegais ) (adjetivo )  
 
E como lidar com o “o” na frase abaixo?  
2. Isto poderá significar, simplesmente, que o controle voluntário da mão direita é mais 
fácil do que o da mão esquerda.  
Em (2) , o “o” pode ser analisado como um pronome substantivo demonstrativo ( 2a) ou  
como um artigo (2b), e neste caso se considera uma  elipse  do substantivo “controle”.  As 
leituras têm como resultado final o mesmo sentido, estão igualmente corretas, e não 
adianta ampliar o contexto para decidir por uma outra análise:  
2a. Isto poderá significar, simplesmente, que o controle voluntário da mão direita é 
mais fácil do que o (= aquele) da mão esquerda.  
2b. Isto poderá significar, simplesmente, que o controle voluntário da mão direita é 
mais fácil do que o (= controle) da mão esquerda.  
A classificação automática de palavras em classes morfossintáticas  é uma tarefa antiga  
da Linguística Computacional /PLN  (existe desde 1958), e inicialmente era encarada 
como uma tarefa de desambiguação , cujo objetivo era eliminar as muitas possibilidades 
de classificação de uma mesma palavra (pensemos nas várias classes que uma palavra 
como “que”  pode ter. Segundo o dicionário Houaiss, são 17 classes possíveis  e há um 
trabalho (Martins, 1985) que  mencion a 27 classes  para o “que” !) 

60 
 Atualmente, o desempenho de ferramentas de anotação de  POS para o português  está em 
torno de 97% , indicando que a análise automática é bastante boa. No entanto, isto não 
significa que ao anotar um texto qualquer iremos obter uma qualidade de 98%. Sabemos 
que textos podem variar de muitas maneiras: cada gênero tex tual e cada domínio têm 
estruturas sintáticas e vocabulário característicos, e esta variação influencia a qualidade 
análise linguística . Como boa parte das ferramentas – baseadas em AM ou em regras – é 
criada a partir de corpora de textos jornalísticos, po demos esperar um bom desempenho 
quando estivermos diante deste tipo de material. Para textos técnicos ou literários, 
veremos alguma  queda na qualidade da anotação . Voltamos aqui à necessidade de corpora 
padrão ouro, neste caso, relativos a textos não jorna lísticos.  
Corpora em português  
Para cada um dos tagsets de POS mencionados, temos pelo menos um corpus padrão ouro 
associado. Para o português dispomos dos seguintes corpora anotados com POS: corpus 
MacMorpho, corpus Bosque ( ambos disponíve is em várias vers ões e formatos ) e o corpus 
Tycho Brahe.   
6.2 Sintaxe  
Corpora anotados sintaticamente são chamados de florestas sintáticas  (em inglês,  
treebanks , literalmente bancos de árvores ), pois  cada frase analisada sintaticamente 
produz uma árvore sintática .  A motivação para a criação de florestas sintática s vem tanto 
do lado teórico quanto do lado aplicado da Linguística Computacional/PLN: do lado 
teórico , trata -se de um recurso para a investigação de teorias linguísticas , que podem ser 
postas à prova em um ambiente real, isto é, com frases naturalmente construídas pelos 
falantes.  E, de maneira complementar , o resultado da anotação – o treebank  – associado 
a uma ferramenta de busca, pode ser um auxiliar valioso para pesquisas linguísticas. Do 
lado aplicado , a anotação sintática  fornece pistas sobre  a frequência de certas estruturas  
linguísticas , facilitando a desambiguação sintática: a seleção d a análise  adequada  pode 
ser feita levando em conta as chances d e a estrutura sintática em questão ser possível na 
língua.  A frase abaixo , embora não seja difícil para nós (sabemos que a palavra Sérvia  
está relacionada  a sanções , e não a ao verbo suspendeu ) tem uma estrutura ambígua, como 
vemos no contraste com a frase (2) , na qual a palavra em negrito se refere ao verbo) .  
(1) O país suspendeu  as sanções  contra a Sérvia  
(2) O inimigo jogou  irmãos  contra irmãos  
Além disso, e  como qualquer outro tipo de anotação em corpus, florestas sintáticas são 
de grande utilidade para a avaliação e treino de sistemas.  
A maioria dos treebanks  é pouco especificada em termos teóricos, e esta decisão tem a 
vantagem de permitir sua utilização por pessoas de diferentes filiações teóricas. Mas, 
assim como no caso das classes de palavras, “leveza” teórica não é sinal de neutrali dade: 
o que se tem é uma forte inspiração (ou influência) de análises derivadas de gramáticas 
tradicionais, que acabam funcionando como metalinguagem supostamente neutra.  
Existem dois grandes tipos de formalismos sintátic os, que levam a dois grupos de 
treebanks : aqueles que levam em conta uma estrutura sintagmática (isto é, que leva em 
conta, na anotação, o conceito de sintagma), e aqueles que levam em conta uma estrutura 
de dependências . Cada formalismo pode ser visualizado de diferentes maneiras, e pode 

61 
 seguir diferentes abordagens gramaticais . Ou seja, uma coisa é um formalismo 
gramatical, isto é, uma maneira formal de representar algum aspecto da gramática (no 
nosso caso, a sintaxe), e outra coisa é uma abordagem de gramática, uma maneira de 
entender co mo se estrutura uma língua (em nosso caso, como se estrutura sintaticamente 
uma língua). Uma abordagem de gramática, portanto, pode ser representada de diferentes 
maneiras.  
Uma abordagem (ou gramática) de dependências comporta teorias gramaticais baseadas  
na relação de dependência , e uma abordagem (ou gramática) sintagmática comporta 
teorias baseadas na relação entre constituintes .  
Nas gramáticas de dependência, os elementos que se conectam para compor a estrutura 
da frase são as palavras , e o verbo é con siderado o núcleo (ou raiz) da oração. Todas as 
outras unidades são direta ou indiretamente conectadas a ele , em relações que são 
chamadas de relações de dependência . Já nas gramáticas de constituintes (ou de estrutura 
sintagmática), os elementos que se conectam para compor a estrutura da frase são 
constituintes . Constituintes são grupos de palavras que possuem alguma hierarquia (por 
exemplo, “vaca amarela” forma o si ntagma nominal – e um sintagma nominal é um 
constituinte –  “vaca amarela”, que por sua vez pode ser decomposto nos constituintes 
menores “vaca” e “amarela”).  
Considerando a frase “A vaca amarela fugiu”, na abordagem de dependências é apenas a 
palavra vaca  que se relaciona – iato é, que “depende de” – fugiu . As palavras a e amarela , 
por sua vez, dependem de vaca . Na abordagem sintagmática, o que se relaciona com fugiu  
é todo o sintagma a vaca amarela , que por sua vez tem como núcleo a palavra vaca .  
Outra d iferença importante diz respeito à estrutura básica da oração: na gramática de 
constituintes  (de inspiração gerativa, sobretudo) , a estrutura é uma divisão binária entre 
sujeito (sintagma nominal  - SN) e predicado (sintagma verbal - SV), o que não acontece  
na gramática de dependências. Podemos usar uma representação de colchetes para 
mostrar essa diferença , e a divisão binária está à esquerda :  
[Eu] [vi a vaca amarela]    [Eu] [vi] [a vaca amarela]  
 
Nos últimos anos, a Linguística Computacional impulsionou o desenvolvimento de 
teorias baseadas na dependência. No entanto, a noção de dependências entre unidades 
gramaticais existe desde as primeiras gramáticas registradas, e se encontra, por exemplo, 
na obra de Paṇini, gramático indiano nascido em 520 a.C. . As gramáticas de dependência 
modernas, por sua vez , começam principalmente com o trabalho do linguista francês 
Lucien Tesnière  (Percival, 1990) .  
A distinção entre gramáticas de dependência e de estrutura sintagmática deriva em grande 
parte da divisão inicial  da oração. Tesnière, no entanto, argumentou contra a divisão 
binária, e posicion ou o verbo como a raiz de todas as estruturas oracionais: seu 
entendimento era de que a divisão sujeito -predicado deriva da lógica aristotélica e não 
faz sentido na Linguística. Seu principal trabalho, Éléments de syntaxe estruturale , 
publicado postumamente em 1959, nunca foi traduzido para o português, e só 
recentemente foi traduzido para o inglês.  

62 
 De volta à diferença entre formalismos e abordagens, podemos ter uma á rvore (um 
formalismo) de constituintes, assim como uma árvore de dependências. E podemos ainda 
ter uma árvore de constituintes que siga ou não a exigência de divisões binárias (exigência 
de modelos associados à abordagem gerativa).  
Como exemplo de treebank s sintagmáticos, temos o Penn Trebank (de língua inglesa) e 
o corpus TychoBrahe (corpus diacrônico de língua portuguesa); como exemplo de 
treebanks de dependência , temos todos os corpora do já mencionado projeto Universal 
Dependencies , que atualmente conta  com mais de 150 treebanks  para as mais variadas 
línguas. O projeto Floresta Sintá(c)tica, pioneiro na construção de treebanks  para a língua 
portuguesa, disponibiliza seus treebanks  em ambos os formatos: árvores sintagmáticas e 
de dependência.  
O quadro ab aixo traz exemplos de análise s (simplificadas)  sintagmática s (esquerda) e 
depende nciais  (direita) . 
 
 
 
  
 
 
    
 
  
 
 
 
  
 


63 
 Desafios linguísticos  
Do mesmo modo que a anotação de POS, a anotação sintática envolve a tomada de uma 
série de decisões linguísticas nem sempre abordadas  em gramáticas  ou teorias 
linguísticas  com o detalhe necessário para a anotação. A frase abaixo ilustra desafios 
simples, e bastante frequente em textos de jornal:  
No realismo moderno de John Cassavetes ( 1929 -89), o ator construía o 
personagem em tempo real diante da câmera, baseado no improviso.  
O trecho sublinhado traz desafios de várias naturezas : quantas palavras temos  em 1929 -
89? Se a escolha for por duas palavras (dois numerais, o que faz sentido, por um lado, 
mas contraria o conceito de palavra gráfica), como se relacionam sintaticamente? 
Estamos diante de uma coordenação? E qual a  relação entre a data e John Cassavetes : 
estamo s diante de um aposto, de um modificador de um nome? Assumir a elipse de uma 
oração (1929 -89 = cujo período de vida foi de 1929 a 1989  ou que nasceu em 19929 e 
morreu em 1989 ) é uma opção delicada para o processamento automático, porque envolve 
analisar el ementos que não estão na frase.   
Em frases  com locuções conjuntivas do tipo quer dizer , por exemplo , isto é , ou seja ... 
estamos diante de coordenação ou subordinação ? A gramática de Mario Vilela e Ingedore 
Koch  (2001)  os considera co nectores discursivos, o  que faz  muito  sentido, mas 
precisamos decidir como exatamente formalizar isto na análise sintática  – ou já  estamos 
na análise discursiva?  
Durante muito tempo, linguistas computacionais acharam que era desnecessário ser muito 
explícito sobre os alvos dos sistemas de análise automática, uma vez que nossa herança 
cultural (linguística) compartilhada já está estabelecida  há muito tempo.  Mas o equívoco 
desta ideia, nos conta  Sampson  (2000) , foi verificado ex perimentalmente . Em 1991, em 
um workshop da já referida ACL , pesquisadores de nove grupos de pesquisa diferentes 
receberam um mesmo conjunto de frases (em inglês)  para analisar (conforme as análises 
que seus grupos de pesquisa considerariam corretas ). Não eram frases especialmente 
complicadas ou confusas, mas eram frases extraídas de corpus. Para simplificar, a 
comparaç ão entre as análises não leva ria em conta os rótulos dos constituintes, apenas a 
estrutura (a forma) das árvores , ou seja, a ideia era  verificar apenas se os pesquisadores 
identificar iam as mesmas subsequências de palavras como constituintes gramaticais, sem 
levar em conta a classificação desses  constituintes. Para uma grande surpresa  na época , o 
nível de concordância entre as análises f oi extremamente baixo. Especificamente, apenas 
as duas subsequências marcadas por colchetes (abaixo) foram identificadas como 
constituintes por todos os nove participantes (e os resultados para outros casos foram na 
mesma direção ): 
One of those capital -gains ventu res, in fact, has saddled him [with [ Gore Court]].  
Um desses empreendimentos de g anhos de capital, na verdade, o sobrecarregou   
[com [Gore Court ]]. 
Se especialistas concordaram tão pouco entre si, e, consequentemente, sobre o que os 
sistemas deveriam fazer  na análise automática , isto sinaliza a necessidade de alguma 
discussão linguística – explícita e pública – sobre estas questões, e não apenas discussões 

64 
 técnica s ou puramente computacionais. Na seção @ Sobre avaliação sintática  abordo a 
avali ação automática da anotação sintática.  
Corpora em português  
Diversos grupos no Brasil estão trabalhando para a criação de treebanks  de qualidade, e 
em breve devemos ter um cenário favorável para a anotação sintática. Atualmente, temos 
o corpus Bosque, que originalmente integra o projeto Floresta Sintá(c)tica, hoje está 
disponível em diferentes formatos e conforme diferentes modelos gramaticais, e o 
também já referido corpus do projeto TychoBrahe, de português diacrônico.  
6.3 Papéis semânticos  
A anotaçã o de papéis semânticos corresponde à formalização de um fenômeno linguístico  
não tão familiar como classes gramaticais ou funções sintáticas: os papéis  semânticos . 
Papéis semânticos atribuem sentido aos constituintes sintáticos , e podem ser entendidos 
como  a contraparte semântica dos argumentos sintáticos , como “cara & coroa ”. É a 
análise de papéis  semânticos que indica que embora  
(1) O lobo mordeu Pedro  
(2) Pedro foi mordido pelo lobo  
tenham  estruturas sintáticas diferentes, veiculam a mesm a ideia . Isto porque e m ambas as 
frases lobo recebe o papel semântico de agente , apesar de ser sujeito em (1) e agente da 
passiva em (2). Do mesmo modo, considerando  
(3) Ela abriu a porta com esta chave  
(4) Esta chave abriu a porta  
chave  é o instrumento  (papel semântico) com que se abre a porta  em ambas as frases , mas 
em (3) exerce a função  sintática  de um adjunto adverbial e, em (4), de sujeito.  
No mundo PLN, a identificação de papéis  semânticos aparece como facilitadora de tarefas 
como a resposta automática a perguntas e extração de informação. Na Linguística , a 
primeira proposta de papéis  semânticos vem de Charles Fillmore e seu projeto  de uma 
Gramática de Casos. Inicialmente , Fillmo re buscou capturar as relações entre padrões 
formais  que, apesar de  diferentes , indicavam  os mesmos significados, como a relação 
entre orações na voz ativa e na voz passiva.  Desde então, surgiram algumas propostas de 
papéis  semânticos – Fillmore (1968), por exemplo, propõe 9 papéis ( agente, 
experimentador, instrumento, objeto, fonte, objetivo, localização, tempo e caminho ). 
A anotação de um corpus com papéis semântic os apresenta dois principais desafios: o 
primeiro é a alta dependência de uma análise sintática  de qualidade , levada a cabo em 
uma etapa anterior ; o segundo desafio é a escolha de quais papéis  usar na anotação, isto 
é, a escolha do  tagset. Diferentemente das anotações anteriores, não há uma tradição 
milenar, ou ensino escolar , subjacente aos papéis  semânticos capaz de disfarçá -los de 
senso comum ou de “levemente teóricos”.  
PropBank é o nome d e um  corpus que contém a anotação de proposições  (novamente 
elas)  e seus argumentos . No Prop Bank, o conceito de proposição  é tomado da semântica 
de Frames proposta por Fillmore  (1968 ), na qual uma proposição é um conjunto de 

65 
 relações entre nomes e verbos, sem informação relativa a tempo, modo, aspecto, negação 
ou modificadores modais . 
O primeiro PropBank foi criado para o inglês, em 2005. Ele é um pedaço do  corpus  
PennTreebank  – ou seja, uma porção de corpus que já foi analisado sintaticamente – , 
com uma camada adicional de papéis  semânticos. Para escolher  o tagset usado na 
anotação  do PropBank , o critério de seleção foi o de convergência, isto é, foram usados  
os papéis  semânticos  comuns às diferentes propostas, o que levou a 5 etiquetas  diferentes 
para os argumentos (Arg0 -Arg5) e 18  etiquetas  para modificadores , padronizadas  “na 
medida do possível” , segundo os autores  (a página eletr ônica deste livro , em @Ponteiros , 
remete ao projeto de anotação do PropBank, que contém a explicação das etiquetas) . São 
chamados argumentos de verbos  os complementos considerados “essenciais”, como os 
objetos, e chamados modificadores  os complementos considerados “acessórios”, como 
os adjuntos adverbiais.  
No PropBank, o s argumentos são  
– Arg0 = agent e 
– Arg1 = pa ciente/tema /experienciador  
– Arg2 = instrument o/beneficiário/ atribut o 
– Arg3 = ponto de partida / atributo  
– Arg4 = ponto de chegada  
– ArgM = modifi cador  
 
Os 18 modificadores  (ArgM) , por sua vez, indica m desde valores tradicionalmente 
associados aos ad verbi os, como causa, tempo, lugar, modo, negação  a elementos como 
discurso direto . Na frase  
• Mês passado, Ana comprou um apartamento à vista.  
Temos os seguintes papéis : 
– Arg0: Ana 
– Arg1: um apartamento  
– ArgM -MNR  (maneira) : à vista  
– ArgM -TMP (tempo) : mês passado . 
Associando agora este tipo de informação  às anteriores (pos e sintaxe), temos o 
seguinte : 
Id palavra  lema  pos Dependência  
sintática  Tipo de relação 
sintática  Anotação  
PropBank  
1 Mês mês SUBST  5 adj adv  ArgM -TMP  
2 passado  passado  ADJ 1 adj adn  
3 , , PUNCT  1 punct   
4 Ana Ama  PROP  5 sujeito  Arg0  
5 comprou  comprar  VERB O 0 núcleo da or.  V* 
6 um um ART  7 adj adn  Arg1  
7 apartamento  apartamento  SUBST  5 objeto 
8 a a PREP  10 caso  
ArgM -MNR  9 a o ART  10 adj adn  
10 vista  vista  SUBST  5 adj adv  
 

66 
 A última coluna da figura do quadro acima não contém os  papéis  semânticos 
propriamente, apenas informações genéricas como Arg0 ou Arg1. Esta opção pela 
generalidade  é uma especificação do modelo  PropBank e não da anotação de papéis  
semânticos  em geral . Ou seja, o que a anotação do PropBank faz é atrel ar, à análise 
sintática – mais especificamente, aos constituintes identificados na  análise sintática  – 
categorias genéricas que vão de Arg0 a Arg4, além dos ArgM . Para saber que valor terá 
o Arg1 ( paciente , experienciador  ou tema ) é preciso consultar o frame   (ver seção 4.2) do 
verbo ao  qual os argumentos se relacionam, ou seja, é preciso consultar o frame do verbo 
comprar . 
Consultar onde? Em um outro recurso, a VerbNet. Como já mencionado  no capítulo 4, a 
VerbNet contém grupos de verbos associados a suas propriedades sintáticas e semânticas.  
O verbo comprar  tem as seguintes informações na VerbNet:  
      Arg0 -Agente:  comprador   
      Arg1 -tema:  coisa comprada  
      Arg2 -fonte:  vendedor  
      Arg3 -valor : preço  pago   
     Arg4 -beneficiário:  beneficiário   
E um verbo como abrir  teria 
      Arg0 -Agente:  “abridor”   
      Arg1 -tema  ou paciente : coisa aberta  
      Arg2 -maneira : instrumento  
      Arg3 - beneficiário:  destinatário /alvo  
O final do processo de alinhamento entre PropBank e VerbNet produz algo como  
(5) Mês passado, ela[agente] comprou  um apartamento[tema] à vista.  
Mais alguns exemplos. Seguindo a anotação  apenas  do Propbank, teríamos  
(6) Ela[Arg0] comprou  um apartamento[Arg1] para os seus netos[Arg4], por um milhão de 
reais[Arg3] . 
(7) A Costa Rica[Arg0] abriu  o mercado de telecomunicações[Arg1] para o capital 
privado[Arg3].  
(8) Ela[Arg0] abriu  a janela[Arg1] com a alavanca[Arg2] . 
(9) A alavanca[Arg2] abriu  a janela[Arg1] . 
(10) A janela[Arg1] abriu . 
A atribuição de papéis semânticos sinaliza que as frases (8) (9) e (10) têm sentidos 
relacionados, mesmo que os argumentos do verbo exerçam funções sintáticas distintas. 
No entanto, sem o alinhamento com a VerbNet  não temos informação sobre a “semântica” 
dos papéis argumentais . A seguir temos as mesmas frases, com os pap éis explicitados.  
(6) Ela[Arg0 :agente ] comprou  um apartamento[Arg1 :tema ] para os seus 
netos[Arg4 :beneficiário ], por um milhão de reais[Arg3 :valor ]. 
(7) A Costa Rica[Arg0:agente]  abriu  o mercado de telecomunicações[Arg1:tema]  para o 
capital privado[Arg3:beneficiário]  
(8) Ela [Arg0 :agente ]  abriu  a janela [Arg1 :tema ]  com a alavanca [Arg2 :instrumento ] 
(9) A alavanca [Arg2:instrumento]  abriu  a janela[Arg1 :tema ] 
(10) A janela[Arg1:tema]  abriu  

67 
  
Desafios linguísticos  
Assim como nos demais casos de anotação, a escolha por um determinado tipo de papel 
semântico durante a anotação pode não ser óbvia:  
(11) Moreira Alves também votou  a favor de Collor  em outros dois 
mandatos de segurança  , mas foi vencido   nos dois julgamentos .  
Qual modificador deve ser usado  no trecho sublinhado: tempo  (foi vencido quando?) ou 
local  (foi vencido onde?)? Seria ótimo poder escolher ambas as análises, mas a 
classificação múltipla não é uma opção de anotação do PropBank (Não ser possível no 
PropBan k não significa, obviamente, que não seja possível na anotação de papéis 
semânticos.). E, como veremos nas próxim as páginas , quanto mais dependente do sentido 
é a anotação, mais chances de interpretações variadas.  
Já deve ter ficado claro por que a anotação de papéis semânticos é dependente de uma 
análise sintática  de qualidade . E uma boa análise sintática, por sua vez, depende da 
identificação adequada das unidades linguísticas básicas, foco do pré -processamento. N as 
frases abaixo,  é important e que  por culpa  e abrir brechas  sejam compostas por duas 
unidades  cada, já que os verbos por e abrir  têm suas propriedades flexionais individuais 
preservadas ( ele põe a culpa ; eles põem a culpa ) . Por outro lado, para a análise do sentido , 
à qual se associa também a atribuição de papéis semânticos, é crucial que por a culpa  e 
abrir brechas  sejam considerados uma unidade.  
(12) A medida provisória [Arg0]   abre brechas  para contestações [Arg1].  
(13) Schumacher[Arg0]   põe culpa [Arg1]   em degrau[Arg2]  
Vale lembrar que não estamos diante de um  fato novo para os estudos linguísticos: é 
justamente esse limite frequentemente  impreciso entre léxico e gramática que motiva a 
abordagem da lexicogramática . No entanto, a formalização que a tarefa de anotação 
dema nda nos obriga a tomar uma decisão acerca de juntar ou separar as unidades . 
O PropBank não é o único recurso a tratar de papéis semânticos  para o português , ainda 
que seja bastante popular. O analisador PALAVRAS, mencionado no capítulo 2 , também 
realiza anotação de papéis semânticos. Para tanto, usa 51  categorias  (a informação sobre 
as etiquetas  (ou tagset)  está disponível na página eletrônica deste livro , na seção 
@Ponteiros ), e ofer ece um índice de acerto de 88% mas, por ter o seu próprio sistema de 
anotação e tagsets, é difícil comparar esse resultado  com os demais sistemas de anotação . 
Corpora  em português  
O único corpus padrão outro para o português com anotação de papeis semânticos é o 
PropBank.Br, cujo esquema de anot ação é bastante similar ao do projeto original a fim 
de facilitar o alinhamento multilíngue.  
6.4 Anotações semânticas  – e algumas considerações sobre o sentido  
 

68 
 Sob o rótulo anotação semântica  podemos agrupar diversos tipos de anotação, que têm 
em comum  a atribuição  de uma informação de natureza  semântica a uma palavra ou 
expressão , em contexto . 
Na anotação semântica  em sentido estrito , as classes  de anotação podem vir de classes 
genéricas , representativas de um campo semântico  (por exemplo, cor, ou emoção , ou 
doenças ), e nesse caso o resultado da anotação é a anotação de um campo sem ântico ; 
podem vir de um inventário de  sentidos  como as acepções de um dicionário  ou os synsets  
(conjunto de sinônimos)  de uma wordnet , vistos na seção 4.1 . 
A anotação semântica  é a que menos precis a de um conhecimento linguístico  teórico  
especializado quando  comparada às anotações anteriores, em que é necessário um 
conhecimento morfossintático profundo (anotação de pos e de sintaxe) ou de uma teoria 
(anotação de papéis semânticos). Ou seja, identificar, em contexto, se uma dada palavra 
se refere  a uma cor, ou a uma parte do corpo, a relações de parentesco,  a uma pessoa ou 
a uma organização  é algo que qualquer  falante nativo com boa capacidade de 
interpretação de texto  é capaz de fazer.  Mesmo assim, e mais uma vez, a anotação nos 
confronta com casos simples mas inesperado s, que a tornam uma tarefa pouco óbvia.  
Os exemplos  (1) e (2)  ilustram a anotação do campo semântico do corpo humano. 
Diferentemente de ( 1), no exemp lo (2) a presença da palavra orelha  não faz referência a 
uma parte do corpo, mas  a um tipo de sorriso.  É uma opção do esquema de  anotação 
decidir se a palavra receberá uma etiqueta relativa a parte s do corpo e, em caso positivo, 
se haverá alguma especificação para indicar que  não se trata de um uso convencional de 
orelha .  
(1) Lampião era cruel o bastante para, pessoalmente, arrancar olhos ou cortar 
línguas  e orelhas . 
(2) Milena era um sorriso só, de orelha  a orelha . 
Existem campos semânticos especialmente complicados, como o das emoções  e 
sentimentos . Neste caso, a dificuldade está menos na anotação  propriamente , e mais na 
própria definição do que seja uma emoção. Em um trabalho as pessoas deveriam 
selecion ar trechos de obras literárias  que contivessem emoção e anotar as emoções 
presentes nesses trechos, os autore s, que inicialmente previam trabalhar com espectro 
amplo de emoções, precisaram reduzir as classes de anotação a apenas duas, medo  e 
felicidade , porque a divergência nas anotações propostas foi tanta que inviabilizou o 
esquema de anotação original  (Heuser et al., 2016) . 
O segundo tipo de anotação semântica  – anotação de sentidos ( word senses ) – busca 
codificar no corpus o sentido de uma palavra, no contexto em que está inserida . Por isso, 
às vezes essa anotação é descrita como um  trabalho  de de sambiguação, visto que a tarefa 
propriamente consistiria em selecionar, dentre os vários sentidos possíveis de uma 
palavra, aquele invocado no contexto da frase. Tomando a palavra trabalho  sublinhada 
na frase anterior , a tarefa consiste em escolher um sentido dentre os vários listados em 
recursos externos, como uma wordnet. Como wordnet s do português ainda não são 100% 
confiáveis, e para que não se pense que a dificuldade está no recurso, e não na tarefa, 
podemos substituir por enquanto uma wordnet por um dicionário. A tarefa de anotação, 
portanto, consiste em escolher, dentre opções listadas no quadro abaixo, retiradas do 

69 
 dicionário Caldas -Aulete online, aquela adequada ao contexto.  Me parece que as 
acepções 1, 2, 3 e 14 são aceitáveis no contexto da frase, o que já é um problema se 
precisamos escolher apenas um sentido.  
 
 
 
 
 
 
 
 
 
Figura 1: Acepções da palavra trabalho  conforme dicionário.  
As figuras 2 e 3 a seguir referem -se aos synsets  (ver 3.1) que contém a palavra trabalho  
conforme a OpenWordNet -PT. 
 
Figura 2: Acepções da palavra trabalho  conforme a OpenWordNet -PT 
1. Emprego  da força  física  ou intelectual  para realizar  alguma  coisa  
2. Aplicação  dessas  forças  como  ocupação  profissional : Seu trabalho  é de gari. 
3. Local  onde  isso se realiza : Mora  longe  do trabalho . 
4. Esmero,  cuidado  que se emprega  na confecção  ou elaboração  de uma obra 
5. A confecção,  elaboração  ou composição  de uma obra 
6. Obra  realizada : Essa  cômoda  é um belo trabalho  de marcenaria.  
7. Grande  esforço;  TRABALHÃO;  TRABALHEIRA  
8. Exercício  para treino : A professora  passou  muito  trabalho  para casa.  
9. Ação  contínu a de uma força  da natureza  e seu efeito : O trabalho  do vento  resulta  na erosão  eólia.  
10. Med.  Fenômeno  orgânico  que se opera  no interior  dos tecidos  (trabalho  inflamatório;  trabalho  de 
cicatrização)  
11. Resultado  do funcionamento  de uma máquina,  um aparelho  etc.: o trabalho  de uma pá mecânica.  
12. Obrigação  ou responsabilidade;  DEVER;  ENCARGO : Seu trabalho  é protegê -lo do assédio  da 
imprensa.  
13. Econ.  Conjunto  das atividades  humanas  empregado  na produção  de bens : O capital  e o trabalho  são 
os pilares  da economia.  
14. Tarefa  a ser realizada : Contratou -o para um trabalho  temporário.  
15. Treinamento  durante  a semana  como  preparação  para o páreo  
 

70 
  
Figura 3: Acepções da palavra trabalho  conforme a OpenWordNet -PT 
Desafios linguísticos  
Para ilustrar os desafios da anotação semântica, retomo brevemente a experiência de 
construção do corpus SemCor . No fim dos anos 1980  e início dos 1990 , quando a 
WordNet se tornou um recurso popular no  PLN, uma equipe de pesquisadores  decidiu 
criar o  corpus  SemCor, um corpus  anotado semanticamente segundo os synsets  da 
WordNet. Nele, cada palavra do corpus ( apenas  substantivo s, verbo s e adjetivo s) estaria 
alinhada ao synset  adequado. Ao olhar retrospectivamente para o processo de criaç ão do 
SemCor, os autores admitem  que o trabalho de anotação  foi feito tendo como pano de 
fundo a  hipótese – reconhecida  agora como  claramente  ingênua  – de que a anotação 
semântica seria um trabalho simples de alinhamento entre uma palavra e um synset  (Baker 
et al., 2017) . No entanto, continuam os autores , a experiência mostrou que o trabalho não 
tinha o menor traço  de simplicidade: em vários casos, nenhum dos sentidos disponíveis 
para o alinhamento parecia se encaixar, ou o sentido da palavra em questã o era capturado 
por mais de um synset  da WordNet.  A conclusão geral a que os autores  chegam é  que 
frequentemente os sentidos das palavras resistem a uma representação discreta e estável 
como a que os dicionários supõem.  
Ainda durante a criação do SemCor, foi feito um outro estudo em que foram comparadas 
as ano tações (alinhamentos) feitas por  alunos  e aquelas feitas por  lexicógrafos e linguistas 
treinados  (Fellbaum et al., 1997) . Os resultados indicaram  
 concordância geral em torno de 70%  na atribuição dos sentidos;  
 concordância mais alta quando as palavras eram substantivos (em oposição a 
adjetivos e verbos);  
 diminuição da concordância relacionada ao aumento da polissemia;  
 preferência por escolher os primeiros synsets  apres entados , isto é,  como se, 
considerando as figuras 2 e 3 com a palavra trabalho , houvesse uma 
preferência  a escolher os synsets  apresentados logo na primeira tela (fig. 2), 
embora haja 4 páginas /telas de synsets .  
Anos depois, a  mesma equipe  fez u m outro estudo . Os autores escolheram uma amostra  
com dez palavras razoavelmente frequentes,  mas moderadamente polissêmicas , para 
serem anotadas/alinhadas com a WordNet . O quadro 1  apresenta as palavras  utilizadas , 


71 
 bem como o número de sentidos que recebem na WordNet  (em inglês)  e a frequência no 
corpus.  Como as palavras estão descontextualizadas , a tradução é apenas uma 
aproximação. O número de sentidos associados a cada uma sinaliza  também  a dificuldade 
de tradução sem o contexto.  
Palavra  POS N. sentidos  N. ocorrê ncias  
fair (~justo)  Adj 10 463 
long (~longo)  Adj 9 2706  
quiet (~quieto)  Adj 6 244 
land (~terra)  Subst  11 1288  
time (~tempo)  Subst  10 21790  
work (~trabalho)  Subst  7 5780  
know (~saber)  Verbo  11 10334  
say (~dizer)  Verbo  11 20372  
show (~mostrar)  Verbo  12 11877  
tell (~dizer)  Verbo  8 4799  
Quadro 1: Lista de palavras usadas no estudo de Passoneu et al., 2009.  
Novamente, os resultados evidenciaram a variação nas interpretações sobre o sentido e, 
novamente, verbos levaram às maiores divergências . Para cada classe, as palavras que 
mais variaram foram long, work  e tell. 
Os autores concluíram que os seguintes fatores contribuem para a baixa concordância:  
(i) quanto maior especificidade do contexto em que a palavra está inserida , maior 
concordância;  
(ii) sentidos mais concretos levam a uma maior concordância que sentidos 
abstratos;  
(iii) um inventário de sentidos (um conjunto de synsets ) com elementos parecidos 
entre si leva a uma menor concordância.  
Considerando o ponto (i), sabendo que a  palavra longo  possui  os seguintes sentidos 
(dentre outros):   
1. extensão esp acial;  
2. extensão temporal ;  
3. maior que o normal ou que o necessário   
 
a frase (a) leva a uma m aior concordância que a frase (b)2: 
a) Durante 18 longos meses, Michael não conseguiu encontrar um emprego.  
b) Depois de enviar o manuscrito, meu editor sugeriu uma série de cortes para agilizar o que 
já era um longo  e complicado capítulo sobre as ideias de Brian.  
Para ilustrar os itens (ii) e (iii) vamos voltar à palavra trabalho , considerando as acepções  
de dicionário (figura 1). Com relação a (ii), podemos imaginar que é mais difícil uma 
concordância a respeito dos sentidos de trabalho  em (c) que em ( d) 
c) Nosso trabalho  foi relevante para a população da cidade  
d) O trabalho  mais famoso de Van Gogh é O Comedor de Batatas  
Com relação ao ponto  (iii), em uma frase como “Eu trabalho como professora”, é 
compreensível uma divergência entre as acepções 2 e 12  .  
Ainda tentando entender por que este tipo de análise é tão difícil (dif ícil quanto à 
concordância nas interpretações) mais um estudo foi feito. Dessa vez, muitos anotadores 
                                                           
2 As frases são traduções das frases usadas  no estudo  original.  

72 
 treinados deveriam anotar as mesmas palavras , nas mesmas frases , para ver se pelo menos 
nessas condições seria possível obter um consenso em torno dos sentidos escolhidos. Os 
resultados sinalizaram que ainda assim a variação entre os anotadores foi bem grande, 
bem maior do que o previsto  (Baker et al., 2017) .  
 
Para uma boa parte da comunidade de PLN e para o  senso comum, os significados das 
palavras são, de certo modo, o que o dicionário diz. O fato de dicionários representarem 
os significados de maneira objetiva, estável e discreta (o que se manifesta nas acepções 
separadas por números) faz parecer que os si gnificados se organizam naturalmente dessa 
maneira. Nos dicionários, porém, as palavras têm os significados que têm não porque 
esses sentidos lhes sejam intrínsecos , mas porque alguém (um conjunto de lexicógrafos) 
assim o decidiu. E assim o decidiu  porque precisavam  delimitar os significados  - 
lexicógrafos são obrigados a descrever palavras como se todas elas tivessem um conjunto 
de significados discretos, que não se sobrepõem, porque é assim  que dicionários 
funcionam , e dicionários são objetos de mercado. Profissionais da Lexicografia  estão 
conscientes de que d ecidir se agrupam ou separam os significados de uma palavra é um 
trabalho inevitavelmente subjetivo  (interpretativo) , e f requentemente  a decisão 
alternativa  a respeito de agrupar ou separar  – a decisã o que não foi tomada - seria 
igualmente válida.  Uma comparação entre dicionários nos mostra exatamente isso , e as 
divergências na anotação de sentidos também.   
Com a anotação, a dificuldade na identificação dos sentidos das palavras como unidades 
discretas não é apenas impressão, ela pode ser medida: todos os trabalhos com anotações 
semânticas que buscam a desambiguação de sentidos costumam ter as mais baixas 
medidas de concordância entre anotadores , em torno de 70%. Na análise sin tática, tipo de 
anotação aparentemente mais complexa,  os números são superiores a 90%.  
Levando em consideração  essas limitações, podemos tentar  um outro ângulo : o sentido 
não é uma propriedade intrínseca das palavras, mas uma abstração  que só irá se 
concretizar no uso – traduzido no PLN como ocorrências da palavra em um corpus – e 
enquanto decorrência de algum objetivo  ou tarefa  (no caso da Lexicografia, o objetivo é 
o de fazer dicionários) . No PLN, c orpora diferentes  e usos diferentes  irão levar a 
representações diferentes de significados, e isso é o que vemos acontecer nos vetores de 
palavras contextuais, apresentados na próxima seção.  
Corpora em português  
Para a língua portuguesa, não há nenhum corpus no estilo do SemCor. Em 2015, participei 
de estudo piloto  com o objetivo de medir tanto o grau de dificuldade que seria enfrentado 
na anotação de um corpus com synsets da OpenWordNet -PT quanto à correção de seus 
synsets (Freitas et al., 2015).  Selecionamos 30 frases do corpus Bosque, anotamos (isto 
é alinhamos) todos os substantivos das frases segundo os synsets  da OpenWordNet.PT. 
Apenas substantivos foram considerados justamente para facilitar a tarefa, já que verbos  
e adjetivos são mais  polissêmicos. Diferente da proposta do SemCor, permitimos uma 
classificação múltipla , e mais de um synset  poderia ser escolhido . O exercício contou com 
a colaboração de alunos de graduação em Letras, e relato algumas de nossas impressões:  
Em 20% dos casos não foi possível escolher um synset  adequado, m as nem sempre a 
impossibilidade era decorrência da inexistência do synset , mas de problemas nas etapas 

73 
 anteriores de  pré-processamento do corpus, co mo tokenização e lematização. Por 
exemplo, há palavras cujos sentidos variam ligeiramente quando estão no si ngular ou no 
plural: recursos  pode ser o plural de recurso , mas com o sentido de bens, riquezas , 
recursos financeiros , será usado sempre no plural . Além disso, quando a tokenização é 
feita palavra por palavra, é difícil apontar para o synset  adequado se ele for composto por 
uma unidade multipalavra.  Alguns desses erros estão no quadro abaixo . 
Palavra  Contexto  
troca  em troca  
realidade  na realidade  
cadeia  transmissão em cadeia nacional  
carteira  carteira de títulos  
ferro  a ferro e fogo  
Exemplos de palavras que não puderam  
ser alinhadas com a OpenWordNet  
 
Com relação  à anotação de campos semânticos, a língua portuguesa conta com os corpora 
do projeto AC/DC (e a sigla significa Acesso a Corpos/Disponibilização de Corpos), 
criado e mantido pela Linguateca. O material contém anotação relativa aos verbos do 
dizer/discurso relatado, campo semântico das cores, do corpo, doenças, relaç ões 
familiares, locais, sentimentos e predicações humanas. Nem todos os corpora contém 
todos os tipos de anotação. O material foi anotado utilizando regras linguísticas  e léxicos , 
e está parcialmente revisto . Como se trata de um acervo dinâmico, novas anotações 
(novos campos semânticos) podem ser incorporadas  (e lembro que o endereço eletrônico 
do AC/DC, bem como dos demais recursos mencionados, estão na página eletrônica , em 
@Ponteiros ).  
 
6.4.1. Palavras viram números – vetores de palavras   
 
A dificuldade de lidar com os significados como unidades discretas e estáveis  deixa aberto 
o caminho para que se invista em formas alternativas de lidar com o sentido, como é o 
caso da abordagem dos vetores de palavras ( word embeddings ). A ideia subjacente é que 
palavras que participam de contextos linguísticos similares tendem a s er similares. Por 
exemplo, carro  e bicicleta  estão próximos porque aparecem em contextos como “fui para 
o trabalho de carro” e “vou para o trabalho de bicicleta”.   
Esta maneira de olhar para as palavras não é nova nos estudos da linguagem, pelo 
contrário (Wittgenstein, 1953; Harris, 1954; Firth, 1957). Na transposição para o PLN, 
uma palavra é representada como um vetor – e imagine um vetor como uma lista de 
números , como na figura abaixo . Cada posição nessa lista de números, que no exemplo 
tem 5 posições (“locomoção”, “lazer”, “sustentável”, “comestível” e “quente”), 
corresponde a uma dimensão.  
 
 
 
 
Locomoção  
Lazer  
Sustentável  
Comestível  
Quente  
 
1 
 0 
 0 
 0 
 0 
 
1 
 1 
 1 
 0 
 0 
 
0 
 1 
 0 
 1 
 0 

74 
  
carro  
bicicleta  
sorvete  
 
Cada  palavra é representada por uma lista que tem lugar para 5 números diferentes, isto 
é, valores para 5 dimensões, e cada palavra tem uma combinação única desses 5 números, 
levando em conta as dimensões “locomoção”, “lazer”, “sustentável”, “comestível” e 
“quente” :  
carro [1,0,0,0,0]  bicicleta [1,1,1,0,0]  sorvete [0,1,0,1,0]  
Continuando com a figura, vemos que bicicleta  está próxima de carro  na dimensão 
“locomoção”, e próxima de sorvete  na dimensão “lazer”. Sorvete  e carro, por sua vez, 
estão mais distantes – distantes conforme as 5 características elencadas, é bom lembrar.  
Assim, quando tratamos de vetores de palavras, o primeiro p asso é entender que cada 
palavra  é representada como um conjunto de números atribuídos em função das palavras 
que co -ocorrem com ela e das dimensões,  e não como uma entrada de dicionário ou como 
um conceito. A transformação em números facilita o processamento pelos computadores.   
Algumas perguntas devem ter surgido aqui:  
De onde vêm esses números? O que significam? E de onde vêm as dimensões? Quem 
determina quais e quantas são as dimensões? Por que, no exemplo, foram usadas as 
dimensões “locomoção”, “lazer”, “sustentável”, “comestível” e “quente”? Afinal, 
sabemos que as pa lavras têm muitas dimensões e podem ser parecidas em várias delas:  
forma ( engenheiro ; motoqueiro ; cinzeiro  – esfarelar ; cantar ; dançar ); sentido ( cerveja ; 
breja ; gelada ); polaridade ( odiar , doença ; pesadelo  -  comemorar ; saúde ; festa ), dentre 
muitas outra s. 
O que cada dimensão codifica (ainda) não sabemos. Trata -se de uma representação que 
foi aprendida pelas máquinas a partir dos dados  – e esta é também uma das críticas que 
se faz este tipo de abordagem, como vimos na seção 2.1.   
O que sabemos com certez a é que quanto mais dimensões, maior o trabalho no 
processamento automático. Há representações que usam 50 dimensões, representações 
que usam 100, representações que usam 300 dimensões...  
Abaixo está um exemplo da representação vetorial da palavra rei, con siderando um vetor 
de 50 dimensões, e por isso temos 50 números (em breve falaremos sobre a origem desses 
números):  
[0.50451 , 0.68607 , -0.59517 , -0.022801, 0.60046 , -0.13498 , -0.08813 , 0.47377 , -
0.61798 , -0.31012 , -0.076666, 1.493 , -0.034189, -0.98173 , 0.68229 , 0.81722 , -
0.51874 , -0.31503 , -0.55809 , 0.66421 , 0.1961 , -0.13495 , -0.11476 , -0.30344 , 0.41177 
, -2.223 , -1.0756 , -1.0783 , -0.34354 , 0.33505 , 1.9927 , -0.04234 , -0.64319 , 0.71125 
, 0.49159 , 0.16754 , 0.34344 , -0.25663 , -0.8523 , 0.1661 , 0.40102 , 1.1685 , -1.0137 , 
-0.21585 , -0.15155 , 0.78321 , -0.91241 , -1.6106 , -0.64426 , -0.51042 ]  

75 
 A lista acima nada nos informa relativamente à palavra rei. Para facilitar o entendimento, 
se transformamos cada número em uma cor, e  comparamos os vetores das palavras rei, 
rainha , homem , mulher , garota , garoto  e água , teríamos o seguinte  (a figura é 
originalmente colorida e foi retirada de Alammar, J. , (2019) . Na página eletrônica do 
livro, em @ Palavras viram número s, é possível ver a  imagem colorida, o que facilita a 
análise ): 
 
- mulher  e garota  são semelhantes em muitas posições; assim como homem  e garot o; 
- garoto  e garota  também têm semelhanças entre si, mas que não são compartilhadas com 
mulher  ou homem . Podemos especular que a dimensão das semelhanças codifica algum 
tipo de juventude ... 
- rei e rainha  também são semelhantes entre si em algumas posições, e distintos de todos 
os outros. Podemos especular que a semelhança codifica uma concepção vaga de 
realeza... 
- Há uma coluna escura (em vermelh o, no original) idêntica  em todas as palavras, 
indicando que ao longo de uma dimensão (ao longo desta coluna escura ), as palavras são 
semelhantes. Não sabemos para que serve cada dimensão, e podemos especular que a  
coluna escura  talvez indique algo relativo ao caráter nominal das palavras representadas...  
 - Exceto pela palavra água , todas as outras representam pessoas . Há uma outra coluna 
escura (em azul escuro, no original)  idêntica em todas as palavras, exceto em água . É 
possível que esta coluna tenha a ver com isso.  
Quanto mais dimensões, mais pulverizada (ou distribuída) é a representação da palavra. 
Voltando à figura 1, podemos imaginar que ao invés de 5, teríamos 8 dime nsões, com 
"locomoção" se distribuindo entre "locomoção terrestre"; "locomoção aérea" e 
"locomoção aquática".  


76 
 Como já foi dito, o que cada dimensão contém será aprendido automaticamente , em 
função do número de dimensões que for estabelecido. E quem estabe lece o número de 
dimensões?   
Até o momento, não há pesquisas que demonstrem uma regra clara para a  escolha da 
quantidade de dimensões; a decisão tem sido baseada na experiência, a partir do teste com 
diferentes números de dimensões. Daí que outra crítica a  este tipo de abordagem é que a 
análise voltada para a quantidade de dimensões dos vetores de palavras não tem recebido 
atenção suficiente.  
E de onde vêm os números que estão em cada uma das dimensões? Voltando aos 50 
números que caracterizam a palavra rei: de onde surgiram?  
Cada um dos números, de cada uma das dimensões, é o resultado de cálculos matemáticos 
que levam em conta as palavras que co -ocorrem com a palavra alvo (que co -ocorrem com 
rei) em um corpus. Este cálculo, por sua vez, é fruto de uma sér ie procedimentos – e é 
quanto aos tipos de procedimentos que as diferentes maneiras de construir de vetores irão 
variar: tamanho do contexto; quantas palavras à direita, ou à esquerda, ou em ambas as 
direções... Em comum, a necessidade de um corpus grande,  sobre o qual serão feitos os 
cálculos.  
Por outro lado, sabemos que uma mesma palavra pode aparecer em contextos diferentes 
– desde contextos completamente distintos, nos casos de homonímia como banco  e 
manga , até contextos ligeiramente diferentes, como os  exemplos de trabalho , que já 
vimos.  
Aliás, é importante notar que, diferentemente do que supõe o senso comum, os casos de 
banco  ou manga , apesar de fartamente citados como exemplos de ambiguidade, estão 
longe de ser prototípicos. Pelo contrário, são raros  os casos em que dois sentidos se 
apresentam tão claramente distintos . O mais comum são casos como a  palavra trabalho , 
em que os sentidos se cruzam e sobrepõem, como vimos na seção sobre anotação 
semântica . Voltando ao nosso exemplo: será que rei do gado, rei da cocada preta  e rei 
da Espanha  podem ser representados pelo mesmo vetor rei? Além disso, rei da cocada 
preta  nos lembra da noção de palavra  e do processo de tokenização , discutid os no capítulo 
5: faz sentido pensar (ou ca lcular) 3 ou 4  vetores para rei (da) cocada preta ? Ou estamos 
diante de uma unidade de sentido, e, portanto, de um vetor?  
Independentemente da noção de palavra e considerando os algoritmos mais complexos , 
cada uso das palavras terá um vetor diferente. Por isso, nesses casos falamos d e vetores 
contextuais  (contextual word embeddings ). Vetores produzidos a partir de conjuntos de 
dados diferentes levarão a representações diferentes.  
Relembro aqui a importância dos dados no PLN, já mencionada nas páginas iniciais. A 
pesquisadora Robyn Spe er comenta que, quando foi aplicar um algoritmo construído por 
ela para análise de sentimento baseado em vetores de palavras, percebe u que o algoritmo 
estava classificando restaurantes mexicanos como ruins. No entanto, esta avaliação 
negativa não encontrava respaldo nem na quantidade de estrelas usadas pelos usuários 
para avaliar os restaurantes, nem no texto das resenhas. Ela então des cobriu que o motivo 
da avaliação ruim era aquilo que o sistema havia aprendido a partir dos dados: o sistema 
aprendeu a palavra “mexicano” a partir dos textos da internet, e na internet (aliás, nos 

77 
 textos da internet  nos Estados Unidos) a palavra mexicano  semp re aparece associada à 
palavra ilegal , especialmente para associar imigrantes mexicanos  a imigrantes ilegais . 
Com isso, o sistema acabou aprendendo que mexicano  significa algo semelhante a ilegal  
e, portanto, deve significar algo ruim.  
 
Mas se a comple ta dependência de dados é um ponto fraco deste tipo de abordagem, é 
também o seu ponto forte, e é incontestável que a incorporação de informação de vetores 
tem levado a resultados melhores  em uma série de tarefas de PLN.  
Por ser dependente dos dados (das palavras de um corpus), a representação do significado 
será, sempre, provisória, posto que outros dados poderiam levar a outras representações.  
Um exemplo esclarecedor a esse respeito, e desconcertante, diz respeito aos trechos 
abaixo: duas traduções consa gradas  dos mesmos versos  (canto XI) da Odisseia , 
comentadas por Jorge Luis Borges no ensaio “As versões homéricas”:  
Tradução 1 (Pope, 1725)  Tradução 2 (Butler, 1900)  
Quando os deuses coroaram de conquista as 
armas, quando os soberbos muros de Troia 
fumegaram por terra, a Grécia, para 
recompensar as galhardas fadigas de seu 
soldado, cumulou sua armada de incontáveis 
despojos. Assim, grande em glória, voltou seguro 
do estrondo marcial, sem uma cicatriz hostil, e 
embora as lanças se fechassem à sua vol ta em 
tormentas de ferro, seu jogo inútil foi inocente de 
ferimentos.  
 Uma vez ocupada a cidade, ele pôde 
apanhar e embarcar sua parte de 
benefícios havidos, que era uma forte 
soma. Saiu sem um arranhão de toda 
essa perigosa campanha. Já se sabe: 
tudo está  em ter sorte.  
 
Quando pensamos no processo de tradução como transporte de significados entre língua 
A e língua B e, de maneira análoga, na representação computacional dos significados 
como o transporte (ou codificação) de significados entre conceitos  e algum tipo de 
representação semântica computacional (como wordnets, framenets, ou outros tipos de 
representação semântica), acreditamos ser o significado do texto original, no primeiro 
caso, e dos conceitos, no segundo caso, um objeto estável, transportável , de contornos 
claros.  No entanto, as traduções acima e os percalços na anotação de sentidos como 
aqueles relatados no projeto SemCor trazem dificuldades para esta visão do significado.  
As traduções 1 e 2 contêm discrepâncias incontestáveis, que não podem ser descritas 
como paráfrases ou variações estilísticas. Tais discrepâncias, como já dito, são um desafio 
para uma visão de significado como a exposta anteriormente: se não estamos diante de 
paráfrases ou sinonímias e se cada tradução traz significados dif erentes, como explicar 
que sejam, apesar de tão distintas, traduções não apenas aceitas, mas igualmente 
consagradas, de um mesmo original?  
Para sair de sse impasse, uma alternativa é compreender  significados e conceitos como 
objetos heterogêneos e instáveis, e não como objetos diferentes apenas na forma ( amor  e 
love) e equivalentes no sentido ou em termos de mesmas entidades mentais. Compreender 
línguas como sistemas dinâmicos indissociáveis de práticas e valores históricos e sociais . 
Deste outro ân gulo, deixamos de ver as palavras (e os textos) como repositório s de 

78 
 significados que se deslocam inalterados  pelo tempo e pelo espaço , mas como marcas de 
um significado que, ao se deslocar no tempo e no espaço , é compreendid o/interpretad o 
segundo contexto s sociohistóricos. Esta alternativa  explica porque interpretações de um 
mesmo texto podem variar, como as diferentes versões da Bíblia  e os versos hom éricos, 
e ajuda a entender por que podemos prescindir de uma representação estável e bem 
delimitada do sig nificado e ainda assim (ou por isso mesmo) ter bons resultados em uma 
série de tarefas, inclusive a tradução automática . Os métodos de representação de palavras 
por meio de vetores contextuais, com a sua dependência dos dados vindos de grandes 
corpora, dão  corpo a este caráter dinâmico próprio dos sentidos (codificam a “estabilidade 
provisória” dos sentidos).   
Mas esta posição não deve ser vista como defendendo a irrelevância de maneiras 
convencionais de representar o sentido. Seria ingenuidade negar a utilidade dos 
dicionários, do mesmo modo que de recursos ao estilo wordnet. São maneiras diferentes 
de lidar com o significado : de um lado, abordagens baseadas em dados; de outro, em 
conhecimento.  
Podemos agora retomar alguns pontos dos capítulos 2 e 5, e fechar o ciclo da geração de 
modelos de língua. A etapa de tokenização transforma o texto em  “unidades de trabalho” 
(palavras, caracteres, subpalavras). Estas unidades são transformadas em vetores, e esses 
vetores serão processados pelas redes neurais . 
6.5 REM  (ou NER) : anotação de entidades  
A anotação de entidades mencionadas (em inglês named entities ) é bem popular  no PLN . 
A sigla NER (em inglês) ou REM  (em português) refere -se à tarefa, e o R vem de 
reconhecimento  (recognition ): Reconhecimento de Entidades Mencionadas  (Named 
Entities Recognition ). Entidades são elementos nominais relevantes para uma área de 
conhecimento ou tarefa.  
A tarefa de REM consiste  em duas tarefas relacionadas: identificar  algo como em 
entidade e classificar  conforme o contexto, isto é, anotar esta entida de de acordo com 
categorias pré -definidas como PESSOA , LUGAR , ORGANIZAÇÃO , TEMPO , QUANTIDADE  
etc. Por isso, o  reconhecimento de entidades  é considerado  a primeira etapa do 
processamento semântico de textos : 
(1) A partida, originalmente marcada para esta noite em [Quito] LOCAL, no [Equador] LOCAL, será 
realizada na próxima sexta, em [Assunção] LOCAL, no [Paraguai] LOCAL. 
(2) Confira os convocados da [Seleção Brasileira] ORG para jogos com [Equador] ORG e 
[Paraguai] ORG. 
Origi nalmente, entidades referiam -se aos nomes próprios, mas a pista da maiúscula tem 
sido deixada de lado e qualquer substantivo incluído no conjunto de classes (o tagset ) 
pré-determinado pode ser anotado como entidade , como vemos no quadro a seguir , que 
apresenta um texto anotado com entidades do domínio da Música .  
 
 
 
 
 

79 
  
 
Quadro 1: Exemplo de anotação de e ntidades do domínio da Música  
 
Quanto mais “novidade” há no domínio que será anotado com entidades, mais trabalho 
será necessário para escolher  as categorias de anotação  (o tagset ) relevantes para o 
domínio . Na Biologia, genes  e processos  genéticos  podem ser entidades; no Direito, le is, 
atores e argumentos podem se r entidades. No quadro  1, que exemplifica uma anotação no 
domínio da Música , além de classes gerais como PESSOA , LUGAR e  TEMPO , há etiquetas 
específicas como GENERO e INSTRUMENTO , e poderia haver outras.  
 
Desafios linguísticos  
 
A anotação de entidades traz os seguintes desafios linguísticos:  
 
 Decidir o que deve ser anotado (identificar uma entidade)  
No exemplo do quadro 1, é poss ível questionar o motivo de  a palavra  biroscas  não ter 
sido anotad a, já que também é um lugar e pode fornecer informações interessantes – 
músicas nascidas em espaços de elite e nascidas em espaços populares, por exemplo. A 
pergunta ou tarefa que  motiva a anotação  é determinante na definição do que vai ser 
considerado entidade.  
Também é em função da motivação que palavras como flautista , compositor , pianista , e 
maestro  serão anotadas. Se forem anotadas, é preciso decidir se serão do tipo PESSOA , 
que já exi ste, ou formarão uma nova classe PROFISSÃO , que pode ser um tipo de PESSOA . 
O mesmo se aplica aos estilos de música: música popular , instrumental . Neste caso, será 
preciso decidir se integram a classe GENERO  ao lado de choro , xote etc. Ou seja, conforme 
as escolhas de anotação o texto do quadro 1 poderá ser anotado de algumas maneiras, 
todas corretas.  
 
 Classificar a entidade  O [choro ]GENERO , popularmente chamado de [chorinho ]GENERO , é um gênero de música 
popular e instrumental brasileira, que surgiu no [Rio de Janeiro ]LOCAL  em meados do [século 
XIX]TEMPO .  
Os primeiros conjuntos de [choro ]GENERO  surgiram por volta da [década de 1870 ]TEMPO , 
nascidos nas biroscas do bairro [Cidade Nov a]LOCAL  e nos quintais dos [subúrbios 
cariocas ]LOCAL . O flautista e compositor [Joaquim Antônio da Silva Calado ]PESSOA , os 
pianistas [Ernesto Nazareth ]PESSOA  e [Chiquinha Gonzaga ]PESSOA , e o maestro [Anacleto de 
Medeiros ]PESSOA  compuseram [quadrilhas ]GENERO , [polcas ]GENERO , [tangos ]GENERO , 
[maxixes ]GENERO , [xotes ]GENERO  e [marchas ]GENERO , estabelecendo os pilares do [choro ]GENERO  
e da música popular carioca da virada do [século XIX ]TEMPO  para o [século XX ]TEMPO.  
Herdeiro de toda essa tradição musical, [Pixinguinha ]PESSOA  consolidou o [choro ]GENERO  
como gênero musical  
, levando o virtuosismo na [flauta ]INSTRUMENTO  e aperfeiçoando a linguagem do contraponto 
com seu [saxofone ]INSTRUMENTO  e organizou inúmeros grupos musicais, tornando -se o maior 
compositor de [choro ]GENERO . 

80 
 Como vimos na anotação de sentido, a polissemia está presente  boa parte das palavras , e 
na anotação  de entidades  este traço é evidente. Os exemplos (1) e (2) no início da seção 
mostram que Equador  e Paraguai  podem ser LOCAL  ou ORGANIZACAO  conforme o 
contexto. Podemos (e devemos) discutir se Seleção Brasileira , Equador  e Paraguai , 
quando utilizados para fazer menção  aos times, são ORGANIZACAO  ou PESSOA  (os 
jogadores ).  Pode ainda ser decidido que é aceitável usar mais de uma classe 
simultaneamente , e todos  no exemplo  ficarão anotados como PESSOA|ORGANIZACAO . Na 
frase abaixo, podemos interpretar (e classificar) Brasil  como  LOCAL , PESSOA  ou amb os, 
já que uma leitura não exclui a outra – e se não fosse a necessidade imposta pela anotação 
de classificar considerando essas etiquetas, a dúvida  sobre  ser LOCAL ou PESSOA  nem 
existiria , uma vez que não impede ou diminui nossa compreensão da frase . Faz parte das 
decisões de anotação aceita r mais de uma classificação simultaneamente.  
Mais de 600 mil pessoas morreram na pandemia que atingiu o Brasil . 
    
 Segmentar a entidade  
Uma entidade também pode ser decomposta de maneiras diferentes. Decisões de 
segmentação dizem respeito a considerar entidades maiores ou entidades mais granulares, 
que correspondem a uma leitura mais composicional. No quadro 1 temos [década de 
1870] e [séc ulo XX]. Mas diferentes alternativas são aceitáveis , como vemos:  
 
[década de 1870]          [século XX]  
década de [1870]          século [XX]  
[década] [de] [1870]          [século] [XX]  
 
No trecho anotado abaixo, é discutível a classificação de Paraguais , mas se não fosse o 
plural dificilmente hesitaríamos. Q uanto à segmentação, as possibilidades abaixo são 
igualmente  aceitáveis  e às vezes  a cada  segmentação  corresponde  uma classificação 
diferente.  
 
[Estado] ORG de [São Paulo] ORG OU LOCAL  
[Secretaria de Educação] ORG de [São Paulo ]ORG OU LOCAL  
[Secretaria] ORG de [Educação] SABER de [São Paulo] ORG OU LOCAL  
 
Outro elemento que contribui para hesitações relativas à segmentação é o uso pouco 
sistemático que costumamos fazer das maiúsculas, como podemos nas frases a seguir  
Mil dólares foram apreendidos anteontem no  porto de Santos (SP). 
O Porto de Santos é um porto estuarino, localizado nos municípios de Santos, Guarujá 
e Cubatão, no estado de São Paulo . 
Estes são exemplo s simples, mas mostra m como é preciso combinar (e documentar) o 
que será anotado, mesmo em casos fáceis.  Um livro de geografia usado nas escolas públicas do [ Estado de São  Paulo ]ORG traz 
dois [ Paraguais ]LOCAL  e exclui o [ Equador ]LOCAL  de um mapa da [ América do 
Sul]LOCAL . A [Secretaria de Educação de São Paulo ]ORG informou que a [ Fundação 
Vanzolini ]ORG, responsável pela impressão dos [ Cadernos do Aluno ]OBRA, substituirá 
os [500 mil ]QUANTD livros que têm o mapa errado.  

81 
  
 
Se dispomos de uma ontologia  e de uma lista de palavras  relativ as à área que estamos 
anotando – e assumindo que ontologia e anotação servem a interesses parecidos – 
podemos pular toda a parte  relativa à identificação , porque já sabemos o que estamos 
procurando ; já sabemos o que devemos identificar . Como vimos no capítul o 4, uma 
ontologia define as classes de um domínio, e podemos usá -las como etiquetas na  
anotação. Mas partir de uma ontologia ou léxico não significa que não existe trabalho a 
ser feito.  Uma vez que  a classificação de uma entidade pode variar conforme o co ntexto, 
a estratégia de lidar com entidades a partir de léxicos (por exemplo, uma lista de entidades 
do tipo LOCAL  ou do tipo PESSOA ) é insuficiente. Na preparação de material padrão ouro, 
a utilização de regras pode ser uma boa estratégia para a desambigu ação.  
Podemos explorar um pouco a formalização simplificada usada em 2.1.1.  e tomar como 
problema a desambiguação das duas frases  exemplo  do início d esta seção  (repetidas 
abaixo) . Sabemos que Quito  e Equador são ambíguos quanto às classes LOCAL  e PESSOA : 
(1) A partida, originalmente marcada para esta noite em [Quito], no [Equador], será realizada 
na próxima sexta, em [Assunção], no [Paraguai].  
(2) Confira os convocados da [Seleção Brasileira] para jogos com [Equador] e [Paraguai].  
Analisando os contextos das frases 1 e 2, podemos criar uma regra  
SELECT ( LOCAL ) IF ( -1 “no”) 
que diz selecione a etiqueta  LOCAL  se à sua esquerda está a palavra “ no”. Mas essa 
regra é muito específica, que só irá selecionar a classe LOCAL  nesse caso exato. 
Analisando as demais ocorrências de LOCAL  no corpus, podemos fazer  
SELECT (LOCAL) IF ( -1 “em |no|na ”) 
Do mesmo modo, podemos fazer  
SELECT (ORGANIZACAO) IF ( -2 “jogo|jogar”) ( -1 “com|contra”)  
que diz selecione a etiqueta  ORGANIZACAO  se 2 palavras à esquerda houver a palavra 
jogo ou a palavra jogar, e se 1 palavra à esquerda está a palavra “com” ou a palavra 
“contra”.  Se a decisão for anotar times como entidades do tipo PESSOA , é preciso alterar 
a regra, trocando ORGANIZACAO por PESSOA.   
Podemos ainda fazer uso de outras camadas de anotação. Na regra abaixo, deixamos de 
lado a informação de posição  (quantidade de palavras à direita ou à esquerda) para usa r 
informação sintática ( ser argumento d e certos verbos , não importa a posição ou a 
quantidade de palavras entre o verbo e o argumento ). A regra indica que se a entidade é 
o argumento  de verbos  como regressar, chegar, voltar  ela será do tipo  LOCAL . O que 
fazemos aqui é usar  a anotação de outras camadas para melhorar a anotação das entidad es, 
e já vislumbramos uma função as anotações gramaticais. E para que a regra faça o que 
queremos, já definimos que a classe ARG não contempla os sujeitos dos verbos.  
SELECT (LOCAL) IF (ARG: “regressar|chegar|voltar”)  
Corpora  em português  

82 
 A língua portuguesa  conta com  o material produzido para a avaliação conjunta HAREM, 
chamado Coleção Dourada do HAREM ( falamos dela em 2.1.2). Os textos da coleção são 
de gêneros variados, e o conjunto de classes e subclasses foi escolhido levando em conta 
os interesses da comunidade de PLN na época, contando  com 10 categorias principais:  
ABSTRACAO, ACONTECIMENTO, COISA, LOCAL, OBRA, ORGANIZACAO, PESSOA , TEMPO, 
VALOR  e OUTRO.  Outras opções de anotação do HAREM foram anotar apenas entidades 
grafadas com nome próprio, aceitar a classificação múltipla e segmentações alternativas  
para uma mesma entidade. Todo o material está disponível na página do HAREM.   
Outro material é o corpus Summit++, um corpus multicamadas que contém anotação de 
entidades  classificadas co nforme as orientações do HAREM.  
6.6 Relações entre entidades  e extração de informação  
 
Após a detecção e classificação das entidades, o passo seguinte na estruturação das 
informações dos textos é relacionar  essas entidades. Por exemplo, uma ORGANIZACAO  
se localiza  em um LOCAL ; uma PESSOA  está vinculada a uma ORGANIZACAO , uma 
PESSOA  nasce em um LOCAL  etc. Neste tipo de anotação o que se faz p ré-defini r tipos de 
relações que interessam  e procurar  essas relações no texto. Por exemplo, interessa detectar  
relações de causalidade  entre entidades do tipo DOENÇA  e SINTOMA , vínculos 
profissionais  entre PESSOA  e ORGANIZACAO , relações de localização entre ACIDENTE 
GEOGRÁFICO  e LOCAL. Assim como na anotação de entidades, o  inventário das relações 
que serão anotad as pode vir da tarefa, dos interesses do domínio ou de recursos externos 
como ontologias. Poder contar com anotação sintática no processo de anotação  (saber 
quem é sujeito, quem é objeto, qu ais elementos estão coordenados com quais elementos  
etc), sempre será uma grande vantagem .  
Vamos continuar com nosso pequeno corpus sobre o choro  usado para exemplificar a 
anotação de entidades (quadro 1), e vamos aproveitar as relações já esboçadas na 
ontologia da música (capítulo 4).  O resultado da anotação seria a criação das seguintes 
relações:  
ARG1 : choro  
ARG2 : chorinho  
RELAÇÃO : identidade  ARG1 : Joaquim Antônio da Silva Calado; 
Ernesto Nazareth; Chiquinha Gonzaga; 
Anacleto de Medeiros; Pixinguinha  
ARG2 : choro  
RELAÇÃO : parte de  
  
ARG1 : Rio de Janeiro; Cidade Nova; 
subúrbios cariocas  
ARG2 : choro  
RELAÇÃO : local de nascimento  
 
ARG1 : década de 1870; século XIX  
ARG2 : choro  
RELAÇÃO : data de nascimento  ARG1 : flauta; saxofone  
ARG2 : choro  
RELAÇÃO : parte de  
 
O resultado da extração de relações pode ser usado para a construção de uma base de 
fatos do domínio, uma base de conhecimento , ou ainda para alimentar uma ontologia com 
instâncias , isto é, exemplos  das classes.  
Do nosso corpus podemos extrair os seguintes fatos  

83 
 Choro é o mesmo que chorinho  
Choro nasceu no Rio de Janeiro  
Choro nasceu na década de 1870  
Joaquim Antônio da Silva Calado é parte de choro  
Flauta é parte de choro  
 
Desafios linguísticos  
Se o conjunto de relações de interesse (o tagset de relações) já está definido, o desafio  
será encontrar as relações no corpus e anotá -las. Uma maneira de conduzir esta anotação 
é buscar padrões linguísticos, utilizando uma estratégia bem parecida com aquela 
apresentada em 4.1, já que relações entre entidades também são relações semânticas. 
Primeiro precisamos analisar o material, e começamos com a frase inicial do nosso 
pequeno corpus, que é rica em informações (e isto não é sorte: frases e parágrafos 
introdutórios de textos informativos/técnicos são carregadas de informação):  
O [choro ], popularmente chamado de [ chorinho ], é um gênero de música popular e 
instrumental brasileira, que surgiu no [ Rio de Janeiro ] em meados do [ século XIX ].  
O trecho  
O choro , popularmente chamado de chorinho , 
Nos fornece o padrão abaixo (parênteses indicam opc ionalidade), que sinaliza uma 
relação de identidade:  
N “,” (ADV) chamado de N “,”  
Após mais alguma exploração do corpus, percebemos que o “conhecido como” ( choro , 
também conhecido como chorinho, ) leva ao mesmo tipo de relação. Teremos então dois 
padrões qu e detectam a presença de uma relação de identidade:  
   N “,” (ADV) chamado de N “,”  
N“,” (ADV) conhecido como N “,”  
Na anotação de relações, sabemos a relação que estamos procurando (no caso, identidade ) 
e vamos procurar exemplos dessa relação no corpus. A busca por padrões típicos de cada 
relação auxilia e acelera o processo de anotação, mas não é uma exigência.  
O segmento seguinte da frase também é informativo, e nos fornece um hiperônimo de 
choro , “música  popular” (também podemos discutir se a entidade é “ música  popular” ou 
apenas “ música ”, isto é, se modificadores do núcleo do sintagma são considerados parte 
da entidade. Esta é uma decisão vinculada à anotação de entidades, e não de relações entre 
entidades).   
O choro , popularmente chamado de chorinho, é um gênero de  música  popular...  
Porém esta relação é mais facilmente capturada se contamos com análise sintática, uma 
vez que sujeito e predicado estão distantes na frase:  
N/SUJ é um  gênero de  SN/PREDICADO  Relação: identidade  
Relação: hiperonímia  

84 
 Se pudermos contar com a anotação de entidades, o processo é mais rápido, pois teremos 
apenas as estruturas que envolvem termos do domínio:  
N/SUJ/GENERO é um gênero de SN/PREDICADO  
Como já foi indicado, é importante saber que nem sempre a relação será materializada 
por um padrão linguístico, mas encontrar padrões ajuda o processo de anotação. A frase 
abaixo deve ser anotada com uma relação entre choro  e gênero musical , mesmo que não 
seja expressa por um padrão.   
Até o surgimento do choro  não existia ainda no país um gênero musical  que pudesse ser 
considerado brasileiro.  
Porque as relações não necessariamente preci sam estar expressas por certos padrões, este 
é um tipo de anotação que precisa ser feito com a colaboração de especialistas da área em 
questão, pois em muitos casos apenas especialistas saberão reconhecer a existência de 
uma relação entre dois termos em uma frase.  
O segmento seguinte de nossa frase inicial também é informati vo, mas é mais complexo 
de ser tratado. O motivo da complexidade é o pronome relativo, que, que é correferente 
de choro . 
O choro  (...) é um gênero de música popular (...), que surgiu no Rio de Janeiro  
Se quisermos identificar que o sujeito de surgir no Rio de Janeiro  é o choro , precisaremos 
resolver a cadeia de correferência entre o que e choro  (e anotação de correferência é a 
próxima seção). Isto é, podemos anotar uma relação entre que e Rio de Janeiro , mas esta 
relação só será útil se soubermos o valor do que. Vamos fingir que isso já foi resolvido, 
e descrever mais um padrão para detectar relações. A anotação de entidades ajuda ainda 
mais aqui, uma vez que surgir  pode ser completado com informações de  tempo e de lugar. 
Por isso também, já podemos criar padrões para relação de lugar e de tempo:  
N/SUJ/GENERO surgir ADJUNTO_ADVERBIAL/LOCAL  
 
N/SUJ/GENERO surgir ADJUNTO_ADVERBIAL/TEMPO  
 
Em ambos os casos também é possível generalizar e incluir o verbo nascer ao lado de 
surgir (dentre alguns outros) como verbos que indicam relação de temporalidade e de 
localização entre as entidades.  
Cada padrão pode levar à criação de uma regra de anotação, e quanto melhores os padrões, 
menos casos errados retornarão. Por outro lado, quanto mais ca madas de anotação 
envolvemos nas regras, mais complexa é também sua formulação. Para os dois últimos 
padrões, podemos ter regras como as abaixo (não estamos nos importando com a 
formalização, a regra é apenas uma maneira de ver o que é preciso codificar):  
 
 Relação: local 
de nascimento  
Relação: data 
de nascimento  
a:[pos= N & func= SUJ & ner= GENERO ] b:[lema=surgir & func= PRED :a] c:[func= AADV :b & 
ner= LOCAL ]  >>  b:[lema=surgir & func= PRED :a & relner=localsurgimento:c_a]  

85 
 Lê-se da seguinte maneira: a regra conta com três elementos: “a”; “b”; “c”. A detecção 
do padrão corresponde ao lado esquerdo, antes do sinal “>>” e o que será preciso fazer 
está do lado direito. Tudo no texto que satisfizer às condições indicadas do lad o esquerdo 
será alvo de uma transformação, indicada do lado direito. As condições do lado esquerdo 
são: 
Elemento a: deve ser um N; deve ter a função sintática de sujeito; deve ser uma entidade 
da classe GENERO . 
Elemento b: deve ter o lema “surgir”; deve te r a função sintática de predicador do 
elemento “a”.  Ou seja, é preciso garantir que o predicador esteja relacionado ao elemento 
“a”, e não a qualquer outro sujeito da frase.  
Elemento c: deve ter a função sintática de adjunto adverbial; deve ser uma entida de da 
classe LOCAL . 
Se essas condições forem satisfeitas, a regra faz o seguinte – neste caso, a regra só altera 
o elemento “b”:  
Elemento b:  continua com o lema “surgir”; continua com a função sintática de predicador 
do elemento “a”; recebe a etiqueta de relações entre entidades “relner” com o valor 
“localsurgimento:c_a”, que indica que se trata de uma relação de “local de surgimento” 
entre os elementos “c” e “a”.  Esta maneira de formalizar indica também que a relação 
entre entidades estará anotada no ver bo que faz a relação. Poderia não ser assim, 
poderíamos estabelecer que a anotação das relações estará em cada entidade envolvida. 
Existem algumas maneiras de fazer as mesmas coisas, mas não iremos nos ocupar disso 
aqui.   
Apesar de muito úteis, não devemo s nos esquecer que padrões muito precisos 
frequentemente têm o inconveniente de deixar muita coisa de fora (podem ser pouco 
abrangentes), e esta é uma característica deste tipo de abordagem de anotação. Sabemos 
que os padrões capturam muitas coisas, mas nã o resolvem tudo, e sempre será necessário 
rever o resultado da aplicação das regras no corpus, pois sempre há casos não previstos 
(como indica a lei de Zipf). Se tudo pudesse ser resolvido com regras, o aprendizado 
estatístico não faria tanto sucesso.  
A an otação de relações torna explícita uma série de relações entre entidades, mas ela 
também é mais complexa do ponto de vista da execução. Para poder usar uma estratégia 
de regras ao invés de ler o corpus completo , frase a frase, buscando relações 
exclusivame nte a partir da nossa leitura estamos pressupondo algumas coisas:     
1. Precisamos do texto lematizado (para saber que surgiram , surgiu  etc) são formas 
do verbo surgir ; 
2. Precisamos de análise de POS para encontrar advérbios, substantivos etc;  
3. Precisamos da análise sintática para encontrar sujeitos e adjuntos adverbiais, 
independentemente da posição que estejam na frase;  
4. Precisamos de cadeias de correferência para poder construir relações informativas 
(apenas a palavra “que” como sujeito não é informativa);  
5. Precisamos de anotação de entidades para facilitar a seleção e a revisão.  

86 
 Nem sempre teremos tudo isso à disposição. Conforme o tipo de texto, podemos confiar 
em ferramentas para fazer automaticamente 1, 2 e 3. O tipo de texto é um aspecto 
importante porque  a maioria das ferramentas para o português é treinada em corpus 
jornalístico, então tudo o que se afasta disso tende a ter um desempenho pior.  
Além disso, é fundamental uma ferramenta que faça a interface entre o corpus e as regras 
que irão procurar os p adrões no corpus, assim como uma forma de procurar os padrões 
no corpus (uma sintaxe) e uma ferramenta para incluir as relações em cada caso.  
Justamente porque demanda uma série de processamentos linguísticos prévios, além da 
necessidade de especialistas d e linguística (porque estão mais aptos a perceber e descrever 
os padrões linguísticos usando informações como adjunto adverbial, objeto etc) e em 
alguns casos especialistas também do domínio, métodos estatísticos irão tentar capturar 
as relações entre enti dades de uma outra maneira, a partir exclusivamente dos dados. 
Observando os contextos em que cada palavra aparece (como vimos em na representação 
do sentido com os vetores de palavras), teríamos como resultado o agrupamento de 
palavras como choro -chorinho ; choro -flauta, choro -música , mas sem rótulos que 
explicitem a relação entre elas. Sabemos “apenas” que são palavras relacionadas (não que 
isto seja pouco). Para sistemas de raciocínio, que irão produzir conhecimento novo – tudo 
o que for dito sobre choro se aplica igualmente a chorinho , já que há uma relação de 
identidade entre eles, por exemplo –, rótulos são importantes; para a estruturação explícita 
da informação em documentos rótulos são importantes, mas isto não se aplica a todas as 
tarefas do PLN.  
Corpora em português  
Para a língua portuguesa, temos três corpora padrão ouro com relações entre entidades.  
O primeiro é dataset relativo ao ReReLEM (Reconhecimento de Relações entre Entidades 
Mencionadas), uma tarefa piloto  da edição de 2008 do HAREM  cujo objetivo  relacionar 
semanticamente as entidades anotadas no HAREM . Originalmente, o ReRelEM contou 
com 4 classes – identidade; inclusão; ocorrência; outra. Posteriormente, a classe outra  foi 
especificada, levando a um total de 24 relações . A página do evento exemplifica cada 
uma das relações, que estão reproduz idas abaixo.  
 
O corpus Summ -it++ também contém anotações padrão ouro com r elações entre as 
entidades. Especificamente, estão anotadas todas as relações  possíveis  entre entidades do 
tipo ORGANIZACAO, PESSOA e LOCAL .  
O corpus de Garcia e Gamallo (2014) é um corpus multilíngue que contém material em 
Português, Galego e Espanhol. O corpus é anotado com relações de IDENTIDADE  entre autor_de/obra_de; causador_de; data_morte; data_nascimento; datado_de/data_de; ident; inclui/incluido; 
local_morte; localizado_em/localizacao_de; natural_de/local_nascimento_de; nome_de/nomeado_por; 
ocorre_em/sede_de; outra_edicao; outrarel; participante_em/ter_participacao_de; periodo_vida; 
personagem_de; praticado_em/pratica_se/praticante_de/praticado_por; produtor_de/produzido_por; 
proprietario_d e/propriedade_de; relacao_familiar; relacao_profissional; residente_em/residencia_de; 
vinculo_inst  

87 
 entidades do tipo PESSOA , e por isso é mais apropriado descrevê -lo como um corpus 
padrão ouro anotado quanto à correferência, tema de 6.7 . 
6.6.1. Extração de informação aberta   
Uma maneira menos restrita de criar relações entre elementos de um texto é por meio da 
extração de informação aberta  (open information extraction ). Neste tipo de extração de 
informação não há predefinição de relações nem exigência de que as relações aconteçam 
entre entidades. As relações são estabelecidas entre os argumentos dos verbos (sujeito e 
objeto) e a partir diss o são criadas relações com a estrutura 
ARGUMENTO1_verbo_ARGUMENTO2, chamadas triplas.  
Este tipo de extração de informação é chamado de “aberta”, justamente por não predefinir 
que tipos de relação devem ser encontradas. Na extração de informação aberta não  está 
em jogo, ao menos inicialmente, a etiquetagem das relações. Como as relações são 
identificadas a partir dos verbos, a quantidade de verbos diferentes corresponderá à 
quantidade de relações diferentes.  Porque depende da informação linguística que está no 
corpus, a  anotação sintática anterior é imprescindível para a extração das triplas .  
6.7 Correferência  
A anotação de correferência é mais uma etapa na estruturação da informação dos textos , 
e se aproxim a da anotação de relação entre entidades quando  consideramos apenas a  
relação de identidade. Por outro lado, na anotação de correferência, nem sempre a cadeia 
de correferência precisa se estabelecer entre as entidades de um texto.   
Apesar das aproximações, a anotação de correferência costuma ser apresentada como uma 
tarefa independente, na qual são estabelecidas cadeias de correferência entre os elementos 
de um texto, que podem ser  substantivos  (comuns ou próprios) , pronomes  (possess ivos, 
demonstrativos , relativos, clíticos)  e elementos elípticos, dentre outros . 
A anotação de correferência  não se restringe aos limites de uma frase, podendo se 
estabelecer entre parágrafos, ou no âmbito do documento inteiro.  Trata -se de uma tarefa 
com uma longa tradição no PLN, visto sua ampla relevância  na extração de informação  e 
na sumarização automática. No entanto, trata -se igualmente de uma tarefa bastante 
complexa do ponto de vista das máquinas, demanda ndo uma boa dose de ‘intepretação’ 
de texto. Cada um dos  trecho s abaixo indica, em negrito, elementos  correferentes:  
 
1. Trump [1] pede recontagem em Wisconsin [2] e tenta suspender apuração na 
Pensilvânia, Geórgia e Michigan.  
Campanha de reeleição do republicano [1]  tenta revisão dos votos no estado 
em que ficou atrás [2] e a interrupção da contagem em dois estados em que está 
na frente e um em que foi superado.  
 
2. O próximo ano ficará marcado como o ano em que o Governo decidiu  
construir um novo aeroporto, uma decisão  polêmica . 
 
Do ponto de vista humano, o grau de dificuldade pode ser medido de uma outra maneira: 
em avaliações (escolares) que têm como objetivo verificar a capacidade de “intepretação 
de texto” dos alunos, costuma  haver pelo menos uma questão que envolva a resolução de 
correferência (por exemplo, saber a quem um determinado pronome se refere). Ou seja, 
se achamos que seres humanos, após mais de uma década de contato com a língua e dos 
anos de escolarização, podem n ão ser capazes de identificar com sucesso as cadeias de 
correferência em um texto, não deve espantar que, para máquinas, esta também seja uma 
atividade complexa. O desempenho da resolução automática da correferência, para o 

88 
 inglês, está em 80%, e a melhori a no desempenho das máquinas se deve à incorporação 
de informação linguística oriunda dos vetores de palavras, comentados na seção 6.4 .1. 
 
Desafios linguísticos  
 
A anotação de correferência tem como desafio o desenvolvimento estratégias para guiar 
e otimizar a anotação. O estabelecimento correto de cadeias de correferência depende de 
informação semântica e contextual, o que explica a melhoria do desempenho das 
máqu inas quando leva em conta informação dos vetores de palavras. Por outro lado, como 
vimos em 6.4, não temos material com anotação de sentidos em português, o que 
facilitaria o desenvolvimento de estratégias de anotação – por exemplo, candidatos a 
correferên cia compartilham o mesmo sentido ou têm um mesmo hiperônimo, como 
Wisconsin  e estado , no exemplo (1) acima.  
 
Corpora disponíveis  
 
A língua portuguesa conta com pelo menos três corpora anotados com correferência: o 
corpus Corref -PT, o corpus Summ -it e o corpus de Garcia e Gammalo (2014), que contém 
anotação de correferência entre entidades do tipo pessoa .  
 
6.8 Inferências e Similaridade semântica  
 
No mundo do PLN, Similaridade Semântica refere -se à determinação do grau de 
semelhança e ntre  duas porções de um texto . A detecção de similaridade é utilizada em 
programas de detecção de plágio e em tarefas de NLI ( Natural Language Inference  – algo 
como Inferências de Linguagem Natural ), que lidam com raciocínio  lógico . A anotação 
é uma tarefa de classificação a respeito d o tipo de relação lógica entre duas frases (ou 
proposições): a frase “hipótese”  e a frase “premissa”. A classificação é feita conforme  
três etiquetas: acarretamento, contradição e neutro.   
acarretamento  - o sentido de uma frase está incluído no sentido de outra . Ou, a 
frase hipótese é verdadeira dada a frase premissa ; 
contradição  - o sentido de uma frase contradiz o sentido de outra . Ou, a frase 
hipótese não é verdadeira dada a frase premissa ; 
neutro  - a frase hipótese pode ser verdadeira dada a frase premissa . 
 
O corpus SICK ( Sentences Involving Compositional Knowledge ) e o corpus SNLI 
(Stanford Natural Language Inference ) exemplifica m este tipo de anotação. Em ambos 
os casos, as fr ases vêm de legendas de imagens .  Esta é uma maneira de garantir frases 
declarativas e no tempo presente (inferências precisam de estabilidade, como vimos em 
3.2).  
No corpus SICK, ambas as frases são fornecidas para os anotadores, que devem também 
avaliar a adequação da relação  semântica atribuída . Cada relação de infer ência é 
classificada em uma escala de 5 pontos, sendo 5 uma relaç ão que está perfeitamente 
exemplifica da pelo par de frases, e 1 uma relação fracamente exemplificada pelo par de 
frases.  Abaixo estão a lguns exemplo s de frases  anotadas  retirad as do corpus SICK -BR.  
Frase p remissa: Uma criança risonha está segurando uma pistola de água e sendo espirrada com 
água . 

89 
 Frase hipótese : Uma criança está segurando uma pistola de água . 
Etiqueta  (anotação) : acarretamento . Pontuação da similaridade: 4.5  
Frase premissa: Não há nenhum homem de jaqueta preta fazendo truques em uma moto.  
Frase hipótese: Uma pessoa de blusa preta está fazendo truques em uma moto.  
Etiqueta (anotação): contradição . Pontuação da similaridade: 3.6  
Frase premissa: Ninguém está dirigindo uma bicicleta com uma roda.  
Frase hipótese: Uma pessoa de blusa preta está fazendo truques em uma moto.  
Etiqueta (anotação): neutro . Pontuação da similaridade: 2.8  
Diferentemente do SICK, n o corpus SNLI não há pontuação de similaridade e as frases 
hipótese são construídas pelos anotadores a partir da frase premissa, conforme algumas 
instruções. Como as instruções são fundamentais para entender a qualidade do que é 
criado, estão reproduzidas abaixo : 
 
 
Desafios linguísticos  
É difícil abordar desafios linguísticos desta tarefa porque tanto o SICK como o SNL I 
foram criados utilizando um processo de anotação colaborativa em grande  escala 
(crowdsourcing annotation ), quando se contrata pessoas  de qualquer formação  para a 
tarefa de classificação . Este tipo de estratégia é comum quando se precisa de um dataset 
grande (o SICK tem 10 mil pares de frases e o SNLI tem quase 560 mil pares) e a tarefa 
de anotação não precisa de conhecimento linguístico específico, como vimos nas 
instruções do SNLI.   
Por outro lado, o olhar linguístico para este tipo de material  é pode perceber características 
linguísticas capazes de impactar o desempenho dos  sistemas . Após analisar as frases de 
datasets como o SNLI, uma equipe de pesquisadores descobriu que as instruções para a 
formulação das frases hipóteses davam margem a que anotadores usassem 
sistematicamente certas estratégias , que os autores chamam de “ artefatos linguísticos”. 
Devido a esses “artefatos”, que foram aprendidos pelas máquinas, era possível acertar a 
relação entre premissa e hipótese olhando apenas para a hipótese e ignorando a premissa. 
Especificamente, Gururangan e sua equipe (2018) descob riram que (i) frases de 
acarretamento tendiam a remover informação de gênero e número e usar termos genéricos Vamos te m ostrar a legenda de uma foto. Não vamos te mostrar a foto. Usando apenas a 
legenda e o seu conhecimento de mundo:  
• Escreva uma legenda alternativa que seja definitivamente uma descrição verdadeira da foto. 
Exemplo: para a legenda “Dois cães correndo em um  campo.” você pode escrever “Existem 
animais ao ar livre”.  
• Escreva uma legenda alternativa que possa ser uma descrição verdadeira da foto. Exemplo: 
para a legenda “Dois cães correndo em um campo.” você pode escrever “Alguns cachorros 
estão correndo para  pegar um graveto.”  
• Escreva uma legenda alternativa seja definitivamente uma descrição falsa da foto. Exemplo: 
para a legenda “Dois cães correndo em um campo.” você poderia escrever “Os animais de 
estimação estão sentados em um sofá”. Isso é diferente da  categoria talvez correta porque é 
impossível para os cães correr e sentar.  

90 
 como pessoa , animal , instrumento ; (ii) frases neutras eram frequentemente construídas 
pela adição de uma oração adverbial final  à oração principal ou pela adição de adjetivos 
modificadores ; e (iii) frases com contradição eram criadas por meio da adição de negações 
como não, nunca, ninguém .  
O uso recorrente destas estratégias é explicado por serem exatamente as estratégias 
present es na s instruç ões da tarefa (que estão no quadro 1).  Os autores então criaram um 
dataset fácil, apenas com as frases construídas por meio dos artefatos linguísticos, um 
dataset difícil, com frases que não faziam uso dos artefatos, e compararam com o datas et 
completo, com ambas as frases.  O que encontraram foi uma piora de 10% quando 
compararam os resultados do dataset completo com o dataset “difícil”. Além disso, 
criaram um sistema muito simples, que deveria apenas prestar atenção nos artefatos 
linguístic os e ignorar a frase premissa, e este sistema conseguiu acertar mais da metade 
dos casos no dataset completo.  
 
No SICK, a atribuição de pontuação é certamente um  ponto delicado. Na construção do 
corpus SICK -BR, os autores comentam sobre discordâncias relat ivas à pontuação 
atribuída pelo corpus original . Uma vez que no corpus SICK não há instruções claras 
indicando  como pontuar a relação de similaridade entre as frases, nem sempre há 
consistência na atribuição das etiquetas. No entanto, como o objetivo do s autores do  
SICK -BR não era reanotar o corpus SICK, mas produzir material para a língua portuguesa 
que também pudesse estar alinhado com o material em inglês, os eventuais deslizes de 
anotação foram mantidos na versão brasileira do material.  
Corpora dispon íveis  
Para avançar com a área de raciocínio semântico em português, foi criado o corpus SICK -
BR, que consistiu na tradução e adaptação do material do corpus SICK. Segundo seus 
autores, a tradução foi especialmente cuidadosa, a fim de garantir que (i) as tr aduções 
mantivessem o mesmo valor de verdade dos pares originais; (ii) os mesmos fenômenos 
discutidos no SICK pudessem ser discutidos no SICK -BR; (iii) houvesse um alinhamento 
entre as frases do SICK e do SICK -BR; (iv) as frases soassem naturais em portugu ês 
(Real et al., 2018). Subjacente à tradução, a hipótese de que os fenômenos lógicos seriam 
similares em ambas as línguas, e que as relações de acarretamento e de contradição 
funcionariam da mesma maneira em inglês e em português, o que se confirmou. Na 
avaliação de 800 pares de frases, em apenas 20 deles os autores discordaram da relação 
atribuída, sendo que em 14 deles a relação já estava errada no material original em inglês, 
e nos restantes 6 pares a atribuição de relações foi considerada discutível ta mbém no 
original.  
O corpus SICK -BR serviu como base para o corpus ASSIN, que por sua vez foi o corpus 
criado para a avaliação conjunta ASSIN -2, que propôs as tarefas Similaridade Semântica 
Textual e Reconhecimento de Inferência para a língua portuguesa  (ver também seção 
2.1.2).  
6.9 Opinião e sentimento  
As áreas de mineração de opinião e análise de sentimento lidam com a identificação de 
opiniões, avaliações e atitudes direcionadas  a entidades como pessoas, produtos e 
organizações. Embora muitos trabalh os da área façam uso de léxicos  de sentimentos e 

91 
 polaridades (o capítulo 4 traz um exemplo de um léxico de sentimentos, o OpLexicon), a 
anotação de corpus também t em um papel relevante  porque, assim como no caso da 
anotação de entidades, léxicos podem ser insuficientes , por um lado, e levar a 
ambiguidades, por outro . Nosso pequeno corpus de música usado para ilustrar a anotação 
de entidades exemplifica o cuidado que precisamos ter na ut ilização ingênua de léxicos: 
o texto é sobre o choro , um gênero musical, e chorões  é o nome dado a quem toca choro. 
Qualquer léxico de polaridade atribui valor negativo a choro  ou chorões , o que levaria o 
texto a ser classificado como negativo.  
A anotação de opiniões e/ ou sentimentos pode assumir diferentes formatos: a anotação 
de polaridades a qualquer palavra , sintagma ou frase  (com a atribuição de valores como 
positivo, negativo e neutro  às palavras ); a inclusão de escalas para cada uma dessas 
categorias (muito positivo, pouco positivo etc);  a atribuição de polaridade apenas a 
palavras indicativas de algum tipo de emoção ou sentimento ( amor  como positivo; ódio 
como negativo ; surpresa  conforme o contexto ); ou ainda a detecção de palavras 
indicativas de emoção ou sentimento conforme uma dada teoria .  
Desafios linguísticos  
Quando a anotação não parte de um léxico prévio que já c ontém as palavras de interesse,  
um desafio é a detecção do elemento que carrega a  polaridade  e a opinião. Abaixo, em 
destaque, alguns trechos selecionados  para ilustrar a questão. Quais elementos devem ser 
anotados  como índices de opinião  – a frase inteira  ou apenas algumas palavras?   
1. Mas em um contexto todo, o livro conseguiu suprir minhas expectativas . 
2. impossível abandonar o livro pela metade  
3. a tradução deveria ser CRAPúsculo.  
No exemplo (1), devemos anotar suprir minhas expectativas , apenas  suprir , ou todo o 
trecho em itálico? Em (2), é difícil apontar  um trecho  específico que carre gue a opinião, 
que é positiva sobre o livro, apesar de, isoladamente, as palavras impossível  e abandonar  
serem consideradas negativas (e ambas estão codificadas como negativas no OpLexicon 
e no Senti -Lex, outro léxico de polaridades para análise de sentime nto). Por fim, em (3), 
é apenas o reconhecimento de que crap é uma palavra da língua inglesa cujo significado 
é lixo que permite compreender  o trocadilho crapúsculo/crepúsculo , que  contém uma 
opinião negativa.  Mas será que apenas a identificação de crapúsculo  é suficiente, ou a 
opinião está no segmento completo?  
Outro ponto importante é a necessidade de levar em conta o contexto na atribuição de 
polaridades , e por isso léxicos podem ser insuficientes . Palavras e expressões 
convencionalmente  considera das negativas podem integrar um comentário positivo, e 
vice-versa , como vimos com impossível  e abandonar . Abaixo, mais alguns exemplos de 
frases com palavras de polaridade convencionalmente negativa, mas que são usadas para 
uma avaliação positiva (o objeto  avaliado era um livro) : 
a. Lamentável tê-lo lido somente agora.  
b. Fui à nocaute, sem direito a (re)contagem. Chorei . Fiquei meio deprê .  
c. Tenso . É o único livro que me deixou nervoso  e apreensivo  enquanto lia.  
d. Dolorido , pavoroso , nojento , repugnante  e nauseante . Por todos esses adjetivos que o 
livro nos causa, ele consegue ser bom.  
 
Corpora disponíveis  

92 
 Para o português, temos atualmente uma série de corpora anotados tendo em vista a tarefa 
de análise de sentimento, sendo talvez pioneiros o Senti -Corpus e o ReLi. O Senti -Corpus 
foi construído a partir da anotação de comentários (posts) em matérias sobre e leições em 
Portugal, no domínio da política e em uma escrita típica da internet. Já o ReLi contém 
resenhas de livros, também publicadas na internet.  
 
6.10 Relações discursivas e retóricas  
 
Também é possível relacionar estruturas retóricas e discursivas em textos por meio da 
anotação. Neste caso, as relações podem se estab elecer entre sintagmas, frases e 
parágrafos. Na anotação de relações discursivas, elementos como mas, e,  portanto , por 
isso, por outro lado,  etc os responsáveis pelas conexões.  
Uma maneira  de indicar relações discursivas em um texto é utilizando a teoria RST 
(Rethorical Structure Theory), proposta por Mann e Thompson (1988), que distribui e 
classifica relações discursivas de acordo com um inventário próprio de relações. Próxima 
à teoria RST  está a teoria CST ( Cross -document Structure Theory ), que envolve o 
estabelecimento de relações entre documentos, o que é bastante relevante na área de 
sumarização automática.  
 
Corpora disponíveis  
Para a língua portuguesa, temos pelo menos dois corpora pad rão-ouro com relações 
discursivas: o já referido corpus Summ -it, que contém relações RST, e o corpus CST -
News, que contém relações CST.  
 
  

