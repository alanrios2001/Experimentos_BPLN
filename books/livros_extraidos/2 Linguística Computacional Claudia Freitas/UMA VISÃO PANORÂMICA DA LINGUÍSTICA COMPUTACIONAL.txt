9 
 2. Uma visão panorâmica da Linguística Computacional  
 
De acordo com  a página da ACL ( Association for Computational Linguistics  - Associação 
para Linguística Computacional), sociedade científica fundada em  1962 “para pessoas 
que trabalham com problemas computacionais envolvendo a linguagem humana” a  
“Linguística computacional é o estudo científico da linguagem de uma perspectiva 
computacional.” Ainda segundo a ACL, trata -se de um campo “muitas vezes referido 
como linguística compu tacional ou processamento de linguagem natural (PLN)”, e 
interessa a este campo fornecer modelos computacionais de fenômenos linguísticos 
"baseados no conhecimento" ou "orientados por dados". O trabalho em linguística 
computacional seria, em alguns casos, motivado por uma perspectiva científica em que 
se tenta fornecer uma explicação computacional para um determinado fenômeno 
linguístico ou psicolinguístico; e em outros casos motivado por uma perspectiva  
tecnológica aplicada, visando  fornecer um componente funcional de um sistema de fala 
ou de linguagem.  
A explicação da ACL menciona modelos computacionais  baseados no conhecimento ou 
orientados por dados . Veremos tudo isso ainda neste capítulo, mas por enquanto 
continuamos com uma apresentação geral.  
Enquanto área aplicada, a Linguística Computacional dedica -se à resolução de  problemas  
ou tarefas  que envolvem centralmente a linguagem – o que não significa, obviamente, 
que para resolver problemas  (dimensão aplicada) não seja necessário investigar questõ es 
(dimensão teórica). Essas tarefas, por sua vez, podem ser vistas como uma simulaç ão das 
variadas práticas linguísticas humanas, como ler um (ou vários) textos para apr eender 
conteúdos, fazer resumos, traduzir, dar opiniões, descrever imagens, contar histórias  etc. 
Algumas das aplicações mais difundidas de PLN são  
• Tradução automática  
• Extração de informação  
• Identificação de opinião  
• Sistemas de Pergunta e Resposta  
• Agentes conversacionais  (chatbots  e assistentes virtuais)  
• Sumarização automática  
• Correção gramatica l e ferramentas de auxílio à escrita  
 
À medida em que os resultados vão se aproximando do desempenho humano , novos 
problemas vão surgindo, demandando novas investigações para novas soluções. Assim, 
se inicialmente muito do interesse no processamento automático de textos esteve em 
encontrar informação factual em textos e coleções de documentos, logo se viu que 
conteúdos não factuais, de caráter subjetivo, eram também relevantes (e porque a natureza 
dos documentos começou a mudar também, com o volume crescente de conteúdo 
produzido por pessoas comuns, que escrevem para expressar suas opiniões sobre pessoas, 
lugares  e produtos , por exemplo ). E associado s a esses dados de natureza diferente, 
problemas mais complexos como identificação automática de humor, sarcasmo e ironia.  
 
2.1 Aplicação, ferramenta  e recurso  
Para que cada uma dessas aplicações seja bem -sucedida, um a série de recursos e 
ferramentas linguístico -computacionais são acionados.   
 

10 
 Segundo os dicionários, aplicação  é execução , prática , utilização . Em nosso caso, 
aplicação é onde queremos chegar . As palavras aplicação e aplicativo têm uma mesma 
raiz, o que já nos diz alguma coisa. Um aplicativo materializa, torna concreta, uma 
aplicação. A aplicação é a ponta do iceberg, é aquilo de interesse mais imediato para a 
sociedade (ou parcela dela). Ferr amentas  são instrumentos com os quais se realizam 
tarefas específicas. Podemos ter ferramenta s que fazem a identificação das classes de 
palavras , que podem ser úteis para as aplicações de correção gramatical e de avaliação de 
complexidade textual . Se vemos  o corretor ortográfico já como uma ferramenta (de 
auxílio a escrita), temos a situação de uma ferramenta que é composta por outras 
ferramentas.  
 
Alguns exemplos de ferramentas de PLN : 
• anotador de  POS – ferramenta que atribui classes morfológicas (subst antivo, verbo, 
preposição etc) às palavras  
• anotador sintático - ferramenta que atribui categorias sintáticas (sujeito, objeto...) 
às palavras  
• lematizador – ferramenta que, para cada palavra de um texto, identifica sua forma 
de dicionário (infinitivo no casos dos verbos; singular para o casos dos 
substantivos; singular masculino para os adjetivos...)  
• anotador de papéis semânticos   – ferramenta q ue identifica, em um texto, os papéis 
semânticos dos verbos, em termos de argumentos e modificadores  
• anotador de entidades – ferramenta que identifica e classifica as entidades de um 
texto (entidades do tipo PESSOA, LOCAL, ORGANIZAÇÃO  etc) 
• anotador de corr eferência – ferramenta que identifica cadeias de correferência 
entre elementos de um texto  
• identificação de similaridade semântica – ferramenta que identifica, dados dois 
‘pedaços’ de texto (frases ou textos inteiros), o quanto são similares.  
 
Já um recurso  é aquilo que irá alimentar as ferramentas. Um corpus anotado  (capítulo 6) 
é um recurso para o desenvolvimento de uma ferramenta. E aqui abro um parêntese para 
reforçar a enorme relevância do trabalho com corpora linguísticos no PLN, e da 
importância, para linguistas (computacionais  ou não ) de dominar  este “objeto 
linguístico”. Um léxico computacional  (capítulo 4) também é um recurso. Um corpus 
anotado é um recurso  que pode ser usado para avaliar  o desempenho de um sistema ou 
método, ou para treinar  uma ferramenta. E um corpus pode ter sido anotado com 
ferramentas  de anotação. O quadro 1 apresenta uma frase anotada com diversas camadas 
de informação . 
 
 
 

11 
  
 
Uma pausa breve para algumas observações sobre o quadro 1.  
 
Como podemos ver, um texto anotado nada mais é que um texto analisado 
linguisticamente, e esta análise pode ser sintática, morfológica, semântica, discursiva etc. 
No quadro 1, a  frase deve ser lida na vertical, cada palavra está em uma linha  e cada 
coluna codifica um tipo de anotação, exceto pelas colunas 1 e 2, que trazem o número 
identificador  de cada elemento da frase  (cada token ) e cada elemento da frase, 
respectivamente  (nem toda a anotação tem esse formato , embora este seja comum ). Todos 
os elementos da frase, mesmo sinais de pontuação, recebem uma etiqueta. As colunas 6 
e 7 codificam infor mação sintática: a coluna 7 informa o tipo de relação sintática, e a 
coluna 6 informa o elemento com o qual a relação sintática se estabelece. As colunas 8 e 
9 codificam informação semântica: classes semânticas na coluna 8, e o que chamamos de 
“polaridade”  na coluna 8. Por agora parece informação demais, mas todos esses tipos de 
anotação serão abordados no capítulo 6.  
Mesmo para quem já tem alguma familiaridade com análise sintática e árvores sintáticas, 
é bem possível que o tipo de análise representada no quadro 1 cause algum estranhamento # text = Em 1964, Karen Sparck Jones publico u o artigo Sinonímia e Classificação Semântica , hoje considerado um 
trabalho seminal no PLN . 
 
1 Em em PREP  _ 2 case   
2 1964  1964  NUM  NumType=Card  7 obl TEMPO   
3 , , PUNCT  _ 2 punct    
4 Karen  Karen  PROPN  Gender=Fem|Number=Sing  7 nsubj   
 
PESSOA   
5 Sparck  Sparck  PROPN  Number=Sing  4 flat:name   
6 Jones  Jones  PROPN  Number=Sing  4 flat:name   
7 publicou  publicar  VERB  Mood=Ind|Number=Sing|Person=3|  
Tense=Past|VerbForm=Fin  0 root   
8 o o DET  Definite=Def|Gender=Masc|  
Number=Sing|PronType=Art  9 det   
9 artigo  artigo  NOUN  Gender=Masc|Number=Sing  7 obj   
10 Sinonímia  Sinonímia  PROPN  Gender=Fem|Number=Sing  9 appos   
 
 
OBRA   
11 e e CCONJ  _ 10 flat:name   
12 Classificação  Classificação  PROPN  Gender=Fem| Number=Sing  10 conj  
13 Semântica  Semântica  PROPN  Gender=Fem| Number=Sing  10 flat:name   
14 , , PUNCT  _ 16 punct    
15 hoje hoje ADV  _ 16 advmod  TEMPO   
16 considerado  considerar  VERB  Gender=Masc|Number=Sing|  
VerbForm=Part  9 acl   
17 um um DET  Definite=Ind|Gender=Masc|Number
=Sing|PronType=Art  18 det   
18 trabalho  trabalho  NOUN  Gender=Masc|Number=Sing  16 xcomp    
19 seminal  seminal  ADJ Gender=Masc|Number=Sing  18 amod   POSITIVO  
20-21 no _ _ _ _ _   
20 em em PREP  _ 22 case   
21 o o DET  Definite=Def|Gender=Masc|  
Number=Sing|PronType=Art  22 det   
22 PLN  PLN  PROPN  Gender=Masc|Number=Sing  16 obl CAMPO   
23 . . PUNCT  _ 7 punct    

12 
 - e essa foi mesmo a intenção. Isto porque a análise está representada segundo um modelo 
de dependências sintáticas, ou de árvores de dependência, pouco  discutido na nossa 
formação linguística, tão habituada ao modelo de árvores sintagmáticas. Por outro lado, 
trata-se de uma  abordagem gramatical  bastante popular no PLN, sendo uma boa ilustração 
de como PLN e a Linguística poderiam conversar mais.  
 
Quando falamos de recursos, estamos, em geral, tratando  de recursos linguísticos . Daí 
que a criação de recursos para o PLN é (ou deveria ser) uma tarefa mais linguística que 
computacional na linguística computacional – e me refiro à natureza da tarefa, e não ao 
perfil de quem a executa, mas é (ou deveria ser) razoável que pessoas com formação em 
linguística estejam instrumentalizadas para isso.  
 
Recursos, em geral, são dependentes de língua, diferentemente das ferramentas e das 
tecnologias. Se uma equipe precisa desenvolver um corretor gramatical ou um sistema 
que identifica opiniões, e para isso precisa de regras linguísticas, de um corpus anotado, 
ou de um léxico de polaridades (uma lista de palavras com a sua orientação ou carga 
semântica, classificada como pos itiva, negativa ou neutra), é difícil imaginar que um 
corpus ou léxico feito para uma língua que não seja o português seja a melhor opção, e o 
mesmo vale para as regras do corretor gramatical. Por outro lado, a construção de recursos 
é sempre uma tarefa tr abalhosa, e há equipes que optam, por praticidade, pela tradução.  
 
 
2.2 Regras linguísticas e Aprendizado de máquina  
 
Ferramentas de PLN são construídas a partir de abordagens baseadas em regras, baseadas 
em aprendizado de máquina, e também  abordagens híbridas  (baseadas em regras 
linguísticas mas que incorporam informação estatística) . Voltando à definição de 
Linguística Computacional que abre este capítulo: as abordagens baseadas em regras 
correspondem aos modelos "baseados no conhecimento" , e abordagens baseadas em 
aprendizado de máquina correspondem aos modelos "orientados por dados".  
 
PLN baseado em regras  
À primeira vista, abordagens baseadas em regras  são de compreensão mais intuitiva para 
linguistas, pois implementam uma visão de língua que nos é familiar: há um componente 
responsável pelas regras, e um componente responsável pelo  léxico. Apesar da aparente 
simplicidade, a implementação de tais sistemas é altamente complexa, e existem vários 
tipos distintos de gramáticas, com filos ofias linguísticas muitíssimo distintas, como por 
exemplo as gramáticas constritivas ( constraint gramar ) e as gramáticas gerativas.  
As primeiras tentativas de criar sistemas de PLN foram baseadas na construção de regras. 
Para serem bem -sucedidas, as regra s subjacentes a tais precisam ser formuladas por 
especialistas, o que ajuda a entender a participação mais intensa de linguistas nos anos 
iniciais do PLN.  
Um sistema de análise gramatical baseado em regras que durante décadas f oi o mais usado  
no PLN de língua portuguesa é o analisador  (ou parser ) PALAVRAS  (Bick, 2000) . 
Chamamos  de parsing  o processo de análise sintática , nome que também é usado na 
Linguística para fazer referência à análise sintática humana . O PALAVRAS  tem um 

13 
 desempenho muito bom, continua bastante ativo  (pode ser consultado online) , e também 
realiza análise semântica, de entidades mencionadas e de papéis semânticos  (veremos 
tudo isso no capítulo 6). O PALAVRAS segue a gramática constritiva, que parte da ideia 
de que isoladamen te a maioria das p alavras é ambígua. A gramática utiliza uma série de 
regras que irão eliminar as ambiguidades , levando em conta o contexto . Por exemplo, 
considerando a frase  
Eu nunca como peixe1 
A palavra como  é ambígua quanto às classes de advérbio, conjunção subordinada ou 
verbo, mas existem regras que irão desfazer a ambiguidade, levando à análise correta. 
Uma regra simplificada seria  
SELECT (VFIN) IF (NOT * -1 VFIN) (NOT *1 VFIN)  
Que deve ser lida como “sel ecione a etiqueta VFIN (verbo finito) se (IF) a palavra à 
esquerda (*-1) não for um verbo finito e se a palavra à direita (*1) não for um verbo 
finito”.  
Neste tipo de abordagem, sistemas contam com um léxico vasto  que contém palavras e 
suas classes , um analisador morfológico, responsável por flexão e derivação, e um 
módulo sintático com as regras de desambiguação. Segundo Bick (2005), uma gramática 
constritiva madura contém milhares de regras, com até 2000 regras para cada nível 
(morfológico e sintá tico). O PALAVRAS, especificamente, contém um analisador 
morfológico baseado em um léxico com cerca de 50.000 formas, e contém cerca de 5000 
regras de gramática restritiva para desambiguação morfológica e sintática.  
Como podemos ver, o desenvolvimento de sistemas com base em regras é algo não apenas 
trabalhoso  e complexo do ponto de vista linguístico , mas também demorado . 
 
PLN com aprendizado de máquina  
O PLN feito com aprendizado de máquina dispensa  todo o  trabalho linguístico 
especializado , como a elabor ação de léxicos e regras . Por outro lado , serão necessários  
muitos dados . Por isso, trata -se de uma maneira de fazer PLN que começa a ganhar força 
nos anos 19 90, com o surgimento da internet , a maior fornecedora de textos já em formato 
eletrônico .  
Embora a ideia de aprender por meio de exemplos seja familiar , trata -se de uma maneira 
de ver a língua diferente da que estamos habituados, e por isso nos demoraremos um 
pouco aqui. As explicações que vêm a seguir são muito simplificadas e têm a intenção  
apenas  de fornecer uma ideia geral do que é feito do lado computacional, e de como é 
possível se obter resultados bons mesmo com quase nenhuma parti cipação linguística.  
A primeira coisa que devemos entender que o aprendizado de máquina representa uma 
mudança de paradigma  na IA. Um paradigma é um modelo, padrão ou exemplo seguido 
                                                           
1 O exemplo é retirado do artigo “Gramática constritiva na análise automática da sintaxe portuguesa”, que 
é uma ótima introdução ao PALAVRAS.  

14 
 em certas situações. Quando falamos em mudança de paradigma, nos referimos a fazer 
algo difere nte do que v inha sendo feito até então.  
E por que aprendizado é uma mudança de paradigma?  
Desde o seu surgimento, a IA se desenvolveu criando sistemas baseados em lógica e 
regras.  No aprendizado de máquina (AM), o objetivo é realizar tarefas (ou resolver 
problemas) com base em exemplos, sem instruções explícitas, sem que os sistemas 
tenham sido explicitamente pro gramados para aquela dada tarefa . As regras (instruções) 
necessárias são aprendidas automaticamente a partir dos dados – a partir dos exemplos. 
Os exemplos são chamados de dados de treinamento , ou dados de treino .  
O AM é um ramo da IA que lida com o desenvolvimento de algoritmos que podem 
aprender a realizar tarefas automaticamente com base em um grande número de 
exemplos, sem a necessidade de regras artesanais  e explícitas  – como as regras para a 
desambiguação da palavra “como” que vimos na seção anterior . Para tanto , o que os 
algoritmos de IA  busca m é a depreensão automática de padrões  nos dados.  
Os algoritmos de AM podem ser agrupados em três tipos: aprendizado supervisionado, 
aprendizado não supervisionado e aprendizado por reforço. E, como mencionado, há 
também o aprendizado profundo, que é um outro paradigma dentro do AM . 
No aprendizado supervi sionado , após ter sido apresentado a um grande número de 
exemplos rotulados – por exemplo, milhares de e -mails já classificados como sendo spam 
ou não , e estes são os dados de treinamento  –  um sistema deve ser capaz de, ao receber 
um novo dado (um novo e-mail, e este é o dado de teste), classificá -lo como spam ou não.  
Neste tipo de AM é como se houvesse a presença de um professor que mostrasse , para 
cada e -mail recebido, “este e-mail é um spam ”, “este não é spam ”. Na fase de teste, o 
professor apresenta um  novo e -mail (que nunca havia sido apresentado antes) e pergunta 
se o e -mail é ou não spam.  Estamos diante de um professor  (ou treinador)  que apenas 
passa exercícios e mostra o gabarito , sem explicar o que faz de um e -mail um spam, e 
espera que o aprendiza do aconteça assim . Se na hora da prova for fornecida a respost a 
esperada (correta) assume -se que houve aprendizado, e não importa que caminhos foram 
percorridos até chegar à resposta. Por isso, a análise realizada  também  é chamada de 
superficial . Se chamamos de parsing o processo de análise sintática feito por analisadores 
com base em regras, chamamos de parsing superficial ( shallow parsing ) a análise 
sintática feita por algoritmos de aprendizado de máquina. No primeiro caso, sabemos o 
que guiou a aná lise (as regras linguísticas); no segundo caso, não.  Como não há 
explicação, boa parte do sucesso no AM está nos dados, nos exercícios que são feitos.  
Aquilo que para linguistas é um corpus anotado , para pessoas de PLN é um conjunto de 
dados  linguístico – um dataset . Podem existir datasets de várias naturezas: de imagens, 
de vídeos etc. Para nós, os dados são sempre de material textual. Por isso, não é 
exatamente correto afirmar que no aprendizado de máquina não há incorporação de 
conhecimento linguístico.  O que acontece é que o conhecimento linguístico assume uma 
outra forma: se não há lugar para as regras linguísticas explícitas, os dados  necessários 
para o aprendizado vêm na forma de um  corpus cuidadosamente anotado,  a que 
chamamos de corpus padrão ouro . Ou seja, o que os sistemas aprendem, na grande 

15 
 maioria dos casos, é aquilo que linguistas analisaram, aquilo que linguistas anotaram. 
Voltando ao exemplo da análise sintática : ao invés de uma regra como  
SELECT (VFIN) IF (NOT * -1 VFIN) (NOT *1 VFIN)  
o sistema é alimentado (treinado) com muitas frases já previamente analisadas , como as 
do quadro abai xo, e a partir delas aprenderá a maneira  correta  de classificar “como”  
conforme o contexto.  
 
 
 
 
Já o aprendizado não supervisionado vai tentar  encontrar padrões em dados sem a ajuda 
de um professor . Aproveitando  o exemplo anterior, é como se o sistema fosse apresentado 
ao mesmo conjunto de e -mails, só que sem os rótulos indicando se cada um é ou não um 
spam. Programamos a  máquina para que distribua o conjunto de e -mails em duas classes  
e como resultad o gostaríamos que uma classe contivesse os e -mails do tipo spam e outra  
classe  os e-mails do tipo não spam. Mas a máquina não “sabe” isso, ela só “sabe” que 
precisa encontrar a melhor maneira de distribuir os e -mails em dois grupos.  O 
aprendizado não supe rvisionado é usado em tarefas  de classificação de documentos  ou de 
identificação de tópicos implícitos em grandes coleções de textos, quando precisamos 
identificar do que tratam ( seus tópicos) , sem tê -los lido. Imagine que você tenha uma 
coleção com todas as decisões julgadas pelo Superior Tribunal Federal (STF) nos últimos 
10 anos, e deseje saber do que tratam  para ter uma primeira ideia do s conteúdos . Dizem os 
que os tópicos são implícitos (ou latentes) porque serão identificado s pelos sistemas  em 
função das palavras contidas nos documentos . Nesta tarefa específica, que se chama 
“modelagem de tópicos”, um tópico não tem um nome e é formado por um conjunto de 
palavras que, ao olhar humano, podem fazer sentido entre si, como “m ar vento praia onda 
tempestade”, “estudante colégio professor diretor livro”, mas também podem ser 
genéricos e pouco informativos, como “vida amor alma olho mundo” ou “dia tempo 
palavra pobre pai”.  
No aprendizado por reforço, a aprendizagem acontece por tentat iva e erro . O objetivo do 
algoritmo é selecionar o passo (a ação) que leva à maior recompensa  ou menor punição . 
É uma abordagem  muito usada para jogos, e não é (ainda) muito comum no PLN.  Eu_PRON nunca_ADV como_VERB peixe_NOUN.  
Eu_PRON como_VERB devagar_ADV.  
Você_PRON sabe_VERB como_CONJ fazer_VERB isso_PRON?  
É_VERB impressionante_ADJ como_CONJ as_ART pessoas_NOUN comem_VERB mal_ADV.  

16 
 Por fim, o a prendizado profundo  (AP),  responsável pelos  rápidos avanços na IA e no 
PLN . O AP se baseia em redes neurais  artificiais, que são modelos computacionais 
(modelos matemáticos) inspirados nas redes neurais biológicas, e que simulam seu 
comportamento. Uma rede é um conjunto de neurônios artificiais con ectados entre si, que 
formam algo parecido com uma teia, ou rede (figura abaixo). A primeira camada da rede 
processa a entrada e passa as informações para as camadas intermediárias até camada de 
saída, que é a camada final.  
 
Uma curiosidade: uma rede neura l artificial pode ter milhares de unidades de 
processamento, enquanto o cérebro de um mamífero pode ter bilhões. As primeiras redes 
neurais tinham uma  única  camada de neurônios conectados, e quando falamos em 
aprendizado profundo, o adjetivo “profundo” se refere às várias camadas das redes 
neurais atuais , como as da figura . Esta profundidade  é também uma maneira de marcar 
diferença com relação à alegada superficialidade dos outros algoritmos de aprendizado 
de máquina.  
Uma característica das redes neurais é serem genéricas, isto é, não temos arquiteturas 
espec íficas para processar linguagem, outras específicas para imagem, ou tras específicas 
para som. Outra característica é que aquilo que cada camada representa ou codifi ca é algo 
que não sabemos, e esse tema será tratado em breve.  
Uma consequência  deste processamento feito pelas  redes neurais profundas  é que elas são 
capazes de gerar modelos de linguagem  muito mais complexos e eficazes que as 
abordagens anteriores.  
A palavra  modelo  tem vários sentidos, e um modelo de linguagem é um esquema teórico 
que permite prever . Sabemos que um modelo é adequado não porque ele se parece com 
“original”, mas porque faz boas previsões. Temos modelos de previsão d o tempo: quanto 
melhor  o modelo , menos chances de errar a previsão. Modelos de linguagem também são 
modelos de previsão, mas ao invés de preverem chuvas, furacões ou períodos de seca, 
preveem tão somente qual a próxima palavra em um texto,  dada a palavra anterior.  
Modelos de lí ngua  devem saber completar uma frase; são modelos que preenchem  
_______.  Se você esperava que após a palavra “preenchem” viesse a palavra “lacunas”, 
fez uma previsão acertada, pois era o que eu havia escrito: modelos de língua são modelos 
que preenchem l acunas em um texto. O preenchimento correto das lacunas – ou, de forma 
mais acurada, a previsão correta relativa à próxima palavra – indica, de maneira indireta, 


17 
 que o modelo capturou aspectos sintáticos  (1) e semânticos (2) da língu a. As palavras 
riscadas  correspondem a previsões erradas, as palavras não riscadas correspondem a 
previsões possíveis : 
(1) Modelos de língua irão preencher  lacunas  
    incrível  
    comprei  
    lentamente  
    nunca  
    o 
 
(2) Modelos de língua irão preencher  lacunas  
    brigadeiro  
    sapo 
    bombeiros  
    espaços  
Um bom modelo de língua (ou de linguagem) será capaz de fazer previsões acertadas e 
com isso sabemos, mesmo que indiretamente, que de alguma maneira ele capturou 
aspectos relevantes da língua . 
Como os modelos são criados ? Com o nos demais tipos de AM, o a prendizado acontece  a 
partir de dados, muitos dados. Os dados são as palavras do texto, e só. Não há dados 
rotulados, não há supervisão  de aprendizagem  – e justamente porque não há ninguém para 
ajudar, é preciso muito mais dados que nas outras formas de AM . De forma bastante  
simplificada, cada palavra do texto  (o que é considerado palavra pode variar, e falaremos 
disso no capítulo 5) é transformada em um  conjunto de números ( essa transformação é 
assunto d a seção 6.4 .1). Esses conjuntos de números (as palavras) são os dados de entrada, 
que serão processados  pelas várias camadas da rede  neural.  
Um modelo de linguagem é , portanto , um mo delo probabilístico que prevê as chances de 
uma determinada sequência de  palavras aparecer em um a língua . Um  bom modelo de 
linguagem codifica a estrutura gramatical  e aspectos de convencionalidade a partir  dos 
dados (ou corpora) de treino. Existem modelos de linguagem simples, usados para tarefas 
simples como correção e de sugestão de palavras dos telefones celulares, e existem 
modelos complexos para tarefas complexas como a tradução. Existem modelos de 
linguagem estatísticos e modelos baseados em redes neu rais. Hoje em dia, modelos de 
linguagem baseados em redes neurais estão levando a resultados às vezes até superiores 
ao desempenho humano , como veremos na seção 2.1.3.  
Embora o AM possa ser feito de diferentes maneiras, alguns procedimentos são comuns. 
O processo  subjacente consiste em dividir o dataset (ou o corpus padrão  ouro, se 
estivermos falando de dados rotulados ) em partes  de tamanhos desiguais : a parte maior  é 
usada para a fase de  treino , e a parte menor é usada para a fase de  teste. Retomando a 
analogia entre os procedimentos do aprendizado de máquina e os procedimentos do 
aprendizado escolar: temos a fase de estudo, quando fazemos exercícios, e a parte de 
verificação de aprendizagem, que é a hora da prova . No AM, a fase do treino é s imilar à 
fase de exercícios: fazemos exercícios , comparamos  nossas respostas com o gabarito, 
vemos o que erramos e tentamos melhorar  fazendo mais exercícios . A fase da avaliação  
é a prova, quando  somos apresentados a questões novas , que não estavam na fase  de 
estudo, e não temos acesso ao gabarito. A nota da prova (nossa avaliação ) será dada 

18 
 comparando a nossa resposta com a resposta do gabarito.  Se o gabarito estiver errado, 
seremos injustamente penalizados caso tenhamos respondido corretamente.   
Mais eficiente , para as máquinas e para o nosso estudo, é dividir a fase de estudo/treino 
em duas : além do s exercício s, fazemos um “simulado”, um teste que nos dá alguma pista 
sobre o que aprendemos, mas ainda não é a avaliação final. É a última chance que temo s 
de perceber nossos pontos fracos e melhorar o desempenho  fazendo os últimos ajustes . 
No AM, isto corresponde a dividir o dataset em 3 partes, e não em duas. Além das 
partições treino e teste, há uma outra parte para “desenvolvimento”, ou ajuste, feito em 
função das respostas do simulado (quanto ao tamanho, esta parte se aproxima ao tamanho 
do material de teste).  
Nas máquinas, o treino acontece comparando as análises fornecidas por algum “chute ” 
inicial, e em seguida ajustando as respostas a partir do contato com o gabarito. Por isso, 
quanto mais exemplos, melhor, e quanto mais variados  os exemplos , melhor  também.  E 
é fundamental que o gabarito esteja correto, para não fornecer pistas erradas sob re o que 
deve ser aprendido. Na  fase de avaliação , escondemos  a anotação da part ição teste do 
corpus padrão ouro (escondemos as respostas das questões)  e deixamos que o sistema  
mostre o que aprendeu. Só ao final do processo  comparamos o resultado da análise 
automática com aquele que estava com o gabarito . Por isso, a qualidade da anotação é tão 
relevante: ela contribui não só para um bom aprendizado, mas também para uma 
avaliação  correta . É igualmente  importante que a partição teste fique guardada e s e 
mantenha inédita até a fase final de avaliação. Do contrário, é possível que haja o 
chamado overfitting , quando os resultados são bons mas o sistema está “roubando”, isto 
é, é como se ele tivesse acesso prévio à prova e ao gabarito, e então ele acerta nã o porque 
tenha aprendido, mas porque já sabendo as perguntas, decorou as respostas.   
 
Na geração de um modelo de língua por redes neurais profundas, em que a tarefa é 
aprender a prever a próxima palavra de um texto, o exercício de treino consiste em 
mascarar (e, portanto, esconder) palavras ao longo do texto. O modelo que está sendo 
criado dá o seu chute com relação a que palavra deverá ser aquela, usando como pista as 
palavras que estão n o entorno da palavra mascarada, e chamamos este entorno de janela . 
O tamanho da janela varia conforme o algoritmo, e modelos complexos têm janelas com 
cerca de 2 mil palavras.  
No aprendizado supervisionado, a qualidade do aprendizado  também  é altamente 
dependente do tamanho e da qualidade do dataset de treino. É por meio dele que o 
algoritmo de aprendizado irá detecta r padrões  e generalizar. Além disso, quanto mais 
complexa a tarefa, mais necessidade de dados (ou: quanto mais difícil o assunto que 
estamos e studando, mais precisamos de exercícios).  
E o que significa muito? O corpus Bosque, por exemplo, tem cerca de 200 mil palavras e 
9 mil  frases. É um tamanho razoável para aprender informação relativa a classe gramatical 
e função sintática . Para tarefas em que nem todas as palavras  recebem uma etiqueta, pode 
ser um corpus considerado pequeno.  
Além disso, outra limitação do aprendizado supervisionado  é que sistemas treinados em 
um tipo de texto não costumam ter um bom desempenho quando levados a analisar um 

19 
 texto de gênero e/ou domínio diferentes, porque estrutura linguística e vocabulário serão 
diferentes. Um sistema que realiza análise sintática treinado em um corpus com textos 
jornalísticos pode ter uma  queda de 10% no desempenho quando realiza a análise si ntática 
em um material diferente, como artigos de medicina . 
O aprendizado profundo oferece soluções  para ambos os problemas. Após a criação de 
um modelo de linguagem “genérico” (porque treinado em textos variados),  a quantidade 
de material de treino necess ária tanto para aprender uma determinada tarefa quanto para 
analisar textos de domínios específicos é muito menor do que aquela que seria necessária 
no aprendizado supervisionado, o que torna a anotação linguística de dados do corpus 
uma tarefa bem mais ra zoável  do ponto de vista da quantidade .  
Na IA, dá-se o nome de transferência de aprendizado  (transfer learning ) à técnica  de 
utilizar  o conhecimento adquirido na resolução de uma  determinada tare fa na resolução 
de uma outra tarefa, aparentada com a anterior. E chamamos de ajuste fino  (fine tunning ) 
a fase de adaptações necessárias. Com isso, um modelo que é ótimo em prever como 
continuar uma frase precisa se esforçar menos (isto é, treinar menos) para aprender a 
identificar opiniões em um texto, do que um modelo de aprendizado supervisionado 
criado para resolver o mesmo problema. (Por outro lado, o modelo de aprendizado 
profundo precisou treinar muito na fase anterior, quando aprende a prever palav ras.) 
O importante aqui é destacar que diferentemente da abordagem com base em regras, em 
que o conhecimento linguístico é explicitamente programado , no aprendizado de máquina 
não há conhecimento linguístico a ser programado: o que é programado é a maneira  pela 
qual um sistema irá aprender, o que levará em conta cálculos matemáticos sobre as 
palavras do corpus. É por isso que se diz que pesquisadores de PLN precisam de 
conhecimento matemático  e estatístico , e não de conhecimento linguístico. O que será 
aprendido dependerá da exposição aos dados e de como foi programada a maneira de 
aprender a partir dos dados, e não de conhecimento específico relativo às línguas.  
Se as várias formas de aprendizado de máqui na são feitas hoje por pessoas com formação 
informática e/ou matemática, e não envolvem o que convencionamos chamar de 
conhecimentos linguísticos , por que falar del as aqui?  Entender o que está por trás dos 
métodos e abordagens bem -sucedidas ajuda a pensar sobre como estudos linguísticos t êm 
mais chances de contribuir de forma efetiva.  Ser bem -sucedido não significa ser perfeito, 
e que não se possa melhorar . 
Além disso, r egras  linguísticas  e conhecimento linguístico têm um papel ativo no PLN 
até hoje , ainda que seja um trabalho de bastidores . Elas s ão uma ótima maneira de 
construir datasets padrão ouro e versões iniciais de sistemas de P LN.    
O anotador de Brill  
Em 1992, Eric Brill lançou a primeira versão de seu etiquetador de POS: um anotador 
baseado em regras, mas que também incorpora informação estatística.  Retomo aqui este 
trabalho porque do ponto de vista da criação de um corpus padrão ouro a abordagem de 
Brill é bastante instrutiva para linguistas.  
De maneira geral, funciona da seguinte maneira:  
Ingredientes:  

20 
 Um corpus grande, padrão ouro , já anotado com  pos. Esse corpus está divido da seguinte 
maneira: 90% é usado para treino; 5% para avaliação (para o t este); e 5% para 
desenvolvimento . No trabalho original, foi utilizado o Brown Corpus , que con tém 1,1 
milhão de palavras de gêneros variados da língua inglesa.  
Como fazer:  
Etapa 1:  
1. Utilizando a partição treino do corpus, é derivada uma lista (um léxico) que 
contém , para cada palavra do corpus , a indicação das classes  ( pos) que podem ser 
atribuídas a essa palavra, associada à frequencia de cada pos. Por exemplo, se 
fôssemos fazer o mesmo com o corpus Floresta (cuj o tamanho é de 6 milhões de 
palav ras) teríamos, uma lista como a abaixo (os números são aproximados, e a 
anotaçã o foi feita de forma automática pelo analisador PALAVRAS):  
que – conjunção: 42%  
que  - pronome relativo: 51% 
um – artigo: 80%  
um – numeral: 14%  
tabuleiro – substantivo: 100%  casa – substantivo: 97%  
casa – verbo: 1.5%  
barata – adjetivo: 60%  
barata – substantivo: 40%  
 
2. Em contato com uma porção do texto sem anotação ( mas ainda na parte de treino ), 
a ferramenta atribui, para cada palavra, a etiqueta de  POS mais provável (ou seja, 
a mais fre quente) de acordo com o léxico criado a partir do corpus de treino. Este 
procedimento olha apenas para a palavra, e não leva em conta o contexto.  
3. Palavras “novas” (não estavam no corpus de treino, e portanto não apareceram no 
léxico) que estiverem em maiú scula e que não estiverem no início da frase 
recebem a etiqueta PROP, relativa aos nomes próprios.  
4. Palavras “novas” (que não estavam no corpus de treino e não viraram PROP no 
procedimento anterior) recebem a etiqueta mais frequentemente  atribuída a 
palavras que terminam da mesma maneira  (especificamente, de palavras que 
compartilham a mesma sequência de 3 últimas letras) Assim, uma sequência como 
xxxxram  será anotada como verbo, porque –ram é uma terminação típica  de verbo.  
Um linguista clássico diria: ok, muito interessante... Mas isso vai falhar! E logo 
apresentaria uma série de casos em que os procedimentos levariam a erros. Se para a 
terminação –ram  o resultado é preciso, o mesmo não se aplica a –rão, por exemplo, com 
formas como camarão  ao lado de amarão . Sim, responderia o linguista computacional. 
Sabemos que vai falhar, essas falhas já estão previstas e, por isso, esses são apenas 
procedimentos iniciais .  
Para a língua inglesa, esses procedimentos simples resultaram em uma taxa de 92% de 
acertos.  
Etapa 2:  
Após a primeira etapa, começa a fase dos “remendos”, cujo objetivo é melhorar o 
desempenho da anotação. São chamados “remendos” porque consertam os furos das 
regras anteriores. O s remendos são adquiridos (ou aprendidos) automaticam ente, e têm a 
seguinte forma:  

21 
 1. pos[0]=ART pos[1]=ART   pos=PREP  
2. pos[0]=N pos[ -1]=N  pos=ADJ  
3. pos[0]=ART pos[1]=V  pos=PREP  
 
Lemos as regras da seguinte maneira: [0] é a palavra para a qual se está olhando (a palavra 
alvo), [ -1] é a palavra que está imediatamente à esquerda da palavra alvo; [+1] é a palavra 
que está imediatamente à direita da palavra alvo. Ou seja:  
1. Se a palavra alvo é um ARTigo e à sua direita está um ARTtigo, então a palavra 
alvo se transforma  em PREPosição (Foi a_ART o_ART cinema  Foi a_PREP 
o_ART cinema)  
2. Se a palavra alvo é um Nome e à sua esquerda está um Nome, então a palavra alvo 
se transform a  em ADJetivo (um filme_N brasileiro_N  um filme_N 
brasileiro_ADJ)  
3. Se a palavra alvo é um Artigo e à sua direita está um Verbo, então a palavra alvo 
se transforma  em PREPosição (demorei a_ART perceber_V  demorei a_PREP 
perceber_V)  
Como são feitos os re mendos? De onde vêm as regras?  
Como vimos, o material é treinado em 90% do corpus, e avaliado em 5%. Os outros 5%  
restantes são usados para construir os remendos  – que é a fase do “simulado”.  Quando 
uma versão preliminar do anotador está treinada, esta ver são é utilizada para anotar esses 
outros 5% do corpus como uma anotação intermediária, feita para ver o quanto o anotador 
acerta . Na anotação desses 5% intermediários (na anotação do “simulado”), é esperado 
que haja erros, e estes erros serão corrigidos au tomaticamente  por meio da consulta a o 
gabarito do simulado. A etapa seguinte é a avaliação, isto é, a prova. Nesse momento, o 
5% do material inédito é apresentado, e então saberemos o que de fato foi aprendido.  
Um aspecto chave dessa estratégia é que “remendo que leva  à maior redução de erros” é 
diferente de “remendo que corrige mais casos”, porque é possível que um remendo corrija 
muitos erros, mas por outro lado introduza mais outros tantos, fazendo com que o 
desempenho, ao final, piore. Reproduzo a explicação do artigo de Brill (1992):  
Por exemplo, quando o etiquetador inicial é aplicado no corpus de remendos, ele 
anota erradamente como verbos 159 palavras que deveriam ser substantivos. Se 
o remendo "mude a etiqueta de verbo para substantivo se uma d entre as duas 
palavras que a precedem for um determinante" for aplicado, ele corrigirá 98 dos 
159 erros. Porém, ele também resultará em 18 erros adicionais (novos erros), 
trocando etiquetas que realmente deveriam ser verbos, para substantivos. Esse 
remendo  resulta, ao final, em uma diminuição de 80 erros no corpus de 
remendos.   
O remendo que leva à maior melhoria, quando aplicado ao corpus de remendos  (os 5% 
intermediários) , é adicionado à lista de remendos. Ele é então aplicado (ainda no corpus 
de remendo s), e assim a aquisição e escolha do melhor remendo continua, até chegar à 
melhor configuração final, isto é, aquela que proporcionará um corpus com a menor 
quantidade de erros, sendo irrelevante a quantidade de remendos necessária para isso. O 

22 
 processo te rmina quando não é mais possível reduzir erros – e, no trabalho original, para 
o inglês, levou a 95% de acertos.  
Uma característica desta estratégia é a ausência de um dicionário – o léxico inicial é 
derivado do corpus. Em um trabalho posterior, um dicion ário foi incorporado à fase inicial 
com o objetivo de melhorar a anotação de palavras não vistas no treinamento, o que levou 
à redução de 0.5% dos erros com relação à estratégia anterior.  
Deixando os resultados de lado, o ponto interessante é que se trata  de uma maneira 
diferente de encarar regras. Quando, linguisticamente, pensamos em regras, nossa ideia 
primeira é pensar em regras precisas, que não produzam erros (ou produzam o mínimo 
possível), e por isso uma proposta de regras que já contém falhas nos parece pouco 
promissora.  A estratégia aqui é oposta : usamos regras para um primeiro chute, sem a 
pretensão de que resolvam tudo . Em seguida, detectamos erros e vamos criando regras 
específicas para corrigi -los. Como já indicado, esta é uma boa estratégia para construir 
corpora padrão ouro. O chute inicial pode ser dado por algum sistema (baseado em regras 
ou em AM) e depois corrigimos.  A ideia de regras que vão progressivamente refinando 
resultados iniciais também está por trás da gramática constritiva, qu e vimos no início 
desta seção.  
2.3 Avaliação 
 
Como podemos ser  suspeitos para avaliar nossas próprias criações  (autoavaliação)  e como 
é importante poder comparar nossos resultados com outros para saber se estamos bem , o 
PLN desenvolveu o hábito das avaliações conjuntas  (shared tasks ). A ideia geral por trás 
das avaliações é fornecer uma estrutura experimental comum ( mesmos conjuntos d e 
dados para avaliar ou para treinar e avaliar ) sobre a qual o desempenho dos sistemas será 
avaliado . O bom é que  esses repositórios de dados permanecem à disposição da 
comunidade mesmo após o encerramento oficial da competição.    
 
Mais do que estabelecer qual é o melhor sistema para uma determinada tarefa, a principal 
vantagem das avaliações  é tornar possível uma  comparação  entre diferentes sistemas, 
métodos ou algoritmos , já que todos são avaliados da mesma maneira, a partir de um 
mesmo material. Com isso, quem cria pode saber como o seu sistema está com relação a 
outros que se propõem fazer a mesma coisa, um fut uro usuário/cliente pode saber qual o 
“melhor” sistema para suas demandas, e pesquisadores têm acesso a como está o estado 
da arte (isto é, qual o melhor resultado, e qual o resultado médio esperado) naquela tarefa.  
O quadro a seguir  traz resultados para a  tarefa de resolução de correferência (seção 6. 7) 
para a língua inglesa. O primeiro resultado, de 2012, se refere ao resultado da equipe 
vencedora quando a tarefa foi lançada em uma avaliação conjunta , o CoNLL  (CoNLL é 
acrônimo de Conference on Computation al Natural Language Learning ). Como 
podemos ver, anos após a realização da competição o seu dataset ( um corpus padrão ouro) 
continua sendo usado para avaliar sistemas, e com isso vemos também a evolução da área. 
Uma curiosidade é que esta era uma tarefa mu ltilíngue, isto é, envolvia a identificação de 
cadeias de correferência em três línguas distintas : inglês, chinês e árabe. A equipe 
vencedora em 2012 (Fernandes et al., 2012) foi uma equipe brasileira . Na equipe não 
havia linguistas ou falantes de  chinês o u árabe , mas isto não os impediu de criarem o 
melhor modelo para as três línguas, na época (os resultados da tabela referem -se apenas 
aos resultados para o  inglês).  
 

23 
 Ano Desempenho na tarefa de 
correferência ( para o  inglês)  
  
2012   63.37  
2017  67.2 
2019  76.6 
2020  80.2 
 
Além das  avaliações conjuntas – que são lançadas, têm prazos e vencedores – também 
existem os  benchmarks  – ou recursos de referência  ou datasets padrão  ouro –, recursos  que 
viabilizam igualmente a avaliação e comparação. Benchmarks  são materiais 
disponibilizados para medir o desempenho de sistemas em determinadas tarefas  sem a 
dimensão temporal das avaliações , mas podem ser lançados como competições, o que 
aliás dará mais publicidade ao material . E, de modo complementar, datasets us ados nas 
competições ficam  disponíveis para utilização em qualquer momento.  
 
Existem  datasets padrão ouro  que avaliam tarefas bastante complexas, e um deles é o 
SQuAD ( Stanford Question Answering Dataset ), lançado em 2019 e criado a partir de 
uma amostra de documentos da Wikipédia (em inglês). Os documentos foram divididos 
em parágrafos, e pessoas, ao receberem os parágrafos, deveriam formular e responder 5 
perguntas sobre o seu conteúdo. Para tornar o trabalho mais difícil para as máquinas, os 
criadores d o SQuAD não permitiram a utilização da funcionalidade copiar & colar , 
forçando assim as pessoas a usarem suas próprias palavras na formulação das perguntas.  
A versão 2.0 do SQuAD conta com 100.000 perguntas “convencionais”, e mais de 50.000 
perguntas irre spondíveis, isto é, perguntas para as quais não há resposta possível, criadas 
para parecerem semelhantes às convencionais. Para ter um bom desempenho  neste 
material , os sistemas devem não apenas responder às perguntas quando possível, mas 
também devem conseguir determinar quando não há resposta . Também foi pedido para 
pessoas que respondessem as perguntas do SQuAD, para que fosse possível ter uma 
medida do desempenho humano na tarefa. Atualmente, m ais de 20 sistemas já superaram 
o desempenho humano no SQuAD . 
 
O GLUE  (General Language Understanding Evaluation ) é um dataset  multitarefas, isto 
é, consiste na compilação de  datasets de  diferentes tarefas , como  resposta a perguntas, 
análise de sentimento e inferência  lógica . Para algumas tarefas, há dados de treinamento 
abundantes, mas para outras não há.  Com isso, espera -se favorece r modelos que podem 
aprender a representar o conhecimento linguístico  de uma forma que facilite o 
aprendizado eficiente  (aprender bem com pouco) e a transferência de conhecimento e ntre 
as tarefas  (ter um bom desempenho em uma tarefa e conseguir ter um bom desempenho 
em uma tarefa próxima, mesmo com poucos dados de treino) .  
 
Tanto o GLUE como o SQuAD têm  tabelas com os desempenh os dos sistemas  (chamadas 
leaderboard ) em suas respectivas páginas eletrônicas . Ao analisar a evolução dos 
resultados vemos que o sucesso das técnicas de aprendizado profundo  chega a lugares até 
então inimagináveis de serem a lcança dos sem a utilização de algum tipo de 
conhecimento, como as inf erências lógicas.  Por outro lado, avaliar os resultados apenas 
pelos números não nos deixa ver questões importantes relativas a como esses resultados 
foram obtidos, assunto que será retomado em 6. 8.  
 
Desde os títulos das conferências de avaliações e dos d atasets padrão ouro  é possível 
perceber que a língua privilegiada é o inglês, mas cada vez mais são comuns competições 

24 
 que envolv em o processamento multilíngue , e o português às vezes integra o conjunto de 
idiomas envolvidos. A principal limitação para a participação da língua portuguesa nesses 
contextos é justamente a inexistência de recursos linguísticos  dedicados às tarefas em 
questão (e voltamos ao início de sta seção: recursos são dependentes de língua). Nesse 
contexto, recursos linguísticos são, sobretudo, corpora anotados.  
 
Mas a língua portuguesa também tem suas próprias competições.  
 
O HAREM (Avaliação e Reconhecimento de Entidades Mencionadas) foi uma a valiação 
conjunta organizada pela Linguateca na área do reconhecimento de entidades 
mencionadas em português (trataremos de entidades mencionadas no capítulo 6 , e 
algumas palavras sobre a Linguateca, que teve um importante papel no desenvolvimento 
do PLN d e língua portuguesa, estão no capítulo Para Saber Mais ). O objetivo do HAREM 
era avaliar o sucesso na identificação e consequente classificação automática de nomes 
próprios em vários textos em língua portuguesa. O HAREM contou  com duas edições, 
em 2003 e 2 008 e, mesmo já tendo se passado há tanto tempo, os recursos produzidos no 
HAREM – corpus com anotação padrão ouro, chamado Coleção Dourada, e uma série de 
ferramentas e métricas associadas – são utilizados até hoje. A Linguateca organizou ainda 
outras ava liações conjuntas, como o Págico , que será mencionado na seção a seguir, em 
2012, e em 2003, as Morfolimpíadas , cujo objetivo foi avaliar analisadores morfológicos 
da língua portuguesa. Todas as avaliações têm suas respectivas páginas eletrônicas, onde 
todo o material - conjuntos de dados, instruções, resultados, publicações  – está disponível 
para consulta e download.  
 
Outra avaliação conjunta é a ASSIN (Avaliação de Similaridade Semântica e de 
Inferência Textual), dedicada às tarefas de Inferência Textual  e Similaridade Semântica . 
Na inferência textual, o desafio consiste em determinar se o significado de um trecho 
implica o outro; na Similaridade Semântica a tarefa consiste em atribuir uma pontuação 
de similaridade semântica a esses trechos, segundo uma e scala de 1 a 5 (falaremos de 
ambos na seção 6.7) A primeira edição da ASSIN aconteceu em 2016, e a segunda, em 
2019.  
 
Além de HAREM, Págico, Morfolimpíadas e ASSI N, também temos os seguintes 
conjuntos de dados que permitem avaliações da língua portuguesa  (e tudo isso está listado 
na seção @ Ponteiros ):  
 
Bosque  - subconjunto do treebank Floresta Sintática, composto por textos de jornal e 
voltado para a tarefa de análise sintática. Disponível em diferentes formatos.  
MacMorpho – composto por textos de jorna l e voltado para a tarefa de classificação de 
palavras . Disponível em três versões.  
CHAVE – resultado da participação da língua portuguesa na avaliação CLEF (Cross -
Language Evaluation Forum), nos anos de 2004 a 2009. Contém textos de jornal e é 
voltada para tarefas de recuperação de informação, resposta a perguntas e recuperação de 
informação geográfica.  
 
Coleções de dados públicas e acessíveis que contenham problemas e seus gabaritos, além 
de permitirem o entendimento de como se comporta um método/ferrame nta em relação 
ao desempenho humano e em relação a outros métodos/ferramentas, são cruciais para o 
desenvolvimento do PLN. Por isso uma das coisas de que precisamos para avançar com 
PLN em português é criar este tipo de recurso.  E quem produz esses conjunt os de dados, 

25 
 que têm um prazo de validade enorme e determinarão a qualidade do aprendizado e da 
avaliação? Com muita frequência, linguistas.  
 
Tipos de avaliação  
 
A avaliação intrínseca  é interna à tarefa que está sendo avaliada. Ou seja, considerando 
que uma tarefa pode ser  uma etapa intermediária para a realização de uma aplicação, a 
avaliação intrínseca avalia o desempenho de um sistema (ou modelo) naquela tarefa 
específica. Já a avaliação extrínseca  avalia o desempenho desse mesmo sistema (ou 
modelo) em tarefas subsequentes .  
 
Podemos avaliar uma ferramenta que anota classes de palavras comparando o resultado 
da sua análise com o resultado do gabarito (do corpus padrão ouro), e esta é  uma avaliação 
intrínseca. Podemos avaliar modelo de língua que prevê a próxima palavra comparando 
a palavra prevista com a palavra efetivamente usada naquela posição, e esta também é 
uma avaliação intrínseca. Podemos avaliar este mesmo modelo de língua  em uma tarefa 
de anotação de classes de palavras, também comparando o resultado da sua análise com 
o resultado do gabarito, e esta é uma avaliação extrínseca. A avaliação é extrínseca porque 
ela não avalia aquilo que, diretamente, o modelo (ou ferramenta ou sistema) faz. Ela 
avalia indiretamente, verificando o quanto outras tarefas se beneficiam do referido 
modelo (ou ferramenta ou sistema).  
 
Para a maioria das tarefas de PLN, a avaliação pode ser feita de forma automatizada, 
comparando os resultados dos sist emas com os resultados dos gabaritos conforme 
algumas métricas (ou medidas).  Algumas delas são precisão , abrangência  e medida F , 
que é uma média harmônica entre a precisão e a abrangência.  
 
A precisão mede a qualidade das respostas, verificando se tudo aquilo que um sistema 
classificou como X é, de fato, X. Mais especificamente, a precisão mede a proporção de 
respostas corretas  fornecidas  considerando o total de respostas fornecidas.  
Já a abrangência mede a  capacidade de encontrar respostas, não importa se corretas ou 
não, verificando a proporção de respostas encontradas considerando todas as respostas 
que deveriam ter sido encontradas. A medida da abrangência indica se tudo aquilo que 
deveria ter sido encon trado foi, de fato, encontrado. Para calcular a abrangência 
precisamos de um gabarito, e não apenas de uma análise de erros, pois uma análise de 
erros só mostra a precisão.  
Para calcular precisão , abrangência  e a medida F , classificamos os resultados da seguinte 
maneira:  
- verdadeiro positivo  (VP) : o elemento foi detectado pela análise automática e foi 
classificado de forma correta.  
- verdadeiro negativo  (VN) : o elemento foi detectado pela análise automática, mas 
foi classificado de forma errada.  
- falso positivo  (FP): o elemento foi detectado pela análise automática, mas não 
deveria.  
- falso negativo  (FN) : o elemento não foi detectado pela análise automática, mas 
deveria.   
Para calcular a precisão fazemos  VP ÷ (VP+FP) 

26 
 Para calcular  a abrangência fazemos  VP ÷ (VP+FN) 
Para calcular a medida F  fazemos         
 
O quadro abaixo traz duas anotações  de entidades para o mesmo texto (veremos este tipo 
de anotação no capítulo 6, mas ela é intuitiva o suficiente para ser compreendida aqui. A 
ideia é fornecer classificações para os nomes próprios).  O primeiro trecho é o gabarito, 
e o segundo é a análise que queremos ava liar.  
  
Quando comparamos a segunda análise com a primeira, encontramos o seguinte:  
VP: 3  (França LOCAL ; Frau Troffea PESSOA Estrasburgo LOCAL ) 
VN: 2  (Estrasburgo ORG; Sacro Império Romano -Germânico LOCAL ) 
FP: 1  (Epidemia LOCAL ) 
FN: 2 (1518 DATA; 1518 DATA) 
 
Precisão: 3 ÷ (3+1) = 0.75    Abrangência: 3 ÷ (3+2) = 0.6  
Medida F:  
 
Ou seja, no exemplo acima, a precisão é melhor que a abrangência , que é baixa . E a 
medida F é de 0.6 . 
Uma ferramenta pode ser muito precisa - todas as classificações que ela faz são  corretas  
-  e pode, igualmente, ter uma baixa  abrangência – apesar de acertar, há muit os casos que 
ficam de fora . Em geral, há uma tensão entre essas duas medidas: se afrouxamos a 
abrangência, para encontrar mais casos, podemos diminuir a precisão, trazendo muitos 
casos errados. E, tentando melhorar a precisão, corremos o risco de perder em 
abrangência. Por isso, um bom desempenho se reflete em um equilíbrio entre essas 
medidas , e esta é a proposta da medida F: indicar em um único número uma combinação 
entre precisão e abrangência que reflita o desempenho geral .   GABARITO: Epidemia de dança de 1518 DATA foi um caso de dançomania e histeria 
coletiva ocorrido em Estrasburgo LOCAL , França LOCAL  (então parte do Sacro Império 
Romano -Germânico ORG) em julho de 1518 DATA. O fenômeno teve início quando uma 
mulher, Frau Troffea PESSOA , começou a interpretar passos frenéticos de dança numa rua 
da cidade de Estrasburgo LOCAL  aparentemente sem qualquer motiv o.  
==============================================================  
ANÁLISE AUTOMÁTICA:  Epidemia LOCAL  de dança de 1518 foi um caso de 
dançomania e histeria coletiva ocorrido em Estrasburgo ORG, França LOCAL (então parte 
do Sacro Império Romano -Germânico LOCAL ) em julho de 1518. O fenômeno teve 
início quando uma mulher, Frau Troffea PESSOA , começou a interpretar passos 
frenéticos de dança numa rua da cidade de Estrasburgo LOCAL  aparentemente sem 
qualquer motivo.  
      precisão * abrangência  
      
2 
      
 precisão + abrangência  
          0.75 + 0.6 
        0.75 * 0.6 
      
2 
      
= 0.6  
      

27 
 Precisão e abrangência  (e medida F)  são apenas algumas maneiras de avaliação, e cada 
tarefa pode ter suas especificidades. Na página do livro, em @ Sobre avaliação sintática  
(e a anotação sintática é abordada em 6.2), apresento como a avaliação da sintaxe tem 
sido realizada.  
Cálculos relativos à precisão e à abrangência, dentre outros, são feitos automaticamente, 
e esta é mais uma das v antagens dos corpora padrã o ouro  na avaliação : basta criar o 
gabarito uma vez, e aplicamos sempre a mesma prova. O que pode mudar são os 
aprendizes (sistemas, modelos ou ferramentas) e os métodos a prendizagem (os 
algoritmos). Além disso, quando fazemos uma análise de erros manual, sem produzir um 
gabarito, é comum não repararmos naquilo que não foi encontrado, mas deveria ter sido 
(os falsos negativos). Ou seja, se analisássemos apenas os resultados  da máquina, é 
possível que não reparássemos nos casos de “1518” que não chegaram a ser classificados. 
Por isso é tão importante a criação dos gabaritos.  
No entanto, para muitos problemas de PLN a criação de um gabarito pode ser complexa , 
como é o caso da  tradução automática, ou pode não haver gabarito disponível.  Vamos 
supor  que eu precise de uma ferramenta de análise sintática para anotar textos jurídicos, 
especificamente decisões de juíze s. Sabemos que se trata  de uma linguagem com suas 
estruturas e léxico próprios , às vezes referida como “juridiquês” . Para saber se podemos 
confiar na análise de uma ferramenta, já sabendo de antemão que ela foi criada (ou 
treinada) tomando por base textos de jornal, precisamos t er uma medida da sua qualidade 
antes de utilizá -la. Neste caso, o que podemos fazer é a análise manual de uma amostra, 
lançando mão de nosso conhecimento linguístico específico relativo à tarefa em questão . 
Posteriormente, e conforme o tamanho da amostra, ela poderá ser usada como padrão 
ouro.  
 
2.4 Textos e informação não -estruturada  
A relevância do PLN é evidente quando nos damos conta da imensa – e em constante 
crescimento - produção e disponibilização de conteúdo na forma de textos. No entanto, 
este cont eúdo está no texto de maneira não estruturada, isto é, não segue nenhum formato 
previsível. A figura abaixo  apresenta três maneiras diferentes de indicar a morada de 
alguém – esta informação  (de que se trata de  três maneiras de dizer a mesma coisa)  é 
óbvia para nós, mas não para as máquinas. E esta mesma informação, por outro lado, 
poderia estar disponível por meio de um formulário, com campos pré -estabelecidos  para 
informações como nome , rua, complemento , bairro , cidade , cep, telefone , código de ár ea. 
Neste caso, seria fácil para as máquinas saber do que se trata , pois a informação já estaria 
organizada (estruturada) . 
 
 
 
 
 
 
 
 
Podemos comparar o preenchimento de formulários com a escrita livre de jornais, peças 
ficcionais, entrevistas, resenhas, tweets...  Felizmente para nós, e infelizmente para o 
processamento automático, a língua (qualquer língua) nos oferece várias maneiras  Batman Silva, Pra ça dos Patos, 160/ 1002 - Humait á, Rio de Janeiro. 22260 -221, tel 
(021) 22555443.  
 Batman Silva. Pra ça dos Patos n . 160 – apto 1002. Humait á – 22260 -221. Rio de 
Janeiro/RJ,  021 2255 -5443  
 Batman Silva mora no n úmero 160 da Pra ça dos Patos, apartamento 1002, no bairro do 
Humait á, Rio de Janeiro. CEP: 22260 -221. O telefone é 22555443, e o c ódigo de área é 
021.  

28 
 diferentes de dizer as mesmas coisas. É compreensível que o processamento automático 
seja um desafio e tanto.  
 
As caixas de informação da Wikipédia ( infoboxes , não confundir com as caixas que são 
índices dos artigos) são um exemplo concreto de informação e struturada. Uma caixa de 
informação é uma tabela com formato fixo que fica no canto superior direito do artigo e 
tem como objetivo apresentar um resumo com aspectos relevantes e comuns (fatos e 
estatísticas) sobre o tema do artigo.   
As caixas de informação  contêm fatos pré -definidos conforme a natureza do artigo. 
Artigos biográficos, por exemplo, devem conter pelo menos certos tipos de informação 
(nome, local e data de nascimento, ocupação etc), artigos de animais devem contar  com 
informações como classific ação científica (gênero e família) etc. Com isso, é mais fácil 
encontrar tais informações e compará -las com a de outros artigos da mesma categoria.  
Além disso, as caixas devem resumir informações que estão presentes no texto principal, 
e não podem conter i nformação que não seja sustentada pelas fontes confiáveis no corpo 
do artigo.  
Por já conterem espaços previstos para certos tipos de informação, podemos dizer que as 
caixas contêm informação estruturada relativa ao conteúdo dos artigos. O quadro abaixo 
contém a informação de uma caixa de informação como nós a vemos (esquerda, imagem 
editada), e como está codificada (direita , na parte de cima ). Ou seja, toda a informação 
que aparece estruturada nas caixas de informação está no texto, mas de maneira não 
estruturada (direita, parte de baixo).  
  
 
  
 
 
 
Alfredo da Rocha Vianna Filho, conhecido como 
Pixinguinha (Rio de Janeiro, 23 de abril de 1897 — Rio de 
Janeiro, 17 de fevereiro de 1973), foi um maestro, flautista, 
saxofonista, compositor e arranjador brasi leiro.  
 
2.5 Leitura, processamento  automático  e compreensão  
 
Encontrar informação em textos é apenas uma das atividades que fazemos com a 
linguagem . Podemos entendê -la de forma genérica  como qualquer processo que encontra , 
estrutura  e combina  dados que estão em textos. Podemos dizer então que a extração de 
informação é responsável por estruturar a informação que aparece espalhada em textos. 
Voltando à figura com acima , a saída de um sistema de ex tração de informação poderia 
corresponder a algo  como o código da caixa de informação, com campos pré -definidos .   


29 
 Em 2011, foi lançada  uma avaliação conjunta chamada Págico, na qual sistemas deveriam 
encontrar as respostas para uma lista de perguntas ou tópicos vasculhando as páginas da 
Wikipédia. A id eia era simular uma situação real de pesquisa complexa, que envolve a 
leitura de uma grande coleção de  documentos com o objetivo de encontrar resposta – no 
caso, respostas – para uma questão, sabendo que as respostas estão espalhadas em 
diversos documentos . Originalmente, o Págico previa a participação também de pessoas, 
para que fosse possível comparar o desempenho humano ao da máquina, mas foram 
poucos os participantes humanos. Alguns exemplos de tópicos do Págico:  
 
Tópico 01: Filmes sobre a ditadura ou sobre o golpe militar no Brasil.   
Tópico 13: Dinossauros carnívoros que habitaram o Brasil.   
Tópico 62: Praias de Portugal boas para a prática de surf   
Tópico 80: Línguas faladas em Timor Leste  
 
Uma especificidade d o Págico é que o nome da página deveria ser a resposta, e não 
simplesmente conter  a resposta (do contrário, teríamos outra tarefa, que seria a geração 
do texto da resposta). O problema subjacente pode ser formulado de duas maneiras:  
 
(a) O que é preciso pa ra que um sistema compreenda o que deve procurar nos texto s? 
(b) O que é preciso para que um sistema forneça respostas adequada s para a pergunta ? 
 
As perguntas formuladas em (a) e (b) são a mesma coisa?  
Sim no que se refere ao resultado final esperado. Não no que se refere à tarefa do 
computador. Isto porque, em (a), a compreensão  é etapa necessária para a obtenção do 
resultado desejado. Para que um sistema realize a tarefa ad equadamente, é preciso que 
ele entenda  o text o (e as perguntas); e para que entend a o texto, é preciso que entenda  as 
palavras.  
 
Do ponto de vista assumido em (a) c abe aqui  a analogia entre a compreensão do PLN e a 
de um papagaio, que repete aquilo que lhe foi ensinado, mas não compreende o que diz. 
Muito da discussão acerca da compreensão das máquinas se deve ao uso que tem sido 
feito, no PLN  e na IA, da palavra  compreensão . A partir de quais critérios concluímos 
que alguém compreendeu  algo?  
 
Mais uma vez podemos usar a analogia com uma situação de  prova: como garantir  que a 
pessoa , ao responder corretamente  uma questão , compreende o que diz , ou compreende 
a resposta ? Como garantir que aquilo  que estamos aferindo ao corrigir é o resultado de 
uma compreensão  verdadeira , em oposição à habilidade de construir paráfrases ou de 
repetir em uma prova o que foi dito  em aula? Que critérios utiliza mos para avaliar a 
compreensão? A única maneira de aferirmos compreensão é por meio do comportamento 
– se a outra pessoa  age segundo a quilo que seria esperado, como se tivesse compreendido . 
Nos termos do filósofo Wittgenstein, “compreende uma ordem aquele que age de acordo 
com ela ” (1953:§6) . Mas essa não é uma visão unânime, e uma quantidade de artigos vem 
(re)discutindo o conceito de compreensão no PLN e na IA ( por exemplo, Bender  e Koller, 
2020; Dunietz et al., 2020).  
 
 
  

