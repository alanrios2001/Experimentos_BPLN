50 
 5. Preparativos para um processamento computacional da língua  
(ou: pré -processamento também é processamento ) 
 
A fase de preparativos para o processamento automático de textos é chamada de pré -
processamento (falamos em “pré -processamento do corpus”), mas este nome é enganoso, 
pois sugere que o verdadeiro “processamento” é o que vai acontecer depois. Embora nem 
sempre explicitada, e às vezes  considerada trivial ou pouco important e, esta fase envolve 
a tomada de decisões linguísticas tão básicas quanto  cruciais, já que irão determinar as 
unidades primeiras com que as demais ferramentas irão trabalhar: palavra  e frase .  
Entretanto, anterior à identificação de palavras e frases é a identificação de tudo o que 
não é texto para o PLN: tabelas, gráficos, figuras, fórmulas, cabeçalhos e rodapés  etc. 
Quando mantidos, esses elementos são transformados em caracteres estranhos que  são 
confundidos com palavras , atrapalhando o processamento automático .  
No mundo do PLN, o s procedimentos relativ os à segmentação de um texto em palavras 
e frases são chamad os, respectivamente, de tokenização  e de sentenciação .    
Tokenização : a etapa de tokenização consiste em segmentar o texto em unidades, 
chamadas tokens .  Um token  pode ser uma palavra ou algum outro sinal que compõe o 
texto, como os sinais de pontuação ou símbolos como $ . Por isso, em um texto, o número 
de tokens é superio r ao número de p alavras. Mas o que é uma palavra?  
Para começar, devemos lembrar que nem palavra , nem frase , são objetos “naturais” 
(como nenhum outro objeto linguístico). Além disso, não há uma definição única do que 
seja palavra , e a Linguística opera co m diferentes conceitos, relativos a diferentes 
perspectivas: a palavra de um ponto de vista fonológico, de um ponto de vista 
morfos sintático e de um ponto de vista semântico. Como  nem sempre haverá 
convergência entre esses diferentes pontos de vista , uma noção prática e de amplo uso é 
a de palavra gráfica. Ou seja, é palavra  tudo aquilo que em um texto escrito aparece entre 
espaços em branco ou sinais de pontuação. Pelo critério gráfico, a frase anterior tem 20 
palavras, e esta frase tem 14. A frase an terior com 20 palavras tem 22 tokens, e a frase 
seguinte com 14 palavras tem 17 tokens . Entretanto, apesar da aparente simplicidade, há 
espaço para discussão e decisão linguística  em ambas as frases .  
Deixar espaços em branco entre as palavras corresponde à possibilidade de pausa entre 
as unidades na fala. Trata -se de uma prática consolidada na Idade Média, e que é cheia 
de distorções e imprecisões, uma vez que os dados fônicos da oralidade nem sempre 
correspondem às unidades lexicais da língua. Em geral, q uanto mais antiga a tradição 
escrita de uma língua, mais teremos segmentações arbitrárias e incoerentes . Este fato traz 
questões relevantes para a fase de tokenização, e voltamos às nossas frases, repetidas 
abaixo:  
(1) Ou seja, é palavra tudo aquilo que em um texto escrito aparece entre espaços em 
branco ou sinais de pontuação.  
(2) Pelo critério gráfico, a frase anterior tem 20 palavras, e esta frase tem 14.  
Do ponto de vista gráfico, ou seja  (frase 1) conta como 2 palavras, mas essa identificação 
interessa? Se  tivermos  2 palavras em “ou seja ”, qual a classe de cada uma? Sem titubear, 

51 
 podemos dizer que “ou” é uma conjunção, mas o que dizer de “seja”? Sabemos que verbos 
flexionam, e sabemos igualmente que “ou sejam”; “ou foi” ou “ou será” não existem no 
contexto da locução (e quem está familiarizado com a gramaticalização já sabe que 
“palavras” do tipo ou seja  estão longe de ser raras).  A frase (2) contém a forma “pelo”, 
que tanto pode ser desmembrada em “por” e “o”, como pode ser trada como uma unidade. 
Diferente mente do “ou seja”, o caso das contrações (presentes em do; da; no; na; neste; 
naquilo etc) não envolve discussão, apenas a tomada de uma decisão (descontrair ou não), 
e há sistemas que trabalham com as contrações desfeitas e sistemas que separam as 
contra ções em duas palavras.  
Voltando ao exemplo (1) , questões de  delimitação associadas ao que chamamos 
genericamente de locução  têm se tornado cada vez mais frequentes, e podem ser 
agrupad as sob o rótulo guarda -chuva das expressões multipalavra  (ou mwe, do inglês 
multi -word expression ). O que fazer com “levar em conta ” ou “abrir mão”? Levando em 
conta o critério gráfico, temos 3 palavras no primeiro caso, e 2 palavras no segundo. Isso 
quer dizer que “ em conta ” ou “mão” são objetos/argumentos dos verbos? A pesar de tantas 
dúvidas, aqui temos uma certeza: não queremos uma análise que indique que “mão” ou 
“conta ” são argumentos de “levar ” e “abrir ”, respectivamente. Além de não fazer sentido, 
estamos criando um problema  para a sintaxe , pois se “mão” for argumento de “abrir”, 
como analisar “prêmio” em “abrir mão do prêmio”?  A linguista Maria Teresa Biderman 
afirma que só a dimensão semântica oferece a chave decisiva para identificar a unidade 
do léxico expressa no discurso (Biderman, 2001). Po rém, nossas intuições semânticas são 
muito menos precisas que as intuições formais. Formas são visíveis, têm materialidade, e 
o sentido não tem. Por isso, a delimitação de unidades lexicais complexas continua sendo 
uma questão nos estudos linguísticos, e u ma questão que tem consequências para o PLN.   
Especificidades de gênero s textua is trazem desafios adicionais: textos técnicos podem 
conter “palavras” como poli(bisfenol A -co-epicloridrina) , que contém hífen, espaço, 
parêntese e travessão, mas que é consid erado um único item. A comunicação digital traz 
hashtags , que com frequência constituem um sintagma ou frase completa (#soquenao),  
emojis e elementos como rsrsrs; ; -) , :-}}, :-P.  
Na criação dos modelos de língua (2.1.1), o primeir o passo  consiste em  traduzir as 
palavras de um texto em representações numéricas , já que lidar com números facilita o 
trabalho das máquinas. Mas quais unidades virarão números?  Qual o c onceito de palavra  
subjacente ? São três as estratégias de tokenização: baseada em palavra,  baseada em 
caractere e baseada em subpalavra.  
A tokenização baseada em palavra usa o critério gráfico na delimitação de tokens. O ponto 
positivo desta abordagem é que as representações criadas a partir das “palavras” serão 
bastante informativas do ponto de vista do sentido e do contexto, já que cada 
representação corresponde a uma palavra. Porém, usando apenas o critério gráfico, a 
abordagem lida com  unidades que apesar de separadas por espaços em branco 
correspondem a uma mesma palavra, como “camisa de f orça”, “de repente” e “engolir 
sapo”. O critério gráfico também não captura relações  importantes entre as palavras, 
como a relação entre as formas  “palavra” e “palavras ”. Isto é, cada uma das formas 
“palavra” e “palavras” é tratada como uma unidade distinta, e receberá um identificador 
diferente. Outro ponto negativo diz respeito  a uma característica da língua que vimos no 

52 
 capítulo 3: a imensa quantidade de palavras difer entes. Se a intenção  é criar um modelo 
que consiga lidar da forma correta com todas frases do corpus (todas as frases contidas 
no material de treino) , precisaríamos de um identificador para cada palavra, e logo a 
quantidade de palavras do modelo ficaria gi gantesca, tornando o processamento mais 
pesado. Uma solução utilizada p ara este problema é ignorar as palavras menos frequentes. 
Neste caso, cada palavra com frequência abaixo de um determinado limite recebe uma 
etiqueta [desconhecida ], o que significa que  palavras pouco usadas como cotidianidade  e 
dançomania  serão representadas exatamente da mesma maneira. E como também já 
vimos no capítulo 3 , essa pode não ser uma boa ideia, pois muito da língua ficaria de fora.  
A tokenização baseada em caracteres (letra s e símbolos), apesar de parecer estranha, tem 
a vantagem de ser mais econômica: ao invés de criar uma representação numérica para 
cada palavra da língua, é criada uma representação para cada letra. Com isso, mesmo 
palavras com erros de ortografia ou com ortografia pouco comum ( ameeeei ) são 
tokenizadas. No entanto, diferentemente das palavras, os caracteres praticamente não 
carregam informação. Ou seja, será necessário um esforço adicional para atribuir sentido 
a um conjunto de tokens que correspond a a uma palavra. Outra característica  dessa 
estratégia é imensa  quantidade de tokens que precisar á ser processad a. Se na abordagem 
anterior contamos 2 tokens em “palavra” e “palavras”, agora passamos a contar  7 e 8 
tokens, respectivamente, o que também tem impacto no processamento : para conseguir 
dar conta de tantos tokens, será necessário limitar a quantidade de texto a ser processada.  
A tokenização baseada em subpalavra é uma abordagem intermediária, e perm ite lidar 
melhor com as palavras não vistas (diferentemente da abordagem de palavras) sem 
aumentar a quantidade de tokens (diferentemente da  abordagem baseada em caracteres ). 
Na tokenização baseada em subpalavra os algoritmos aprendem, por meio de estatíst ica, 
aquilo que chamamos de morfema – os algoritmos aprendem a separar as palavras em 
pedaços que têm algum significado a partir da recorrência desses pedaços.  Palavras 
frequentes são consideradas na íntegra,  isto é, são indecomponíveis, e palavras pouco 
frequentes são quebradas em partes significativas, as tais “subpalavras” que d ão nome à 
estratégia. Assim, palavras raras como cotidianidade  e dançomania , que na abordagem 
baseada em palavras são igualmente tratadas como [desconhecida], ganham alguma 
repres entação devi do à recorrência de _idade e _mania , e a quantidade de palavras 
desconhecidas que consegue se r tratada aumenta. Pelo mesmo princípio, “palavras” será 
quebrada em “palavra” e “s”, evitando com isso duas representações dissociadas para o 
caso de singular e de plural. Os tokenizadores baseados em subpalavras conseguem 
identificar prefixos e sufixos, dando conta, por exemplo, de “desmereci mento ”, que será 
transformado em três tokens . O fato de levar em conta a frequência das palavras é uma 
maneira i nteressante de evitar segmentações equivocadas, como em “desenvolvimento ”, 
em que não queremos des_ e _envolvimento  ou des_ confiar , mas queremos des_  
proteger . Atualmente, os modelos de língua com os melhores resultados utiliza m essa 
estratégia de tokenização  e, linguisticamente, entendemos o motivo de ela levar a os 
melhores resultados . 
Abordagens baseadas em regras linguísticas também utilizam pistas morfológicas no 
reconhecimento de palavras nunca vistas. A diferença é que pref ixos e sufixos serão 
manualmente inseridos pelo programador.  

53 
  
Sentenciação : A sentenciação diz respeito à identificação de frases, e seu primeiro 
desafio está em reconhecer quando um ponto final não está sendo usado para delimitar 
frases . É frequente o uso de ponto fina l para abreviar  nomes próprios (Philip B. Morris), 
ou comuns  (“em conformidade com o art. 4°), e não queremos, em nenhum desses casos, 
que o ponto seja considerado um separador de frases. Também há  a situação oposta , 
quando o  fim da frase não é indicado por qualquer sinal de pontuação, mas por uma 
mudança de linha, caso de manchetes  de jornais , títulos ou nomes de seção  em textos 
técnicos ou acadêmicos. Uma consequência da ausênc ia do sinal de pontuação é que 
diferentes frases  ficam  concatenadas como se fossem uma única frase , dificultando as 
etapas posteriores de análise linguística . A figura a seguir ilustra o ponto, com um trecho 
do Estatuto da Criança e do Adolescente . 
 
 
 
Exemplo  de um pré-processamento  mal feito no que se refere  à sentenciação , em 3 momentos .  
 
A figura  traz o mesmo trecho em 3 momentos: o documento original (em pdf) com as 
marcas de parágrafo; a versão convertida  em texto simples  (texto simples  é um arquivo 
de texto sem informações de estilo ou de formatação. Arquivos criados no “B loco de 


54 
 Notas”, o editor de texto encontrado no sistema operacional Windows, são exemplos de 
arquivo s de texto simples, que têm a extensão “.txt”) ; e a saída de um programa de análise 
linguística . Exceto pela primeira fras e, “Parágrafo único .”, todo o trecho que vem a seguir  
não tem ponto final  e é a formatação que nos informa que estamos diante  de unidades de 
informação distintas. Porém, a consequência disso é que tudo o que vem após “Parágrafo 
único .” pode ser considerado uma única frase ou, no exemplo do terceiro momento da 
figura, o trecho foi considerado duas frases, com uma segmentação completamente 
aleatória.  
Onde gostaríamos de identificar frases no trecho selecionado? Qual a segmentação certa? 
Cada parágrafo do documento original deve ser considerado uma frase, mesmo que esteja 
separado do parágrafo seguinte por ponto e vírgula?  Já em t extos literários , os dois pontos 
podem sinalizar o início da fala de personagens no discurso direto , frequentem ente 
introduzid a com um novo parágrafo . Do mesmo modo que na tokenização, diferentes 
gêneros textu ais trarão diferentes desafios e precisarão de soluções diferenciadas.  
5.1 StopWords: por que retirá -las? 
É comum, em trabalhos com processamento automático de textos , a menção à retirada de 
stopwords  do texto  como uma das etapas de pré -processamento. Independentemente  da 
decisão de retirá -las ou não, o importante é que esta seja uma decisão informada.  
Stopwords  são palavras consideradas irrelevantes ( ... e o que é irrelevante para uma 
pessoa /tarefa  pode não ser para outra ...) e que, por isso, podem ser descartadas. As 
palavras mais frequentes de um corpus – artigos, preposições, conjunções, verbos 
auxil iares, por exemplo – costumam ser consideradas irrelevantes, porque pouco 
significativas com relação ao conteúdo de um documento.   
A ideia de trabalhar com uma lista  de palavras que podem ser eliminadas do texto surge  
no conte xto da indexação automática de documentos : utilizar estas palavras frequentes 
como elementos de um índice, além de não ser informativo, exige um espaço maior de 
armazenamento. A ideia subjacente às stopwords é que  ao se deparar com as palavras  de 
uma lista, deve -se interromper ( stop) a indexação e o processamento dos documentos.  
Porque as palavras mais frequentes irão corresponder ao que  chamamos de palavras 
gramaticais  (em oposição a palavras lexicais ), é comum também uma c onfusão entre 
esses termos. Qualquer palavra que se deseje eliminar de um documento por ser pouco 
informativa pode ser considerada uma stopword , independentemente de sua classe 
gramatical, e poderá integrar uma lista de palavras que serão canceladas (uma stoplist ). 
Uma crítica que se pode fazer  à eliminação irrefl etida de palavras gramaticais tem a ver 
com a noção de palavra. Quando se justifica a remoção de palavras gramaticais com o 
argumento de que palavras lexicais e palavras gramaticais têm valores semânticos 
diferentes – comparemos o sentido de lagoa  e o sentido de de – fica explícito que o critério 
semântico é um critério relevante. Entretanto , se eliminamos a palavra “de”, camisa de 
força  vira duas palavras,  camisa  e força , e isso poderá ter impacto em tarefas que 
envolvem sentido ou conteúdo dos te xtos. Se levamos em conta que terminologias, ou 
palavras específicas de um domínio, são com frequência unidades de mais de uma palavra 
é importante ter ciência do que se ganha e do que se perde com a decisão de eli minar ou 
manter as palavras  gramaticais . 

