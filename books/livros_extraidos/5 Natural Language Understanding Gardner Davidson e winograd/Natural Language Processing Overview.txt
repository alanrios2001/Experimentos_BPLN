____________
A. Naturai Language Processing Overview
The most common way that human beings communicate Is by speaking or writing In one
of the “natural” languages , like English, French, or Chinese. Computer programming
languages , on the other hand, seem awkward to humans . These “artificial” languages are
designed to have a rigid format, or s,ntax, so that a computer program reading and compiling
code written in an artificial language can understand what the programmer means. In addition
to being structurally simpler than natural ianguages , the artificial languages can express
easily only those concepts that are Important in programming: “Do this then do that,” “See if
such and such Is true,” etc. The thin gs that can be expressed in a language are referred to
as the semantics of the language.
The research on understanding natural language described in this section of the
Handbook Is concerned with programs that deal with the full range of meaning of languages
like English. Computers that can understand what people mean when typiil~ (or speaking)
English sentences will be easier to use and will fit more naturally into people ’s lives. In
addition , artificial intelligence (Ai) research in natural language processing alms to extend our
knowledge of the nature of language as a human activity. Programs have been written that
are quite successful at understanding somewhat constrained input: the user Is limited in
either the structural variation of his sentences (syntax constrained by an artificial grammar)
or in the number of things he can “mean” (in domains with constrained semantics). Some of
these programs are adequate for many useful computer-interface tasks and are available
commercially. But the fluent use of language as humans use it is still elusive , and natural
language (NI) processing Is an active area of research in Al.
ml. article presents a brief sketch of the history of natural language processing
research in Al, and it attempts to give some idea of the current state of the art in NI and
related research In representing knowledge about the world within the language
understanding programs. The next article Is a historical sketch of the very earliest ideas
about processing language with computers , to achieve meckanical tranilatlon of one language
Into another. It is followed by two sections containing technical articles on some of the
grammars and parsin g techniques that Al researchers have used in their programs. Then,
after an article on text generation , which involves the creation of sentences by the program to
express what It wants to say, there are a half dozen articies describing some of the most
important NI systems.
0
Two other sections of the Handbook are especially relevant to NI research. SpIsch
Undsra ten~ng research attempts to build computer interfaces that actually unders land
spoken language. Speech and natural language understanding research have been ciosely
linked. Increasingly Inseparable from NI research Is the study of Knowledge Rspresentation ,
because Al researchers have come to believe that a very large amount of know ledge about
the world Is used in even simple dialogue. Research in the representation of knowledge
explores ways of making this world knowledge accessible to the computer program by
“represen ting” it in Internal data structures.
History
Research in computational linguistics, the use of computers In the study of language ,
started In the 1940s , soon after computers became available commercially. The machine ’s~
.  A

- —.-.------- ... --.-..-- —- --.--. ~~~—,-.-—----- —-.----—--.—-.“-“ ~~~~~~~ -~~
2 Natural LanØ uags
ability to manipulate symbols was first used to compile lists of word occurrences (word lists)
and concordances (their contexts in written texts). Such surface-level machine processing
of text was of some value In linguistic research , but it soon became apparent that the
computer could perform much more powerful linguistic 4unction s than merely counting and
rearranging data.
In 1949, Warren Weaver proposed that computers might be useful for “the solution of
the world-wide translation problem ” (Weaver , 1949, p. 15). The resulting research effort,
called mechanical translation , attempted to simulate with a computer the presumed functions of
a human translator : looking up each word In a bilingual dictionary; choosing an equivalent
word in the output language; and, after processing each sentence , arranging the resulting
strIng of words to fit the output language ’s word order. Despite the attractive simplicity of
the idea, many unforeseen problems arose, both in selecting appropriate word equivalences
and in arranging them to produce a sentence in the output language. Article B discusses the
hIstory, problems , and current state of research on mechanical translation.
In the 1 960s a new group of computer programs was developed that attempted to deal
with some of the more complex issues of language that had led to the difficulties in the
mechanical translation efforts. These early natural language programs mark the beginning of
artificial intelligence work in understanding language. They no longer assume that human
communication is a process of word manipulation . Instead , they view human language as a
complex cognitive ability involv ing many different kinds of knowledge: the structure of
sentences , the meaning of words, a model of the listener , the rules of conversation , and an
extensive shared body of general information about the world. Several of these programs
are described briefly in Article Fl.
The focus of modern work in natural language processing in Al Is “understanding ”
language. Several different tasks have been used as the criterion for defining what
constitutes a demonstration that the program understands a piece of text; these tasks
include paraphrasing, question answering, mechanical translation , and Information retrieval . Many
design issues depend on which type of task the program is to perform , but the general
approach has been to model human language as a knowledge-based system for processing
communications and to create a computer program that serves as a working model of this
system.
Al researchers in natural language processing expect their work to lead both to the
development of practical , useful language understanding systems and to a better
understanding of language and the nature of intelligence. The computer , like the human mind,
has the ability to. manipulate symbols in complex processes , including processes that involve
decision making based on stored knowledge. It is an assumption of the field that the human
use of language I. a cognitive process of this sort. By developing and testing computer-
based models of language processing that approximate human performanc e, researchers
hope to understand better how human language works.
Approaches to ML Processing
Natural language research projects have had divers e goais and used diverse methods ,
making their categorization somewhat difficult. One coherent scheme , borrowed from
Wlnograd (1972), groups natural ianguage programs according to how they represent and

- ~~~~~~~~~~~~~~~~~~~~~~~~~~~
A Natural language Process ing Overview S
use knowledge of their subject matter. On this basis , natural language programs can be
divided into four hIstorical categories.
The earliest natural language prog~ams sought to achieve only limited results in
specific , constrained domains. These programs used ad hoc data structures to represent
“knowledge. ” Programs like BASEBALL , SAD-SAM , STUDENT , and ELIZA (see Article Fl)
searched their input sentences , which were restricted to simple declarative and
interrogative forms, for key words or patterns representing known objects and relationships.
Domain-specific rules, called heuristics , were used to derive the required infnrmat ion from the
key words in the sentence and the knowledge in the database. Though they performed
relatively small tasks and avoided or ignored many of the complexities In language , their
results and methods were the Impetus to dealing with more difficult problems.
The second category can be called text-based systems. These programs , such as
PROTOSYNTHEX i (Simmons , Kieln, & McConlogue , 1964) and the Teachable Language
Comprehender , TIC (Quillian, 1969), attempted to expand beyond the limits of a specific
domain. The programs dealt with full EnglIsh text as a base, rather than with key words or
phrases, input text was interpreted as a request to access a structured Information store,
and a variety of clever methods were used to identify the proper response. Though more
general than their predecessors , these programs still failed to deal with the underlying
meanin g of the English language Input. They were able to give only responses that had been
pre-stored as data--they had no deductive power.
To try to deal with the problem of how to characterize and use the meaning of
sentences , a group of programs was developed called limited logic systems. in systems like
SIR (Raphael , 1968), DEACON (Thompson , 1966), and CONVERSE (Kellogg, 1968), the
informatio n In the database is stored in a formal, albeit ad hoc, notation , and mechanisms are
provided for translating input sentences Into the same form. The function of the formal
notation is to attempt to liberate the informational content of the input from the structure of
English. The overall goal of these systems was to accept complex input information (e.g.,
information containing quantifiers and relationships), use it to perform Inferences on the
database , and thus realize answers to complex questions . Problems , however , aros e from
the fact that the complexity of the stored Information was not really part of the database
but was built into the system ’s routines for manipulating the database. PROTOSYNTHEX ii
(Simmons , 1966; Simmons , Burger, & Long, 1966, for example , contained statements of the
form “A Is X” and “X Is B” and tried to answer “is A B?”, based on transitivity. The
deductive mechanism required for these inferences was embedded in specIal-purpose
subroutines , rather than In the database as a “theorem ,” end thus was not available to be
used to perform more Involved Inferences , which require a longer chain of reasoning.
Representing Knowledge in ML Programs
The fourth approach to building language understanding programs might be called
knowled ge-based systems and is closely intertwined with current research on the representation
of knowledge (see the Knowle dge Re,.~rsaintetion section of the Handbook). Among the most
important knowledge representation schemes explored In NI research have been: procedural
semantics , semantic networks , case systems , and frame systems.
in the early I 97O~, two systems were built that attem pted to deal with both syntactic
_____ _____ ______ ______ ~~~~~~~~~~~~~~~~~~~ ~~

-—--—-—~-~~ .— - .
~~~--- - - - , “~~-,-—- -—~~~~~~~~~
4 Natural Language
and semantic problems in a comprehensive way. William Woods ’s LUNAR system (Article F3)
answered questions about the samples of rock brought back from the moon, using a large
database provIded by the National Aeronautics and Space Agency. It was one of the first
programs to attack the problems of English grammar using an augmented transition network
parser (Article D2). It used a notion of procedura l semantics In which queries were first
converted in a systematic way into a “program ” to be executed by the retrieval component.
Terry Wlnograd’ s SHROLU system (Article F4) carried on a dialogue with a user in which the
system simulated a robot manipulating a set of simple objects on a table top. The
naturalness of the dialog ue, as well as SHRDIU ’s apparent reasoning ability, made It
particularly Influential In the development of Al ideas. These two systems Integrate
syntactic and semantic analysis with a body of world knowledge about a limIted domaIn ,
enabling them to deal with more sophisticated aspects of language and discourse than had
previously been possible.
Central to these two systems Is the representation of know lege about the world as
procedures within the system. The meanings of words and sentences were expressed as
programs in a computer language , and the execution of these programs corresponded to
reasonIng from the meanings. DIr~ct procedural representations are often the most
straightforward way to implement the specific reasoning steps needed for a natural language
system. Most of the actual working systems that have been developed have made heavy
use of specialized procedural representations , to fill in those pieces where the more
declarative representation schemes--those where the “knowledge ” Is encoded in passive
data structures that are Interpreted by other procedures--are insufficient. (The
procedural/declarative controvers y has been an Important focus In the history of Al. See Article
Rep-esente tion.B.)
Perhaps the most InfluentIal declarative representation scheme is the semantic network.
Semantic networks were first proposed by Ouililan (1968) as a model for human associative
memory. They used the concepts of graph theory, representing words and mean ings as a set
of linked nodes. By using a systematic set of link types, It was possible to program simple
operations (such as following chains of links) that corresponde d to drawing Inferences.
Another important declarative scheme Is the use of standard logic formulas (Article
Rspreeent ation.Cl), which are subject to mathematical rules of deduction for drawing
inferences. The advantage of semantic networks over standard logic is that some selected
set of the possible inferences can readily be done in a specialized and efficient way. if
these correspond to the Inferences that people make easily, then the system will be able to
do a more natural sort of reasoning than can be easily achieved using formal logical
deduction .
SemantIc networks have been the basis for a number of systems, Including most of the
speech understanding systems (see Speech Lhideratardng) . Recently there has been a good
deal of work on formalizing the network notions so that there Is a clear correspondence
between the graph operations and the formal semantics of the statements represented (see
Article Ra,.~1 a, ,lation.C2).
Case representations extend the basic notions of semantic nets with the idea of a case
frame , a cluster of the properties of an object or event Into a single concept (see Article
C4). There have been a large number of variations on this notion, some of which remain close
to the linguistic forms. Others such as conceptual dependenc y are based on the notion of
semantic primitives, the construction of all semantic notions from a small set of *prImltIveN

-~~ —--- . -— — .-
~~. -~~- . — . -
A Natural Language Processing Overview 6
concepts. The MARGIE sytem (Article F5), built in the early 1 970s by Roger Schank and his
students , uses the conceptual dependency representation.
As with semantic networks , the advantage of case representations lies in their focus
on clustering relevant sets of relationships Into single data structures. The idea of
clustering structures in a coherent and efficIent way has been carried much further In
representation schemes based on the notion of a frame (Minsky, 1975; see also Article
Rprssntatlori.C8). Where case representat ions deal primarily with single sentences or
acts, frames are applied to whole situat ions or complex objects or series of events. In
analyzing a sentence , narrative , or dIalogue , a language understanding system based on
frame representations tries to match the Inp~at to prototypes far the objects and events In
its domain that are stored In its database.
For example , Roger Schank ’s SAM system (Article FB) makes use of simple, linear
scripts , which represent stereotyped sequences of events, to understand simple stories. It
assumes that the events being described will fit (roughly) into one of the scripts In its
knowledge bass , wh ich it then uses to fill In missing pieces in the story. The GUS system
(Bobrow St al., 1977) Is a prototyp e travel consuftant , carrying on a dialogue to help a
person schedule an air trip. It uses frames representing standard trIp plane. GUS uses the
experimental frame language KRL (Bobrow & WInograd , 1977; see also Article
Repreeent.tion.C8).
The Important common element in all of these systems Is that the existence of
prototyp e frames makes It possible to use expectations In analysis. When a sentence or
phrase Is input that Is ambiguous or underspecified , it can be compared to a description of
what would be expected based on the prototype. Assumptions can be made about whet was
meant, If there Is a plausible fit to the expectation. This expectation-driven processing seems
to be an important aspect of the human use of language , where incomplete os ungrammatIcal
sentences can be understood In appropriate contexts. Research on script- and frame-based
systems is the most active area of Al research in natural language understanding at the
present time.
The current state-of-the-art in working (non-experimental) NI systems is exemplified
by ROBOT (Harr Is, 1977), LIFER (Hend rix, 1977b), and PHLl~~1 (Landsbergen , 1976).
References
General discussions of natural language processing research In Al can be found In
Boden (1977), WIIks (1974) , Wlnograd (1974), Charn lak & Wilks (1976) , Schank & Abelson
(1977), and Winograd (forthcoming). Waltz (1977) contaIns more than fifty brief summaries
of current projects and systems. in addition , many historically important NI systems are
described in Feigenbaum & Feldman (1963), Minsky (1966), Rustln (1973), Schank & Colby
- (1973), and Wlnograd (1972). COLING (1976), TINIAP-1 (1976), Bobrow & Collins (1976) ,
and TINIAP-2 (1978) are proceedings of recent conferences describing current work in the
field. 
-.-.~~~~~~~~~ -.-~~~~~-~~~~~~~~~~~ --.--~~~~~ 

