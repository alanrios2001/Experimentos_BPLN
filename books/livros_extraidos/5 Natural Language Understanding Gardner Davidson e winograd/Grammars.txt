p —
~~~~~~~--
~~~~~~~~- --. ,-- - -.-
~~~~~~- —- -—.- .-— -
~~~~~ 
C Gram mars 11
C. Grammars
A grammar of a language Is a scheme for specIfyIng the sentences allowed In the
language , Indicating the rules for combining words into phrases and clauses. In natural
language processing programs , the grammar is used in parsing to “pick apart” the sentences
in the Input to the program to help deter ajlne their meaning and thus an appr opriate response.
Several very different types of grammars hive been used in NL programs and are described
in the articles which follow.
CI. Formal Grammars
One of the more important contributions to the study of language ~vas the theory of
formal langua ges Introduced by Noam Chomsky In the 1 960a. The theory has developed as a
mathematical area, not a linguistic one, and has strongly influenced computer science In the
design of computer programming languages (artificial langua ges). Nevertheless , it is useful In
connection with natural languag, understanding systems , both as a theoretical and a practical
tool.
Definitions
A formal language Is defined as a (possIbly infinite) set of strings of finite length formed
from a finite vocabulary of symbols. (For example , the strings might be sentences composed
from a vocabulary of words.) The grammar of a formal language is specified in terms of the
following concepts:
1. The syntactic categories , such as <SENTENCE> and (NOUN PHRASE>. These
— syntactic categories are referred to as nontermlnal symbols or variables. Notetionally, the
nonterminals of a grammar are often indicated by enclosi ng the category nales in angle
brackets , as above.
2. The terminal symbols of the language , for example the words In English. The
terminal symbols are to be concatenated Into strings calied sentences (if the terminals are
words). A language Is then just a subset of the set of all the strings that can be formed by
combining the terminal symbols in all possible ways. Exactly which subset Is permitted in the
lang uage Is specifIed by the rewrite rules:
3. The reunite rules or productions specify the relationships that exist between certain
strings of terminals and nonterm lnal symbols. Boise examples of productions are:
(SENTENCE> —> (NOUN PHRASE ) <VERB PHRASE>(NOUN PHRASE> —) the <NOUN)(NOUN> —>dog H
<NOUN> —> cat
<VERB PHRASE ) -4 runs
The first production says that the (non-terminal) symbol (SENTENCE) may be “rewritten~ as
the symbol <NOUN PHRASE> followed by the symbol (VERB PHRASE). The second permits
(NOUN PHRASE) to be replaced by a string composed of the word ~~~~~, which is a terminal
symbol, followed by the nonterm lnal (NOUN). The next two allow <NOUN> to be replaced by
- ~-
~-
~~~~~~~_ .-- 
~~~--- .- - I

12 Natural Language
either ~~~ or ç~~. Since there are sequences of productions permitting <NOUN PHRASE> to
be replaced by the doc or the cat, the symbol <NOUN PHRASE> is said to generate these two
terminal strings. FInaHy, <VERB PHRASE> can be replaced by the terminal ~~~~~~~~~ .
4. The start symbol. One nonterminal Is distinguished and called the “sentence ” or
“start” symbol , typically denoted (SENTENCE> or ~. The set of strings of terminals that can
be derived from this distinguished symbol, by applying sequences of productions , Is called
the language generated by the grammar. in our simple example grammar , exactly two sentences
are generated:
The cat runs.
The dog runs.
The important aspect of defining languages formally, from the point of view of computational
linguistics and natural language processing, Is that if the structure of the sentences Is well
understood , then a parsing algorithm for analyzing the input sentences will be relatively easy
to write (see Section 01 on parsing).
The Four Types of Formal Grammars
Within the framework outlIned above, Chomsky delineated four types of grammars , - 
-
numbered 0 through 3. The most general class of grammar is type 0, whIch has no
restrictions on the form that rewrite rules can take. For successIve grammar types, the form
of the rewriting rules allowed is increasI ngly restricted , and the languages that are
generated are correspond ingly simpler. The simplest formal languages (types 2 and 3) are, -
as It turns out, Inadequate for describing the complexities of human languages. (See Article
C2 for a fuller dIscussion.) On the other hand, the most general formal languages are difficult
to handle cosnputationa lly. There is an ‘intImate and interesting connection between the
theory of formal languages and the theoly of computational complexity (see Hopcroft &
UlIman, 1989). The following discussion givóe a formal account of the dIfferent restrictions
applied In each of the four grammar types. \
Formally, a. grammar G is defined by a quadruple (VN, VT, P, 5) representing the
nontermlnals , terminais , productions , and the start symbol, respectively. The symbol ~~, for
“vocabulary, ” is used to represent the union of the sets ~f and Yl. which are assumed to
have no elements In common. Each production In ~ is of the form
x-> Y
where A and ! are strings of elements in ~j, and A is not the empty string.
Type 0. A type-O grammar is defined as above: a set of productions over a given
vocabulary of symbols with no restrictions on the form of the productions . It has been shown
that a language can be generated by a type-O grammar If and only if It can be recognized by
a Turing machine; that Is, we can build a Turing machine which will halt In an ACCEPT state
for exactly those Input sentences that can be generate d by the language.
Type 1, A type-O grammar ii also of type 1 if the form of the rewrite rules is
restricted so that, for each production X.) V of the grammar the right-hand side ~ contain~
- - - _______ — - -
______

_____ -.-,-,--—-.- -— -----.---, — -.--—-— -- “ ?~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~—i.
Cl Formai Grammars 13
at least as many symbols as the left-hand side A. Type-I grammars are also caUed context -
sensItlv grammars. An example of a context-sensitive grammar with start symbol ~ and
terminals a. ~ and ~ is the fo llowing:
S— > aSBC
S —) aBC
C8—>BC
aB—>ab
bB—>bb
bC—)bc
cC—>cc
The language generated by this grammar is the set of strings ~~~~~~~, ~~~~ aaabbbccc
This language , where each symbol must occur the same number of tImes and must appear in
the right position in the string, cannot be generated by any grammar of a more restricted
type (I..., type 2or type 3).
An alternate (equivalent) definition tot context-sens itive grammars is that the
productions must be of the form
uXv -) uVv
where A Is a single nonterm lnai symbol; ~ and y are arbitrary strings , possibly empty, of
elements of ~; and ~ is a nonempty string over ~~. It can be shown that this restriction
generates the same languages as the first restriction , but this latter definition clarifies the
term cons ext-se’uulva: ~ may be rewritten as ~ only in the context of ~ and y.
Type 2. Context-ft.. grammars or type-2 grammars are grammars In which each
production must have only a sIngle non-term lnai symbol on its left-hand side. For example , a
context-free grammar generat ing the sentences ~~, ~~~~ ~~~~~~~~~~~~~~ is:
S -) aSb
S -> ab
Again, It is not possible to write a context-free grammar for the language composed of the
sentences ~~~, ~~~~~ ~~~~~~~~ . . --having the same number of ~‘s at the end makes
the langua ge sore oompl. x. The simpler language here, in turn, cannot be generated by a
mote restricted (typ..3) grammar.
An example of a context-fr.. prammar that might be used to generate some sentences
in natural language is the following:
(SENTENCE> —> (NOUN PHRASE) (VERB PHRASE ><NOUN PHRASE> —> (DETERNINE R) (NOUN>
<NOUN PHRASE > -4 <NOUN)
<VERB PHRASE ) —) (VERB) <NOUN PHRASE>
(DETERMINER> —) the
<NOUN) —) boys
<NOUN > —> a~ol.s(VERB) —) eat
In this example, tha~ ~~~ ~~~~ and ~~ are the terminals In the language and
(SENTENCE> Is the start symbol.
__________________ ____________________________________-. —-- —-.-~~-.-—-- .---..—. —- —.-- -~~~~
-- .:.. . —.

,—. ,—
~--—= -.
~~~--.--——
~~~ .__
~~~__
~~_
~.__ ~.. ,..--...--- ~~~~~~~~
14 Natural Language
An Important property of context-free grammars in their use in NL programs is that
every derivation can conveniently be represented as a tree, which can be thought of as
displaying the structure of the derive d sentence. Using the grammar above, the sentence
9he boys eat apples ” has the following d.rivatl4m tree:
(SENTENCE)
<NOUN PHR~SE) (~ERB PHRASE>
<OETERMINE~> <~OUN) <VE(B) <~OUN PHRASE>
Ut. b~ys e~t (N&IN>
apples
Of course , “the apples eat boys” is also a legal sentence In this language. Derivation trees
can also be used with context-sensit ive (type-i) grammars , provided the productions have
the alternate form uXv -> uYv, described above. For this reason, context-free and context -
sensitIve grammars are often called pAras.-str ucture grammars (see Chomsky, 1959, pp. 143-
144, and Lyons, 1968, p. 236).
Type 3. Finally, if every production is either of the form
X-) aY or X-) a
where A and ! are single variables and ~ is a slngie terminal , the grammar Is a type-3 or
regular grammar. For example , a regular grammar can be given to generate the set of strings
of one or more a’ followed by one or more ~a (but with no guarantee of an equal number of
~s and us):
S— > aS
5—7.1
T— > b
T —7 bT
Discussion: Language and Computational Algorithms
Becaus e of the increasingly restricted forms of productions in grammars of type 0, 1, 2,
and 3, each type Is a proper subset of the type above it In the hierarchy. A corresponding
hierarchy exists for formal languages. A language is said to be of type I If It can be
generated by a type-I grammar. It can be shown that languages exist that are context-free
(type 2) but not regular (type 3); context-sensitIve (type 1) but not context-free; and
type 0 but not context-sens itive. Examples of the first two have been given above .
For regular and context-free grammars , there are practical parsing algorithms to
determine whether or not a given string is an element of th. language and, If so, to assign to
It a syntactic structure In the form of a derivation tree. Context-free grammar. have
considerable application to programming languages. Natural lang uages , however , are not
general l y conte xt-free (Chomsky, 1963; Postal , 1964), and they also contain features that
can be handled mote conveniently, it not exclusively, by a more powerful grammar. An
example is the requirement that the subject and verb of a sentence be both singular or both 
.
.--- ~~~~~~~~ ~--~~- .
- .~~~~~~~~~~~~~ —-~~--- ~-~ 

~~--—.- -- ~~—.-- ——--.,— .~~~~-- ____________
Cl Formal Grammar s 16
plural. Some of the types of grammars and parsing algorithms that have ben explored as
more suitable for natural ianguage are discussed in the articles that follow.
References
For a general discussion of the theory of formal grammars and their relation to automata
theory, see Hopcroft & Uliman (1969). Their use in NI research i. discussed in Wlnograd
(forthcomIng).
Also of Interest are the works of Chomsky (especIally 1966, 1967, and 1969), as well
as Lyons (1968), Lyons (1970), and Postal (1964). 

--- .-. . ~~~~~~~~~~~~~~~~~~~~~~
16 Natural Language
C2. Transformational Grammars
The term transformational grammar refers to a theory of language introduced by Noam
Chomsky in Syntactic Structures (1957). In the theory an utterance Is characterized as the
surface manifestation of a “deeper ” structure representing the “meaning ” of the sentence.
The deep structure can undergo a variety of “transformations ” of form (word order, endings ,
etc.) on its way up, while retaining its essential meaning. The theory assumes that an
adequate grammar of a language like Engiish must be a generative grammar , that is, that it
must be a state~ent of finite length capable of (a) accounting for the Infinite number of
possIble sentences In the language and (b) assigning to each a structural description that
captures the underlying knowledge of the language possessed by an Idealized native user. A
formal system of rules is such a statement; it “can be viewed as a device of some sort for
producing the sentences of the language under analysis ” (Chomsky, 1957, p. 11). The
operation of the device is not intended to reflect the processes by which people actually
speak or understand sentences , just as a formal proof in mathematIcs does not purport to
reflect the processes by which the proof was discovered. As a model of abstract knowledge
and not of human behavior , generative grammar is said to be concerned with competence , as
opposed to performance.
The Inadequacy of Phrase-structure Grammars
Given that a grammar is a generative rule-system , It becomes a central task of
linguistic theory to discover what the rules should look like. In Syntactic Structures (1957)
and elsewhere (see Chomsky, 1963, Postal, 1984), It was shown that English is neither a
regular nor a context-free language. The reason is that those restricted types of grammars
(defined in Article Cl) cannot generate certain common constructions In everyday English,
such as the one using “respectively ”:
Arthur, Barry, Charles , and David are the husbands of Jane, Joan, Jill,
and Jennifer , respectively.
it was not determined whether a more powerful (i.e., context-sensItIve) grammar could be
written to generate precisely the sentences of English; rather, such a grammar was rejected
for the following reasons.
1. It made the description of English unnecessarily clumsy and complex--for
example, in the treatment required for conjunction , auxiliary verbs, and
passive sentences.
2. It assigned identical structures (derivat ion trees) to b ~nces that are
understood differently, as in the pair:
The picture was painted by a new technique .
The picture was painted by a new artist.
~~~~~~~~~~~ ~~~~~~~~~~ 

-“ !rrn-- ~ r ---~— -
C2 Transfor mational Grammars 17
3. It provIded no basis for IdentIfyIng as similar sentences h~eing different
surface structures but much of their “meaning ” In common :
John ate an apple.
Did John eat an apple?
What did John eat?
Who ate an apple?
The failure of phrase-structure grammar to explain such similarities and differences was
taken to indIcate the need for analysis on a higher level, which transformational grammar
provides.
Transformational Rules
In Syntactic Structures , Chomsky proposed that grammars should have a tripartite
organization . The first pert was to be a phrase-structure grammar generating strings of
morphemes representing simple, declarative , active sentences , each with an associated
phrase marker or derivation tree. Second, there would be a sequence of transformational
rules rearranging the strings and adding or deleting morphemes to form representations of the
full varIety of sentences. FInally, a sequence of morphophonem lc rules would map each
sentence representation to a string of phonemes. Although later work has changed this
model of the grammar , as well as the content of the transformational rules, It provides a basis
for a simple Illustration .
Suppose the phrase-structure grammar is used to produce the following derivation tree:
SENTENCE
NOUN PHRAS~ ~VERB PHRASE
NP—S ILGULAR V~R8 \IOUN PHRASE
DETERNINE~ ~OUN AU/ “V NIL-PLURAL
tI’e b~y TEIISE Lat DETERNINE~ NbUN \\the app’e $
To generate “the boy bte the apples,” one would apply transformations mapping “TENSE •
~g~” to “~~~~ + PAST”; a morphophonemlc rule would then map “~~~~ + PAST” to g~~. To derive
“the boy eats the apples,” the transformational rule used would select present tense and,
because the verb follows a singular noun phrase, would map “TENSE + ~~~~“ to “~~~~~ + ~~.“ It Is
noteworthy that the trans formational rule must look at nontermInal nodes In the derivation
tree to determine that “the boy” is in fact a singular noun phrase. This example illustrates
one respect In which transformational rules are broader than the rulee of a phrase-structure
grammar.
The transformations mentioned so far are examples of obligalor, transformations , insuring
agreement In number of the subject and the verb. To obtain “the apples were eaten by the
- —~
~~~~~~~~~~ —~~~~- - -— -~~~~~~~~~-~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~ -—. -~~ -~~~~~~~~~~~~~~~ —

18 katura l Language
boy,” It would be necessary first to apply the optional passive transformation , changing a
strIng analyzed as
NOUN-PHRASE-i + AUX . V. NOIJN-PHRASE-2
to
• NOUN-PHRASE-2 + (MiX + be) + (en + V) + by + NOUN-PHRASE-i
In other words, this optional transformation changes “the boy TENSE eat the apples” to ~~~~~apples TENSE be (en eat) by the boy,” and then forces agreement of the auxiliary verb with
the new plural subject. Further obligatory transformations would yield “the apples be PAST
eaten by the boy” (where “~~~ + PAST,” as opposed to “~~~ + + PAST,” is ultimatel y mapped
to were). The orderin g of transformational rules is thus an essential feature of the grammar.
Revisions to the Model
In Aspects of the Theory of Syntax (1965), Chomsky made several revIsions to the
model presented In Syntactic Structures. The version outlined in the more recent book has
been called the “standard theory ” of generative grammar and has served as a common
starting-point for further discussion. In the standard theory (as summarized in Chomsky ,
1971), sentence generation begins from a context-free grammar generating a sentence
structure and is followed by a selection of words for the structure from a lexicon. The
context-free grammar and lexicon are said to form the base of the grammar; their output is
called a deep structure. A system of transformational rules maps deep structures to surface
structures; together , the base and transformational parts of the grammar form its syntactic
corn ponent. The sound of a sentence Is determined by its surface structure , which Is
Interpreted by the phonolo gical component of the grammar; deep structure , interpreted by the
semantic component , determines sentence meaning. it follows that the application of
transformational rules to deep structures must preserve meaning: This was the Katz-Postal
hypothesis , which required enlarging the generative capacity of the base and revising many
of the transformational rules suggested earlier (Katz & Postal, 1964).
The place of the semantic component in the standard theory has been the major source
of current issues. For example , the following pairs of sentences have different meanings ,
but their deep structures , In the standard theory, are the same.
Not many arrows hit the target.
Many arrows didn’t hIt the target.
Each of Mary’s sons loves his brothers .
His brothers are loved by each of Mary’s sons.
Chomsky ’s response was to revise the standard theory so that both the deep structure of a
sentence and its subsequent transformations are input to the semantic component (Chomsky,
1971). He exemplifies the position of Interpretive semantics , which keeps the syntactic
component an autonomous system. The opposing view, called generative semantics , is that
syntax and semantics cannot be sharply separated and, consequently, that a distinct level
of syntactic deep structure does not exist. (This Issue is discussed in Charniak & WiIks,
1976.)
____ ______ 
-~~-.-.~~~~~~~~ ~ A

____ ________________ -, ~~~--
~~~~~~~~~~ _________
Ca Transfo rmational Grammars 19
There have been a number of developments within the theory of transformational
grammar since the work reviewed here, and current debates have called Into question many
of the basic assumptions about the role of transformations In a grammar. For current
discussions of these Issues , see Akmajian, Culicover and Wasow (1977) and Bresnan
(1978).
Refere nces
The classIc references here are, of course, Chomsky (195?) and Chamsky (1965).
Chomsky (1971) Is a shorter and more recent dIscussIon. Culicover, Wasow , & Akmaj lan
(1977) and Bresnan (1978) are the latest word on transformation theory.
Also see Akmajian & Heny (1976), Charn lak & Wilks (1976), Chomsky (1956 ), Chomsky
(1959), Choinsky (1963), Harman (1974), Katz & Postal (1964), Lyons (1968), Lyons
(1970), Postal (1964), and Steinberg & Jakobovits (1971).
________ ~

20 Natural Language
Ca. Systemic Grammar
Systemic grammar , developed by Michael Halilday and others at the University of
London , is a theory within which linguistic structure as related to the function or use of
language, often termed pragmatics, is studied. According to Halllday (198 1, p. 141), an
account of linguistic structure that pays no attention to the functional demands we make on
language is lacking in perspicacity, since it offers no principles for expialrlng why the
structure Is organized one way rather than another. ThI$ viewpo int is In contrast to that of
transformational grammar , which has ~een concerned with the syntactic structure of an -
utterance apart from Its intended use.
The Functions of Language
Halliday distinguishes three general functions of language , all of which are ordinarily
served by every act of speech.
The ideational function serves for the expression of content. It says something about
the speaker ’s experience of the world. Analyzing a clause in terms of its ideational functionInvolves asking questions like: What kind of process does the clause describe--an action, a
mental process , or a relation? Who Is the actor (the logical subject)? Are there other
participants in the process , such as goal (direct object) or beneficiary (Indirect object)? Arethere adverbial phrases expressing circumstances like time and place? The organization of
this set of questions is descrIbed by what Haiiiday calls the transitivity system of the grammar.
(This Is related to the ideas of case grammars discussed In Article C4.)
The interperson al function relates to the purpose of the utterance. The speaker may be
asking a question , answering one, making a request , giving informatIon , or expressing an
opinion. The mood system of English grammar expresses these possibilities in terms of
categories such as statement , question , command , and exclamation.
The textual function reflects the need for coherence in language use (e.g., how a given
sentence Is related to preceding ones). Concepts used for analysis In textual termsInclude: (1) theme, the element that the speaker chooses to put at the beginning of aclause; end (2) the distinction between what is new In a message and what is gIven--the
latter being the point of contact with what the hearer already knows.
Categories of Systemic Gramma l
The model of a grammar proposed by Halliday has four primitive categories:
1. The units of lenpuaae, which form a hierarchy. In English, these are the sentence ,
clause, group, word, and morpheme . The “rank” of a unit refers to Its position in thehierarchy.
2. The structure of units. Each unit Is composed of one or more units at the rankbelow, and each of these components fills a particular role. The English clause, for example ,Is made up of four groups, which serve as subject , predicator , complement, and adjunct. 
——.——— 
— .———— — —— .—————,— —. 

---—- • —-- —------
~- 
- ~~~~~~~~~~~~ — —--- -- --
C3 Syste mic Grammar 21
4
.
3. The classification of units, as determined by the roles to be filled at the level
above. The ciasses of English groups, for instance , are the verbal, which serves as
predicator ; the nominal , which may be subject or complement; and the adverbial , which fills
the adjunct function .
4. The system. A system is a list of choices representing the options available to the
speaker. Since some sets of choices are available only If other choices have already been
made, the relationship between systems ii shown by combining them into networks , as in the
simple example below:
imperative
independentIndicative —.1 declaratIve
clause — ‘ I— .
dependent ‘I interrogative
The interpretation is that each clause Is independent or dependent; if independent , it is
either Imperative or Indicative; and if either indicative or dependent , then It Is either
declarative or interrogative. in general , system networks can be defined for units of any
rank, and entry to a system of cho ices may be made to depend on any Boolean combination
of previous choices.
Conclusion
Sys~~mic grammar views the act of speech as a simultaneous selection from among a
large number of interrelated options , which represent the “meaning potential” of the
language. If system networks representing these options are suitably combined end carried
to enough detail , they provide a way of writing a generative grammar quite distinct from that
proposed by transformational grammar (see Hudson , 1971, 1976; McCord , 1975; and Self,
1976). Furthermore , this formalism has been found more readily adaptable for use In naturallanguage understanding programs in Al (see especially Winograd’s SHADLU system , Article
F4).
Referenc es
Halliday (t961) and HallIday (1970b) are the most general original referenA~s.
• WIflo grad (1972) dIscusses the application of systemic grammar in an NL program.
Also see itaiNday (1987-68), Haiiiday (1970a), Hudson (1971), Hudson (1976), McCord
(1976), Mcintosh & Haillday (1968) , and Self (1975).
1-• •~ 
-~~~--—--— -. _____ __o
_______________ _________ - _

~
22 Natural Language
C4. Case Grammars
Case systems , as used both in modern ilngulstics and in artificial intelligence, are
descendants of the concept of case that occurs in traditional grammar. Traditionally, the case
of a noun was denoted by an inflectional ending indicating th. noun’s role in the sentence.
Latin, for example , has at least six cases: the nominative, accusatIve , genitive, dative,
ablative , and vocative. The rules for case endings make the meaning of a Latin sentence
almost independent of word order: The function of a noun depends on Its inflection rather
than Its position in the sentence. Some present-da y languages , including Russian and
German , have sImilar Inflection systems~ but English limits case forms mainly to the personal
pronoun , as In 
~~, 
~~~~~~, g~, and to the possessive ending ~~. Case functions for nouns are
Indicated In English by using word order or by the cho ice of preposition to precede a noun
phrase--as in “~~~ the people, ~~ the people, and f~ the p.ople.”
The examples above describe what have been called “surface ” cases; they are
aspects of the surface structure of the sentence. Case systems that have attracted more
recent attention are “deep ” cases, proposed by Fillmore (1968) in his paper The Case for
Case, as a revision to the framework of transformational grammar. The central idea is that the
proposition embodied in a simple sentence has a deep structure consisting of a verb (the
central component) and one or more noun phrases. Each noun phrase is associated with the
verb In a particular relationship. These relationships , which FIllmore characterized as
“semantically relevant syntactic relationships ,” are called cases. For example , in the
sentence
John opened the door with the key,
~~~ would be the AGENT of the verb opened, the door wouid be the OBJECT , and the keY
would be the INSTRUMENT. For the sentence
The door was opened by John with the key,
the case assignments would be the same, even though the surface st ructure has changed.
It was Important to Fillmore ’s theory that the number of possibie case relationships be
small and fixed. FIllmore (1971 b) proposed the following cases:
Agent -- the instigator of the event.
Counter-Agent -- the force or resistance against which the action is
carried out.
Object -- the entity that moves or changes or whose position
or existence is in consideration.
Result -- the entity that comes into existence as a result of
the action.
instrument -- the stimulus or immediate physical cause of an
- event.
Source -- the place from which something moves.
Goal -- the place to which something moves.
Experiencer -- the entity which receives or accepts or
experiences or undergoes the effect of an action.~
-- ~~~~~~~~~~~~~~~ -- -~~~
---~~ ~~~~~-

C4 Case Grammars 23
Still another proposal (Fillmore, 19?la) recognizes 9 cases: Agent , Experience , , instrument ,
Object , Source , Goal, Location , Time, and Path.
Verbs were classified accordin g to the cases that could occur with them. The cases
for any particular verb formed an ordered set called a case frame. For example, the verb
“open ” was proposed to have the case frame
[OBJECT (INSTRUMENT) (AGENT) )
indicating that the object Is oblIgato ry in the deep structure of the sentence , whereas It is
permissible to omit the instrument (“John opened the door” ) or the agent (“ The key opened
the door”), or both (“The door opened”). Thus, verbs provide templates within which the
remainder of the sentence can be understood.
The Case for Case
The following are some of the kinds of questions for which case analysis was intended
to provide answers:
1. in a sentence that is to contain several noun phrases , what determines
which noun phrase should be the subject in the surface structure? Cases
are ordered , and the highest ranking case that is present becomes the
• subject.
2. Since one may say “Mother is baking ” or “The pie is baking, ” what is wrong
with ‘Mother and the pie are baking ”? Different cases may not be
conjoined.
3. What is the precise relationship between pairs of words like “buy” and
“sell” or “teach” end “learn ”? They have the same basic meaning but
dIfferent case frames.
One way of lookIng at deep cases is to view the verb as a predicate taking an
appropriate array of arguments. Filimore has extended the class of predicates to Include
other parts of speech , such as nouns and adjectives , as well as verbs. Viewing warm as a
predicate , for example , enabled case distinctions to account for the differences amon g the
following sentences:
I am warm. (experiencer]
This jacket Is warm. [instrument]
Summer is warm. (time]
The room is warm. (locatio n)
The Representation of Case Frames
in artificial intelligence programs, such predicates and their arguments can readily be
equated to nodes in seman& networks; and the case relations , to the kinds of links betwee n
- 
-, ~4~~~~~:~~~-- - - _________________________ 
~~~~~~~~~~~~~~~~~~~~~~~~~~ - -
_________ -- ——— ~- ~~---- ---—-&~ -— --—— -

——- -~~--——--- — .~~•—• .-. ----‘—-- -—-~~ --~- -- -~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
24 Natural Language
them. Systems making such identifications include those of Simmons (1973), Schank (1975),
Schank & Abelson (1977), and Norman & Rumethart (1975). Semantic nets and related work
on semantic primitives and frames are discussed in the section on Knowledge Representation
and in Articles F5 and FB which describe the MARGIE and SAM systems.
Many other systems using case representations exist. As pointed out in an extensive
survey by Bruce (1975), considerable variation exists in both the sets of cases adopted and
the ways In which case representation is used. The number of cases used varies from four
or five (Schank) to over thirty (Martin). Bruce’s proposal on criteria for choosing cases ,
which departs significant l y from Fillmore ’s original goal of finding a small, fixed set of
relationships , is that:
A case is a relation which is “important” for an event in the context in
which It is described. (Bruce , 1976)
Case notation has been used to record various levels of sentence structure. As
Fillmore Introduced it, within the transformational grammar framework , deep cases were
“deep ” in the sense that “John opened the door” and “the door was opened by John” were
given the same representation. They can also be viewed as relatively superficial , however ,
In that “John bought a car from Bill” and “Bill sold a car to John” could have distinct
representations since they have different verbs. At this level, cases have been used in
parsing (Wllks, 1976; Taylor & Rosenberg, 1976); in the representation of English
sentences , as opposed to their underlying meanings , as discussed above (Simmons , 1973);
and in text generation (see Article E).
Systems using case at the deepest level, on the other hand, may represent the
meaning of sentences in a way that collapses k~ and ~~ for example , into a single
predicate (Schank , 1976; Norman & Rume lhart, 1975). A typical problem attacked by these
systems is paraphrasing, where Identifying sentences with the same deep structure is the
goal. Schank (1976) also requires that all cases be filled, even if the information required
was not explicItly given in the sentences represented. Charniak (1975) suggests that the
appropriate use of case at this level of representation is In inferencin g: The “meaning ” of a
case would then be the set of Inferences one could draw about an entity knowing only its
case. in the view of some writers , however , the function of case in natural language
understanding systems is usuall y only as a convenIent notation (see Charniak , 1975; Welin,
1976).
References
FIllmore (1968) is the classic reference on case grammars . Bruce (1975) Is a thorough
review of different approaches to case grammar.
Also see Charn iak (1975), Fillmore (1971a), Fillmore (1971b), Norman & Rumeihart
(1975), Samiowaki (1976), Schank (1973), Schank (1975), Schank & Abelson (1977),
SImmons (1973), Taylor & Rosenberg (1976), Weiin (1975), and Wilks (1976).
~~~~~~~~~~~~~~~~~ ---.•--— -~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 

