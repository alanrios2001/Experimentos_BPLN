E Text Generation 39
E. Text Generation
Text generation Is, in a sense, the opposite of natural. language understanding by
machine--It Is the process of constructing text (i.e., phrases, sentences, paragraphs) in a
natural language. Although this field has been pursued for fifteen years, few coherent
principles have emerged , and the approaches have varied widely. Attempts at generating
text have been made with two general research goals: (a) generating random sentences to
test a grammar or grammatical theory and (b) convertIng Information from an internal
representation Into a natural language.
Random Generation
This approach , the random generation of text constrained by the rules of a test
grammar, is of limited interest to workers In Artificial intelligence , being oriented more toward
theoretical linguistics than functional natural language processing systems. The objective of
implementing a generation system of this sort is to test the descriptive adequacy of the test
grammar , as Illustrated by the followIng two systems.
VIctor Yngve (1962) was one of the first researchers to attempt English text
generation; the work was seen as preliminary to a full program for machine translation (see
Article B). Yngve used a generative context-fr.. grammar and a random-number generator to
produce “grammatical” sentences: The system selected one production randomly from among
those that were applicable at each point in the generation process , starting from those
productions that “produced” (SENTENCE) , and finally randomly selecting words to fill In the
<NOUN) , (VERB), etc., positions. This is an example of the text produced by the system~
The water under the wheels In oiled whIstles and Its
polished shiny big and big trains is black.
Joyce Friedman ’s (1969, 1971) system was designed to test the effectiveness of
• transformational grammars (Article C2). it operated by generating ~~ras. marke rs (derivation
trees) and by performing transformations on them until a surface structure was generated. Th’~generation was random , but the user could specify an input phrase marker and semantic
restrictions between various terminals In order to test specific rules for grammatical validity.
These two systems , while releva nt to work in linguistics , are only periphe rally related
to recent work in ArtificIal intellIgence. The fundamental emphasis In Al text-generation work
has been on the meaning, as opposed to the syntactic form, of language.
Surface RealIzatIon of Meaning
The general goal of text-generation programs in the Al paradigm Is to take some
internal representation of the “meaning ” of a sentence and convert It to surface structure
form, i.e. into an appropriate string of words. There has been considerable variety among
such systems , reflecting differences both in the type of internal representation used and In
the overall purpose for which the text is generated. Repres entation schemes have included
largely syntactic dependency trees, stored generation patterns of different degrees of
complexity, and severa l versions of semantic nets (see the Kruowls dgs Ru~rsuntstion section

‘-•—— .-
~~~~~- ___
40 Natural Language
of the Handbook). Purposes have included automatic pare phrasing or mechanical translation of
an input text; providing natural-sounding communicat ion with the user of an inte ractive
program; and simply testing the adequacy of the internal representat ion.
Sheldon Klein (1966) made a first step beyond the random generation of sentences .
by means of a program that attempted to generate a frare pares, of a paragraph of text via
an internal representation of that text (see also Klein & Simmons , 1963). The program used a
• type of grammar called dep.nd~ncy grammar , a context-free grammar with word-dependency
information attached to each production. That is, th. right-hand side of each rule in the
grammar has a “distinguished symbol ”; the “head” of the phrase associated with that rule is
the head of the phrase that is associated with the distin guished symbol. All other words that
are part of the phrase associated with the production are said to depend on this head.
For Instance , given the following simple dependency grammar and the sentence “the
fierce tigers In India eat meat,” Klein’s parser would produce both an ordinary phrase-
structure derivation tree (see Article Cl) and also the dependency tree shown below:
TICERS
THE’ S
NP I. DET ADJ + N. PP
FIER E N ‘EAT PP.PREPe+NOUNI I VP.V* +OBJ
IN6IA N~AT
The symbols followed by * are the distinguishe d symbols in the productions. The dependency
trees from the individual sentences of the Input paragra ph were bound together with “two-
way dependency ” links between similar nouns. For example , the Input paragra ph
The man rides a bicycle. The man Is tail. A bicycle is a vehicle with
wheels.
would yield the following dependency structure:
fiNd’ eNAN
THE’ \IDES
ICYCLE u ‘ BICYCLE \ALL
A” A’
V~H ICLE
A’ ~‘WITH
~‘WHEELS
One paraphrase generated from the given paragraph was:
The tail man rides a vehicle with wheels .
The grammar used in generation was similar to the one used for analysis. Rule selection —
_____________________________ —• ~~~~~~~~—-~ ~~~~. A

E Text Generation 41
was random (as in Yngve’s method ) but with the added constraint that au dependencies
among the words that were generated must be derivable from the Initial dependency trees.
in the example above, vehicle could be generated as the object of rJ~~ because vehicle
depends on 
~~~~, f~ on bicycle, and bicYcle on ~~~ Two restrictions were imposed on the
transitivity of dependency relations: Dependency did not cross verbs other than ~~ or
• prepositions other than p~. Thus “the man rides wheels ” could not be generated.
The use of dependency trees was expected to insure that the Output sentences would
“reflect the meaning of the source text” (Klein, 1966, p. 74). A difficulty , however, was that
the trees encoded only the crudest of semantic relationships present In the paragraph. In
tact, the dependency relationship between words only indicates that some semantic relation
exists between them without really specifying the nature of the relationshIp.
Ross Quillian (1988), in contrast, emphasized the expression of semantic relationships
almost to the exclusion of concern for syntactic well-formedness. Quillian did pioneering
work in the representation of knowledge (see the Knowledge Rnprssintetion section of the
Handboo~c)) and was also one of the first to deal with the problems of text generation. His
system used a semantic net to represent the relations between words, which can be
Interpreted as their meaning. The task the system was then to perform was to compare two
words, that Is, find some semantic relation between them, and then to express the
comparison in “understandable , though not necessarily grammaticall y perfect , sentences ” (p.
247). For example:
Compare: Plant, Live
Answer: PLANT IS A LIVE STRUCTURE. 
• -
This relationship between the two words was discovered as a path in the net between The
nodes that represented the words. Although this was a primitive semantic net scheme , many
fundamental issues were first raised by Quillian’s system.
One important point was that paths in the semantic net did not necessarily correspond
to Input sentences. Instead, the discovery of paths between two nodes amounted to making
inferences on the knowledge in memory. For example , another relationship the system found
between pjgj~ and fi~~ was:
PLANT IS STRUCTURE WHICH GET-FOOD FROM AIR. THIS FOOD IS THING
WHICH BEING HAS-TO TAKE INTO ITSELF TO KEEP LIVE.
In order to have found this connection , the system had to discover a connection between
PLANT and LIVE, by way of FOOD, that was not directly input.
Although Quilllan’s semantic net system was limIted, it strongly influenced much of the
later work in Ni. and the representation of knowledge in Al (see Article Rsprs s.ntatian.C2).
This influence reflected QuilIlan’s stress on the importance of the semantic versus the
surface components of language:
As a theory, the program implies that a person first has something to
say, expressed somehow in his own conceptual terms (which is what a
“path” Is to the program), and that .11 his decisions about the
L,. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ . •. A

• 42 Natural Langua ge
• syntactic form that a generated sentence Is to take are then made in
the service of this intention . (Quililan, 1968, p. 265)
• This is a strong statement about langua ge, and this view, of a cognitive process manipulating
an internal representation , is perhaps the essence of the Al perspective.
Terry Winograd ’s block s world program , SHRDLU (1972), contained several text-
generation devices. Their function was to enable the system , which is described In Article
F4, to answer questions about the state of Its table-top domain and certain of the system ’s
Internal states.
The basic text-generation techniques used were “fill-in-the-blank ” and stored
response patterns. For example , if an unfamiliar word was used, SHROLU responded “I don’t
know the word... ”. More complex responses were called for by questions asking why or
how an action had been done. For “why”, the system answered with “because (event>” or
“in order to (event> ,” where (event) referred to a goal that the program had had when the
action was taken. For example, “Why dId you clear of f that cube?” might be answered by
“To put it on a large green cube.” The program retrieved the appropriate event from its
history list and then used a generation pattern associated with events of that type. For an
event of the type “(PUTON OBJ1 08J2),” the pattern would be:
((correct form of to put), (noun phrase for 08.11>, ON, (noun phrase for 08.12)).
Noun phrases in the pattern were generated by associating an English word with every
known object ; adjectives and relative clauses were added until a unique object (within the
domain of discourse) was described.
The stilted text generated by this scheme was moderated by the (heuristic) use of
pronouns for noun phrases. For example , if the referent of a noun phrase had been
mentioned In the same answer or in the previous one, an appropriate pronoun could be
selected for it. SHRDLU’s limited domain of discourse allowed it to exhibit surprisingly natural
•di~alogue with such simple techniques.
Simmons and Slocum (1972) developed a natural language system that generated
sentences from a semantic network representation of knowledge , based on a case grammar (see
Article C4). The program produced surface structure from the network by means of an
augmented transit ion net, adapted for the purpose of generation rather than parsing (see
Article 02). The object of the work was to substantiate the claim that “the semantic
networ k adequatel y represents some Important aspect of the meaning of discourse ”; if the
claim was true, then “the very least requirement” was that “the nets be able to preserve
enough Information to allow regeneration of the sentences--and some of their syntactic
paraphrases--from which th. nets were derived” (p. 903).
An i~!ustra tion of the capabilIties of the system Is given by the paragraph below, which
was Initially hand-coded into semantic network notation. (For a later version of the program in
which the parsIng was done automatical ly, see SImmons , 1973.)
John saw Mary wrestling with a bottle it the liquor bar. He went over
to help her with it. He drew the cork and they drank champagne
together.

E Text Generation 43
The network notation, in simplIfied form, Is indicated by the following representation of “John
saw Mary wrestling ”:
Cl TOKEN (see) C3 TOKEN (wrestle)
TINE PAST TINE PROGRESSIVE PAST
DATIVE C2 AGENT C4
OBJECT C3
C2 TOKEN (John C4 TOKEN (Mary)
NUMBER SINGU R NUMBER SINGULAR
Here ~j, Q~, ~~~~~~, and Q4 are nodes in the network representing concepts which are tokens
of meanings of “see”, “wrestle ,” “John”, and “Mary”. ~~I and SINGULAR are also nodes.
TOKEN, IJM~. OBJECT, and the like are types of arcs, or relations.
The representation shown was augmented by other relations , attached to verb nodes.
such as MOOD (Indicative or interrogative), VOICE (active or passive), and informat ion about
the relative times of events. Using this representation , the system was able to reconstruct
several versions of the original paragraph. One read:
John se Mary wrestling with a bottle at the liquor bar. John went
over to help her wIth It before he drew the cork. John and Mary
together drank the champagne.
The actual generation was accomplished by an ATN in which the arcs were labelled with
the names of relations that might occur In the semantic net. The actual path followed
through the ATN--and thus the exact text generated--depended both on which relations
were actually present and on which node or nodes were chosen as a starting point.
Wong (1976) has extended this approach, IncorporatIng features to handle extended
dIscourse.
Nell Goldman ’s (1975) program generates surface structure from a database of
conceptual dependency networks, as the text-generatIon part of Roger Schank’ s MARGIE
system , described In Article F5. The Udflceptua l dependency (CD) knowledge representation
scheme, discussed further in Article FB on Schank’ s SAM system , Is based on semantic
primitives (Article Repre sentaticn.C 5) and is therefore language Independent , so the actual
word selection for output must be performed by Goldman ’s text-generation subsystem , called
BABEL. This Is accomplished by means of a discrimination net (a kind of binary decision tree- -
see Article lnformeticn Proce ssing P.ychciog y.C) that operates on a CD network that is to be
verbalized. This discrimination net is used to select an appropriate verb sense to represent
the event specified by the CD. (A verb sense is a meaning of the verb: DRINK, for example
has two senses , to drink a fluid and to drink alcohol.) Essentially, there are only a small
number of possible verbs that can represent the event, and a set of predicates determines
which one to use. For instance , DRINK can be used to describe an INGEST event if the
<object) has the property FLUID. The section of the discrimination net that handles DRINK
might look like this:
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ • • A

44 Natural Language
• (EQ (ACTION ) INGEST)(PROP (OBJECT) FLUID)
F/. \T
(EQ (OBJECT) ALCOHOL)I
DRI~K1 DRINK Z
Once a verb sense has been selected , an associated framework Is used to generate a
case-oriented syntax net, which Is a structure similar to the semantic net of Simmons and
Slocum. These frameworks Include information concerning the form of the net and where In
the conceptualIzation the necessary information Is located. After the framework has been
filled out, other language-specific functions operate on the syntax net to complet e It
syntactically with respect to such things as tense, form, mood, and voice. Finally, an ATN is
used to generate the surface structure , as in the Simmons and Siocum program.
Yorick WIlks (1973) has developed a program that generates French from a semantic
base of templates and pars plates. This is part of a complete machine translation system
described in Article F2.
Discussion
The key point is that, as the richness and completeness of the underlying semantic
representation of the information has Increased , the quality of the resulting paraphrase has
improved. Like other areas of Al, the basic problem Is to determine exactly what the salient
points are and to obtain a good representation of them; progress in generation seems to be
closely tied to progress in knowledge representation. Future work in generation will also
have to address areas such as extended discourse , stylistics, etc. In this direction , Clippinger
(1975) has looked at psychological mechanisms underlying discourse production , and
Perrault , Alien, & Cohen (1978) have studied the planning of speech acts for communIcation
in context.
References
See Clippinger (1975), Friedman (1969) , Friedman (1971), Goldman (1976), KleIn &
SImmons (1963), Klein (1986), Perrau it Al ien, & Cohen (1978), QuIllian (1968), Quillhan
(1969), Simmons & Siocum (1972), Simmons (1973), Wllks (1973), Winograd (1972), Wang
(1976), and Yngve (1962).
A

