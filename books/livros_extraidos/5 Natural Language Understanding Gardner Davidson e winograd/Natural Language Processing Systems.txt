---~r~— --— ~~~~•—
~~ •.
F Natural Language Processing 8ystems 45
F. Natural Language Processing Systems
Ft. Early Natural Language Systems
Early work on machine processing of natural language assumed that the syntactic
Information In the sentence , along with the meaning of a finite set of words, was sufficient to
perform certain language tasks--in particular , answering questions posed In English. Several
of these early natural language programs are reviewed here: their techniques , their
successes , and their shortcomings. These programs were restricted to dialogues about
limited-knowledge domains In simple English and ignored most of the hard grammatical
problems In the complex constructions found In unrestricted English. Through work with
programs of this genre, it became apparent that people constantly use extensive world-
knowledge In processing language and that a computer could not hope to be competent
without “understanding ” language. These programs bridge the gap between the early
mechanical translation attempts of the 1 950s and current , semantics-based natural language
systems (see the Overview Article, Art icle B, and the Articles on recent NI systems in this
section).
SAD-SAM
SAD-SAM (Syntactic Appraiser & Diagrammer - Semantic Analyzing Machine) was
programmed by Robert LIndsay (1 983a) at Carnegie Institute of Technology in the lPL-V list-
processing language (see Article Al Lsn~.m~~s.A). The program accepts English sentences
about kInship relationships , builds a database, and answers questions about the facts It has
stored.
It accepts a vocabulary of Basic English (about 1,700 words) and follows a simple
context-free grammar. The SAD module parses the input from left to right, builds a syntactic
tree structure , and passes this structure on to SAM, which extracts the semantically
relevant (kinship-related) InformatIon to build the family trees and find answers to questions.
• Though the subset of English processed by SAD is quite ImpressIve in volume and
complexity of structure , only kinship relations are considered by SAM; all other semantic
informat ion Is Ignored. SAM does not depend on the order of the Input for buildIng the family
trees; if a fIrst input assigns offspring ~ and ~ to ~~~, and offspring Q and ~~ to ~~~, two “family
units” will be constructed , but they will be collapsed Into one If we lesrn later that ~ and ~are siblings. (Multi ple marriages are Illegal.) However, SAM carnot handle certain
ambiguities; the sentence “Joe plays In his Aunt Jane’s yard” indicates that Jane Is either
the sister or sister-in-law of Joe’s father, but SAM assigns one and only one connection at a
time and therefore cannot use the ambiguous information: The structure of the model permits
storing definite links but not possible Inferences.
BASEBALL
Also in the early 1960s, Bert Green and his colleag ues at Lincoln Labs wrote a program
• . called BASEBALL (Green at ai., 1983), agaIn using the 1PL-V programming language. BASEBALL
1
L. ~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~

• - 
~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ r— - -~~~~~~ ‘ ,-----. _ - _—‘-- --,---- ••• -
48 Natural Language
is essentially an information retrieval program, since its database of facts about eli of the
American League games during one year is not modified by the program. Acceptable input
questions from the user must have only one clause, no logical connectives (g~g. ~~~. i~i), no
comparatives (highest, 
~~~~~~~~~~~~ and no facts about sequences of events; and most words mu~t
• be recognized by the (extensive) dictionary.
The parsing system uses 14 categories of parts of speech and right-to-left scanning to
structure the input question into functional phrases. Using this structure and the key-words
found In the question , the input is transformed Into a specification list that is the canonical
expression for the meaning of the question. For example , the question “How many games did
the Yankees play In July?” becomes:
TEAM “ YANKEES
MONTH z ~~~GAMES (number of) • ?
The answer is found by searching the database for data items matching the specification list,
storing them on a “found” list, and eventually processing and outputting them.
The size of the dictionary and the heuristics used for resolving syntactic ambiguities
(score can be a verb or a noun; Boston, a place or the name of a team) make BASEBALL able
to answer most reasonable English questions about the games stored in the database.
However, BASEBALL does not return to the user for clarification of semantic ambiguities. it
simply fails to parse the sentence and asks for a rephrasing of the question. Though the
database is not modified by the program , BASEBALL exhibits deductive abilitIes In parsing
questions and processing the “found-Nat. ”
SIR
SIR (Semantic Information Retrieval) was written in LISP by Bertram Raphael (1968) as
part of his thesis research at MIT. It Is a prototype “understanding ” machine since It can
accumulate facts and then make deductions about them In order to answer questions .
SIR accepts a restricted subset of English. It matches sentences against 24 matching
patterns of the following sort:
“ is “
“ Is part of *
Is””?
How many * does “have?
What is the of “
where the elements matched against * are nouns, possibly modified by a “quantifier ” such
as: !~ fl~, every, ~~~~ or a number. The match with a rule triggers the appropriate action In
the program. A sample conversation might look like this:
Every boy is a person
I UNDERSTAND
A finger is part of a hand

-.•.-~ 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
Ft Early Natur al Language Systems 47
I UNDERSTAND
There are two hands on each person
I UNDERSTAND
How many fingers does John have ?
THE ABOVE SENTENCE IS AMBIGUOUS BUT I ASSUME (has) MEANS
(has as parts)
I DON’T KNOW WHETHER FINGER IS PART OF JOHN
John is a boy
I UNDERST AND
How many fingers does John have?
THE ABOVE SENTENCE IS AMBIGUOUS BUT I ASSUME (has) MEANS
(has as parts)
HOW MANY FINGERS PER HAND ?
Every hand has 5 fIngers
THE ABOVE SENTENCE IS AMBIGUOUS BUT I ASSUME (has) MEANS
(has 58 parts)
THE ANSWE R iS 10
Who Is President of the United States ?
• STATEMENT FORM NOT RECOGNIZED
SIR’s world Is concerned with only a small group of relations between objects , as
specified by the 25 matching patterns. Declarative input statements are used to build the
model, and the patterns have been chosen to express concepts that commonly occur in
human conversation : set inclusion, spatial relationship , etc. The program has certain built-in
knowledge (like transitivity of set Inclusion) that enables -It to answer questions about some
facts not explicitly stated during input. SIR can also Interact with the user to gather more
Information or to tesoive ambIguities.
STUDENT
STUDENT is another pattern-matching natural iangua g. program , written by Daniel
Bobrow (1988) as his doctor al research project at MIT. STUD ENT is abl. to read and solve
high-scho ol- level algebra story problems ks the following :
11 the number 0! customers Tom gets Is twice the squa re of 20 per
cent of the number of advertiseme nts he runs, and the numbe r of
advertisements he runs is 46, what is the number of custo mers Tom
gets?
The entire subset of English recognized by STUDENT Is derived from the following set of
basic patterns:
---•—--•~
—-- -
~~--
~— —• —
~~— — ---• -~~~--~~~~~~- -——-— ,
~~•-— -~~~-..
~--••
~~~~-•- -

46 Natura l Langua ge
(WHAT ARE “AND”) (FIND “ AND”)
(WHAT IS”) (“IS MULTIPLIED BY”)
(HOW MANY “1 iS”) (“IS DIVIDED BY”)
(HOW MANY”DO ”IiAVE) (“IS”)
(HOW MANY “ DOES “HAVE) (“(“1/VERB) “1 “)
(FIND ”)
(“(“1/VERB) “AS MANY “AS “(“1/VERB) ”)
A sign IndIcates a string of words of any length, ~J. Indicates one word, and (“1/VERB)
means the matching element must be recognized as a verb by the dictionary .
To construct the algebraic equations that will lead to the solution , the problem
statement Is scanned , first for linguistic forms associated with the equality relation (such as
(“ IS “3), then for algebraic operators. STUDENT then builds a list of the answers required ,
the units Involved in the problem , and a list of all the variables in the equations. Then
STUDENT Invokes the SOLVE module with the set of equations and the desired unknowns.
if SOLVE fails, STUDENT applies heuristics such as: expanding idioms, identifying two
previously “slightly different” variables , or Invoking the REMEMBER module that contains
special facts like:
(FEET IS THE PLURAL OF FOOT)
(ONE HALF ALWAYS MEANS .5)
(SUCCESSFUL CANDIDATES SOMETIMES MEANS STUDENTS WHO
PASSED THE ADMISSTIONS TEST)
(DISTANCE EQUALS SPEED TIMES TIME)
(ONE FOOT EQUALS 12 INCHES)
If all else falls, STUDENT can request more Information from the user.
Each time a heuristIc succeeds in findIng a new equation , the augmented set is sent to
• SOLVE. STUDENT finally prInts the solution (using the pattern “variable IS value”) or reports
Its inability to solve, in the example. STUDENT would print -
(THE NUMBER OF CUSTOMERS TOM GETS IS 162)
STUDENT’ s simple pattern-matching scheme , combined with its small set of well-chosen
heurIstics , does an Impressive job of parsing typical high-school algebra word problems. The
program can be tricked easily (“Bill has 3 apples , 2 pears, and one dog” would be parsed into
two clauses); but as implemente d, It solved the problems submitte d as fast as humans could
(In one case, several times faster than an MIT graduate student who was not using the
• special “age problem ” heuristics). A slightly modified version was found to be a good model
for human problem solving, In a study conJucts d by Paige & Simon (1966).
ELIZA
Perhaps the most famous of these “pattern-matching ” natural language programs was
ELIZA, wrItten by Joseph Weizsnbaum (1966) at MIT. This progra m assumes the role Of a
Rogeri.n or “non-dI rective ” thera pist In Its dialogue with the user. Although ELiZA’ s dialo gue
L ~~~~~~~~~~~~~~~~~~~~~~~~ _________________ __

- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ —.•---- ---——---• -—•——--—----• -—— ~~—-•- •••
~~- —
~~•—• --—-‘--•--•- -
~-— L~•~~~~- ________________ -
Fl Early Natural Language Systems 4~
with the user appears surprisingl y realist ic , the program uses no more Intelligent mechanisms
than those described above. Rather , the nature of therapist-pat ient dialogue lends itself to
simulation in this manner. A sample dialogue might look like this (with ELIZA In caps):
Men are all alike.
IN WHAT WAYS
They’r. always bugging us about something or other.
CAN YOU THINK OF A SPECIFIC EXAMPLE
Well, my boyfriend made me come here.
YOUR BOYFRIEND MADE YOU COME HERE
He says I’m depressed much of the time.I AM SORRY TO HEAR THAT YOU ARE DEPRESSED
it’s true. I’m unhappy.
DO YOU THINK COMING HERE WILL HELP YOU NOT TO BE UNHAPPY
I need some help, that much seems certain.
WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP
Perhaps I could learn to get along with my mother.
TELL ME MORE ABOUT YOUR FAMILY
WHAT ELSE COMES TO MIND WHEN YOU THINK OF YOUR FATHER
• . Bullies.
DOES THAT HAVE ANYTHING TO DO WITH THE FACT THAT YOUR
BOYFRIEND MADE YOU COME HERE
ELIZA is programmed In the SLIP language (Symmetric List Processor), developed by
Welzenbaum (1963). An offline script composed of hr,lists specifies the nature of the Input
-: language by specifying for each keyword a set of patterns to look for and the possIble
transformations assocIated with each pattern. For example , assuming the keyword Is !!~~ a
pattern used for matching might be
(0 YOU 0 ME)
and one of the transformation rules associated with thIs pattern might be
(WHAT MAKES YOU THINK I 3 YOU)
where Q in the pattern matches any string of words, and ~ In the transformation rule
designates the third element matched (in this case, everything between !2!J and ~~~ The
Input Is scanned from left to right for a keyword; a systematic hashing procedure very
quIckly eliminates words that cannot be keywords. ELIZA deletes the part of text preceding
a punctuation delimiter If no keyword has been found; otherwise , the part following It
(Insuring thus that the transformation will be applied to ens single phrase or sentence ). If
• __ _
____ ____________

r ~~- -- 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
60 Natural Language -
several keywords are found, they are stored In turn In a “keystack” according to the rank of
precedence associated with each of them; then the Input Is matched agaInst each
decomposition ruie In turn. These patterns can be ordered in the keyiist so that the more
complex ones are tried first; for the keyword ‘I” the pattern
(0 I 0 DEPRESSED 0)
Is hard to match, but If a match Is achieved , the answe r can be more spectacular than the
transformations for the “general match ” pattern
(0 I 0).
When a match Is found, EL1ZA generates a response, using the reassembly rules for this
decompositIon rule In a cyclic manner. If no decomposition rule matches for a given keyword,
the keystack is popped and the pattern-matching procedure is repeated for the new
keyword. If the keystack Is smpty, a response like “Please go on,” “ i see,” or “Very
interesting ” will always do.
Several other tricks--like substituting for keywords In Its response, associatin g
keywords with a class or situation (Mother ImplIes I amilv), and remembering these keyword
affiliates over the course of the conversation-- h&p enhance the illusion of intelligent
dialogue.
Conclusions
None of these early natural language systems dealt with the syntax of language in any
sophisticated way. In these early programs , the semantic knowledge needed to respond to
the user was implicit in the patterns and the ad hoc rules used for parsing. Modern natural
language programs maintain large databases of explicit world-knowledge that they use to
assist in parsing the sente nce as well as in interpretIng it.
References
For general reference , see Boden (1977), for lucid discussions of several of these
systems; also, Simmons (1966), Simmon s (1970), and Winograd (1974). The collections in
Feigenbaum & Feldman (1963) and Minsky (1968) contaIn much of the original materi al.

- --~~~--~~---•-— •~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 
F2 WlIks’s MechanIcal Translation System 61
F2. Wilks ’s Mechanical Translation Syste m
Current work in machine translation of languages is exemplified by Wllks’s system
(1973) , whIch can produce good French from small English paragraphs. The system is entirely
semantics based; that Is, no use Is made of conventional linguistic syntax In either the
-
• analysis or the generation stages. The input English text Is first converted to a semantic
representation and then converted to the final translated text. (The use of an intermediate
representation bears some similarity to the Weaver ’s Idea of interlingus, discussed in Article
B.) Wilks stresses that his semantic representation Is designed for mechanical translation
and may not be appropriate for other NL tasks like question answering. The rationale for this
Is that an explIcit representation of the logical implications of a sentence , which is
necessary for some tasks, may not be necessary for translation: If the two languages are
similar, an approprIate target sentence with the same implications can often be found in a
more straightforward way.
Wilks’s system first fragments the Input text Into substrings of words; It then matches
the fragments against a set of standard templates , that Is, deep semantic forms that try to
pick out the meaning conveyed by the input-text fragments. The output of this stage is a
first approximation to a semantic representation of each of these fragments. The system
then tries to tie together these representations to produce a more densely connected
representation for the complete text. When this process has been completed , the
generation of the output text Is accomplished by unwinding the Interlingual representation
using functions that interpret It in the target language.
The Interlingual representation is based on semantic primitives (see Article
Representation.C5) that Wilks calls elements. Elements express the entities, states, qua lities ,
and actions about which humans communicate. In the system as reported in Wilks (1973),
there were 60 of these elements , which fall Into 5 classes , as shown In the following
examples.
1. Entitles: MAN (human being),
PART (parts of things),
STUFF (substances).
2. Cases: TO (direction),
IN (containment).
3. Sorts: CONT (being a container),
THRU (being an aperture).
4. Type indicators : KIND (being a quality),
HOW (being a type of action).
5. Actions: CAUSE (causes to happen),
BE (exists),
FLOW (moving as liquids do).
The elements are used to build up “formulas ,” which each represent one sense ot a word.
The verb ~~~~ for example , is represented by the following formula:
_______________

-
~~~~ •~•--•--••--• ——•
~~---• ---•.- -— -—
~~~~~— --• ••-- - •
52 Natural Language
((“ANI SUBJ)
(((FLOW STUFF) OBJE)
((“ANI IN) U(THIS (‘ANt (THRU PART))) TO) (BE CAUSE))))) .
Drink is thus an action, (BE CAUSE), done by animate subjects , (“ANI SUBJ) , to liquids , ((FLOW
STUFF) OBJE). It causes the liquid to be In the animate object, (“ANI IN), via a particular
aperture of the animate object, ((THIS (‘ANI (THRU PART))) TO).
Formulas are understood as expressing preferences rather than absolute requirements.
In the formula for ~~~~~~~~~~ for example, It is only a preference that the agent be animate and
the object liquid; the system could accept a sentence about cars that drink gasoline. The
function of preferences , nevertheless , Is to help determine the correct word-senses in the
input text. In “John drank a whole pitcher ,” the preference for a liquid object would select
the formula for pitcher as a container of liquid rather than the one for a baseball player.
The system ’s dictionary contains formulas for all the word-senses paired with
stereot ypes for producing the translated words in the target Ianguage.The following Is an
example of two stereotypes for the word advise (into French):
(ADVISE (CONSE ILLER A (FI41 FOLK MAN))
(CONSEILLER (FN2 ACT STATE STUFF)))
The two functions , FN1 and FN2, are used to distinguish the two possible constructions in
French Involving conseBier: conseiller a... and simply conseWer .... The first woul d be used
in translating “I advise John to have patience ”; the second , for “I advise patience. ”
Functions like these In stereotypes are evaluated by the generation routines. Each function
evaluates either to NIL, in which case the stereotype fails, or to words that will appear In the
output text. The stereotypes serve the purpos e of a text generation grammar , providing
complex context-sensitive rules where required , without search of a large store of such
rules. This is an example of procedural representation of knowled ge (see Article
Represent etion.C4).
Analysis of an English sentence by the system proceeds in several stages. First the
text is separated Into fragments , where the fragment boundaries are determined by
punctuation marks, conjunctions , prepositions, and so on.
For each word in the fragment , the dictionary may contain several word-sense
formulas; therefore one of many possible sequences of formulas must be selected to
represent the fragment. For this purpos e, the formula sequences are matched against a
buIlt-in list of templates , which are networks of formulas based on a basic actor-action-object
triple called a bare template. Examples of such triples are MAN CAUSE THING and MAN DO
THING. Special forms of templates are available to match fragments like prepositional
phrases, it is assumed that It is possible to build up a finIte inventory of bare templates that
would be adequate for the analysis of ordinary language. The Inventory for the system has
been determined empIrically and is easIly modified.
At the initIal stage of template matching some senses of the words In the fragment can
be rejected for failure to match any bare template , but more than one candidate template
may remain. For example , if the fragment is “the policeman interrogated the crook,” there
will still be two possible templates , MAN FORCE MAN and MAN FORCE THING, which take
“crook” to be a person and a shepherd ’s staff, respectively.
-- -~~•a~~~-••••— ~~~~--—---~~~~•—- — -•• ~~~~~~~~~~~~~~ •.-~- 

r-~~~~~~~~- --~~~~~~~~~~~~~~~~~~~~~ ~~-- - --~~~~~~~~~~~~~~~~ - ~~~-
P2 Wilks ’s Mechanical Translation System 63
At the next stage of the analysIs , called expansion , a more detailed matching algorithm
is used. The principle Is that the template representation chosen for a fragment is the one In
which the most preferences are satisfied. In the example , the preference of “Interrogate ”
for an object representing a human being Is decisive. The result of this stage Is a full
template (a network of formulas) for each fragment , in which semantic dependencies among
the formulas have been noted. The overall goal of semantic densi ty--that is, of maximizing the
interdependence of formulas--is one of the key Ideas in WiIks’s work and produces a good
solution to many problems of ambiguity.
In the succeeding stage of analysis , the templates for individual fragments are tied
together wIth higher level dependencies , expressed in terms of pare plates , or patterns that
span two templates. The use of parapIates is to resolve prepositional or case ambiguities
(see Article C4). For example , the fragments “he ran the mile” and “in four minutes ” would
be tied together by a paraplate for the TIMELOCATION case; had the second fragment been
“In a plastic bag,” a CONTAINMENT case paraplate would have matched Instead. A similar
technique Is used to resolve simple problems of pronoun reference, as in “I bough t the wine,
sat on a rock, and drank it.” in both cases, the chief preference of the system Is for
semantic density.
Finally, the system uses some commonsense inference rules to deal with situations in
which more explicit world-knowledge is required to resolve pronoun references than formulas ,
templates , and paraplates provide. At the completion of this analysis , the Input text has
been replaced by an Interlingual representation with suitabie markers , and other information
Is used by the generation routines in a relatively straightforward manner to produce the final
output text.
References
This descriptio n of Wiiks’s work is based primarily on Wilks (1973). Other description s
Include Wilks (1975a ), WIIks (1975b), WIlks (19?5c), Wiiks (1977b) , and Wilks (1978).
Also of interest: Charn lak & Wllks (1978) and Schank (1975).

-~~~~ - -- 
~-~~~~~~----~r - -~~~~~~~~~~~~~ - 
64 Natural Language
F3. LUNAR
LUNAR is an experimental , natural language information retrieval system designed by
WillIam Woods at B8N (Woods , 1 973b; Woods, Kaplan, & Nash-Webber , 1972) to allow
geologists to access , compare , and evaluate chemical- oinalysls data on moon rock and soil
composition obtained from the Apollo 11 mission (see Article Apphcation s.F4 for a discussion
of Al Information retreival systems). The primary goal of the desIgners was research on the
problems Involved In building a man-machine interface that would allow communication in
ordinary English. A “real-world” application was chosen for two reasons: First, it tends to
focus effort on the problems really in need of solution (sometimes this is implicItly avoided in
“toy” problems); second , the possibility of producing a system capable of performing a
worthwhil e task lends some additional Impetus to the work.
LUNAR operates by translating a question entered in English into an expression in a
formal query language (Codd, 1974). The translation Is done using an augmented transition
network parser coupled with a rule-driven semantic Interpretation procedure , which is used to
guide the analysis of the question. The “query” that results from this analysis is then
applied to the database to produce the answer to the request. The query language is a
generalizatIon of the predicate calculus (Article Rsprsssntetion.C1). Its central feature is a
quantifier function that Is able to express , in a simple manner, the restrictions placed on a
database-retrieval request by the user. This function is used in concert with special
enumeration functions for classes of database objects , freeing the quantifier function from
explicit dependence on the structure of the database. LUNAR also served as a foundation
for the early work done on speec h understanding at BBN (see ArtIcle Speech,B3).
Detailed Description
The following list of requests Is IndicatIve of the types of English constructions that
can be handled by LUNAR (shown as they would actually be presented to the system):
1. (WHAT IS THE AVERAGE CONCENTRATION OF ALUMINUM IN
HIGH ALKALI ROCKS?)
2. (WHAT SAMPLES CONTAIN P205?)
3. (GIVE ME THE MODAL ANALYSES OF P205 IN THOSE SAMPLES)
4. (GiVE ME EU DETERMINATIONS IN SAMPLES WHICH CONTAIN ILM)
LUNAR processes these requests In the following manner:
Syntactic analysis using an augmented transition network parser and
heuristic information (including semantics ) to produce the most likely derivation
tree for the request;
Semantic interpretation to produce a representation of the meaning of the
request In a formal query language; and
Execution of the query language expressio n on the database to produce the
answer to the request.
LUNAR ’s language processor contains a grammar for a large subset of English, the

— --—--— —- .,-,- -
PS LUNAR 65
semantic rules for interpreting database requests , and a dictionary of approximately 3,500
words. As an IndIcation of the capabilities of the processor, It is able to deal with tense and
modality, some anaphorlc references and comparatives , restrictive relative clauses , certain
adjective modifiers (some of which alter the range of quantification or InterpretatIon of a
noun phrase), and embedded complement constructions. Some problems do arIse In parsing
conjunctive constructions and in resolving ambiguity In the scope of quantifiers. Emphasis
has been placed on the types of English constructions actually used by geologIsts so that
the system knows how they habitually refer to th. objects In its database.
The Query Language
The formal query language contains three types of obJects~ “designators ,” which name
classes of objects in the database (including functionally defined objects); “propositions, ”
which are formed from predicates with designators as arguments; and “commands ,” which
InItiate actions. Thus, If $10048 is a designator for a particular samp le, QLII~~ is a designator
for the mineral olivIne, CONTAIN Is a predicate , and J~~I is a truth-value testing command .
then “(TEST (CONTAIN $10046 Oily))” is a sample expression in the query language. The
primary function in the language is the quantifier function fQ~, which Is used in expressions
of the following type:
(FOR QUANT X F CI.ASS: PX ; OX)
where QUANT Is a quantifier like ~~~~~ or every, or a numerical or comparative quantifier; ~ Is
a variable of quantification; CLASS determines the class of objects over which the
quantifIcation Is to range; ~~ specifies a restriction on the range; and Q~ Is the proposit ion
or command being quantifIed. fQfi is used with enumeration functions that can access the
database. Thus, FOR Itself Is independent of the database structure. As an example (taken
from Woods , 1 973b), If SEQ is an enumeration function used to enumerate a precomputed list,
and if PRINTOUT is a command that prInts a representation for the designator given as its
argument, then
(FOR EVERY Xl / (SEQ TYPECS) : I ; (PRINTOUT Xl))
prints the sample numbers for all type-C samples. In this case there is no restriction on the
range of quantIfication in that PX ~ I, the unIversally true proposition.
A fuller example of the operation of LUNAR (simplified slightly from the same source) is
shown below.
Request:
(DO ANY SAMPLES HAVE GREATER ThAN 13 PERCEN T ALUMINUM )
Query Language Translation (after parsing):
(TEST (FOR SOME Xl! (SEQ SAMPLES) : I ; (CONTAIN Xl
(NPR” X2/ ‘AL203) (GREATERTHAN 13 PCT))))
Respons e: -
YES
-J 
- - -
~~~~~-
~~~ --- -
.
~ 

-.. - - —.~-—---,-
~~~~--—--— .——- -!~~‘—— ~~~~~~~~~~~~~~~~~ -•
~~~~ ___________________________________________________________________________________
- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -~~~~~--~~
58 Natura l Language
LUNAR is perhaps the best operatIonal example of a fInely tuned ATN parsing system
applied to a real-world problem. Since the system has limited performance goals (i.e.,
facilitating database inquiry as opposed to holding an interesting conversation), many of the
complications inherent In language understanding are avoided.
References
See Codd (1974), Wood s (1973b ), Woods & Kapln (1971), and Woods, Kaplan, &
Nash-Webber (1972).
I
I 
— ~~~~~~ --— -----~-----~~-- - ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~

----~~~ -~~~~~~~~~~~--~~~~~~~~~~~ -~~~~ ~~-— ~~~~~~~~~ — ~~~-- -~- -- ~~~.---—- -
P4 SHROLU 67
P4. SHRDLU
SHRDLU was written by Terry Wlnograd (1972) as his doctoral research at MIT. It was
written In LiSP and MICRO-PLANNER , a LISP-based programming language (see Article Al
Lengusgss. C2). The design of the system is based on the belief that to understand
language , a program must deal in an Integ rated way with syntax , semantIcs , and reasoning.
The basic viewpoint guiding Its implementation is that meanings (of words , phrases , and
sentences) can be embodied in procedural structur• s and that language is a way of
activating appropriate procedures withIn the hearer. Thus, Instead of representing
knowledge about syntax and meaning as rules in a grammar or as patterns to be matched
agains t the Input, Winograd embodied the knowled ge in SHRDLU in pieces of executable
computer code. For example, the context-free rule saying that a sentence is composed of a
noun phrase and a verb phrase,
S -> NP VP
Is embodied in the MICRO-PLANNER proced ure:
(PDEFINE SENTENCE(((PARSE NP) NIL FAIL)
((PARSE VP) FAIL FAIL RETURN)))
When called, this program, called SENTENCE, uses Independent procedures for parsing a nou n
phrase followed by a verb phrase. These, in turn, can call other procedures. The process
FAILs If the required constituents are not found. With such special procedural representations
for syntactic , semantic , end reason ing knowledge , SHRDLU was able to achieve
unprecedented performance levels in dialogues simulating a blocks world robot.
SHRDLU operates within a small “toy” domain so that It can have an extensive model of
the structures and processes allowed in the domain. The program simulates the operation of
a robot arm that manipulates toy blocks on a table. The system maintains an Interactive
dialogue with the user: It can accept statements and commands as well as answer questions
about the state of Its world and the reasons for its actions. The implemented system
consists of four basic elements: a parser, a recognition grammar for English , programs for
semantic analysis (to change a sentence into a sequence of commands to the robot or Into a
query of the database), and a problem solver (which knows about how to accomplish tasks in
the blocks world).
Each procedure can make any checks on the sentence being parsed , perform any
actions , or call on other procedures that may be required to accomplish its goal. For
example , the VERB PHRASE procedure called above contains calls to functions that establish
verb-subject agreement by searchIng through the entire derivation tree for other
constituents while still being In the middle of parsing the VP. SHRDLU’ s knowledge base
Includes a detailed model of the blocks world it manipulates , as well as a simple model of its
own reasoning processes , so that It can explain Its actions.
Reason ing In SHROLU
SHRDLU’ s model of the world and reasoning about ft are done in the MICRO-PLANNER 
~~~~~~~~~- --— — *. ---~.--. ~~--.- 

- ~~----,- - ~~~--.—- ,--~~~~-- ----- ~~~-—- -~~~~ ~~---~~ ~~~~~~~~~~~ --~~~~~~~~~~~~~ - ~~~— - . ~~~~~~~~~~~~~~ ---~~~~~~~~~~ -~~ --
~~~,
68 Natural Language
programming language , which facilitates the representation of problem-solving procedures .
allowing the user to specify his own heuristics and strategies for a particular domain.
Knowledge about the state of the world Is translated Into MICRO-PLANNER assertions , and
manipulative and reasoning knowledge is embodied In MICRO-PLANNER programs. For
example , the Input sentence “The pyramid is on the table” might be translated into an
assertion of the form:
(ON PYRAMID TABLE)
SHRDLU’s problem solver consists of a group of “theorems ” about the robot’ s
environment and actions , represented as MICRO-PLANNER procedures. In operation , the
theorem prover manipulates the state of the domain by running MICRO-PLANNER programs
that perform the actions requested by the user.
The philosophy and implementation of PLANNER are described in the Al Progrem rrwng
Languages section of the Handbook , but a brief discussion here will Illustrate Its use in
SHRDLU. The main idea of PLANNER Is to solve problems using specific procedures built into
the problem statements themselves , as well as using general problem -solving rules. The
advantage of using these problem-specific rules or heuristics Is that they can radically
increase the efficiency of the process. Furthermore , the problem statements are programs
and thus can carry out actions in the problem-solving process. Thus, to put one block on
another , there might be a MICRO-PL ANNER program of the form:
(THGOAL (ON ?X 1Y )
(OR (ON—TOP YX ?Y)
(AND CLEAR—TOP ?X
CLEAR—TOP ?Y
PUT—OH ?X ?Y )))
This means that, If X is not already on Y, that state can be achieved by clearing off
everything that is stacked on top of X (so that the robot can move X), clearing of f V (so that
X car. be placed on top of V) and then putting X on V. The procedure resembles a predicate
calculus theorem , but there are important differences . The PLANNER procedure Is a program ,
and its operators carry out actions. The THGOAL procedure finds an assertion in the
database or proves it using other procedures. ~Q and QB are logIcal connectives. The
crucial element Is that though PLANNER may end up doing a proof , It does so only after
checking some conditions that may make the proof trivial, or impossible , and It only performs
the proof on relevant arguments , rather than checking all entities in the database as a blind
theorem prover might. Moreover , no sharp distinction is drawn between proof by showing
that a desired assertion Is already true and proof by finding a sequence of actions
(manipulating blocks) that will make the assertion true. In addition to the article on PLANNER
(Al Lenguoge s.C2), the reader Is referred to the Knowledge Rsprsesntsticn section for a
general discussion of these Issues.
Grammar , Syntax , and Semantics
SHRDLU’ s grammar Is based on the notion of s,st.mlc grammar , a system of choice
networks that specifies the features of a syntactic unit, how the unit functions , and how it
influences other units, discussed In Article C3. Thus, a systemic grammar contains not only
the constituent elements of a syntactic group but also higher level features such as mood,
tense, and voice.~
——--— ~~~~----- -- -~~~- . ~~~-

F4 SHROLU 59
in order to facilitate the analysis , the parsing process looks for syntactic units that
play a major role in meaning, and the semantic programs are organized into groups of
procedures that are applicable to a certain type of syntactic unit. In addition , the database
definitions contain semantic markers that can be used by the syntactic programs to rule out
grammatical but semantically Incorrect sentences such as “The table picks up blocks. ”
These markers are calls to semantic procedures that check for restrictions , such as that only
animate objects pick up things. These semantic programs can also examine the context of
discourse to clarIfy meanings, establish pronoun rsfgre rits, and InitIate other semantically
guided parsing functions.
Parsing
To write SHRDLU’ s parser, Winograd first wrote a programmIng language , embedded In
LISP, which he called PROGRAMMAR. PROGRAMMAR supplies primitive functions for building
systemically described syntactic structures. The theory behind PROGRAMMAR Is that basic
programming methods , such as procedures , iteration , and recursion , are also basic to the
cognItive process. Thus, a grammar can be Implemented in PROGRAMMAR without additional
programming paraphernalia; special syntactic Items (such as conjunctions) are dealt with
through calls to special procedures. PROGRAMMAR operates basically In a top-down , left-to-
F right fashion but uses neither a parallel processing nor backtracking strategy In dealing with
multiple alternatives (see Article 01). PROGRAMMAR finds one parsing rather directly, since
decisions at choIce-points are guided by the semantic procedures. By functionally
integratIng its knowledge of syntax and semantics , SHRDLU can avoid tryIng all choices in an
ambiguous situation. If the choice made does fall, PROGRAMMAR has primitives tore returning
to the choice-point with the reasons for the failure and informing the parser of the next best
choice based on these reasons. This “directed backup” is far different from PLANNER’s
automatic backtracking in that the design philosophy of the parser is oriented toward making
an original correct choice rather than establishing exhaustive backtracking.
The key to the system ’s successful operation is the Interaction of PLANNER reasoning
procedures , semantic analysis, and PROGRAMMAR. All three of these elements examine the
input and help direct the parsing process. By making use of this multiple-source knowledge
and programmed-in “hints” (heur istics), SHRDLIJ successfully dealt with language issues such
as Jronouns and referents. The reader Is referred to Winograd’ s Understanding Natural
Language (1972), pages 8-15, for an illustrative sample dialogue with SHROLU.
Discussion
SHRDLU was a significant step forward In natural language processing research
because of its attempts to combine models of human linguistic and reasoning methods in the
language understanding process. Before SHRDLU , most Al language programs were
linguistically simple; they used keyword and pattern-oriented grammars. Furthermore , even
the more powerful grammar models used by linguists made little use of inference methods and
semantic knowledge in the analysis of sentence structure. A union of these two techniques
gives SHADLU impressive results end makes It a more viable theoretical model of human
language processing.
SHRDIU does have Its problems, however. Like most existing natural language
_

~ - -
SO Natural Language
systems , SHRDLU lacks the ability to handle many of the more complex features of English.
Some of the problem areas are agreement , dealing with hypotheses, and handling words such
as fl~ and and.
Wilks (1974) has argued that SHRDLU’ s power does not come from linguisti c analy sis
but from the use of problem-solving methods in a simple, logIcal, and closed domain (blocks
world), thus eliminating the need to face some of the more difficult language issues. It
seems doubtful that if SHRDLU were extended to a larger domain, it would be able to deal
with these problems. Further , the level at which SHROLU seeks to simulate the intermixing of
knowledge sources typical of human reasoning is embedded In its processes rather than
made explicit In its control structure , where It would he most powerful. Lastly, its problem
solving Is still highly oriented to predicate calculus and limited in its use of inferential and
heurIstic data (Winograd , 1974, pp. 40-48).
References
Winograd (1972) is the principal reference on SHRDLU. The original version of the —
thesis is Winograd (1971). A convenient summary is given In Winograd (1973). Boden
(197?) also presents a clear and concise discussion of the system.
Also of Interest are Sussman , Wino grad , & Charniak (1970) , the MICRO-PLANNER
manual; Wilks (1974); Wlnograd (1974); and Winograd (forthcoming).

- -
F6 MARGIE 61
F6. MARGIE
MARGIE (Meaning Analysis , Response Generation , and Inference on English) was a
program developed by Roger Schank and his students at the Stanford Al Lab (Schank , 1975).
Its intent was to provide an intuitive model of the process of natural language understanding.
More recent work by Schank and his colleagues at Yale on story understanding and conceptual
dependenc y theory are described In Article F8 on their SAM and PAM systems.
Conceptual Dependency Theory
The central feature of the MARGIE system was the use of a knowledge representation
scheme called Conceptual Dependency. Conceptual dependency is intended to represent
meaning in a sufficiently deep manner so that all ambiguity is eliminated. Every sentence
maps into a canonical form, and any two sentences with the same “meaning ” will have the
same representation . This goal was approached by designing a graph-structure formalism
based on a set of primitive concepts. There are 6 basic types of concepts: things, actions.
attributes of things, attributes of actions , times, and locations (the first four correspond
roughly to nouns, verbs, adjectives , and adverbs). Relations among concepts are called
dependencies , and there are 15 types of these. Among them are case relationships such as
those between an act and Its object , its direction , or Its recipient and donor (see Article C4
on case grammars). Graphically, each type of dependency is denoted with a special arrow
symbol (link), and each concept is denoted by a word representing it. For example , “John
gives Mary a book” would be expressed as:
o R Mary
John (~~.i) ATRANS ~— book 
~E John
where John, book, and ~~y are concept nodes. Also, the concept node ATRANS (abstract
transfer--I.e., transfer of possession) is one of a small set of primitive verbs (about twelve)
from which all actions must be built up. Other primitives Include PTRANS (physical transfer--
I.e., movement) and PROPEL (apply a force). The complicated , three-pointed arrow labeled B
indicates a recipient-donor dependency between Mary and John and the book, since Mary
got the book from John. The arrow labeled ~ indicates an “objective ” dependency; that Is,
the book Is the object of the ATRANS , since it Is the thing being given. Dependency links
may link concepts or other conceptual dependency networks.
Another example , “John eats the Ice cream with a spoon,” would be represented as:
o D John I JohnJohn <.u> INGEST Ice cream
spoon
P1 VEIce cream
AIII ‘- , spoon
V
CONTAIN (spoon) 0
Ice creL .L~h
L —-- ~~~- --- --t-.--~--~~ - - -

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
62 Natural Language
where the Q and I arrows Indicate DIRECTION and INSTRUMENT , respectively. Notice that in
this example , “mouth” has entered the diagram as part of the conceptualization , even though
It was not In the original sentence. This is part of the fundamental difference between
conceptual dependency networks and the syntactic tree that a grammar may produce in
parsing a sentence. John’s mouth as the recipient of the ice cream Is inherent in the
“meaning ” of the sentence , whether it Is expressed or not. in fact, the diagram can never be
finished , because we could add such details as “John INGESTed the ice cream by TRANSing
the Ice cream on a spoon to his mouth, by TRANSIng the spoon to the Ice cream, by GRASPIng
the spoon, by MOVing his hand to the spoon, by MOVing his hand muscles ,” and so on. Such
an analysis Is known to both the speaker and the hearer of the sentence and normally would
not need to be expanded. (However , if we were actually designing a robot to perform such
an action , we would want access to a more detailed network that would represent the
robot’ s procedural knowledge about eating.)
For some tasks, like paraphrasing and question answering, this style of representation ¶
has a number of advantages over more surface-oriented system s . in particuiar , sentences
like
Shakespeare wrote Hamlet
and
The author of Hamlet was Shakespeare
which in some sense have the same meaning, map into the same deep structure. They can
thus be seen to be paraphrases of each other. Another Important aspect of conceptual
dependency theory is its Independence from syntax; In contrast with earlier work in the
paradigms of transfor mallonal grammar or phrase-structure grammar , a “parse” of a sentence in
conceptual dependency bears little relation to the syntactic structure. Schank (1975) also
claims that conceptual dependency has a certain amount of psychological validity, In that it
reflects Intuitive notions of human cognit ion.
MARGIE
The MARGIE system , programmed in LISP 1.8, was divided into three components. The
first, written by Chris Riesbeck , was a conceptual analyzer , which took English sentences end
converted them into an internal conceptual dependency representation. This was done
through a system of “requests ,” which were similar to demons or production systems. A request
is essentially a pIece of code that looks for some surface linguis tic construct and takes a
specific action If it is found. It consists of a “test condition ,” to be searched for In the input,
and an “action,” to be executed if the test Is successful. The test might be as specific as a
particular word or as general as an entire conceptualizatIon . The action might contain
Information about: (a) what to look for next in the Input, (b) what to do with the input just
found, and (c) how to organize the representation. The flexibility of this formalism allows the
system to function without depending heavily on syntax, although it is otherwise quite similar
to the tests and actions that make ATNs such a powerfui parsing mechanism.
The middle phase of the system, written by Chuck Pleger, was an Inferencer designed
to accept a proposItIon (stated in conceptual dependency ) and deduce a large number of
facts from the proposition In the current context of the system ’s memory. The motivation for
this component was the assumption that humans “understand” far more from a sentence than

F6 MARGIE 63
is actually stated. Sixteen types of inferences were identified , Including “cause, ” “effect ,”
“specification,” and 9unction. ” The inference knowledge was represented in memory in a
modified semantic net. Inferences were organized into “molecules,” for the purpose ofapplying them. An example of this process might be:
John hit Mary.
from which the system might infer (among many other things):
John was angry with Mary.
Mary might hit John back.
Mary might get hurt.
The module does relatively unrestricted forward Inferencing, which tended to produce large
numbers of Inferences for any given Input.
The last part of the system was a text generat ion module written by Neil Goldman. This
took an internal conceptual dependency representation and converted It into English-like
output, In a two-part process:
1. A discrimination net was used to distinguish between different word-senses.
This permitted the system to use English-specific contextual criteria for
selecting words (especially verbs) to “name” conceptual patterns.
2. An ATN was used to linearize the conceptual dependency representation Into a
surface-like structure.
The text generation module is also discussed In Article E.
MARGIE ran In two modes: inference mode and paraphrase mode. In inference mode, it
would accept a sentence and attempt to make Inferences from that sentence , as described
above. in paraphrase mode, it would attempt to restate the sentence In as many equivalent
ways as possible. For example , given the input
John killed Mary by choking her.
it might produce the paraphrases
John strangled Mary.
John choked Mary and she died because she was unable to breathe.
• - Discussion
MARGIE is not, and was not intended to be, a “finished” production-level system.
Rather , the goal was to provide ~ foundation for further work In computational linguistics. Of
particular interest In MARGIE was the use of conceptual dependency as an lnterllngua, a
language-Independent representation scheme for encoding the meaning of sentences. Once
the sentence was processed, the surface structure was dropped and all further work was
done with the conceptual dependen cy notation. This method has certain beneficial effects

S - ~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~ •~~~~~~~~~~flfl S -— - ~-~-—---‘r— - 
~~~~~~~~~~~~~~ —
5
- - ---- - -
64 Natural Language
on the control structure: All interprocess communication can be done through conceptual
dependency, without the need to resort to the surface level, although the more subtle 
- --
information in the surface structure may be lost. Since the intermediate representation is 
-
“ language-free ,” it should facilitate translation of the original sentence Into anothe r I
language , as Weaver indicated in his original discuss ion of Machinese (see Article B). As
mentioned above, the existence of a unique representation for any fact should also facilitate
tasks like paraphrasing and question answering.
ii
S References
Conceptual dependency theory and all three parts of the MARGIE system are described
in detail in Schank (1975). Since the version descrIbed In this article , the theory has
evolved considerably, and several new systems have been built using the CD formalisms , all
described very well in Schank & Abelson (1977 ). Other references for MARGIE include S
Schank (1973) and Schank et al (1973).
____________ - ±11 -J

5 -~~~~~~~
FO 8AM and PAM 66
F6. SAM and PAM
Story Understanding
SAM (Script Applier Mechanism) and PAM (Plan Applier Mechanism) are computer
programs developed by Roger Schank, Robert Abelson and their students at Yale to
demonstrate the use of scripts and plans in understanding simple stories (Schank et al.,
1975; Schank & Abelson, 1977). Most work in natural language understanding prior to 1973
involved parsing individual sentences In isolation; It was thought that text composed of
paragraphs could be understood simply as collections of sentences. But just as words are
not formed from the unconstrained juxtaposition of morphemes, and sentences are not
unconstrained collections of words, so paragraphs and stories are not without structure. The
structures of stories have been analyzed (Propp, 1988; Rume lhart, 1976; Thorndyke , 1977),
and it Is clear that the context provided by these structures facilitates sentence
comprehension, just as the context provided by sentence structure facilitates word
comprehension (see the Overview; also, the Sp.sch. A article discusses lop-down processing in
speech understanding research). For example, If we have been told in a story that John Is
very poor, we can expect later sentences to deal with the consequences of John’s poverty ,
or steps he takes to alleviate It.
Different researchers have very different ideas about what constitutes the structure
of a story. Some story grammars are rather “syntactic”; that is, they describe a story as a
collection of parts like setting, characters, goal Introduction, and plans, deter mined by their
sequential position In the story rather than by their meaning. The work of Schank and
Abelson reported here has a more semantic orientation. They propose an underlying
representation of each phrase in a story which is based on a set of semantic primitives. This
representation , called conceptual dependency, is the theoretical basis for more complex story
structures such as sclipts, plans, goals, and themes . The SAM and PAM programs understand
• stories using these higher level structures. (Article F5 describes the early work on
conceptual dependency theory, and Articl es Repr.aent ation.C5 and Rspreeantatiori.C6
discuss related representation schemes.)
Parsing : A Brief Introduction to Conceptual O.p.ndency
Prior to his work with stories , Schank (1973) developed conceptual dependency fCD)
for representing the meaning of phrases or sentences. The “basic axiom” of conceptual
dependency theory is:
For any two sentences that are identical In meaning, regardless of
language , there should be only one representation of that meaning In
CD. (See Schank & Abelson , 1977, p. 11.)
Schank thus allies himself with the early machine translation concept of tnteilingua, or
intermediate language (see Articles B and Overview), and has In fact done some mechanical
translation research In conjunction with the story understanding project. A second important
idea is:
Any information In a sentence that Is Implicit must be mad. explicit in
S -S- S

~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ --S-S
68 - Natural Language
the representation of the meaning of that sentence. (Schank &
Abelson , 1977. p. 11)
This idea Is the basis for much of the sophisticated inferential ability of SAM and PAM: We
s~ail see a sense In whIch the fact that “John ate food” is Impiicit in the sentence “John
went to a restaurant ,” and how the former sentence can be Inferred at the time that the
program reads In the latter.
A third important idea Is that conceptual dependency representations are made up of a
very small number of semantic primitives , which inciude primitive acts and primitive states 
S(with associated attribute values). Examples of primitive acts are:
ACTS:
PTRANS The transfer of the physical location of an
object. For one to ‘go’ Is to PTRANS oneself.
“Putting’ an object soaewhere Is to PTRANS It
to that place.
PROPEL The application of physical force to an object.
ATRANS The transfer of an abstract relationship. To‘give’ Is to ATRANS the relationship of possession
or ownership.
$TRANS The transfer of nental information between people
or within a person. ‘Telling’ Is an HTRANS between
people; ‘seeing’ Is an NTRANS within a person.
MBUILD The conitruction of new Information from old.
“Imagining,’ ‘inferring, and ‘decidin g ’ are MBUILOs .
in the most recent version of CO theory (1977). Schank and Abelson Included 11 of these
primitive acts.
Examples of primitive states include:
STATES:
Mary HEALTH(-1$) Mary is dead.
John MENTAL STATE( +1S) John Is ecstatic.
Vase PHYSICAL STATE(- 1B) The vase is broken .
F The number of primitive states In conceptual dependency theory Is much larger than the
number of primitive actions . States and actions can be combined; for example , th. sentence
John told Mary that Bill was happy
can be represented as
John MTRANS (Bill BE MENTAL-STATE(5)) to Mary.
An important class of sente nces involves causal chains, and Sohank and Abelson have
worked out some rules about causality that apply to conceptual dependency theory. Five
important rules are:
S~~
---~~~~~~~~
-•~~~
-S~~~~~
S --~~~ 5S~~~~~~~~~~~~~~~~~~~ ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ ~~

Pr -Sj - S ~~~~~~~S~~~~~~~~~~~~~~~~~555~~~~~~~S 5 5~~~~5~~~~ __________
P8 SAM and PAM 87
1. Actions can result in state changes.
2. States can enable actions.
3. States can disable actions .
4. States (or acts) can Initiate mental events.
5. Mental events can be reasons for actions.
These are fundamental pieces of knowiedge about the world, and conceptual dependency
theory includes a shorthand representation of each (and combinations of some) called causal
links.
Conceptual dependency representation Is, in fact, the interlIngua that is produced
when SAM or PAM parses sentences. The parser which is used by these programs Is an
extension of the one developed by Chris Riesbeck (1976) for the MARGIE system (Article
Fe). As this program encounters words, it translates them into conceptual dependency
representation; but, in addition, it makes predictions about what words and linguistic
structuros (verbs, prepositions, etc.) can be expected to occur and what conceptual
dependency structures should be built in that eventuality.
Conceptual dependency is the underlying representation of the meaning of sentences
upon whIch SAM and PAM operate. We turn now to higher level knowledge structures:
scripts, plans, goals, and themes. Schank and Abelson make a distinction between scripts
and plans that must be clear before the differences between SAM and PAM become
apparent.
Scripts
• A script Is a standardized sequence of events that describes some stereotypical
human activity, such as going to a restaurant. Schank and Abelson ’s assumption is that
people know many such scripts and use them to establish the context of events. A script is
functionally similar to a frame (Minsky, 1975) or a schema (Bartlett , 1932; Rume lhart, 1975),
In the sense that It can be used to anticIpate the events It represents. For example , the
RESTAURANT script (see FIgure 1) involves going to a restaurant , being seated , consulting
the menu, and so on. People who are presented with an abbreviated description of this
activity, e.g., the sentence “John went out to dinner,’ Infer from their own knowledge about
restaurants that John ordered , ate, and paid for food. Moreover , they anticipate from a
sentence which fills part of the script (“John was given a menu’) what sort of sentences are
likely to follow, e.g., “John ordered the lamb.” Scripts attempt to capture the kind of
knowledge that people use to make these Inferences. (Article Rspr.ma ntetion.CB discusses
scripts , frames and related representation schemes.)
__________ ~~~~~~~S-

— ~~~~____S_~~~w_S -S S 
-
•8 Natural Language
Players: customer , server , cashier
Props: restaurant , table, menu, food, check, payment , tip
Events :
1. customer goes to restaurant
2. customer goes to table3. server brings menu
4. custome r orders food
5. server brings food
6. custome r eats food
7. server brings check8. customer leaves ti p for server
9. customer giv es payment to cashier
1$. custom er leaves resta urant
Header: event 1
Main concept: event 6
Figure 1. Restaurant Script
Two components of scripts are of special Importance. We will discuss later how the script
header is used by SAM to match scripts to parsed sentences. The second Important
component is the main concept or goal of the scri pt. In the restaurant script the goal is to eat
food.
The scripts used In SAM grew out of Abelson ’s (1973) notIon of scri pts as networks of —
causal connections. However , they do not depend on explicit causal connections between
their events , in hearing or observing events that fit a standard script, one need not analyze
the sequence of events In terms of causes , since they can be expected just from knowing
that the script applies. The Identificat ion of events as filling their slots in the script gives us
the intuition of “understanding what happened. ’
ScrIpts describe everyday events , but frequentl y these events (or our relating ~them ) do not run to completion. For example:
I went to the restaurant. I had a hamburger.
Then I bought some groceries.
This story presents several problems for a system like SAM that matches scripts to input
sentences. One proble m Is that the restaurant scri pt Is ‘left danglIng’ by the Introduction of
the last sente nce. it Is not clear to the system whethe r the restaurant scri pt - (a) has
terminated , and a new (grocery shopping) script has started; (b) has been distracted by a
“fleet ing ” (one-se ntence ) grocery script; or (c) is Interactin g with a new grocery script
L ____________

Fe SAM and PAM 69
(e.g., buying groceries in the restau rant). Another thing that can happen to everyday scripts
is that they can be thwarted , as In:
I went to the gas station to flU up my car.
But the owner said he was out of gas.
This is called an “obstacle ”.
Scripts descrIbe rather specific events, and although It Is assumed that adults know
thousands of them, story comprehension cannot be simply a matter of finding a script to
match a story. There are just too many possible stories. Moreover , there are clear cases
where people comprehend a story even though it does not give enough information to cause
a program to Invoke a script, as in
John needed money. He got a gun and went to a liquor store.
Schank and Abelson point out that even if the program had a script for Robbery , this story
offers no basis for invoking It. Nonetheless, people understand John’s goals and his intended
actions.
There must be relevant knowledge available to tie together sentences
that otherwise have no obvious connection.... The problem Is that
there are a great many stories where the connection cannot be made
by the techniques of causal chaining nor by reference to a script. Yet
they are obviously connectable. Their connectability comes from these
sto ries ’ implicit reference to plans. (Schank & abelson, 1917, p. 15)
Plans
Schank and Abelson introduce plans as the means by which goals are accomplished , end
they say that understanding plan-based stories involves discerning the goals of the actor and
the methods by which the actor chooses to fulfill those goals. The distInction between
script-based and plan-based stories Is very simple: In a script-based story, parts or all of
the story correspond to one or more scripts available to the story understander; in a plan-
based story, the understander must discern the goals of the main actor and the actions that
accomplish those goals. An understander might process the same story by matching It with a
script or scripts , or by figuring out the plans that are represented in the story. The
difference Is that the first method is very specialized , because a script refers to a specific
sequence of actions , while plans can be very general because the goals they accomplish are
general. For example , in
John wanted to go to a movie, lie walked to the bus-stop .
we understand that John’s immediate goal (called a delta- goal because it brings about a
change necessary for accomplishment of the ultImate goal) is to get to the movie theater.
Going somewhere is a very genera l goal hnd does not apply just to going to the movies. in
Schank and Abelson ’s theory, this goal has associated with It a set of planboxes , which are
standard ways of accomplishing the goal. Pianboxes for going somewhere include riding an
animal, taking public transportation , driving a car, etc.
L. ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -- . - --~~~~~~--—-- -5~~~~~~~~~~~~S S

70 Natural Language
Obviously, a story understander might have a “go to the movies ” script In Its repertoire ,
so that analysis of John’s goals would be unnecessary--the system would just “recognize ’
the situation and retrieve the script. This script would be the standardized intersection of a
number of more or less general goals and their associated pianboxes. It would be a
“routinized plan’ made up of a set of general subplans: Go to somewhere (the theater),
Purchase something (a ticket), Purchase something (some popcorn), etc.
A routlnized plan can become a script, at least from the planner ’s
personal point of view.
Thus, plans are where scripts come from. They compete to. the same
role in the understanding process , namely as explanations of
sequences of actions that are intended to achieve a goal. (Schank &
Abelson , 1977, p. 72)
The process of understanding plan-based stories involves determining the actor’s goal,
establishing the subgoals (delta- or 0-goals) that will lead to the main goal, and matching the
actor’s actions with planboxes associated with the 0-goals. For example , in
- John was very thirsty. He hunted for a glass.
we recognize the 0-goal of PTRANS 1ng liquid, and the lower level goal (specified in the
planbox for PTRANS Ing liquid) of finding a container to do It with.
Goals and Themes
In story comprehension , goals and subgoals may arise from a number of sources. For
example, they may be stated explicitly, as in
John wanted to eat;
they may be nested in a planbox; or they may arise from Memes . For example , If a LOVE
theme holds between John and Mary, it is reasonable to expect the implicit, mutual goal of
protecting each other from harm: “Themes , in other words, contain the background
information upon which we base our predIctions that an Individual will have a certain goal”
(Schank & Abelson, 1977, p. 132).
Themes are rather lIke production systems in their situation-action nature. A theme
specifies a set of actors, the situations they may be in, and the actions that will resolve the
situation in a way consistent with the theme. The goals of a theme are to accomplish these
actions. Schank and Abelson have proposed seven types of goals; we have already
considered 0-goals. Other examples are:
A- or Achievement- goals. To desire wealth is to have an
A-Mone y goal.
P- or Preservation-goal. To protect somsons may be a P-Health
or P-Menta l State goal.
C- or Crisis- goal. A special case of P-goals , when action
Is immediatel y necessary .
_________ __________ ___ 
-- - -~~~~ ~~

—~
--
SAM and PAM 71
The LOVE theme can be stated In terms of some of these goals:
X is the lover; Y Is the loved one; Z Is another person.
SITUATION ACTION
Z causs Y harm A-Health(Y) and possibly
caus e Z harm
or C-Health(Y)
not-Lovs(Y .X) A-Love(Y ,X)
Genera l goa ls: A-Rssp .ct(Y)
A-Narry( Y)
A-Approva l(Y)
To summarize the knowledge-structures we have discussed , we note their
InterrelationshIps:
Themes give rise to goals.
A plan Is understood when Its goals are identified and its actions are consistent
with the accomplishment of those goals.
Scripts are standardized models of events.
Scripts are specific; plans are general.
Plans originate from scripts .
Plans are ways of representing a person ’s goals. These goals are Implicit in
scripts, which represent only the actions.
A script has a header , which Is pattern-matched to an input sentence. Plans do
not have headers , but each plan is subsumed under a goal.
SAM
Both SAM and PAM accept stories as Input; both use an English-to-CO parser to
produce an Internal representation of the story (in conceptual dependency). Both are able
to paraphrase the story and to make intellIgent inferences from it. They differ with respect
to the processing that goes on after the CD representat ion has been built.
SAM understands stories by fitting them into one or more scripts. After this match is
completed , It makes summaries of the stories. The process of fitting a story Into a script has
three parts, a PARSER , a memory module (MEMTOK), and the script applier (APPLY). These
modules cooperate: The parser generates a CD representation of each sentence, but APPLY
gives it a set of Verb-senses to use once a script has been identified. For example , once
the restaurant script has been established , APPLY tells the parser that the appropriate
sense of the verb ‘to serve ” Is ‘to serve food” rather than, fo r example, ‘to serve in the
army.”
- ____________ S__~~~~~
S~~~ 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ SSS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ __
_
.S __~__S•SS ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ —~~

72 Natural Languag e
The parser does not make many inferences; thus it does not realize that “it” refers to
the hot dog In “The hot dog was burned. it tasted awful. ” This task is left to MEMTO$(. This
module takes references to people , places , things, etc., and fills in information about them. It
recognizes that the “it” In the sentence above refers to the hot dog, and “instantiates ” the
“it” node In the CD representation of the second sentence with the “hot dog” node from the
first sentence. Similarly, In a story about John, MEMTOK would replace “he” with “John”where appropriate , and would continually update the “John” node as more information became
available about him.
The APPLY module has three functions. First, it takes a sentence from the parser and
checks whether it matches the current script, a concurrent (Interacting) script, or any script
In the database. if this matching is successful , it makes a set of predictions about likely
inputs to follow, Its third task Is to Instantiate any steps In the current script that were
“skipped over” in the story. For example , if the first sentence of a story is “John went to a
restaurant ,” APPLY finds a match with the script header of the restaurant script In Its
database (see Figure 1). APPLY then sets up predictions for seeing the other events In the
restau rant script In the input. if the next sentence Is “John had a hamburger ,” then APPLY
successfully matches this sentence Into the restaurant script (event 6). It then assumes
events 2-5 happened , and Instantiates structures in its CO representation of the story to
this effect. Events 7-10 remaIn as predictions.
When the whole story has been mapped into a CD representation in this manner , the
SAM program can produce a summary of the story, or answer questions about it. (See
Schank & Abelson , 1977, pp. 190-204 , for en annotated sample protocol with the program.)
Consistent with the Idea of interlingua, SAM can produce summaries In English , Chinese ,
Russian , Dutch, and Spanish. An example of a SAM paraphrase follows; note the powerful
inferences made by Instant iating intermed iate script steps:
ORIGINAL: John went to a restaurant. He sat down. He got mad.
He left.
PARAPHRASE: JOHN WAS HUNGRY. HE DECIDED TO GO TO A RESTAURANT.
HE WENT TO ONE. HE SAT DOWN IN A CHAIR. A WAITER
DID NOT GO TO THE TABLE. JOHN BECAME UPSET. HE
DECIDED HE WAS GOING TO LEAVE THE RESTAURANT . HE
LEFT IT.
SAM inferred that John left the restaurant because he did not get any service. The basis for
this Inference Is that In the restaurant script, event 3 represents the waiter coming over to
the table after the main actor has been seated. SAM knows that people can get mad If their
expectations are not fulfilled , and infers that John’s anger results from the nonoccurrence of
event 3.
PAM
Wllensky’s (1978) PAM system understands stories by determining the goals that are
to be achieved In the story and attempting to match the actions of the story with the
methods that it knows will achieve the goals. More formally:
The process of understa nding plan-based stories is as
follows:
L -~~~~~~~~~~~~~~~ ~~

~ —~~ -- --!- - -- - —
F8 SAM and PAM 73
a) Determine the goal.
b) Determine the 0-goals that will satisfy that goal,
c) Analyze input conceptuailzatlons for their potential realization of one of the
planbo xes that are called by one of the determined 0-goats. (Schank &
Abelson , 1977, p. 75)
PAM utilizes two kinds of knowledge structure In understanding goals: named plans and themes.
A named plan is a set of actions and subgoals for accomplishing a main goal. it is not very
different from a script, although the emphasis In named plans Is on goals end the means to
accomplish them. For example , a script for rescuing a person from a dragon would involve
riding to the dragon’s lair and slaying it--a sequence of actions--but a named plan would be
a list of subgoals (find some way of getting to the lair, find some way of killing the dragon ,
etc.) and their associated pianboxes. When PAM encounters a goal In a story for which it
has a named plan, It can make predictions about the D-goals and the actions that will follow .it will look for these 0-goals and actions In subsequent Inputs. Finding them is equivalent to
understanding the story.
Themes provide another source of goals for PAM. Consider the sentences:
a) John wanted to rescue Mary from the dragon.
b) John loves Mary. Mary was stolen away by a dragon.
In both of these cases, PAM will expect John to take actions that are consistent with the
goal of rescuing Mary from the dragon, even though this goal was not explicitly mentioned In
(b). The source of this goal in (b) is the LOVE theme mentioned above, because In this
theme , If another actor tries to cause harm to a loved one, the main acto# sets up the goal of
Achieving-Health of the loved one and possibly harming the evil party. (It is assumed that
the dragon stole Mary In order to hurt her.)
PAM determines the goals of an actor by (a) their explicit mention In the text of the
story, (b) establishing them as D-goals for some known goal, or (c) inferring them from a
theme mentioned in the story. To understand a story is to “keep track of the goals of each
of the characters in a story and to Interpret their actions as means of achieving those goals”(Schank & Abelson, 1977, p. 217). The program begins with written English text, converts it
into CD representation , and then interprets each sentence in terms of goals (predicting D-
goals and actions to accomplish them) or actions themselves (marking the 0-goals as
p “accomplished”). Woen this process is completed , PAM can summarize the story and answer
question s about the goals and actions of the characters.
Summary
Scripts , plans, goals, and themes are knowledge structures built upon conceptual
dependency theory. SAM is a program for understanding script-based stories, It matches
the input sentences of a story to events In one or more of the scripts In its database. As
such, It processes Input based on expectat ions it has built up from the scripts. PAM
understands plan-based stories by determining the goals of the characters of the story and
by interpreting subsequent actions in terms of those goals or subgoals that will achieve~
--~
-— -~
- .... --‘---- -- -. -——--~-- -

F- --. - - - - - -;~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~--~~ — - ~— ---- 
- -~-—~~-.
74 Natural Language
them. A great deal of Inference can be required of PAM simply to establish the goals and
subgoals of the story from the Input text.
Schank and Abelson argue that human story understanding is a mixture of applying
known scripts and Inferring goals (where no script Is available or of obvious applicability).
They are experimenting with interactions of SAM and PAM, in particular , with using SAM to
handle script-based sub-stories under the control of PAM.
References
The recent book by Schank & Abelson (1977) is the most complete and readable
source on both of these systems and on the current state of Conceptu nl Dependency theory.
For the whole truth about PAM, see the doctoral dissertation by WiIensky (1978a).
Also of interest: Abelson (1973), Bartlett (1932), MInsky (1975), Propp (1968),
Riesbeck (1975), Rumeihart (1975), Schank (1973), Schank St al. (1975), Thorndyke
(1977), and Wilensky (1978b).  
-~~~~~~~~~~--- ---~~~~~~~ ~~

~
--- - -
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -
Fl LIFER 76
F?. LIFER
The natural language systems described in the preceding articles fall into two
categories: those built to study natural language processing Issues in general and those built
with a particular task domain In mind. in contrast, LIFER, built by Gary Hendrix (1977a) as
part of the internal research and development program of SRI International , is designed to be
an “oil-the-shelf” natural language utilIty avaita bie to system builders who want to
Incorporate an NI front-end Intertace to improve the usability of their various applications
systems. The bare LIFER system is a system for generating natural language interface .; the
Interface builder can augment LIFER to fit his particular application , and even the eventual
users can tailor the LiFER-supported front-end to meet their individual styles and needs.
Language Specification and Parsing
The LIFER system has two major components: a set of Interactive functions for
specifying a language , and a parser. InitIally it contains neither a grammar nor the semantics
of any language domain. An Interface builder uses the language specification functions to
define an application language , a subset of English that is appropriate for interacting with his
application system. The LIFER system then uses this language specificat ion to ilUe’ p ri
natural language Inputs as commands for the applIcation system.
The interface builder specifies the language primarily In terms of grammatical rewrite
rules (see Article Cl). LIFER automatically translates these into transition trees, a simplified
form of augmented transit ion networks (Article 02). Using the transition tree, the parser
Interprets inputs In the application language. The result Is an interpretation in terms of the
appropriate routines from the applications system , as specified by the interface builder. The
parser attempts to parse an Input string top-down and left to right (see Article Dl) by
nondeterm lnistically tracing down the transItion tree whos e root node is the start symbol
(known as (L.T.G.> for “LiFER top grammar ”). For example , suppose the interface builder has
specified the following three production rules as part of his application language:
(L.T.G.) -) WHAT IS THE (ATTRIBUTE) OF <PERSON) .1
<L.T.G.) -) WHAT iS <PERSON > (ATTRIBUTE> e2
(L.T.G.> -) HOW (ATTRIBUTE) IS <PERSON> I e3
If an Input matches one of these patterns , the corresponding expression (a 1, e2, or e3) Is
evaluated--these are the appropriate Inter pret 411055 that the system Is to make for the
corresponding input. The transition tree built by the language specification functions would
look like this:
/.—THE-<ATTRIBUTE > OF (PERSON> I .1
/—WHAT ISI \—<PERSON > <ATTRIBUTE) .2
<L.T.G.)
\ HOW <ATTRIBUTE) IS <PERSON> ) .3
Sentences such as:
What Is the age of Mary’s sister?
- ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ .~~~ 

_
76 Natural Language
How old Is Mary’s sister?
What is John’s height?
How tail I. John?
might be parsed using this simple transition tree, depending on how the nonterm inal symbols
or mets-symbols , <ATTRIBUTE) and <PERSON ), are defined. (The interface builder can supply
a preprocessing function which Is applied to the input string before LIFER attempts to parse
it. Typically the preprocessor strips trailing apostrophes and s’s so that LIFER sees “John’s”
as “John”.)
During parsing, LIFER starts at the symbol (L.T.G.) and attempts to move toward the
expressions to be evaluated at the right. The parser follows a branch only if some portion at
the left of the remaining input string can be matched to the first symbol on the branch.
Actual words (such as what or of in the above example) can be matched only by themselves.
Mets-symbols (such as <ATTRIBUTE> or <PERSON>) can be matched in a number of ways,
depending on how the interface builder has defined them:
(a) as a simple set (for example , <PERSON> • the set (Mary, John, Bill));
(b) as a predicate that Is applied to the string to test for satisfaction (for
example , some mets-symbol used In a piece of grammar to recognize dates
might test whether the next string of characters is a string of digits, and
• thus a number); or
• (c) by another transition tree which has this mets-symbol as its root node.
The above example is typical: A large amount of semantic Information is embedded in
the syntactic description of the application language. JOHN and HEIGHT are not defined as
instances of the single mets-symbol <NOUN) as they would be In a more formal grammar , but
rather are separated Into the semantic categories indicated by the mets-symbols <PERSON>
and <ATTRIBUTE>. The technique of embedding such semantic information In the syntax has
been referred to as semantic grammar (Burton , 1976), and It greatly Increases the
performance of LIFER’ s automatic spelling correction , ellipsis, and paraphrase facilities ,
described below.
Applications
LIFER has been used to build a number of natural language Interfaces , including a
medical database , a task scheduling and resource allocation system , and a computer-based
expert system. The most complex system built with a LIFER interface Involved a few man-
months of development of the natural language front-end: The LADDER system (Language
Access to Distributed Data with Error Recovery) developed at SRI, which provides real-time
natural language access to a very large database spread over many smaller databases in
computers scattered throughout the United States (Sacerdoti , 1977; Hendr lx at al, 1978).
Users of the system need have no knowledge of how the data is organized nor where It is
stored. More Importantly, from the point of view of this article, users do not need to know a
data query language: They use English, or rather a subset that Is “natural” for the domdin of
discourse and which is usually understood by the LIFER front-end. The interpretations of the
inputs by LIFER are translations Into a general database query language , which the rest of
— —~
-- 

• -~~~~~~~~~~~~~~~~~~~~~~ •-~~~ ~~~~~-— ~~~~~~~~~•——~~ ~~~---— --~~~~~~~~~~~~~~~~~~--,-~~~~~~~~~~ 
F7 LIFER 77
the LADDER system converts to a query of the appropriate databases on the appropriate
computers (see Article Applice tions.F4 on Al in information retrieval systems).
Another Interesting system to use a LIFER front-end was the HAWKEYE system (Barrow
at al. 1977), also developed at SRI. This Is an Integrated Interactive system for cartography
or intelligence , which combines aerial photographs and generic descriptions of objects and
situations with the topographical and cultural information found in traditional maps. The user
queries the database and invokes image-processing tasks via a LIFER natural language
Interface. A unique feature of this interface is the combination of natural language and
nontextual forms of input. For instance , using a cursor to point to places within an image, the
user can ask questions such as “What Is this?” and “What is the distance between here and
here?” The interpretation of such expressions results In requests for coordinates from the
subsystem providing graphical input, which are then handed to subsystems that have access
to the coordinates-to-object correspondences.
Human Engineering
LIFER Is Intended as a system which both facilitates an Interface builder in describing
an appropriate subset of a language and its interpretation in his system , and also helps a
non-expert user to communicate with the application system In whatever language has been
defined. For this reason , close attention was paid to the human engineering aspects of
LIFER. Experience with the system has shown that, for some applications , users previously
unfamiliar with LIFER have been able to create usable natural language interfaces to their
systems In a few days. The resulting systems have been directly usable by people whose
field of expertise Is not computer science.
The Interface builder. Unlike PROGRAMMAR (in SHRDLU , Article F4), there is no
“compilation ” phase during which the language specification Is converted into a program.
Instead , changes are made incrementally every time a call to the language specification
functions is made. Furthermore , it is easy (by typing a prefix character) to intermix
statements to be interpreted by the specification functions , statements to be parsed using
the partially specified grammar , and statements to be evaluated in the underlying
implementatIon language of LIFER, namely INTERLISP (see Article Al Lengi.mgea .CO. Thus, the
interface builder can define a new rewrite rule for the grammar or write a predicate for some
meta-symbol and test It Immediately, which leads to a highly Interactive style of language
definItion and debugging. A grammar editor allows mistakes to be undone. The abiIt~y~to
intermix language definition with parsing allows the interface user to extend the interface
language to personal needs or taste during a session using the application system. This
extension can be done either by directly Invoking the language specifIcation functions , or, if
the Interface builder has provided the facility, by typing natural language sentences whose
Interpretations Invoke the same language specIfication functions.
The interface user. LIFER provides many features to ease the task of the user typing
In sentences to be understood by the system. First of all, it provides feedback indicating
when LIFER is parsing the Input sentence and when the applications software Is running.
When LIFER fails to parse a sentence , it tries to give the user useful Information on how it
failed. It tells the user how much of the Input was understood and what it was expecting
when it got to the point where it could no longer understand. Interactions with the user are
numbered , and the user can refer back to a previous question and specify some substitution
to be made. For instance: 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ -

___ 
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
18 Natural Langua ge
12. How many minority students took 21 or more units of credit last
quarter?
PARSED!
87
13. Use women for minority in 12
PARSED I
156
Notice the “PARSED!” printed by LIFER to indicate parsing success. This facility can be used
to save typing (and more errors), both when similar questions are being asked and when
errors In previous inputs are being corrected. The user can simply specify synonyms to be
used. For instance:
28. DefIne Bill like William
will cause LIFER to treat the word BILL’ the same as WILLIAM. LIFER also allows for easy
inspection of the language definition , which is useful for both interface builders and
sophIstIcated users.
There are three more sophisticated aspects of LIFER designed to make interactions
easier for the user--the spelling correction , ellipsis , and paraphrase mechanisms. Spelling
correction is attempted when LIFER fails to parse an input. When the parser Is following
along a branch of a transition tree and reaches a point where It can go no further , It recordsit. failure in a failure list. If the input is eventually parsed correctly, the failure list Is
forgotten. However , if no successful parse can be found, the parser goes back to the test
(rightmost) fall point and attempts to see If a misspelling has occurred. (Fail points to the
left In the sentence are at first assumed not to be caused by spelling errors, since at least
one transition using the word must have been successful to get to the fall point further to
the right. This Is not toolproof, however, and sometimes LIFER will fall on a spelling mistake).
F The 1NTERLISP spelling correction facility is used to find candidate words that closely match
the spelling of the suspect word. The use of semantically significant syntactic categories
(such as <PERSON)) greatly restricts the allowable word susbstitutlons and improves the
efficiency of the spelling corrector:
While Interacting wIth an applications system , the user may want to carry out many
similar tasks (for example , in a database query system , one often asks several questions
about the same object). The LIFER system automatically allows the user to type Incomp letØ
input fragments and attempts to Interpret them in the context of the previous Input (i.e., the
interface builder need not consider this Issue). For instance , the following three questions
mIght be entered success i vely and unders tood by LIFER:
42. What is the height of John
• 43. the weight
44. age of Mary’s sister
If an input faIls normal parsing and spelling correction , LIFER tries elliptIc processing. Again,
because languages defined in LIFER tend to encode semantic information in the syntax
• definition , similar syntactic structures tend to have similar semant Ics. Therefore LIFER
accepts any Input str ing that is syntactically analogous to any contiguous substrtng of words

PT LIFER 79
• In the last input that parsed without •INpsls. Tb. analogies do not have to be In terms of
complete subtrees of the syntactic tree, but they do have to correspond to contiguous
words In the previous input. The elliptical processing allows for quite natural and powerful
Interact ions to take place, without any effort from the interface builder.
The paraphrase facility allows users to define new syntactic structures In terms of old
structures. The user gives an example of the structure and Interpretation desired , and the
system builds the most general new syntactic rule allowed by the syntactic rules already
known. The similarit y between the semantics and synta x Is usually sufficient to ensure that a
usable syntax rule is generated. The following example assumes that the interface builder
has included a rule to Interpret the construction shown to Invoke a call to the language
specIfication function PARAPHRASE with appropriately bound arguments. After typing
63. Let “Describe John” be a paraphras, of “Print th. height, weight
and age of John’
the user could expect the system to understand the requests
64. Describe Mary
65. DescrIbe the tallest person
66. Describe Mary’s sister
even with a fairly sImply designed LIFER grammar. (In the context of the earlier examples,
this example assumes that “the tallest person ” can correspond to the meta-symbol
<PERSON>.) The method used to carry out paraphrase (whIch, as can be seen, is a much more
general form of synonym lc reference) is quite complex. Basically it invokes the parser to
parse the model (the second form of 63) that is already understood. All proper subphrases
(i.e., subphrasee that are complete expans ions of a syntactic category) of the model that
also appear in the paraphrase are assumed to play the same role. A new syntactic rule can
then be written, and the actions invoked by the model can be appropriately attached to the
paraphrase rule.
Conclusions
Although grammars constructed wIth LIFER may not be as powerful as speciall y
constructed grammars , LIFER demonstrates that useful natural language systems for a wide
variety of domains can be built simply and routinel y wIthout a large-scale programming effort.
Human engineering features and the ability of the naive user to extend the system ’s
capabilities are Important issues in the usefulness of th. system.
References
Ilendrlx (19?Ta), Hendrix (1977b), and Hendrix (19770) all describe the LIFER system.
The LADDER information retrieval application 1* descr ibed In Hendrix et al. (1978) and
Sac•rdotl (1977). Barrow •t ii. (1977) descrlb.s the HAWKEYE system.
— ---~

