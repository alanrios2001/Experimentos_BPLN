Chapter /two.pnum
Text Preprocessing
/two.pnum./zero.pnum./one.pnum Text Cleaning
Cleaning the text involves various steps like removing digi ts, punctuation, converting to lowercase,
and removing extra spaces. Below is an example of text cleani ng in Python:
/one.pnumimport re
/two.pnum
/three.pnumdef preprocess_text(text):
/four.pnumtext = text.lower() # Convert text to lowercase
/five.pnumtext = re.sub( r’\d+’,’’, text) # Remove digits
6text = re.sub( r’[^\w\s]’ ,’’, text) # Remove punctuation
/seven.pnumtext = re.sub( r’\s+’,’ ’, text).strip() # Remove extra spaces
8return text
/nine.pnum
/one.pnum/zero.pnumsample_text = "Hello, World! This is an example, 2023."
/one.pnum/one.pnumprint(preprocess_text(sample_text))
/one.pnum/two.pnum
/one.pnum/three.pnum# Output:
/one.pnum/four.pnum# hello world this is an example
/two.pnum./zero.pnum./two.pnum Tokenization
Tokenization is the process of splitting a text into individ ual words or phrases, known as tokens.
/one.pnumtokenized_text = "Marie Curie was a physicist" .split()
/two.pnumprint(tokenized_text)
/three.pnum
/four.pnumoutput:
/five.pnum[’Marie’,’Curie’,’was’,’a’,’physicist’ ]
/two.pnum./zero.pnum./three.pnum Stopword Removal
Stopwords are common words (e.g., "and", "the") that usuall y do not add signiﬁcant meaning to the
text. We can remove them to focus on meaningful words.
/two.pnum/two.pnum

CHAPTER /two.pnum. TEXT PREPROCESSING /two.pnum/three.pnum
/one.pnumfrom nltk.corpus import stopwords
/two.pnum
/three.pnumtext ="The cat is sitting on the mat"
/four.pnumwords = text.split()
/five.pnumstop_words = set(stopwords.words( ’english’ ))
6filtered_sentence = [w for w in words if not w in stop_words]
/seven.pnumprint(filtered_sentence)
8
/nine.pnumoutput:
/one.pnum/zero.pnum[’cat’,’sitting’ ,’mat’]
/two.pnum./zero.pnum./four.pnum Lemmatization and Stemming
Lemmatizationreduceswordstotheirbaseformorlemma,whe reasstemmingreduceswordstotheir
root form, which may not always be a valid word.
Lemmatization Example:
/one.pnumfrom nltk.stem import WordNetLemmatizer
/two.pnum
/three.pnumlemmatizer = WordNetLemmatizer()
/four.pnumwords = [ "running" ,"ran","better" ,"cats"]
/five.pnumlemmatized_words = [lemmatizer.lemmatize(word, pos= ’v’) for word in words]
6print(lemmatized_words)
/seven.pnum
8output:
/nine.pnum[’run’,’run’,’better’ ,’cats’]
Stemming Example:
/one.pnumfrom nltk.stem import PorterStemmer
/two.pnum
/three.pnumstemmer = PorterStemmer()
/four.pnumwords = [ "running" ,"ran","better" ,"cats"]
/five.pnumstemmed_words = [stemmer.stem(word) for word in words]
6print(stemmed_words)
/seven.pnum
8output:
/nine.pnum[’run’,’ran’,’better’ ,’cat’]
/two.pnum./zero.pnum./five.pnum N-grams
An n-gram is a sequence of nconsecutive words. For example, a bigram consists of two consecutive
words, while a trigram consists of three.
Bigram Example:
/one.pnumfrom nltk import ngrams
/two.pnum
/three.pnumsentence = "Marie Curie was a physicist"
/four.pnumn = 2# Bigram

CHAPTER /two.pnum. TEXT PREPROCESSING /two.pnum/four.pnum
/five.pnumbigrams = list(ngrams(sentence.split(), n))
6print(bigrams)
/seven.pnum
8output:
/nine.pnum[(’Marie’,’Curie’), (’Curie’,’was’), (’was’,’a’), (’a’,’physicist’ )]
/two.pnum./zero.pnum.6 TF-IDF (Term Frequency-Inverse Document Frequency)
TF-IDF[ /four.pnum8]evaluateshowimportantawordisinadocumentrelativetoa corpus. Itiscommonlyused
for feature extraction.
/one.pnumfrom sklearn.feature_extraction.text import TfidfVecto rizer
/two.pnum
/three.pnumdocuments = [
/four.pnum"The cat sat on the mat" ,
/five.pnum"The dog barked at the cat" ,
6"The cat and the dog played together"
/seven.pnum]
8
/nine.pnumvectorizer = TfidfVectorizer()
/one.pnum/zero.pnumtfidf_matrix = vectorizer.fit_transform(documents)
/one.pnum/one.pnum
/one.pnum/two.pnum# Convert to dense matrix and print the TF-IDF matrix
/one.pnum/three.pnumprint(tfidf_matrix.toarray())
/one.pnum/four.pnum
/one.pnum/five.pnumoutput:
/one.pnum6[[0.607, 0.000, 0.485, 0.485, 0.000, 0.000, 0.607],
/one.pnum/seven.pnum[0.455, 0.692, 0.363, 0.363, 0.000, 0.455, 0.000],
/one.pnum8[0.378, 0.000, 0.302, 0.302, 0.504, 0.378, 0.504]]
/two.pnum./zero.pnum./seven.pnum Text Normalization
Textnormalizationensuresconsistencybytransformingth etextintoauniformformat,includingtasks
such as lowercasing, removing punctuation, and removing UR Ls or HTML tags.
/one.pnumimport re
/two.pnum
/three.pnumdef normalize_text(text):
/four.pnumtext = text.lower() # Convert to lowercase
/five.pnumtext = re.sub( r"http\S+" ,"", text) # Remove URLs
6text = re.sub( r"<.*?>" ,"", text) # Remove HTML tags
/seven.pnumtext = re.sub( r"[^\w\s]" ,"", text) # Remove punctuation
8return text
/nine.pnum
/one.pnum/zero.pnumsample_text = "Here’s a link to my site: https://example.com <b>Welcome! </b>"
/one.pnum/one.pnumprint(normalize_text(sample_text))
/one.pnum/two.pnum
/one.pnum/three.pnumoutput:
/one.pnum/four.pnumheres a link to my site welcome

CHAPTER /two.pnum. TEXT PREPROCESSING /two.pnum/five.pnum
/two.pnum./zero.pnum.8 Bag of Words (BoW)
The Bag of Words model [ /four.pnum/nine.pnum] converts text into a vector of word counts. Each unique word becomes
a feature.
/one.pnumfrom sklearn.feature_extraction.text import CountVecto rizer
/two.pnum
/three.pnumdocuments = [
/four.pnum"The cat sat on the mat" ,
/five.pnum"The dog barked at the cat" ,
6"The cat and the dog played together"
/seven.pnum]
8
/nine.pnumvectorizer = CountVectorizer()
/one.pnum/zero.pnumbow_matrix = vectorizer.fit_transform(documents)
/one.pnum/one.pnum
/one.pnum/two.pnum# Convert the matrix to a dense format and print
/one.pnum/three.pnumprint(bow_matrix.toarray())
/one.pnum/four.pnum
/one.pnum/five.pnumoutput:
/one.pnum6[[1 0 1 1 0 0 1]
/one.pnum/seven.pnum[1 1 1 1 0 1 0]
/one.pnum8[1 0 1 1 1 1 1]]
/two.pnum./zero.pnum./nine.pnum Part-of-Speech (POS) Tagging
Part-of-Speech (POS) tagging is the process of assigning wo rd classes, such as nouns, verbs, adjec-
tives, etc., to each word in a sentence. This helps in underst anding the grammatical structure of a
sentence.
/one.pnumimport nltk
/two.pnumnltk.download( ’punkt’)
/three.pnumnltk.download( ’averaged_perceptron_tagger’ )
/four.pnum
/five.pnumtext ="Marie Curie was a brilliant physicist"
6tokens = nltk.word_tokenize(text)
/seven.pnumpos_tags = nltk.pos_tag(tokens)
8print(pos_tags)
/nine.pnum
/one.pnum/zero.pnumoutput:
/one.pnum/one.pnum[(’Marie’,’NNP’), (’Curie’,’NNP’), (’was’,’VBD’),
/one.pnum/two.pnum(’a’,’DT’), (’brilliant’ ,’JJ’), (’physicist’ ,’NN’)]
In this example, each word is tagged with a corresponding POS tag, such as NNP(proper noun),
VBD(verb, past tense), JJ(adjective), and NN(noun).
/two.pnum./zero.pnum./one.pnum/zero.pnum Named Entity Recognition (NER)
NamedEntityRecognition(NER)[ /five.pnum/zero.pnum]identiﬁespropernounssuchaspeople,organizations,loc ations,
and other entities in a text.

CHAPTER /two.pnum. TEXT PREPROCESSING /two.pnum6
/one.pnumimport nltk
/two.pnumnltk.download( ’maxent_ne_chunker’ )
/three.pnumnltk.download( ’words’)
/four.pnum
/five.pnumtext ="Marie Curie won the Nobel Prize in Physics in 1903."
6tokens = nltk.word_tokenize(text)
/seven.pnumpos_tags = nltk.pos_tag(tokens)
8named_entities = nltk.ne_chunk(pos_tags)
/nine.pnumprint(named_entities)
/one.pnum/zero.pnum
/one.pnum/one.pnumoutput:
/one.pnum/two.pnum(S
/one.pnum/three.pnum(PERSON Marie/NNP)
/one.pnum/four.pnum(PERSON Curie/NNP)
/one.pnum/five.pnumwon/VBD
/one.pnum6the/DT
/one.pnum/seven.pnum(ORGANIZATION Nobel/NNP Prize/NNP)
/one.pnum8in/IN
/one.pnum/nine.pnum(GPE Physics/NNP)
/two.pnum/zero.pnumin/IN
/two.pnum/one.pnum1903/CD)
Thisidentiﬁes MarieCurie asaPERSON ,NobelPrize asanORGANIZATION ,andPhysics asaGPE
(geopolitical entity).
/two.pnum./zero.pnum./one.pnum/one.pnum Word Embeddings (Word/two.pnumVec and GloVe)
Wordembeddingsarevectorrepresentationsofwordsthatca pturetheirmeaningsbyplacingseman-
tically similar words closer in vector space. Common method s for generating word embeddings are
Word/two.pnumVec [/five.pnum/one.pnum] andGloVe[/five.pnum/two.pnum].
Word/two.pnumVec Example:
/one.pnumfrom gensim.models import Word2Vec
/two.pnum
/three.pnumsentences = [
/four.pnum["cat","sat","mat"],
/five.pnum["dog","barked" ,"cat"],
6["cat","and","dog","played" ]
/seven.pnum]
8
/nine.pnummodel = Word2Vec(sentences, vector_size=100, window=5, m in_count=1, workers=4)
/one.pnum/zero.pnumvector = model.wv[ ’cat’]
/one.pnum/one.pnumprint(vector)
/one.pnum/two.pnum
/one.pnum/three.pnumoutput:
/one.pnum/four.pnum# A 100-dimensional vector representing the word "cat"
GloVe Example:

CHAPTER /two.pnum. TEXT PREPROCESSING /two.pnum/seven.pnum
GloVe embeddingscan beloadedfrom pre-trainedmodels, whi ch are available online. Below is an
example of how to load and use GloVe embeddings:
/one.pnumimport numpy as np
/two.pnum
/three.pnum# Load pre-trained GloVe embeddings
/four.pnumdef load_glove_embeddings(filepath):
/five.pnumembeddings = {}
6with open(filepath, ’r’) as f:
/seven.pnum for line in f:
8 values = line.split()
/nine.pnum word = values[0]
/one.pnum/zero.pnum vector = np.asarray(values[1:], dtype= ’float32’ )
/one.pnum/one.pnum embeddings[word] = vector
/one.pnum/two.pnumreturn embeddings
/one.pnum/three.pnum
/one.pnum/four.pnumembeddings_index = load_glove_embeddings( "glove.6B.100d.txt" )
/one.pnum/five.pnumprint(embeddings_index[ ’cat’])
/one.pnum6
/one.pnum/seven.pnumoutput:
/one.pnum8# A 100-dimensional vector representing the word "cat"
/two.pnum./zero.pnum./one.pnum/two.pnum Text Augmentation
Text augmentationis a technique usedto artiﬁcially increa se the size of the training datasetby apply-
ing transformations to existing text data. This is especial ly useful when working with small datasets.
Common augmentation techniques include:
•Synonym Replacement : Replacing words with their synonyms.
•Random Insertion : Inserting random words in the sentence.
•Random Deletion : Removing random words.
•Back Translation : Translating the sentence into another language and then ba ck to the original
language.
Here is an example of synonym replacement using the ‘nlpaug‘ library:
/one.pnumimport nlpaug.augmenter.word as naw
/two.pnum
/three.pnumtext ="The quick brown fox jumps over the lazy dog"
/four.pnumaug = naw.SynonymAug(aug_src= ’wordnet’ )
/five.pnumaugmented_text = aug.augment(text)
6print(augmented_text)
/seven.pnum
8# Output:
/nine.pnum# "The quick brown fox leaps over the lazy dog"
This increasesthe variety ofthe training data without need ingto collect more samples,enhancing
model generalization.

CHAPTER /two.pnum. TEXT PREPROCESSING /two.pnum8
/two.pnum./zero.pnum./one.pnum/three.pnum Handling Imbalanced Data
InmanyNLPtasks,suchastextclassiﬁcation,imbalancedda tasetsareacommonissuewherecertain
classes are over-represented compared to others. There are several strategies to handle imbalanced
text data:
•Resampling : Either over-sampling the minority class or under-samplin g the majority class.
•Class Weighting : Assigning higher weights to the minority class during trai ning.
•Data Augmentation : Generating more samples for the minority class.
For example, using ‘imbalanced-learn‘ to oversample the mi nority class:
/one.pnumfrom imblearn.over_sampling import RandomOverSampler
/two.pnumfrom sklearn.feature_extraction.text import CountVecto rizer
/three.pnum
/four.pnumtexts = [ "I love coding" ,"Python is great" ,"I dislike bugs" ]
/five.pnumlabels = [1, 1, 0] # Imbalanced dataset
6
/seven.pnumvectorizer = CountVectorizer()
8X = vectorizer.fit_transform(texts)
/nine.pnumros = RandomOverSampler(random_state=42)
/one.pnum/zero.pnumX_res, y_res = ros.fit_resample(X, labels)
/one.pnum/one.pnumprint(y_res)
/one.pnum/two.pnum
/one.pnum/three.pnum# Output:
/one.pnum/four.pnum# [1, 1, 0, 0] # Balanced dataset after oversampling
Resampling techniques can help improve the performance of m odels on imbalanced datasets by
balancing the classes.
/two.pnum./zero.pnum./one.pnum/four.pnum Topic Modeling (LDA)
Latent Dirichlet Allocation (LDA) [/five.pnum/three.pnum] is a popular method for topic modeling, which uncovers hidd en
topics in large collections of documents. It is often used to discover the underlying themes in text
data, such as a set of news articles.
LDA assumes that documents are a mixture of topics, and each t opic is a mixture of words.
/one.pnumfrom sklearn.decomposition import LatentDirichletAlloc ation
/two.pnumfrom sklearn.feature_extraction.text import CountVecto rizer
/three.pnum
/four.pnumdocuments = [
/five.pnum"Cats are great pets" ,
6"Dogs are loyal companions" ,
/seven.pnum"I love my cat and dog" ,
8"Dogs and cats both make good pets"
/nine.pnum]
/one.pnum/zero.pnum
/one.pnum/one.pnumvectorizer = CountVectorizer()
/one.pnum/two.pnumX = vectorizer.fit_transform(documents)
/one.pnum/three.pnum

CHAPTER /two.pnum. TEXT PREPROCESSING /two.pnum/nine.pnum
/one.pnum/four.pnumlda = LatentDirichletAllocation(n_components=2, random _state=42)
/one.pnum/five.pnumlda.fit(X)
/one.pnum6
/one.pnum/seven.pnum# Print the topics
/one.pnum8terms = vectorizer.get_feature_names_out()
/one.pnum/nine.pnumfor idx, topic in enumerate(lda.components_):
/two.pnum/zero.pnumprint(f"Topic {idx}:" )
/two.pnum/one.pnumprint(" ".join([terms[i] for i in topic.argsort()[-5:]]))
/two.pnum/two.pnum
/two.pnum/three.pnum# Output:
/two.pnum/four.pnum# Topic 0: cats dog pets cat
/two.pnum/five.pnum# Topic 1: dog dogs pets loyal
LDA helps us identify topics such as "pets" and "loyalty" fro m the above collection of documents,
making it a powerful tool for discovering latent themes in la rge text corpora.
/two.pnum./zero.pnum./one.pnum/five.pnum Sentiment Analysis Preprocessing
Sentiment analysis aims to detect the sentiment expressed i n a piece of text (positive, negative, or
neutral). Preprocessing for sentiment analysis often invo lves:
•Removing Emojis and Special Characters : Emojis and special symbols can interfere with anal-
ysis.
•HandlingNegation : Negationdetectioniscrucialsince"nothappy"meanstheo ppositeof"happy".
•Expanding Contractions : Words like "can’t" should be expanded to "cannot" for clari ty.
Example of handling contractions and negation:
/one.pnumfrom contractions import fix
/two.pnum
/three.pnumdef preprocess_for_sentiment(text):
/four.pnumtext = fix(text) # Expand contractions
/five.pnumtext = text.replace( "n’t"," not")# Handle negation
6return text
/seven.pnum
8sample_text = "I can’t believe it’s not good!"
/nine.pnumprint(preprocess_for_sentiment(sample_text))
/one.pnum/zero.pnum
/one.pnum/one.pnum# Output:
/one.pnum/two.pnum# "I cannot believe it is not good!"
Preprocessinginthiswayensuresthatthesentimentmodelc orrectlyinterpretsphraseswithnega-
tion and contractions, leading to better sentiment detecti on.
/two.pnum./zero.pnum./one.pnum6 Handling Out-of-Vocabulary (OOV) Words
Inmanymachinelearningmodels,especiallywhenusingword embeddings,wordsthatdonotappear
inthetrainingvocabulary arereferredtoas Out-of-Vocabulary (OOV) words. Handlingtheseiscritical
for improving model performance:

CHAPTER /two.pnum. TEXT PREPROCESSING /three.pnum/zero.pnum
•SubwordEmbeddings : Usingsubwordunitslike BytePairEncoding(BPE) [/five.pnum/four.pnum]orFastText ,which
break words into smaller chunks and represent them as embedd ings, even if the entire word is
not seen during training.
•Character-level Models : Using character-level features helps models handle unsee n words by
understanding them from their characters.
Example of using FastText for subword embeddings:
/one.pnumfrom gensim.models import FastText
/two.pnum
/three.pnumsentences = [
/four.pnum["I","love","machine" ,"learning" ],
/five.pnum["AI","is","the","future" ]
6]
/seven.pnum
8model = FastText(sentences, vector_size=100, window=5, m in_count=1, workers=4)
/nine.pnumvector = model.wv[ ’machine’ ]
/one.pnum/zero.pnumprint(vector)
/one.pnum/one.pnum
/one.pnum/two.pnum# Output:
/one.pnum/three.pnum# A 100-dimensional vector for the word "machine", even if it is broken into subwords.
FastText models are especially robust to OOV words because t hey are capable of constructing
word vectors from the character n-grams, allowing for bette r generalization when the model encoun-
ters unfamiliar words.
/two.pnum./zero.pnum./one.pnum/seven.pnum Dealing with Noise in Text Data
Real-world text data often contains noise, such as typos, sl ang, abbreviations, and formatting errors.
Handling noisy data is crucial for improving the accuracy of NLP models. Techniques to deal with
noise include:
•Spelling Correction : Automatically correct misspelled words.
•Handling Abbreviations and Slang : Expanding common abbreviations (e.g., "u" to "you") and
converting slang to standard language.
•Removing HTML Tags : Especially important when working with web data.
Example of spelling correction using ‘TextBlob‘:
/one.pnumfrom textblob import TextBlob
/two.pnum
/three.pnumtext ="I lovee natural lnguage prossessing"
/four.pnumblob = TextBlob(text)
/five.pnumcorrected_text = str(blob.correct())
6print(corrected_text)
/seven.pnum
8# Output:
/nine.pnum# "I love natural language processing"

CHAPTER /two.pnum. TEXT PREPROCESSING /three.pnum/one.pnum
Cleaning noisy text ensures that the models can focus on the c ontent without being misled by
noise, leading to better performance on downstream tasks.
/two.pnum./zero.pnum./one.pnum8 Advanced Text Vectorization Techniques
Beyond Bag of Words (BoW) and TF-IDF, there are other advance d text vectorization techniques that
convert text into a numerical representation while preserv ing semantic information:
•WordEmbeddings : AsdiscussedwithWord/two.pnumVecandGloVe, word embeddingscapt urethecon-
textual meaning of words.
•DocumentEmbeddings : Extendswordembeddingstoentiredocuments. Methodslike Doc/two.pnumVec
capture the semanticsof entire documents, making them suit ablefor tasks like document clas-
siﬁcation and similarity.
•TF-IDF Weighted Word Embeddings : Combining the strengths of both TF-IDF and word embed-
dings, we can weight word vectors by their TF-IDF scores.
Example: Doc/two.pnumVec for Document Embedding
/one.pnumfrom gensim.models.doc2vec import Doc2Vec, TaggedDocume nt
/two.pnum
/three.pnumdocuments = [
/four.pnum"The cat sat on the mat" ,
/five.pnum"Dogs are loyal pets" ,
6"Cats and dogs are common pets"
/seven.pnum]
8
/nine.pnumtagged_data = [TaggedDocument(words=doc.split(), tags= [str(i)]) for i, doc in enumerate(documents)
]
/one.pnum/zero.pnummodel = Doc2Vec(tagged_data, vector_size=50, window=2, m in_count=1, epochs=100)
/one.pnum/one.pnumdoc_vector = model.infer_vector( "Cats and dogs are pets" .split())
/one.pnum/two.pnumprint(doc_vector)
/one.pnum/three.pnum
/one.pnum/four.pnum# Output:
/one.pnum/five.pnum# A 50-dimensional vector representing the document.
Doc/two.pnumVec extends the idea of word embeddings to entire documents and i s particularly useful for
document-level tasks such as classiﬁcation and clustering .
/two.pnum./zero.pnum./one.pnum/nine.pnum Handling Large Text Corpora
When working with large text corpora, such as billions of doc uments, several techniques can be ap-
plied to handle computational and memory limitations:
•Batch Processing : Process text insmall batches instead of loading theentire corpus into mem-
ory.
•StreamingAlgorithms : Algorithmslike OnlineLDA [/five.pnum/five.pnum]andOnlineWord/two.pnumVec canupdatemodels
incrementally without having to process the entire dataset at once.

CHAPTER /two.pnum. TEXT PREPROCESSING /three.pnum/two.pnum
•DataCompression : Applytechniquessuchas PrincipalComponentAnalysis(PCA) [/five.pnum6]ort-SNE
[/five.pnum/seven.pnum] to reduce the dimensionality of word embeddingsor text vec tors.
Example: Online Training with Gensim’s Word/two.pnumVec
/one.pnumfrom gensim.models import Word2Vec
/two.pnum
/three.pnum# Assume we have a very large corpus broken into batches
/four.pnumdef generate_batches(corpus, batch_size):
/five.pnumfor i in range(0, len(corpus), batch_size):
6 yield corpus[i:i + batch_size]
/seven.pnum
8model = Word2Vec(vector_size=100, window=5, min_count=1 )
/nine.pnum
/one.pnum/zero.pnum# Train model in batches
/one.pnum/one.pnumfor batch in generate_batches(large_corpus, batch_size= 1000):
/one.pnum/two.pnummodel.build_vocab(batch, update=True)
/one.pnum/three.pnummodel.train(batch, total_examples=len(batch), epochs= model.epochs)
This approach allows us to train models on large corpora with out overwhelming system memory.
/two.pnum./zero.pnum./two.pnum/zero.pnum Language Detection and Translation
In multilingual datasets, it is often necessary to detect th e language of a given text and potentially
translate it into a common language for analysis. This is cri tical for tasks like sentiment analysis or
topic modeling when working with global datasets.
Example: Language Detection with ‘langdetect‘ Library
/one.pnumfrom langdetect import detect
/two.pnum
/three.pnumtext ="Bonjour tout le monde"
/four.pnumlanguage = detect(text)
/five.pnumprint(language)
6
/seven.pnum# Output:
8# ’fr’ (French)
Once the language is detected, text can be translated to a com mon language using libraries such
asGoogletrans :
/one.pnumfrom googletrans import Translator
/two.pnum
/three.pnumtranslator = Translator()
/four.pnumtranslated = translator.translate( "Bonjour tout le monde" , src="fr", dest="en")
/five.pnumprint(translated.text)
6
/seven.pnum# Output:
8# ’Hello everyone’
Languagedetectionandtranslationpreprocessingstepsal lowustoworkwithmultilingualdatasets
more efﬁciently.

CHAPTER /two.pnum. TEXT PREPROCESSING /three.pnum/three.pnum
/two.pnum./zero.pnum./two.pnum/one.pnum Text Summarization Preprocessing
Before applying text summarization algorithms (such as ext ractive or abstractive methods), certain
preprocessing steps are required to ensure the text is clean and optimized for summarization:
•Sentence Splitting : Break the document into sentences, which is necessary for e xtractive sum-
marization.
•Removing Redundancies : Repetitive sentences or paragraphs should be removed to en sure a
concise summary.
•Handling Coreferences : Resolve pronounsto the entities they refer to, so summarie s are coher-
ent.
Example: Sentence Splitting using ‘nltk‘
/one.pnumimport nltk
/two.pnumnltk.download( ’punkt’)
/three.pnum
/four.pnumtext ="Marie Curie was a physicist. She won the Nobel Prize."
/five.pnumsentences = nltk.sent_tokenize(text)
6print(sentences)
/seven.pnum
8# Output:
/nine.pnum# [’Marie Curie was a physicist.’, ’She won the Nobel Prize.’ ]
In text summarization tasks, preprocessing helps improve t he quality of both extractive and ab-
stractive summaries.
/two.pnum./zero.pnum./two.pnum/two.pnum Custom Tokenization Strategies
Standardtokenizationmethods(e.g.,splittingbyspaces) arenotalwaysidealforalllanguagesortypes
of text (e.g., code or URLs). Custom tokenization strategie s can be implemented to handle speciﬁc
use cases:
•Tokenizing by Punctuation : Useful in legal documents where punctuation separates imp ortant
clauses.
•Tokenizing URLs and Hashtags : Common in social media analysis where URLs and hashtags
contain key information.
•SubwordTokenization : Techniqueslike BytePairEncoding(BPE) ,usedinmodelssuchas GPT-/three.pnum
[/five.pnum8] andBERT[/five.pnum/nine.pnum], tokenize words into subword units.
Example: Subword Tokenization with Hugging Face’s Tokenizers
/one.pnumfrom tokenizers import BertWordPieceTokenizer
/two.pnum
/three.pnumtokenizer = BertWordPieceTokenizer()
/four.pnumtokenizer.train([ "data/corpus.txt" ], vocab_size=30_000, min_frequency=2)
/five.pnum
6encoding = tokenizer.encode( "Natural Language Processing is awesome!" )

CHAPTER /two.pnum. TEXT PREPROCESSING /three.pnum/four.pnum
/seven.pnumprint(encoding.tokens)
8
/nine.pnum# Output:
/one.pnum/zero.pnum# [’natural’, ’language’, ’processing’, ’is’, ’awe’, ’##s ome’, ’!’]
Subword tokenization allows usto handlerare words and OOVw ords more effectively bybreaking
them into smaller units, which is especially important in mo dern transformer models like BERT and
GPT.
/two.pnum./zero.pnum./two.pnum/three.pnum Entity Linking
Entity Linking (also knownasNamedEntityDisambiguation)istheprocesso f connecting recognized
entitiesintexttoknowledgebaseentries,suchasWikipedi aorDBpedia,toresolveambiguitybetween
entities. For instance, if a text contains "Apple", we need t o determine if it refers to the fruit or the
technology company.
Entity linking generally involves the following steps:
•Named Entity Recognition (NER) : Detect entities like people, organizations, locations, e tc.
•Disambiguation : Assign the recognized entities to entries in a knowledge ba se.
•Contextual Matching : Use the surrounding context to resolve ambiguities.
Example: Entity Linking using ‘spaCy‘ and ‘Wikipedia-API‘
/one.pnumimport spacy
/two.pnumimport wikipediaapi
/three.pnum
/four.pnumnlp = spacy.load( "en_core_web_sm" )
/five.pnumwiki_wiki = wikipediaapi.Wikipedia( ’en’)
6
/seven.pnumtext ="Apple was founded by Steve Jobs."
8
/nine.pnum# Step 1: Entity Recognition
/one.pnum/zero.pnumdoc = nlp(text)
/one.pnum/one.pnumentities = [(ent.text, ent.label_) for ent in doc.ents]
/one.pnum/two.pnum
/one.pnum/three.pnum# Step 2: Linking entities to Wikipedia
/one.pnum/four.pnumfor entity in entities:
/one.pnum/five.pnumpage = wiki_wiki.page(entity[0])
/one.pnum6if page.exists():
/one.pnum/seven.pnum print(f"Entity: {entity[0]}, Wikipedia Link: {page.fullurl}" )
/one.pnum8
/one.pnum/nine.pnum# Output:
/two.pnum/zero.pnum# Entity: Apple, Wikipedia Link: https://en.wikipedia.or g/wiki/Apple_Inc.
/two.pnum/one.pnum# Entity: Steve Jobs, Wikipedia Link: https://en.wikipedi a.org/wiki/Steve_Jobs
Entity linking enablesus to enrich the data with additional knowledge from external sources, mak-
ing it a powerful tool for NLP tasks involving knowledge base s or ontologies.

CHAPTER /two.pnum. TEXT PREPROCESSING /three.pnum/five.pnum
/two.pnum./zero.pnum./two.pnum/four.pnum Contextual Word Embeddings (BERT)
Unlike traditional word embeddings (like Word/two.pnumVec or GloVe ) that produce a single vector for each
word regardless of context, Contextual Word Embeddings generate vectors based on the context in
which a word appears. The BERT(Bidirectional Encoder Representations from Transformer s) model
[/five.pnum/nine.pnum],forinstance,generatesdynamicembeddingsthatchangeba sedonthesurroundingtext,enabling
deeper understanding of nuances like polysemy.
Example: Using Pre-trained BERT from Hugging Face
/one.pnumfrom transformers import BertTokenizer, BertModel
/two.pnumimport torch
/three.pnum
/four.pnum# Load pre-trained BERT model and tokenizer
/five.pnumtokenizer = BertTokenizer.from_pretrained( ’bert-base-uncased’ )
6model = BertModel.from_pretrained( ’bert-base-uncased’ )
/seven.pnum
8text ="The bank was by the river bank."
/nine.pnuminputs = tokenizer(text, return_tensors= ’pt’)
/one.pnum/zero.pnum
/one.pnum/one.pnum# Forward pass through BERT
/one.pnum/two.pnumwith torch.no_grad():
/one.pnum/three.pnumoutputs = model(**inputs)
/one.pnum/four.pnum
/one.pnum/five.pnum# Get contextual embeddings for each token
/one.pnum6embeddings = outputs.last_hidden_state
/one.pnum/seven.pnumprint(embeddings.shape) # Shape: [batch_size, sequence_length, hidden_size]
/one.pnum8
/one.pnum/nine.pnum# Output:
/two.pnum/zero.pnum# torch.Size([1, 10, 768])
BERT’s contextual embeddings are now the foundation of many state-of-the-art NLP models and
applications, such as question-answering systems, text cl assiﬁcation, and more.
Social media preprocessing also often requires additional stepslike URL removal, hashtag extrac-
tion, and emoji interpretation to make the text suitable for NLP models.
/two.pnum./zero.pnum./two.pnum/five.pnum Text Data Anonymization
Text Data Anonymization is the process of removing personally identiﬁable informat ion (PII) from a
text,makingitsafetoshareorprocess. Thisisimportantin industrieslikehealthcareorﬁnance,where
sensitive data like names, addresses, or phone numbers are c ommonly found.
Techniques used for anonymization include:
•Named Entity Recognition (NER) :Detecting and redacting personalidentiﬁers like names, o rga-
nizations, or locations.
•Pattern Matching : Detecting phone numbers, email addresses, or other identi ﬁable information
using regex patterns.
•SyntheticDataGeneration : ReplacingPIIwithrealistic,syntheticdatathatcannotb etracedback
to the original individuals.

CHAPTER /two.pnum. TEXT PREPROCESSING /three.pnum6
Example: Anonymizing PII Using Named Entity Recognition (NER)
/one.pnumimport spacy
/two.pnum
/three.pnumnlp = spacy.load( "en_core_web_sm" )
/four.pnumtext ="John Smith lives in New York and works at Google."
/five.pnum
6# Detect Named Entities
/seven.pnumdoc = nlp(text)
8anonymized_text = text
/nine.pnum
/one.pnum/zero.pnum# Replace Named Entities with placeholders
/one.pnum/one.pnumfor ent in doc.ents:
/one.pnum/two.pnumif ent.label_ in [ ’PERSON’ ,’ORG’,’GPE’]:
/one.pnum/three.pnum anonymized_text = anonymized_text.replace(ent.text, "[REDACTED]" )
/one.pnum/four.pnum
/one.pnum/five.pnumprint(anonymized_text)
/one.pnum6
/one.pnum/seven.pnum# Output:
/one.pnum8# [REDACTED] lives in [REDACTED] and works at [REDACTED].
Anonymizationensuresthatthetextcanbesharedandproces sedincompliancewithdataprivacy
regulations like GDPR.
/two.pnum./zero.pnum./two.pnum6 Handling Temporal and Spatial Information in Text
Many NLP tasks require an understanding of temporal (time-b ased) and spatial (location-based) in-
formation. Extracting and normalizing these elements is cr ucial in applications like event detection,
timeline generation, and location-based analysis.
•Temporal Information : Recognizing and normalizing date/time expressions like " next Monday"
or "two days ago."
•Spatial Information : Extracting and resolving geographical locations mention ed in text.
Example: Extracting Temporal Information using ‘dateparser‘
/one.pnumimport dateparser
/two.pnum
/three.pnumtext ="The meeting will be held next Monday at 10am."
/four.pnumdate = dateparser.parse(text)
/five.pnumprint(date)
6
/seven.pnum# Output:
8# 2023-09-25 10:00:00 # (Assuming today is September 18, 202 3)
Handlingtemporalandspatialinformationisparticularly importantinindustrieslikeﬁnance,health-
care, and logistics, where event timing and locations matte r.

CHAPTER /two.pnum. TEXT PREPROCESSING /three.pnum/seven.pnum
/two.pnum./zero.pnum./two.pnum/seven.pnum Text Normalization for Low-Resource Languages
Working with low-resource languages (languages with limit ed NLP tools or datasets) often requires
specialized preprocessing techniques. These languages mi ght have less formalized grammar, varied
dialects, or insufﬁcient annotated datasets.
Key approaches include:
•TransferLearning : Usingpre-trainedmodelsfromhigh-resourcelanguagesan dﬁne-tuningthem
for low-resource languages.
•Data Augmentation : Creating synthetic data for training using back-translat ion or paraphrasing.
•Zero-shotLearning : Applyingmodelstrainedinonelanguagetoanotherwithout speciﬁctraining
data.
Example: UsingTransferLearningforLow-ResourceLanguages w ithHuggingFace’sMultilingual
BERT
/one.pnumfrom transformers import BertTokenizer, BertForSequence Classification
/two.pnum
/three.pnum# Load Multilingual BERT, which supports over 100 languages
/four.pnumtokenizer = BertTokenizer.from_pretrained( ’bert-base-multilingual-uncased’ )
/five.pnummodel = BertForSequenceClassification.from_pretrained (’bert-base-multilingual-uncased’ )
6
/seven.pnumtext ="Example sentence in low-resource language"
8inputs = tokenizer(text, return_tensors= ’pt’)
/nine.pnum
/one.pnum/zero.pnumoutputs = model(**inputs)
Preprocessingforlow-resourcelanguagesenablestheappl icationofmodernNLPtechniqueseven
in underrepresented linguistic communities.
/two.pnum./zero.pnum./two.pnum8 Text Classiﬁcation Preprocessing
For text classiﬁcation tasks (e.g., spam detection, sentim ent analysis, or news categorization), pre-
processingiscrucial toensuretheinputdataisintheright formatforclassiﬁcationmodels. Common
steps include:
•Text Normalization : Converting text to lowercase, expanding contractions, an d removing non-
alphanumeric characters.
•Class Balancing : Ensuring that each class has a sufﬁcient numberof examples to preventclas-
siﬁcation bias.
•Vectorization : Convertingtextintonumericalformatusingtechniquesli keBagofWords,TF-IDF,
or word embeddings.
Example: Preprocessing for Text Classiﬁcation using TF-IDF
/one.pnumfrom sklearn.feature_extraction.text import TfidfVecto rizer
/two.pnumfrom sklearn.model_selection import train_test_split
/three.pnum

CHAPTER /two.pnum. TEXT PREPROCESSING /three.pnum8
/four.pnum# Sample data for binary text classification
/five.pnumdocuments = [ "Spam message about free money" ,"Important work email" ,"Spam link to a malicious
website" ]
6labels = [1, 0, 1] # 1 for spam, 0 for non-spam
/seven.pnum
8# Vectorization using TF-IDF
/nine.pnumvectorizer = TfidfVectorizer()
/one.pnum/zero.pnumX = vectorizer.fit_transform(documents)
/one.pnum/one.pnumX_train, X_test, y_train, y_test = train_test_split(X, la bels, test_size=0.3, random_state=42)
/one.pnum/two.pnum
/one.pnum/three.pnumprint(X_train.shape, y_train)
/one.pnum/four.pnum
/one.pnum/five.pnum# Output:
/one.pnum6# (2, 10) # Two training samples with 10 TF-IDF features each
By applying TF-IDF vectorization and train-test splitting , the data is ready for classiﬁcation tasks
such as spam detection or sentiment analysis.
/two.pnum./zero.pnum./two.pnum/nine.pnum Aspect-Based Sentiment Analysis Preprocessing
Aspect-BasedSentimentAnalysis(ABSA) [6/zero.pnum]focusesonextracting opinionsaboutspeciﬁcaspects
or entities within a text. For instance, in a restaurant revi ew, we may want to separate sentiments
related to food, service, and ambiance. Preprocessing for A BSA involves several unique steps:
•Aspect Term Extraction : Identifying the speciﬁc terms (e.g., "food", "service") r elated to each
aspect.
•Aspect-Speciﬁc Tokenization : Ensuring tokenization preserves key phrases related to ea ch as-
pect.
•Context-Dependent Sentiment : Ensuring that the sentiment is correctly assigned to the ri ght
aspect (e.g., "good food but poor service").
Example: Extracting Aspect Terms using NER
/one.pnumimport spacy
/two.pnumnlp = spacy.load( "en_core_web_sm" )
/three.pnum
/four.pnumtext ="The food was great, but the service was slow."
/five.pnumdoc = nlp(text)
6
/seven.pnumaspect_terms = [ent.text for ent in doc.ents if ent.label_ i n [’FOOD’,’SERVICE’ ]]
8print(aspect_terms)
/nine.pnum
/one.pnum/zero.pnum# Output:
/one.pnum/one.pnum# [’food’, ’service’]
Aspect-based sentiment analysis preprocessing helps in ta rgeting sentiments towards speciﬁc
entities or attributes, providing more granular insights.

CHAPTER /two.pnum. TEXT PREPROCESSING /three.pnum/nine.pnum
/two.pnum./zero.pnum./three.pnum/zero.pnum Textual Data Augmentation for Deep Learning
For deep learning models, having large amounts of data is cru cial for achieving high performance.
When data is limited, textual data augmentation techniques can be employed to generate synthetic
data. These techniques help improve model generalization a nd reduce overﬁtting.
•Back Translation : Translating text to another language and back to generate d ifferent versions
of the same text.
•Word Swap : Replacing words with synonyms to introduce variation.
•Random Deletion : Removing words at random to simulate noise in the data.
Example: Back Translation for Data Augmentation
/one.pnumfrom googletrans import Translator
/two.pnum
/three.pnumtranslator = Translator()
/four.pnumtext ="Deep learning models require a lot of data."
/five.pnum
6# Translate text to French and back to English
/seven.pnumtranslated_text = translator.translate(text, src= "en", dest="fr").text
8back_translated_text = translator.translate(translate d_text, src= "fr", dest="en").text
/nine.pnumprint(back_translated_text)
/one.pnum/zero.pnum
/one.pnum/one.pnum# Output:
/one.pnum/two.pnum# "Deep learning models need a large amount of data."
Augmenting textual data for deep learning models helps expa nd the training set, improving the
robustness and accuracy of the models.
/two.pnum./zero.pnum./three.pnum/one.pnum Domain-Speciﬁc Text Preprocessing (Legal, Medical , etc.)
Different domains such as legal, medical, or ﬁnancial have s peciﬁc terminologies and structures that
require tailored preprocessing techniques. For example:
•Legal Text Preprocessing : Legal documents contain structured sections, clauses, an d termi-
nologiesthat mustbepreservedduringpreprocessing. Name dEntity Recognition (NER) isused
to identify legal entities like laws, regulations, and part ies involved.
•Medical Text Preprocessing : Medical text oftencontains abbreviations, acronyms, and medical
jargon. Specialized tokenizers and NER modelsare required to identifymedications, symptoms,
diseases, etc.
•FinancialTextPreprocessing : Financialdocumentsmaycontainnumericdata,monetaryva lues,
and speciﬁc ﬁnancial terminology that require careful hand ling.
Example: Medical Text Preprocessing using ScispaCy
/one.pnumimport scispacy
/two.pnumimport spacy
/three.pnumfrom scispacy.linking import EntityLinker

CHAPTER /two.pnum. TEXT PREPROCESSING /four.pnum/zero.pnum
/four.pnum
/five.pnumnlp = spacy.load( "en_core_sci_sm" )
6linker = EntityLinker(resolve_abbreviations=True, name ="umls")
/seven.pnum
8text ="The patient was prescribed amoxicillin for bacterial infe ction."
/nine.pnumdoc = nlp(text)
/one.pnum/zero.pnumentities = [(ent.text, ent._.umls_ents) for ent in doc.ent s]
/one.pnum/one.pnum
/one.pnum/two.pnumprint(entities)
/one.pnum/three.pnum
/one.pnum/four.pnum# Output:
/one.pnum/five.pnum# [(’amoxicillin’, [(’C0002510’, ’Amoxicillin’)]), (’ba cterial infection’, [(’C0005436’, ’
Bacterial Infection’)])]
Domain-speciﬁcpreprocessingensuresthatthetextistrea tedinawaythatrespectstheintricacies
of the ﬁeld, enabling more accurate downstream tasks.
/two.pnum./zero.pnum./three.pnum/two.pnum Handling Multimodal Data (Text with Images or Audio)
Multimodal data involves working with text inconjunction withother data mo dalities, such asimages
or audio. This is common in applications like caption genera tion, visual question answering, or tran-
scribing spoken content.
Preprocessing for multimodal data typically involves:
•Text Preprocessing : Standard text cleaning, tokenization, and vectorization .
•Image Preprocessing : Resizing, normalization, and data augmentation for image data.
•Audio Preprocessing : Extracting features such as spectrograms or mel-frequenc y cepstral co-
efﬁcients (MFCCs).
Example: Caption Generation for Images using CNN + RNN Model Prep rocessing
/one.pnumimport torchvision.transforms as transforms
/two.pnumfrom PIL import Image
/three.pnum
/four.pnum# Load and preprocess the image
/five.pnumimage_path = "example_image.jpg"
6image = Image.open(image_path)
/seven.pnum
8transform = transforms.Compose([
/nine.pnumtransforms.Resize((224, 224)),
/one.pnum/zero.pnumtransforms.ToTensor(),
/one.pnum/one.pnumtransforms.Normalize(mean=[0.485, 0.456, 0.406], std=[ 0.229, 0.224, 0.225])
/one.pnum/two.pnum])
/one.pnum/three.pnum
/one.pnum/four.pnumimage_tensor = transform(image).unsqueeze(0) # Add batch dimension
/one.pnum/five.pnumprint(image_tensor.shape)
/one.pnum6
/one.pnum/seven.pnum# Output:
/one.pnum8# torch.Size([1, 3, 224, 224])

CHAPTER /two.pnum. TEXT PREPROCESSING /four.pnum/one.pnum
Handling multimodal data requires preprocessing text and n on-text data simultaneously, which is
crucial for tasks such as image captioning, speech recognit ion, and more.
/two.pnum./zero.pnum./three.pnum/three.pnum Handling Class Imbalance in Text Classiﬁcation
Class imbalance is a common issue in text classiﬁcation task s where one class is overrepresented
compared to others (e.g., spam vs. non-spam). This imbalanc e can lead to poor model performance
on minority classes. Common strategies include:
•Resampling : Oversampling the minority class or undersampling the majo rity class.
•Class Weighting : Assigning higher weights to minority classes during train ing.
•Synthetic Data Generation : Using techniques like SMOTE to create synthetic examples for the
minority class.
Example: Handling Imbalance using SMOTE for Text Classiﬁcation
/one.pnumfrom imblearn.over_sampling import SMOTE
/two.pnumfrom sklearn.feature_extraction.text import TfidfVecto rizer
/three.pnumfrom sklearn.model_selection import train_test_split
/four.pnum
/five.pnum# Sample imbalanced data
6documents = [ "Spam message" ,"Another spam" ,"Important work email" ]
/seven.pnumlabels = [1, 1, 0] # Imbalanced: 2 spam, 1 non-spam
8
/nine.pnum# Vectorization using TF-IDF
/one.pnum/zero.pnumvectorizer = TfidfVectorizer()
/one.pnum/one.pnumX = vectorizer.fit_transform(documents)
/one.pnum/two.pnum
/one.pnum/three.pnum# Apply SMOTE for oversampling the minority class
/one.pnum/four.pnumsmote = SMOTE(random_state=42)
/one.pnum/five.pnumX_res, y_res = smote.fit_resample(X, labels)
/one.pnum6
/one.pnum/seven.pnumprint(y_res)
/one.pnum8
/one.pnum/nine.pnum# Output:
/two.pnum/zero.pnum# [1 1 0 0] # Balanced data after applying SMOTE
Handling class imbalance ensures that the model can general ize better and perform well on both
majority and minority classes, improving the fairness of th e classiﬁcation task.
/two.pnum./zero.pnum./three.pnum/four.pnum Handling Imbalanced Data with Class Weighting
In addition to oversampling techniques like SMOTE, class we ighting in the loss function is a crucial
method for handling imbalanced datasets. By assigning high er weights to underrepresentedclasses,
the model can learn to give them more importance during train ing.
Example in PyTorch:
/one.pnumimport torch
/two.pnumfrom torch import nn

CHAPTER /two.pnum. TEXT PREPROCESSING /four.pnum/two.pnum
/three.pnum
/four.pnum# Assign higher weight to the minority class
/five.pnumclass_weights = torch.tensor([1.0, 2.0]) # Class 0 is majority, class 1 is minority
6criterion = nn.CrossEntropyLoss(weight=class_weights)
Thistechniqueisespeciallyusefulindeeplearningmodels wheredirectlyresamplingthedatamay
lead to overﬁtting.
/two.pnum./zero.pnum./three.pnum/five.pnum Contractions in Non-English Languages
Whenworkingwithnon-Englishtext,contractionhandlingr equiresdifferentstrategies. Languageslike
FrenchorSpanishhaveuniquecontraction patterns. Forexa mple,inFrench,"l’amour"isacontraction
of "le amour", which should be expanded correctly during pre processing.
Example of handling French contractions:
/one.pnumimport re
/two.pnum
/three.pnumdef expand_french_contractions(text):
/four.pnum# French contraction for "the"
/five.pnumtext = re.sub( r"\bl’","le ", text)
6return text
/seven.pnum
8sample_text = "l’amour est beau"
/nine.pnumprint(expand_french_contractions(sample_text))
/one.pnum/zero.pnum
/one.pnum/one.pnum# Output:
/one.pnum/two.pnum# "le amour est beau"
/two.pnum./zero.pnum./three.pnum6 Cross-Lingual Text Preprocessing
In multilingual NLP tasks, leveraging cross-lingual embeddings such as MUSE[6/one.pnum] orXLM-R[6/two.pnum] al-
lows for consistent vector representations across languag es. These embeddings align words from
different languagesinto a common vector space, facilitati ng cross-lingual tasks like sentiment analy-
sis or machine translation.
Example using XLM-R:
/one.pnumfrom transformers import XLMRobertaTokenizer, XLMRobert aModel
/two.pnum
/three.pnum# Load tokenizer and model for cross-lingual embeddings
/four.pnumtokenizer = XLMRobertaTokenizer.from_pretrained( ’xlm-roberta-base’ )
/five.pnummodel = XLMRobertaModel.from_pretrained( ’xlm-roberta-base’ )
6
/seven.pnumtext ="Bonjour tout le monde"
8inputs = tokenizer(text, return_tensors= ’pt’)
/nine.pnumoutputs = model(**inputs)

CHAPTER /two.pnum. TEXT PREPROCESSING /four.pnum/three.pnum
/two.pnum./zero.pnum./three.pnum/seven.pnum Noisy Channel Model for Spelling Correction
An advanced approach to spelling correction involves the noisy channel model , which calculates the
probability of a misspelledword given the intended word. Th is method requires two probabilities: the
likelihood of the intended word and the likelihood of the obs erved (misspelled) word.
The noisy channel model is represented as:
argmax
wP(w|observed ) = argmax
wP(observed |w)P(w)
WhereP(w)is the probability of the correct word and P(observed |w)is the probability of the
observed misspelling given the correct word.
/two.pnum./zero.pnum./three.pnum8 Handling Dialects and Regional Variations
In additionto handling spellingerrors, preprocessing tex t from diverseregions oftenrequires normal-
izing dialects orregional variations . For example, converting "colour" (British) to "color" (Am erican)
ensures consistency across the dataset.
A dictionary-based approach can be used for this purpose:
/one.pnum# Dictionary to normalize British English to American Engli sh
/two.pnumbritish_to_american = {
/three.pnum"colour" :"color",
/four.pnum"favourite" :"favorite"
/five.pnum}
6
/seven.pnumdef normalize_dialect(text):
8words = text.split()
/nine.pnumnormalized_words = [british_to_american.get(word, word ) for word in words]
/one.pnum/zero.pnumreturn" ".join(normalized_words)
/one.pnum/one.pnum
/one.pnum/two.pnumsample_text = "My favourite colour is blue."
/one.pnum/three.pnumprint(normalize_dialect(sample_text))
/one.pnum/four.pnum
/one.pnum/five.pnum# Output:
/one.pnum6# "My favorite color is blue."
/two.pnum./zero.pnum./three.pnum/nine.pnum Context-Aware Preprocessing
Context-aware preprocessing ensures that word meanings ar e understood in context. For example,
handling negation (e.g., "not good" should be interpreted d ifferently from "good"). Polysemous words
(words with multiple meanings) also need to be handled caref ully.
Negation handling example :
/one.pnumdef handle_negation(text):
/two.pnumtext = text.replace( "n’t"," not")
/three.pnumreturn text
/four.pnum
/five.pnumsample_text = "I can’t believe it’s not good!"
6print(handle_negation(sample_text))

CHAPTER /two.pnum. TEXT PREPROCESSING /four.pnum/four.pnum
/seven.pnum
8# Output:
/nine.pnum# "I cannot believe it is not good!"
/two.pnum./zero.pnum./four.pnum/zero.pnum Advanced Tokenization for Programming Code and Soci al Media Text
Tokenizing programming code or text from social media requi res special handling. CamelCase tok-
enization isoftenusedinprogramminglanguages,and hashtagextraction oremojihandling iscrucial
in social media text.
Example of CamelCase tokenization :
/one.pnumimport re
/two.pnum
/three.pnumdef camel_case_split(identifier):
/four.pnummatches = re.finditer( ’.+?(?:(?<=.)(?=[A-Z])|(?<=[A-Z])(?=[A-Z][a-z])|$)’ , identifier)
/five.pnumreturn [m.group(0) for m in matches]
6
/seven.pnumidentifier = "CamelCaseExample"
8print(camel_case_split(identifier))
/nine.pnum
/one.pnum/zero.pnum# Output:
/one.pnum/one.pnum# [’Camel’, ’Case’, ’Example’]
/two.pnum./zero.pnum./four.pnum/one.pnum Error Handling in Large-Scale Text Preprocessing
When dealing with large datasets, error handling becomes essential. Introducing try-except blocks
and logging mechanisms helps ensure that malformed or noisy data does not interrupt the prepro-
cessing pipeline.
Error handling example :
/one.pnumdef safe_preprocess(text):
/two.pnumtry:
/three.pnum # Sample text normalization process
/four.pnum processed_text = preprocess_for_sentiment(text)
/five.pnum return processed_text
6except Exception as e:
/seven.pnum print(f"Error processing text: {e}" )
8 return None
/nine.pnum
/one.pnum/zero.pnumsample_text = "Invalid input 12345"
/one.pnum/one.pnumprint(safe_preprocess(sample_text))
/two.pnum./zero.pnum./four.pnum/two.pnum Custom Tokenization for Speciﬁc Text Types
In addition to standard tokenization methods, certain text types such as code, URLs, or social me-
dia content (e.g., hashtags) require customized tokenizat ion approaches. Subword tokenization is
frequently employed in transformer models like BERTandGPT.
Subword tokenization example :

CHAPTER /two.pnum. TEXT PREPROCESSING /four.pnum/five.pnum
/one.pnumfrom tokenizers import BertWordPieceTokenizer
/two.pnum
/three.pnum# Training a subword tokenizer
/four.pnumtokenizer = BertWordPieceTokenizer()
/five.pnumtokenizer.train([ "data/corpus.txt" ], vocab_size=30_000, min_frequency=2)
6
/seven.pnumencoding = tokenizer.encode( "Natural Language Processing is amazing!" )
8print(encoding.tokens)
/nine.pnum
/one.pnum/zero.pnum# Output:
/one.pnum/one.pnum# [’natural’, ’language’, ’processing’, ’is’, ’amaz’, ’## ing’, ’!’]
/two.pnum./zero.pnum./four.pnum/three.pnum Cross-lingual Named Entity Recognition (NER)
In multilingual datasets, Named Entity Recognition (NER) m ust handle different languages with vary-
ingsyntacticstructures. Cross-lingualNERmodels,sucha sXLM-R(XLM-Roberta),offerthecapability
to recognize entities across multiple languages without th e need for separate models per language.
These models leverage a shared vocabulary across languages , allowing for the identiﬁcation of enti-
ties such as persons, organizations, and locations regardl ess of the language used.
Example using XLM-R for Cross-lingual NER :
/one.pnumfrom transformers import XLMRobertaTokenizer, XLMRobert aForTokenClassification
/two.pnumfrom transformers import pipeline
/three.pnum
/four.pnum# Load XLM-R model for NER
/five.pnumtokenizer = XLMRobertaTokenizer.from_pretrained( ’xlm-roberta-large-finetuned-conll03-english’ )
6model = XLMRobertaForTokenClassification.from_pretrai ned(’xlm-roberta-large-finetuned-conll03-
english’ )
/seven.pnum
8# Use pipeline for cross-lingual NER
/nine.pnumner_pipeline = pipeline( ’ner’, model=model, tokenizer=tokenizer)
/one.pnum/zero.pnumsentence = "Marie Curie a prix Nobel en 1903."
/one.pnum/one.pnumentities = ner_pipeline(sentence)
/one.pnum/two.pnumprint(entities)
/one.pnum/three.pnum
/one.pnum/four.pnum# Output:
/one.pnum/five.pnum# [{’word’: ’Marie’, ’entity’: ’I-PER’, ’score’: 0.99},
/one.pnum6# {’word’: ’Curie’, ’entity’: ’I-PER’, ’score’: 0.99},
/one.pnum/seven.pnum# {’word’: ’Nobel’, ’entity’: ’I-MISC’, ’score’: 0.98}]
This cross-lingual NER exampleshows how to detect namedent ities like people, organizations, or
events across multiple languages, enhancing the scope of mu ltilingual NLP tasks.
/two.pnum./zero.pnum./four.pnum/four.pnum Noisy Data Detection and Handling
Textdatacollected fromreal-worldsourcessuchassocial m edia,customerreviews, ortranscriptions
oftencontains noisydata ,suchasincompletesentences,randomsymbols,ornon-stan dardgrammar.

CHAPTER /two.pnum. TEXT PREPROCESSING /four.pnum6
Detecting and handling this noise is crucial for improving m odel performance. Common techniques
for noisy data handling include:
•Spell-checking and correction : Correcting typos and misspellings.
•Removing or normalizing special characters : Dealing with symbols or non-standard punctua-
tion.
•Handling abbreviations and contractions : Expanding or replacing informal language.
Example: Detecting Noisy Text :
/one.pnumimport re
/two.pnum
/three.pnumdef detect_noise(text):
/four.pnum# Define a noise pattern (e.g., random non-alphabetical cha racters)
/five.pnumnoise_pattern = r’[^a-zA-Z0-9\s]’
6noisy_characters = re.findall(noise_pattern, text)
/seven.pnum
8if noisy_characters:
/nine.pnum print(f"Detected noisy characters: {noisy_characters}" )
/one.pnum/zero.pnumelse:
/one.pnum/one.pnum print("No noise detected" )
/one.pnum/two.pnum
/one.pnum/three.pnumsample_text = "I love NLP! :) #AI @2023"
/one.pnum/four.pnumdetect_noise(sample_text)
/one.pnum/five.pnum
/one.pnum6# Output:
/one.pnum/seven.pnum# Detected noisy characters: [’!’, ’:’, ’)’, ’#’, ’@’]
Removing noise ensurescleanerdata forthemodel, leadingt o betterperformanceindownstream
tasks like sentiment analysis or classiﬁcation.
/two.pnum./zero.pnum./four.pnum/five.pnum Named Entity Linking (NEL)
Named Entity Linking (NEL) [ 6/three.pnum] is the process of linking recognized named entities in text to entries
in a knowledge base, such as Wikipedia ,DBpedia , orWikidata . This disambiguation process allows
NLPsystemstoassociateentitieswiththeirrespectivemea ningsoridentiﬁers,suchasdistinguishing
between "Apple" (the company) and "apple" (the fruit).
Example using ‘spaCy‘ and ‘Wikipedia-API‘ for Entity Linking :
/one.pnumimport spacy
/two.pnumimport wikipediaapi
/three.pnum
/four.pnumnlp = spacy.load( "en_core_web_sm" )
/five.pnumwiki_wiki = wikipediaapi.Wikipedia( ’en’)
6
/seven.pnumtext ="Apple was founded by Steve Jobs."
8doc = nlp(text)
/nine.pnum
/one.pnum/zero.pnum# Step 1: Entity Recognition

CHAPTER /two.pnum. TEXT PREPROCESSING /four.pnum/seven.pnum
/one.pnum/one.pnumentities = [(ent.text, ent.label_) for ent in doc.ents]
/one.pnum/two.pnum
/one.pnum/three.pnum# Step 2: Linking entities to Wikipedia
/one.pnum/four.pnumfor entity in entities:
/one.pnum/five.pnumpage = wiki_wiki.page(entity[0])
/one.pnum6if page.exists():
/one.pnum/seven.pnum print(f"Entity: {entity[0]}, Wikipedia Link: {page.fullurl}" )
/one.pnum8
/one.pnum/nine.pnum# Output:
/two.pnum/zero.pnum# Entity: Apple, Wikipedia Link: https://en.wikipedia.or g/wiki/Apple_Inc.
/two.pnum/one.pnum# Entity: Steve Jobs, Wikipedia Link: https://en.wikipedi a.org/wiki/Steve_Jobs
By linking entities to a knowledge base, we enrich the data wi th additional context, which can be
especially useful in applications like question-answering systems orsemantic search .
/two.pnum./zero.pnum./four.pnum6 Contextual Word Embeddings in Low-Resource Languag es
When working with low-resource languages , there may not be sufﬁcient data to train effective word
embeddings. In such cases, contextual embeddings likemBERT (Multilingual BERT) [ 6/four.pnum] andXLM-R
can be utilized to generate meaningful word vectors in these languages by transferring knowledge
from high-resource languages. This approach beneﬁts from m ultilingual pretraining on large corpora,
allowing for robust performance on low-resource languages without requiring massive datasets.
Example: Using mBERT for Low-Resource Languages :
/one.pnumfrom transformers import BertTokenizer, BertModel
/two.pnum
/three.pnum# Load Multilingual BERT
/four.pnumtokenizer = BertTokenizer.from_pretrained( ’bert-base-multilingual-cased’ )
/five.pnummodel = BertModel.from_pretrained( ’bert-base-multilingual-cased’ )
6
/seven.pnumtext ="Ejemplo de texto en un idioma con pocos recursos."
8inputs = tokenizer(text, return_tensors= ’pt’)
/nine.pnum
/one.pnum/zero.pnum# Forward pass through BERT
/one.pnum/one.pnumoutputs = model(**inputs)
/one.pnum/two.pnum
/one.pnum/three.pnum# Get contextual embeddings for each token
/one.pnum/four.pnumembeddings = outputs.last_hidden_state
/one.pnum/five.pnumprint(embeddings.shape)
/one.pnum6
/one.pnum/seven.pnum# Output:
/one.pnum8# torch.Size([1, 9, 768])
Contextual word embeddings from models like mBERT or XLM-R c an help NLP tasks in underrep-
resented languages, including classiﬁcation, translatio n, and entity recognition.
/two.pnum./zero.pnum./four.pnum/seven.pnum Domain-Speciﬁc Preprocessing for Financial Text
Financial documents often contain structured data such as n umbers, dates, and speciﬁc terminology
(e.g.,"grossdomesticproduct","stockoptions"). Prepro cessingﬁnancialtextrequirescarefulhandling

CHAPTER /two.pnum. TEXT PREPROCESSING /four.pnum8
of numeric data, abbreviations, and domain-speciﬁc terms. Techniques include:
•Handling numbers and percentages : Extracting and normalizing ﬁnancial data points.
•Handling domain-speciﬁc terms : Using predeﬁned vocabularies or ﬁnancial glossaries.
•EntityRecognitionforﬁnancialterms : Identifyingcompanynames,stocksymbols,andﬁnancial
instruments.
Example: Extracting Financial Data :
/one.pnumimport re
/two.pnum
/three.pnumdef extract_financial_data(text):
/four.pnum# Extract numbers and percentages
/five.pnumnumbers = re.findall( r’\b\d+(?:,\d+)?(?:\.\d+)?%\b’ , text) # Match percentages
6return numbers
/seven.pnum
8sample_text = "The company’s revenue grew by 15.5% in the last quarter."
/nine.pnumfinancial_data = extract_financial_data(sample_text)
/one.pnum/zero.pnumprint(financial_data)
/one.pnum/one.pnum
/one.pnum/two.pnum# Output:
/one.pnum/three.pnum# [’15.5%’]
Preprocessing ﬁnancial text ensures that numerical data is accurately extracted and normalized,
which is crucial for tasks such as sentiment analysis in stoc k market predictions or parsing earnings
reports.
/two.pnum./zero.pnum./four.pnum8 Handling Temporal Expressions in Text
Temporal expressions, such as dates, durations, and times, are common in text and require special
handlingfor taskslike eventextraction or timelinegenera tion. Normalizing temporal expressionsinto
standard formats (e.g., converting "next Monday" into a spe ciﬁc date) allows for accurate time-based
analysis.
Example: Normalizing Temporal Expressions Using ‘dateparser‘ :
/one.pnumimport dateparser
/two.pnum
/three.pnumtext ="The meeting will take place next Monday at 10am."
/four.pnumparsed_date = dateparser.parse(text)
/five.pnumprint(parsed_date)
6
/seven.pnum# Output:
8# 2024-09-23 10:00:00 # (Assuming today is September 18, 202 4)
Handlingtemporalinformationisessentialforusecasessu chaseventextraction, schedulingsys-
tems, or analyzing historical data trends.

CHAPTER /two.pnum. TEXT PREPROCESSING /four.pnum/nine.pnum
/two.pnum./zero.pnum./four.pnum/nine.pnum Handling Emojis and Special Characters in Social Med ia Text
Social media text often contains emojis and special charact ers, which may either carry sentiment
or introduce noise. Handling emojis effectively involves e ither interpreting them or removing them
depending on the task.
Example: Handling Emojis Using ‘emoji‘ Library :
/one.pnumimport emoji
/two.pnum
/three.pnumdef remove_emojis(text):
/four.pnumreturn emoji.replace_emoji(text, replace= ’’)
/five.pnum
6sample_text = "I love NLP! xxxx #AI @2023" # assume xxxx is emoji
/seven.pnumclean_text = remove_emojis(sample_text)
Emojisoftenplayasigniﬁcantroleinsentimentanalysis,e speciallyforinformaldatasetslikesocial
media posts, so carefully handling or interpreting them is c rucial in downstream tasks.
/two.pnum./zero.pnum./five.pnum/zero.pnum Back Translation for Data Augmentation
Back translation is a powerful data augmentation technique, especially usef ul for creating synthetic
data for training. It involves translating text to another l anguage and back to the original language,
generating a paraphrased version of the text while preservi ng meaning.
Example: Back Translation for Augmentation :
/one.pnumfrom googletrans import Translator
/two.pnum
/three.pnumtranslator = Translator()
/four.pnumtext ="Data augmentation techniques improve model performance. "
/five.pnum
6# Translate text to French and back to English
/seven.pnumtranslated = translator.translate(text, src= ’en’, dest=’fr’).text
8back_translated = translator.translate(translated, src =’fr’, dest=’en’).text
/nine.pnumprint(back_translated)
/one.pnum/zero.pnum
/one.pnum/one.pnum# Output:
/one.pnum/two.pnum# "Techniques for augmenting data improve model performanc e."
Back translation is commonly used in tasks like machine tran slation, sentiment analysis, and text
classiﬁcation, where data scarcity can affect model genera lization.
/two.pnum./zero.pnum./five.pnum/one.pnum Text Summarization Preprocessing
Preprocessing for text summarization is essential to ensure the input text is in the best possible f or-
mat for extractive or abstractive summarization models. Pr eprocessing steps may include sentence
splitting, removing redundancies, and handling coreferen ces to ensure coherence in the summary.
/two.pnum./zero.pnum./five.pnum/one.pnum./one.pnum Coreference Resolution
Coreference resolution involves replacing pronouns with t he entities they refer to. For example, in
the sentence "Marie Curie won the Nobel Prize. She was the ﬁrs t woman to do so," the word "She"

CHAPTER /two.pnum. TEXT PREPROCESSING /five.pnum/zero.pnum
refersto"MarieCurie."Resolvingsuchcoreferenceshelps abstractivesummarizationmodelsmaintain
coherence and clarity.
Example using ‘coreferee‘ library for coreference resolution:
/one.pnumimport spacy
/two.pnumimport coreferee
/three.pnum
/four.pnumnlp = spacy.load( "en_core_web_sm" )
/five.pnumnlp.add_pipe( ’coreferee’ )
6
/seven.pnumtext ="Marie Curie won the Nobel Prize. She was the first woman to do so."
8doc = nlp(text)
/nine.pnum
/one.pnum/zero.pnum# Display resolved coreferences
/one.pnum/one.pnumfor cluster in doc._.coref_chains:
/one.pnum/two.pnumprint(f"Coreference: {cluster}" )
/one.pnum/three.pnum
/one.pnum/four.pnum# Output:
/one.pnum/five.pnum# Coreference: [Marie Curie, She]
By resolving coreferences, text summarization models can g enerate clearer and more accurate
summaries, particularly in longer texts where the use of pro nouns is common.
/two.pnum./zero.pnum./five.pnum/one.pnum./two.pnum Sentence Splitting for Summarization
Forextractivesummarization,thetextisoftensplitintoi ndividualsentences,andthemostinformative
sentences are selected for the summary. Accurate sentence t okenization is critical to avoid splitting
sentences at incorrect points, which could lead to incomple te or nonsensical summaries.
Example of sentence splitting using ‘nltk‘:
/one.pnumimport nltk
/two.pnumnltk.download( ’punkt’)
/three.pnum
/four.pnumtext ="Marie Curie was a physicist. She won the Nobel Prize."
/five.pnumsentences = nltk.sent_tokenize(text)
6print(sentences)
/seven.pnum
8# Output:
/nine.pnum# [’Marie Curie was a physicist.’, ’She won the Nobel Prize.’ ]
Accurate sentence splitting ensures that each sentence is p roperly extracted, facilitating effective
extractive summarization.
/two.pnum./zero.pnum./five.pnum/two.pnum Aspect-Based Sentiment Analysis (ABSA)
Aspect-Based Sentiment Analysis (ABSA) goes beyond general sentiment analysis by focusing on
identifying sentiment towards speciﬁc entities or aspects of an entity. For example, in a restaurant
review, ABSA can distinguish between the sentiments toward s food, service, and ambiance.

CHAPTER /two.pnum. TEXT PREPROCESSING /five.pnum/one.pnum
/two.pnum./zero.pnum./five.pnum/two.pnum./one.pnum Aspect Term Extraction
Theﬁrststep inABSAisextracting theaspectterms(e.g.,"f ood", "service") fromthetext. Thisisoften
done using Named Entity Recognition (NER) or custom diction aries.
Example of extracting aspect terms using a custom dictionary:
/one.pnumaspects = { "food": ["pizza","pasta","burger" ],"service" : ["waiter" ,"staff","service" ]}
/two.pnum
/three.pnumdef extract_aspect_terms(text):
/four.pnumwords = text.split()
/five.pnumextracted_aspects = [aspect for word in words for aspect, ke ywords in aspects.items() if word
in keywords]
6return extracted_aspects
/seven.pnum
8text ="The pizza was great but the service was slow."
/nine.pnumaspect_terms = extract_aspect_terms(text)
/one.pnum/zero.pnumprint(aspect_terms)
/one.pnum/one.pnum
/one.pnum/two.pnum# Output:
/one.pnum/three.pnum# [’food’, ’service’]
Aspect term extraction ensures that the model can target spe ciﬁc opinions expressed about enti-
ties or their components.
/two.pnum./zero.pnum./five.pnum/two.pnum./two.pnum Context-Dependent Sentiment Classiﬁcation
Afterextracting theaspects,thesentimentassociatedwit heachaspectmustbedetermined. Inmany
cases, the sentiment towards one aspect may differ from anot her within the same sentence.
Example of sentiment classiﬁcation per aspect:
/one.pnumfrom nltk.sentiment import SentimentIntensityAnalyzer
/two.pnum
/three.pnumanalyzer = SentimentIntensityAnalyzer()
/four.pnum
/five.pnumdef classify_aspect_sentiment(text, aspect_terms):
6for aspect in aspect_terms:
/seven.pnum aspect_sentiment = analyzer.polarity_scores(text)
8 print(f"Aspect: {aspect}, Sentiment: {aspect_sentiment[’compo und’]}")
/nine.pnum
/one.pnum/zero.pnumtext ="The pizza was great but the service was slow."
/one.pnum/one.pnumaspect_terms = extract_aspect_terms(text)
/one.pnum/two.pnumclassify_aspect_sentiment(text, aspect_terms)
/one.pnum/three.pnum
/one.pnum/four.pnum# Output:
/one.pnum/five.pnum# Aspect: food, Sentiment: 0.6369
/one.pnum6# Aspect: service, Sentiment: -0.4767
ABSA provides a more granular understanding of sentiments w ithin the text, which can be highly
beneﬁcial in applications like customer feedback analysis or product reviews.

CHAPTER /two.pnum. TEXT PREPROCESSING /five.pnum/two.pnum
/two.pnum./zero.pnum./five.pnum/three.pnum Handling Out-of-Vocabulary (OOV) Words with Subwor d Embeddings
Out-of-Vocabulary(OOV)wordscanposeasigniﬁcantchalle ngeformodelstrainedonalimitedvocab-
ulary.Subword embeddings , such as those used in BERTorFastText , mitigate this issue by breaking
down rare or unseen words into smaller, meaningful subunits (e.g., morphemes or syllables).
Example: Using FastText for OOV words :
/one.pnumfrom gensim.models import FastText
/two.pnum
/three.pnum# Train FastText on a small corpus
/four.pnumsentences = [[ "I","love","machine" ,"learning" ], ["AI","is","the","future" ]]
/five.pnummodel = FastText(sentences, vector_size=100, window=5, m in_count=1, workers=4)
6
/seven.pnum# Get the vector for an OOV word
8vector = model.wv[ ’unseenword’ ]
/nine.pnumprint(vector)
/one.pnum/zero.pnum
/one.pnum/one.pnum# Output:
/one.pnum/two.pnum# A 100-dimensional vector for the subwords of "unseenword"
Subword embeddings allow the model to generalize better to O OV words by leveraging subword
information, which is particularly useful for handling lar ge vocabularies in languages with complex
morphology.
/two.pnum./zero.pnum./five.pnum/four.pnum Textual Data Augmentation for Low-Resource Scenari os
In low-resource scenarios, where labeled data is scarce, textual data augmentation techniques such
asparaphrasing ,word swapping , andback translation are often employed to expand the training set
and improve model generalization.
/two.pnum./zero.pnum./five.pnum/four.pnum./one.pnum Word Swap Augmentation
Word swapping involves replacing certain words in a sentenc e with their synonyms to introduce vari-
ability into the dataset.
Example of synonym-based word swapping using ‘nlpaug‘:
/one.pnumimport nlpaug.augmenter.word as naw
/two.pnum
/three.pnumtext ="The quick brown fox jumps over the lazy dog"
/four.pnumaug = naw.SynonymAug(aug_src= ’wordnet’ )
/five.pnumaugmented_text = aug.augment(text)
6print(augmented_text)
/seven.pnum
8# Output:
/nine.pnum# "The quick brown fox leaps over the lazy dog"
Word swap augmentation helps to improve the robustness of th e model by introducing linguistic
variation into the training data.

CHAPTER /two.pnum. TEXT PREPROCESSING /five.pnum/three.pnum
/two.pnum./zero.pnum./five.pnum/four.pnum./two.pnum Back Translation
Back translation, previously discussed, is particularly v aluable for **low-resource languages** where
directly labeleddatamay notbeavailable. Bytranslating t hetextintoanotherlanguageandthenback
to the original language, one can generate new training data that preserves meaning but varies in
expression.
/two.pnum./zero.pnum./five.pnum/five.pnum Handling Class Imbalance with Synthetic Data Genera tion
When facing class imbalance in text classiﬁcation tasks, sy nthetic data generation techniques like
**SMOTE (Synthetic Minority Over-sampling Technique)** [ 6/five.pnum] can be employed to create additional
samples for the minority class.
Example using ‘imbalanced-learn‘ for SMOTE:
/one.pnumfrom imblearn.over_sampling import SMOTE
/two.pnumfrom sklearn.feature_extraction.text import TfidfVecto rizer
/three.pnumfrom sklearn.model_selection import train_test_split
/four.pnum
/five.pnum# Sample imbalanced data
6documents = [ "Spam message" ,"Another spam" ,"Important work email" ]
/seven.pnumlabels = [1, 1, 0] # Imbalanced: 2 spam, 1 non-spam
8
/nine.pnum# Vectorization using TF-IDF
/one.pnum/zero.pnumvectorizer = TfidfVectorizer()
/one.pnum/one.pnumX = vectorizer.fit_transform(documents)
/one.pnum/two.pnum
/one.pnum/three.pnum# Apply SMOTE for oversampling the minority class
/one.pnum/four.pnumsmote = SMOTE(random_state=42)
/one.pnum/five.pnumX_res, y_res = smote.fit_resample(X, labels)
/one.pnum6
/one.pnum/seven.pnumprint(y_res)
/one.pnum8
/one.pnum/nine.pnum# Output:
/two.pnum/zero.pnum# [1, 1, 0, 0] # Balanced data after applying SMOTE
Synthetic data generation helps ensure that models can gene ralize better and avoid bias towards
the majority class.
/two.pnum./zero.pnum./five.pnum6 Custom Tokenization for Legal and Medical Text
Legaland medical documents often contain specialized term inologies, abbreviations, and formatting
conventions. Customtokenization strategiesmustbedesig nedto handlethesedomain-speciﬁcchal-
lenges effectively.
Example: Tokenizing medical text using ‘scispaCy‘:
/one.pnumimport spacy
/two.pnumimport scispacy
/three.pnum
/four.pnumnlp = spacy.load( "en_core_sci_sm" )
/five.pnum

CHAPTER /two.pnum. TEXT PREPROCESSING /five.pnum/four.pnum
6text ="The patient was prescribed amoxicillin for bacterial infe ction."
/seven.pnumdoc = nlp(text)
8
/nine.pnumfor token in doc:
/one.pnum/zero.pnumprint(token.text)
/one.pnum/one.pnum
/one.pnum/two.pnum# Output:
/one.pnum/three.pnum# The patient was prescribed amoxicillin for bacterial infe ction
By leveraging domain-speciﬁc tokenizers, models can more a ccurately process texts in ﬁelds like
healthcare and law, where tokenization errors can lead to si gniﬁcant misinterpretations of the data.
/two.pnum./one.pnum Conclusion
Inthischapter,weexpandedonvariousadvancedtechniques fortextpreprocessing,includingdomain-
speciﬁcapproaches,handlingnoisyorimbalanceddata,and employingdataaugmentationstrategies.
These techniques are critical in improving the performance and accuracy of NLP models, especially
when dealing with real-world data in ﬁelds like ﬁnance, heal thcare, and legal systems. Preprocessing
isnot a one-size-ﬁts-allsolution; instead, it mustbecare fully tailored to the speciﬁc characteristics of
the text and the NLP tasks at hand.
In the next chapter, we will focus on how to integrate these pr eprocessing techniques with state-
of-the-artmodels,exploring theimplementationofdeep le arning architectures suchasTransformers,
Recurrent Neural Networks (RNNs), and Convolutional Neura l Networks (CNNs) for various text pro-
cessing applications.

