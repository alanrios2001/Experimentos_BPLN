CHAPTER /four.pnum. HUGGING FACE FOR NLP 6/four.pnum
/four.pnum./three.pnum pipeline
/four.pnum./four.pnum taxonomy of pipeline
/four.pnum./five.pnum loading models
/four.pnum.6 Tokenizers
/four.pnum./seven.pnum Tokenization in Natural Language Processing
Tokenizationisafundamentalstepinpreparingtextdatafo ruseinnaturallanguageprocessing(NLP)
models, especially transformer-based models such as BERT, GPT-/two.pnum, and RoBERTa. In this section,
we will explore the concept of tokenization, why it is necess ary, and how to use the Hugging Face
Transformers library to handle tokenization effectively.
/four.pnum./seven.pnum./one.pnum What is Tokenization?
Tokenization is the process of converting a piece of text int o smaller components, known as tokens.
These tokens can be words, subwords, or even characters. Mod ern transformer models often rely
on subword tokenization techniques, such as Byte-Pair Enco ding (BPE) and WordPiece, to handle the
complexities of natural languages, including dealing with rare words and morphological variations.
/four.pnum./seven.pnum./one.pnum./zero.pnum./one.pnum Why Tokenization Matters Tokenization helps break down a sentence like:
“Hugging Face Transformers is awesome!”
into smaller units that a machine learning model can underst and, like:
["hugging", "face", "transform", "##ers", "is", "awesome" , "!"]
Withouttokenization, theinputwouldnotbeinterpretable bymodelsbecausetheyrequirenumeric
representations, not raw text.
/four.pnum./seven.pnum./two.pnum Hugging Face Transformers: Tokenizers
TheHuggingFacelibraryprovidesmultipletokenizersopti mizedforvarioustransformermodels,such
as BERT, GPT-/two.pnum, RoBERTa, and more. Each tokenizer comes with pre-deﬁned vocabulary, subword
tokenization algorithms, and ways to handle special tokens .
/four.pnum./seven.pnum./two.pnum./zero.pnum./one.pnum Installing Hugging Face Transformers You can install the Hugging Face Transformers
library using:
/one.pnum!pip install transformers
/four.pnum./seven.pnum./three.pnum Using Pre-Trained Tokenizers
Hugging Face provides pre-trained tokenizers, which can be easily loaded and used with just a few
lines of code. Let’s see how you can use the AutoTokenizer class to load and use a BERT tokenizer.

CHAPTER /four.pnum. HUGGING FACE FOR NLP 6/five.pnum
/four.pnum./seven.pnum./three.pnum./zero.pnum./one.pnum Loading a Tokenizer
/one.pnumfrom transformers import AutoTokenizer
/two.pnum
/three.pnum% Load the tokenizer for BERT (bert-base-uncased)
/four.pnumtokenizer = AutoTokenizer.from_pretrained( "bert-base-uncased" )
In this example, the AutoTokenizer is used to automatically load the appropriate tokenizer for the
bert-base-uncased model.
/four.pnum./seven.pnum./three.pnum./zero.pnum./two.pnum Tokenizing Text Once the tokenizer is loaded, you can use it to tokenize any in put text:
/one.pnumtext ="Hugging Face Transformers is awesome!"
/two.pnumtokens = tokenizer.tokenize(text)
/three.pnumprint(tokens)
This will output:
[’hugging’,’face’, ’transform’, ’##ers’, ’is’, ’awesome’ , ’!’]
The word Transformers has beensplit into two tokens: transform and##ers,using the WordPiece
subword tokenization method.
/four.pnum./seven.pnum./three.pnum./zero.pnum./three.pnum ConvertingTokenstoInputIDs Inatransformermodel,tokensneedtobeconvertedinto
numerical representations called token IDs:
Listing /four.pnum./one.pnum: Converting tokens to input IDs
input_ids = tokenizer.convert_tokens_to_ids(tokens)
print(input_ids)
Theinput_ids are the integer representations of the tokens. These IDs can be fed into a trans-
former model for further processing.
/four.pnum./seven.pnum./four.pnum Padding and Truncation
When working with batches of data, tokenized sequences must often be padded to a uniform length.
You can automatically handle this by setting padding andtruncation options when tokenizing:
/one.pnumencoded_inputs = tokenizer(text, padding= ’max_length’ , max_length=10, truncation=True)
/two.pnumprint(encoded_inputs)
Thepadding=’max_length’ ensuresthattheoutputispaddedtoalengthof/one.pnum/zero.pnum,and truncation=True
ensures that sequences longer than the maximum length are tr uncated.
/four.pnum./seven.pnum./five.pnum Batch Tokenization
HuggingFace’stokenizersalsosupportbatchtokenization ,whichisusefulforhandlingmultipleinputs
at once. This can be done by passing a list of texts to the token izer.
Listing /four.pnum./two.pnum: Batch tokenization example
texts = ["Hello,␣world!", "Transformers␣are␣powerful." ]
batch_inputs = tokenizer(texts, padding=True, truncatio n=True, return_tensors="pt")
print(batch_inputs)

CHAPTER /four.pnum. HUGGING FACE FOR NLP 66
Thereturn_tensors="pt" option converts the output into PyTorch tensors, making it e asy to feed
into models.
/four.pnum./seven.pnum.6 Special Tokens
Hugging Face models often use special tokens like [CLS],[SEP], and[PAD]for speciﬁc purposes in
NLP tasks.
/four.pnum./seven.pnum.6./zero.pnum./one.pnum Example of Special Tokens For example, in BERT:
•[CLS]is added at the beginning of the sentence for classiﬁcation t asks.
•[SEP]isusedtoseparatesentencesintaskslikequestionansweri ngorsequenceclassiﬁcation.
•[PAD]is used for padding sequences to a uniform length.
These special tokens are automatically added by the tokeniz er when processing inputs:
/one.pnumencoded_inputs = tokenizer(text, add_special_tokens=Tr ue)
/two.pnumprint(encoded_inputs)
/four.pnum./seven.pnum./seven.pnum Conclusion
Tokenizationisacrucialpreprocessingstepwhenworkingw ithtransformers.HuggingFace’s AutoTokenizer
and related classes provide a seamless interface for tokeni zing text and preparing it for input to vari-
oustransformer models. Byusing pre-trained tokenizers, y ou ensurethat themodel andtokenizer are
aligned, making it easier to achieve high performance in NLP tasks.
/four.pnum./seven.pnum.8 Understanding Token IDs and the Vocabulary
Every tokenizer comes with a vocabulary that maps tokens (wo rds, subwords, or symbols) to unique
integer IDs. These token IDs are what the transformer models expect as input.
/four.pnum./seven.pnum.8./zero.pnum./one.pnum AccessingtheVocabulary Youcaneasilyaccessthevocabularysizeandlookupspeciﬁc
token IDs or tokens:
/one.pnum% Access the vocabulary size
/two.pnumvocab_size = tokenizer.vocab_size
/three.pnumprint(f"Vocabulary size: {vocab_size}" )
/four.pnum
/five.pnum% Look up token ID for a specific token
6token_id = tokenizer.convert_tokens_to_ids( "hugging" )
/seven.pnumprint(f"ID for ’hugging’: {token_id}" )
8
/nine.pnum% Look up the token corresponding to a specific ID
/one.pnum/zero.pnumtoken = tokenizer.convert_ids_to_tokens(token_id)
/one.pnum/one.pnumprint(f"Token for ID {token_id}: {token}" )

CHAPTER /four.pnum. HUGGING FACE FOR NLP 6/seven.pnum
/four.pnum./seven.pnum.8./zero.pnum./two.pnum Exploring Vocabulary Here’s how you can print out the ﬁrst few tokens in the tokeniz er’s
vocabulary, to get a sense of what the tokenizer has learned:
/one.pnum% Print the first 10 tokens in the vocabulary
/two.pnumfor i in range(10):
/three.pnumprint(f"Token ID {i}: {tokenizer.convert_ids_to_tokens(i)}" )
This canbeusefulfor understandingwhat tokensthetokeniz er prioritizes (e.g.,common words or
special tokens like [PAD]).
/four.pnum./seven.pnum./nine.pnum Handling Long Sequences: Chunking and Sliding Window s
Transformer models like BERT and GPT-/two.pnum have a maximum input l ength (e.g., /five.pnum/one.pnum/two.pnum tokens for BERT).
If a sequence exceeds this length, truncation is applied, bu t sometimes we need to preserve context
across chunks. The sliding window technique can help with this.
/four.pnum./seven.pnum./nine.pnum./zero.pnum./one.pnum Sliding Window Tokenization Using a sliding window, we can split long sequences into
chunks while maintaining overlap between consecutive segm ents:
/one.pnumtext ="This is a long document that exceeds the maximum token lengt h of the transformer model..."
/two.pnummax_len = 512
/three.pnumstride = 50
/four.pnum
/five.pnumencoded_inputs = tokenizer(text, return_tensors= "pt", max_length=max_len, stride=stride,
truncation=True, return_overflowing_tokens=True)
6
/seven.pnum% Print each chunk ’s token IDs
8for chunk_idx, input_ids in enumerate(encoded_inputs[’ input_ids ’]):
/nine.pnumprint(f"Chunk {chunk_idx} Token IDs: {input_ids}")
This way, the model can process the entire text, but no critic al context is lost between chunks.
/four.pnum./seven.pnum./one.pnum/zero.pnum Handling Multiple Text Inputs: Pairing and Encoding Sentences
In tasks like question answering orsentence-pair classiﬁcation , transformer models expect two se-
quences as input. The tokenizer can handle this by adding a sp ecial separator token ( [SEP])between
the sequences.
/four.pnum./seven.pnum./one.pnum/zero.pnum./zero.pnum./one.pnum Encoding Sentence Pairs To tokenize sentence pairs, pass two texts to the tokenizer:
/one.pnumsentence1 = "What is the capital of France?"
/two.pnumsentence2 = "The capital of France is Paris."
/three.pnum
/four.pnumencoded_pair = tokenizer(sentence1, sentence2, return_t ensors="pt")
/five.pnumprint(encoded_pair)
This will add the necessary [CLS]and[SEP]tokens to the encoded pair.
/four.pnum./seven.pnum./one.pnum/zero.pnum./zero.pnum./two.pnum Tokenized Sentence Pair The resulting tokenized sequence will look like this:
[CLS] what is the capital of france ? [SEP] the capital of fran ce is paris .
[SEP]
This format allows models like BERT to process the relations hip between two sequences.

CHAPTER /four.pnum. HUGGING FACE FOR NLP 68
/four.pnum./seven.pnum./one.pnum/one.pnum Attention Masks: Handling Padding in Models
When dealing with variable-length sequences, padding is ap plied to ensure uniform input sizes. The
attention mask isabinarymaskthattellsthemodelwhichtokensarerealand whicharejustpadding.
/four.pnum./seven.pnum./one.pnum/one.pnum./zero.pnum./one.pnum Using Attention Masks When padding sequences, the attention mask indicates where
the actual tokens end and the padding begins:
/one.pnumencoded_inputs = tokenizer(text, padding= ’max_length’ , max_length=10, truncation=True,
return_tensors= "pt")
/two.pnum
/three.pnuminput_ids = encoded_inputs[ ’input_ids’ ]
/four.pnumattention_mask = encoded_inputs[ ’attention_mask’ ]
/five.pnum
6print(f"Input IDs: {input_ids}" )
/seven.pnumprint(f"Attention Mask: {attention_mask}" )
The attention mask will look like a list of /one.pnums and /zero.pnums, where /one.pnum in dicates a real token and /zero.pnum indicates
padding.
For example:
[CLS] hugging face is awesome ! [PAD] [PAD] [PAD]
Will have the following attention mask:
[1, 1, 1, 1, 1, 1, 0, 0, 0]
/four.pnum./seven.pnum./one.pnum/one.pnum./zero.pnum./two.pnum WhyAttentionMasksMatter Attentionmaskspreventthemodelfromfocusingonpadded
tokens, which would otherwise confuse the model’s self-att ention mechanism.
/four.pnum./seven.pnum./one.pnum/two.pnum Creating and Using Custom Tokenizers
Sometimes pre-trained tokenizers may not suit your speciﬁc needs (e.g., working with a specialized
domainlikemedicalorlegaltext). Youcancreatecustomtok enizersusingthe tokenizers libraryfrom
Hugging Face.
/four.pnum./seven.pnum./one.pnum/two.pnum./zero.pnum./one.pnum Training a Custom Byte-Level BPE Tokenizer Here’s how you can train a Byte-Pair En-
coding (BPE) tokenizer on your custom dataset:
/one.pnumfrom tokenizers import ByteLevelBPETokenizer
/two.pnum
/three.pnum% Initialize a Byte-Pair Encoding tokenizer
/four.pnumtokenizer = ByteLevelBPETokenizer()
/five.pnum
6% Train the tokenizer on your dataset
/seven.pnumtokenizer.train(files=[ "path_to_your_text_data.txt" ], vocab_size=52_000, min_frequency=2)
8
/nine.pnum% Save the tokenizer
/one.pnum/zero.pnumtokenizer.save_model( "tokenizer_directory" )
This creates a custom tokenizer based on your dataset and all ows you to tailor the vocabulary to
speciﬁc tasks.

CHAPTER /four.pnum. HUGGING FACE FOR NLP 6/nine.pnum
/four.pnum./seven.pnum./one.pnum/two.pnum./zero.pnum./two.pnum Loading and Using the Custom Tokenizer Once trained, you can load and useyour cus-
tom tokenizer:
/one.pnum% Load the custom tokenizer
/two.pnumcustom_tokenizer = ByteLevelBPETokenizer( "tokenizer_directory/vocab.json" ,"tokenizer_directory/
merges.txt" )
/three.pnum
/four.pnum% Tokenize some text
/five.pnumcustom_tokens = custom_tokenizer.encode( "Custom text for testing" ).tokens
6print(custom_tokens)
This allows you to tokenize text using a vocabulary that is sp eciﬁc to your domain.
/four.pnum./seven.pnum./one.pnum/three.pnum Multilingual Tokenization: Handling Multiple Lang uages
Multilingual transformer models like mBERTandXLM-Rsupport tokenizing multiple languages simulta-
neously, thanks to a shared vocabulary trained on many langu ages.
/four.pnum./seven.pnum./one.pnum/three.pnum./one.pnum Multilingual Tokenization Example
Here’s how you can tokenize text in different languages usin g a multilingual tokenizer:
/one.pnum# Load multilingual BERT tokenizer
/two.pnumtokenizer = AutoTokenizer.from_pretrained( "bert-base-multilingual-cased" )
/three.pnum
/four.pnum# Tokenize text in multiple languages
/five.pnumenglish_text = "Hello, how are you?"
6french_text = "Bonjour, comment \c{c}a va?"
/seven.pnumgerman_text = "Hallo, wie geht’s?"
8
/nine.pnumtokens_en = tokenizer.tokenize(english_text)
/one.pnum/zero.pnumtokens_fr = tokenizer.tokenize(french_text)
/one.pnum/one.pnumtokens_de = tokenizer.tokenize(german_text)
/one.pnum/two.pnum
/one.pnum/three.pnumprint(tokens_en)
/one.pnum/four.pnumprint(tokens_fr)
/one.pnum/five.pnumprint(tokens_de)
Thistokenizercanhandleinputsinvariouslanguageswitho utneedingseparatetokenizers,making
it ideal for cross-lingual tasks.
/four.pnum./seven.pnum./one.pnum/four.pnum Speeding Up Tokenization with Fast Tokenizers
Hugging Face provides Fast Tokenizers , written in Rust, that signiﬁcantly improve the speed of tok -
enization whilemaintaining accuracy. Thesetokenizers ar e especiallyusefulwhen working with large
datasets.
/four.pnum./seven.pnum./one.pnum/four.pnum./zero.pnum./one.pnum Using a Fast Tokenizer Fast tokenizers are used just like normal tokenizers, but wi th
much better performance:

CHAPTER /four.pnum. HUGGING FACE FOR NLP /seven.pnum/zero.pnum
/one.pnum# Load a fast tokenizer
/two.pnumfast_tokenizer = AutoTokenizer.from_pretrained( "bert-base-uncased" , use_fast=True)
/three.pnum
/four.pnum# Tokenize some text
/five.pnumencoded_fast = fast_tokenizer( "Tokenization with fast tokenizer." , return_tensors= "pt")
6print(encoded_fast)
Thesefasttokenizerscanreducepreprocessingtimes,espe ciallywhenworkingwithhugeamounts
of text data, without sacriﬁcing accuracy.
/four.pnum./seven.pnum./one.pnum/five.pnum Tokenization and Word Embeddings
Tokenization, encoding, and embeddings are key steps in tra nsforming raw text into representations
that machine learning models can process. Each step has a dis tinct role in the natural language pro-
cessing (NLP) pipeline.
Tokenization : Tokenization is the process of splitting the text into smal ler units like words, sub-
words, or characters. This is necessary for handling text da ta and breaking it down into manageable
partsthatmodelscanprocess. Commonapproachesincludewo rd-leveltokenization(e.g.,splittingby
spaces), subword tokenization (used in BPE or WordPiece mod els), and character-level tokenization.
Encoding : Once the text is tokenized, the next step is to map each token to a unique identiﬁer
(token ID). This transformation is deterministic, meaning the same text input will always result in the
samesequenceoftokenIDs. Encodedtextservesasinputform odels,andtheIDsareusedtoretrieve
speciﬁc embeddings from a pre-trained embedding matrix.
Word Embeddings : Embeddings transform tokens into continuous, dense vecto r spaces, captur-
ing semantic relationships between words. Unlike encoding , embeddings are learned during model
training and representwords in terms of numerical vectors, placing semantically similar words closer
together in the vector space. Techniques such as Word/two.pnumVec, G loVe, and transformer-based embed-
dings (e.g., BERT or GPT) are commonly used to generate these vectors.
/four.pnum./seven.pnum./one.pnum/five.pnum./one.pnum Coding Example with Hugging Face Transformers
To illustrate these concepts, let’s use Hugging Face’s transformers library to demonstrate tokeniza-
tion, encoding, and extracting embeddingsusing a pre-trai ned model like BERT.
First, install the necessary libraries:
pip install transformers torch
/four.pnum./seven.pnum./one.pnum/five.pnum./one.pnum./one.pnum /one.pnum. Tokenization Example Using the BERT tokenizer, we can split the input sentence int o
subword tokens:
/one.pnumfrom transformers import BertTokenizer
/two.pnum
/three.pnum# Initialize tokenizer for BERT
/four.pnumtokenizer = BertTokenizer.from_pretrained( ’bert-base-uncased’ )
/five.pnum
6# Sample sentence
/seven.pnumsentence = "Hello world, this is an NLP example."
8

CHAPTER /four.pnum. HUGGING FACE FOR NLP /seven.pnum/one.pnum
Aspect Tokenization Encoding Embedding
DeﬁnitionSplitting text into smaller
units like words, subwords,
or charactersMapping text into token IDs
or indicesMapping words/tokens into
dense vector space
Type of
OutputTokens (subwords, words,
characters)Integer values (e.g., token
IDs)Continuous values (vectors
of ﬂoats)
Deterministic?Yes (same input, same
output)Yes (same input, same
output)No, embeddings are learned
during training
PurposePrepares text for further
processing by splitting it into
manageable partsText preprocessing for
modelsCapturing semantic
relationships in data
MethodWord-level, subword-level, or
character-level
segmentationLookup in a token-to-ID
mapping tableDense representation
learned through algorithms
like Word/two.pnumVec, GloVe, or
BERT
ExampleTokenizing "Hello world" into
[’Hello’, ’world’]Tokenizing "Hello" to /one.pnum/five.pnum/four.pnum/nine.pnum6Embedding "cat" as [/zero.pnum./two.pnum/three.pnum/four.pnum,
-/zero.pnum.6/five.pnum/seven.pnum, /zero.pnum./nine.pnum8/two.pnum]
Similarity
AwarenessNo inherent notion of
similarityNo inherent notion of
similarityEmbeddings capture
semantic similarity
Impact of
Out-of-
Vocabulary
(OOV)
WordsMay require handling OOV
with special tokens like
‘[UNK]‘OOV tokens may get
mapped to an unknown IDEmbedding layer will either
assign random vectors or
ignore OOV tokens
Common
AlgorithmsWordPiece, Byte-Pair
Encoding (BPE),
SentencePieceNone (simple lookup table)Word/two.pnumVec, GloVe, FastText,
transformer embeddings
Table /four.pnum./one.pnum: Comparison between Tokenization, Encoding, and Embedding
/nine.pnum# Tokenizing sentence
/one.pnum/zero.pnumtokens = tokenizer.tokenize(sentence)
/one.pnum/one.pnumprint("Tokens:" , tokens)
Output:
Tokens: [ /quotesingle.Varhello/quotesingle.Var,/quotesingle.Varworld/quotesingle.Var,/quotesingle.Var,/quotesingle.Var,/quotesingle.Varthis/quotesingle.Var,/quotesingle.Varis/quotesingle.Var,/quotesingle.Varan/quotesingle.Var,/quotesingle.Varnl/quotesingle.Var,/quotesingle.Var\#\#p/quotesingle.Var,/quotesingle.Varexample/quotesingle.Var,/quotesingle.Var./quotesingle.Var]
/four.pnum./seven.pnum./one.pnum/five.pnum./one.pnum./two.pnum /two.pnum. Encoding Example The tokenized output is then converted into token IDs:
/one.pnum# Encoding tokens into token IDs
/two.pnumtoken_ids = tokenizer.encode(sentence)
/three.pnumprint("Token IDs:" , token_ids)
Output:
Token IDs: [101, 7592, 2088, 1010, 2023, 2003, 2019, 17953, 2 361, 2742, 1012, 102]
Here,[101]and[102]are special tokens added by BERT, representing the start and end of a sen-
tence, respectively.

CHAPTER /four.pnum. HUGGING FACE FOR NLP /seven.pnum/two.pnum
/four.pnum./seven.pnum./one.pnum/five.pnum./one.pnum./three.pnum /three.pnum. Embedding Example Next, we obtain the embeddings for the encoded tokens using
a pre-trained BERT model:
/one.pnumfrom transformers import BertModel
/two.pnumimport torch
/three.pnum
/four.pnum# Load pre-trained BERT model
/five.pnummodel = BertModel.from_pretrained( ’bert-base-uncased’ )
6
/seven.pnum# Convert token IDs to tensors
8input_ids = torch.tensor([token_ids])
/nine.pnum
/one.pnum/zero.pnum# Get embeddings from the model
/one.pnum/one.pnumwith torch.no_grad():
/one.pnum/two.pnumoutputs = model(input_ids)
/one.pnum/three.pnumembeddings = outputs.last_hidden_state
/one.pnum/four.pnum
/one.pnum/five.pnumprint("Embedding shape:" , embeddings.shape)
Output:
Embedding shape: torch.Size([1, 12, 768])
In this example, the shape (1, 12, 768) indicates that we have /one.pnum/two.pnum tokens, each represented as a
/seven.pnum68-dimensionalvector, as BERT outputs embeddings in a den se vector space.
/four.pnum./seven.pnum./one.pnum/five.pnum./two.pnum Contextual vs. Static Embeddings
One major advancement in modern NLP models is the shift from s tatic embeddings (like Word/two.pnumVec
or GloVe) to contextual embeddings generated by models like BERT or GPT. In static embeddings, a
word always has the same vector regardless of its context, wh ereas in contextual embeddings, the
vector representation depends on the word’s surrounding co ntext.
For instance:
/one.pnumsentence_1 = "He went to the bank to withdraw money."
/two.pnumsentence_2 = "The river bank was full of birds."
/three.pnum
/four.pnumtokens_1 = tokenizer.tokenize(sentence_1)
/five.pnumtokens_2 = tokenizer.tokenize(sentence_2)
6
/seven.pnumprint("Tokens for sentence 1:" , tokens_1)
8print("Tokens for sentence 2:" , tokens_2)
Eventhough"bank"appearsinbothsentences,itsmeaningdi ffersbasedonthecontext,andthere-
fore, the embeddingsgenerated by BERT will be different for the two occurrences of "bank."
/four.pnum.8 How to Fine-Tune a Pretrained Model?
Fine-tuning a pretrained model is a common practice in Natur al Language Processing (NLP) that al-
lows leveraging the knowledge learned from large datasets ( during pretraining) and adapting it to a

