CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum8/zero.pnum
/four.pnum./one.pnum/seven.pnum Translation
In this section, we will explore how to perform machine trans lation using Hugging Face’s Transform-
ers library. Hugging Face provides pre-trained models that make it easy to translate text between
languages without needing to train your own model from scrat ch.
/four.pnum./one.pnum/seven.pnum./one.pnum Introduction to Machine Translation
Machine translation [ 8/nine.pnum] is the task of automatically converting text from one langu age to another.
Traditionally, machine translation relied on complex rule -based systems, but modern systems use
deep learning models, such as Transformers, to achieve stat e-of-the-art performance.
/four.pnum./one.pnum/seven.pnum./two.pnum Hugging Face Transformers for Translation
Hugging Face offers a variety of pre-trained models speciﬁc ally for translation. For this example, we
will use the MarianMT model, which supports translation bet ween many languages.
/four.pnum./one.pnum/seven.pnum./two.pnum./one.pnum Step /one.pnum: Installing the Required Libraries
Before we begin, ensure you have the necessary libraries ins talled. You can install the Hugging Face
Transformers library by running the following command:
/one.pnumpip install transformers
/four.pnum./one.pnum/seven.pnum./two.pnum./two.pnum Step /two.pnum: Loading the MarianMT Model
We will use the MarianMT model for translation. The ﬁrst step is to load the pre-trained model and
tokenizer. In this example, we will translate from English t o French.
/one.pnumfrom transformers import MarianMTModel, MarianTokenizer
/two.pnum
/three.pnum# Load the pre-trained MarianMT model and tokenizer for Engl ish to French
/four.pnummodel_name = ’Helsinki-NLP/opus-mt-en-fr’
/five.pnumtokenizer = MarianTokenizer.from_pretrained(model_nam e)
6model = MarianMTModel.from_pretrained(model_name)
In the code above, we load the MarianMT model for English-to- French translation using the model
name ‘Helsinki-NLP/opus-mt-en-fr‘. Hugging Face’s ‘Mari anMTModel‘ and ‘MarianTokenizer‘ classes
allow us to easily work with this translation model.
/four.pnum./one.pnum/seven.pnum./two.pnum./three.pnum Step /three.pnum: Tokenizing the Input Text
Before feeding the text into the model, we need to tokenize it . The tokenizer will convert the inputtext
into a format that the model can understand.
/one.pnum# Example text to translate
/two.pnumtext ="Hugging Face is creating state-of-the-art NLP tools."
/three.pnum
/four.pnum# Tokenize the input text
/five.pnumencoded_text = tokenizer([text], return_tensors= "pt")

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum8/one.pnum
Here,the‘tokenizer‘takestheinputsentenceandconverts itintotensors. The‘return_tensors="pt"‘
argument speciﬁes that the output should be in PyTorch tenso r format.
/four.pnum./one.pnum/seven.pnum./two.pnum./four.pnum Step /four.pnum: Generating the Translation
Now, we can generate the translation using the pre-trained m odel. The model’s output will be the
translation of the input text.
/one.pnum# Generate the translation
/two.pnumtranslated = model.generate(**encoded_text)
/three.pnum
/four.pnum# Decode the generated tokens back to text
/five.pnumtranslated_text = tokenizer.decode(translated[0], skip _special_tokens=True)
6print(translated_text)
The‘model.generate()‘functiongeneratesthetranslated output,andweusethe‘tokenizer.decode()‘
functiontoconvertthetokenizedoutputbackintohuman-re adabletext.The‘skip_special_tokens=True‘
argument ensures that any special tokens (such as padding or end-of-sequence tokens) are not in-
cluded in the ﬁnal output.
/four.pnum./one.pnum/seven.pnum./two.pnum./five.pnum Step /five.pnum: Running the Full Example
Here’s the complete example of how to translate an English se ntence to French using Hugging Face’s
MarianMT model:
/one.pnumfrom transformers import MarianMTModel, MarianTokenizer
/two.pnum
/three.pnum# Load the pre-trained MarianMT model and tokenizer for Engl ish to French
/four.pnummodel_name = ’Helsinki-NLP/opus-mt-en-fr’
/five.pnumtokenizer = MarianTokenizer.from_pretrained(model_nam e)
6model = MarianMTModel.from_pretrained(model_name)
/seven.pnum
8# Example text to translate
/nine.pnumtext ="Hugging Face is creating state-of-the-art NLP tools."
/one.pnum/zero.pnum
/one.pnum/one.pnum# Tokenize the input text
/one.pnum/two.pnumencoded_text = tokenizer([text], return_tensors= "pt")
/one.pnum/three.pnum
/one.pnum/four.pnum# Generate the translation
/one.pnum/five.pnumtranslated = model.generate(**encoded_text)
/one.pnum6
/one.pnum/seven.pnum# Decode the generated tokens back to text
/one.pnum8translated_text = tokenizer.decode(translated[0], skip _special_tokens=True)
/one.pnum/nine.pnum
/two.pnum/zero.pnum# Print the translated text
/two.pnum/one.pnumprint(f"Translated Text: {translated_text}" )
Running this example will print the translated French sente nce:
Translated Text: Hugging Face crée des outils NLP de pointe.

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum8/two.pnum
/four.pnum./one.pnum/seven.pnum./three.pnum Conclusion
In this section, we learned how to performmachine translati on using the Hugging Face Transformers
library. By using pre-trained models like MarianMT, we can e asily translate text between different lan-
guageswith just a few lines of code. This approach can be exte nded to support many language pairs
bysimplychanging themodelnameto thecorresponding pair a vailable in HuggingFace’smodel hub.
/four.pnum./one.pnum/seven.pnum./four.pnum Batch Translation
While translating a single sentence is useful, in real-worl d applications, we often need to translate
entire paragraphs or multiple sentences at once. Hugging Fa ce’s transformers can handle batch pro-
cessing efﬁciently.
/four.pnum./one.pnum/seven.pnum./four.pnum./one.pnum Step /one.pnum: Translating Multiple Sentences
To translate multiple sentences in a batch, we simply need to provide a list of sentences to the tok-
enizer.
/one.pnum# List of sentences to translate
/two.pnumtexts = [
/three.pnum"Hugging Face is a great company." ,
/four.pnum"Machine translation is an exciting field." ,
/five.pnum"Natural language processing is evolving rapidly."
6]
/seven.pnum
8# Tokenize the input sentences
/nine.pnumencoded_texts = tokenizer(texts, return_tensors= "pt", padding=True, truncation=True)
/one.pnum/zero.pnum
/one.pnum/one.pnum# Generate the translations
/one.pnum/two.pnumtranslated_batch = model.generate(**encoded_texts)
/one.pnum/three.pnum
/one.pnum/four.pnum# Decode the translations back to text
/one.pnum/five.pnumtranslated_texts = [tokenizer.decode(t, skip_special_t okens=True) for t in translated_batch]
/one.pnum6
/one.pnum/seven.pnum# Print each translated sentence
/one.pnum8for i, text in enumerate(translated_texts):
/one.pnum/nine.pnumprint(f"Original: {texts[i]}" )
/two.pnum/zero.pnumprint(f"Translated: {text}\n" )
In this code, we pass a list of sentences to the tokenizer. The ‘padding=True‘ argument ensures
that all sentences in the batch are padded to the same length, while ‘truncation=True‘ ensures that
sentences longer than the maximum allowed length are trunca ted.
The output is a list of translated sentences that correspond to each input sentence. This method
allows us to efﬁciently translate multiple sentences at onc e.
/four.pnum./one.pnum/seven.pnum./four.pnum./two.pnum Performance Considerations
When working with batch translation, it’s important to cons ider performance. Processing sentences
in batches, as shown above, is signiﬁcantly faster than tran slating each sentence individually. This is
because it reduces the overhead of running the model multipl e times.

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum8/three.pnum
For very large text datasets, it might also be useful to lever age GPUs or distributed processing to
further accelerate translation.
/four.pnum./one.pnum/seven.pnum./five.pnum Using Other Translation Models
Hugging Face’s model hub provides access to many other pre-t rained translation models. While we
haveusedthe‘opus-mt-en-fr‘modelforEnglishtoFrench,t herearemodelsforvariousotherlanguage
pairs.
/four.pnum./one.pnum/seven.pnum./five.pnum./one.pnum Step /one.pnum: Changing the Model for a Different Language Pair
Suppose we want to translate from English to German. We can ea sily switch the model to ‘opus-mt-
en-de‘ (English to German) by loading the appropriate model name.
/one.pnum# Load the MarianMT model for English to German
/two.pnummodel_name_de = ’Helsinki-NLP/opus-mt-en-de’
/three.pnumtokenizer_de = MarianTokenizer.from_pretrained(model_ name_de)
/four.pnummodel_de = MarianMTModel.from_pretrained(model_name_d e)
/five.pnum
6# Example text to translate
/seven.pnumtext_de = "Machine learning is transforming industries."
8
/nine.pnum# Tokenize the input text
/one.pnum/zero.pnumencoded_text_de = tokenizer_de([text_de], return_tenso rs="pt")
/one.pnum/one.pnum
/one.pnum/two.pnum# Generate the translation
/one.pnum/three.pnumtranslated_de = model_de.generate(**encoded_text_de)
/one.pnum/four.pnum
/one.pnum/five.pnum# Decode the translation
/one.pnum6translated_text_de = tokenizer_de.decode(translated_d e[0], skip_special_tokens=True)
/one.pnum/seven.pnum
/one.pnum8# Print the translated text
/one.pnum/nine.pnumprint(f"Translated Text: {translated_text_de}" )
In this example, we switched the model from ‘opus-mt-en-fr‘ to ‘opus-mt-en-de‘to perform English
to German translation. Hugging Face’s model hub has a wide va riety of language pairs available, and
you can explore them at [Hugging Face Model Hub](https://hugg ingface.co/models).
/four.pnum./one.pnum/seven.pnum.6 Optimization for Large-Scale Translation
For large-scale translation tasks, where we might be workin g with thousands of sentences or entire
documents, certain optimizations can improve efﬁciency.
/four.pnum./one.pnum/seven.pnum.6./one.pnum /one.pnum. Using GPUs for Acceleration
Running the model on a GPU can drastically reduce the time tak en for translation. [ /seven.pnum/five.pnum] Hugging Face
transformers supportGPU acceleration via PyTorch. Here’s how to transfer the model anddata to the
GPU:

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum8/four.pnum
/one.pnumimport torch
/two.pnum
/three.pnum# Check if GPU is available
/four.pnumdevice = torch.device( "cuda"if torch.cuda.is_available() else "cpu")
/five.pnum
6# Load model and tokenizer
/seven.pnummodel_name = ’Helsinki-NLP/opus-mt-en-fr’
8tokenizer = MarianTokenizer.from_pretrained(model_nam e)
/nine.pnummodel = MarianMTModel.from_pretrained(model_name)
/one.pnum/zero.pnum
/one.pnum/one.pnum# Move model to GPU
/one.pnum/two.pnummodel.to(device)
/one.pnum/three.pnum
/one.pnum/four.pnum# Example text
/one.pnum/five.pnumtext ="Hugging Face is creating state-of-the-art NLP tools."
/one.pnum6
/one.pnum/seven.pnum# Tokenize the input text and move tensors to GPU
/one.pnum8encoded_text = tokenizer([text], return_tensors= "pt").to(device)
/one.pnum/nine.pnum
/two.pnum/zero.pnum# Generate the translation
/two.pnum/one.pnumtranslated = model.generate(**encoded_text)
/two.pnum/two.pnum
/two.pnum/three.pnum# Decode and print the translated text
/two.pnum/four.pnumtranslated_text = tokenizer.decode(translated[0], skip _special_tokens=True)
/two.pnum/five.pnumprint(f"Translated Text: {translated_text}" )
In this code, the model is moved to the GPU using the ‘.to(devi ce)‘ function, and the input tensors
are also moved to the GPU for faster processing. By using the G PU, we can signiﬁcantly speed up the
translation process, especially for large datasets.
/four.pnum./one.pnum/seven.pnum.6./two.pnum /two.pnum. Handling Long Texts
Hugging Face models are often trained with a maximum input le ngth, which can lead to truncation
if the text is too long. For longer texts, such as entire docum ents, we need to break the input into
smaller chunks before passing them to the model. Here’s an ex ample of how to split a long text into
manageable chunks for translation:
/one.pnumdef chunk_text(text, max_length=512):
/two.pnum"""Splits long text into smaller chunks."""
/three.pnumreturn [text[i:i+max_length] for i in range(0, len(text), max_length)]
/four.pnum
/five.pnum# Long text example
6long_text = "This is a very long text that exceeds the maximum token lengt h allowed by the model.
"\
/seven.pnum "We will need to break it down into smaller pieces to translat e each part individually.
"
8
/nine.pnum# Split text into chunks
/one.pnum/zero.pnumtext_chunks = chunk_text(long_text, max_length=400)

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum8/five.pnum
/one.pnum/one.pnum
/one.pnum/two.pnum# Translate each chunk
/one.pnum/three.pnumtranslated_chunks = []
/one.pnum/four.pnumfor chunk in text_chunks:
/one.pnum/five.pnumencoded_chunk = tokenizer([chunk], return_tensors= "pt", truncation=True).to(device)
/one.pnum6translated_chunk = model.generate(**encoded_chunk)
/one.pnum/seven.pnumtranslated_text_chunk = tokenizer.decode(translated_c hunk[0], skip_special_tokens=True)
/one.pnum8translated_chunks.append(translated_text_chunk)
/one.pnum/nine.pnum
/two.pnum/zero.pnum# Join the translated chunks back together
/two.pnum/one.pnumfinal_translation = " ".join(translated_chunks)
/two.pnum/two.pnumprint(f"Final Translated Text: {final_translation}" )
In this approach, we use a function to split long texts into sm aller chunks based on the model’s
maximum input length. We translate each chunk separately an d then combine the translated chunks
back into a coherent text. This method ensures that even very large texts can be translated without
losing information due to truncation.
/four.pnum./one.pnum/seven.pnum./seven.pnum Further Exploration
Hugging Face’s transformer models support a wide variety of advanced features and can be ﬁne-
tuned on speciﬁc tasks for even better performance. Users ca n also explore using beam search for
generatinghigher-qualitytranslationsorexperimentwit hothermodelsfordomain-speciﬁctranslation
tasks (e.g., medical, legal, or technical texts).
/four.pnum./one.pnum/seven.pnum./seven.pnum./one.pnum Using Beam Search for Translation
Beamsearchisanadvanceddecodingtechniquethatallowsfo rgeneratingbettertranslationsbycon-
sideringmultiplepossibletranslationsateachstep. Byin creasing thebeamwidth,youcanpotentially
improve the quality of the output, although at the cost of add itional computation time.
Here’s an example of how to use beam search with Hugging Face:
/one.pnum# Generate the translation using beam search
/two.pnumtranslated_beam = model.generate(**encoded_text, num_b eams=5, early_stopping=True)
/three.pnum
/four.pnum# Decode the translated text
/five.pnumtranslated_text_beam = tokenizer.decode(translated_be am[0], skip_special_tokens=True)
6print(f"Translated Text with Beam Search: {translated_text_beam }")
In this code, ‘num_beams=/five.pnum‘ sets the beam width to /five.pnum, which me ans the model will consider /five.pnum
different possible translations at each step and select the best one. This can improve translation
accuracy, especially for complex sentences or less common l anguage pairs.
/four.pnum./one.pnum/seven.pnum.8 Fine-Tuning a Translation Model
AlthoughHuggingFaceprovidesmanypre-trainedtranslati onmodels,theremightbecaseswherethe
speciﬁc translation task requires domain-speciﬁc knowled ge or improved performance. Fine-tuning
allowsustoadaptapre-trainedmodeltoacustomdataset,en suringthatthemodellearnsthenuances
of the speciﬁc translation task.

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum86
/four.pnum./one.pnum/seven.pnum.8./one.pnum Step /one.pnum: Preparing a Custom Dataset
Toﬁne-tuneatranslationmodel,weﬁrstneedaparalleldata set—onethatcontainspairsofsentences
in the source and target languages. Datasets for translatio n typically follow a structure where each
entry consists of a sentence in the source language and its eq uivalent in the target language.
For example, let’s assume we have a dataset ‘custom_dataset .csv‘ with two columns: ‘source‘
(English) and ‘target‘ (French).
/one.pnumimport pandas as pd
/two.pnum
/three.pnum# Load the dataset
/four.pnumdata = pd.read_csv( ’custom_dataset.csv’ )
/five.pnum
6# Display the first few rows
/seven.pnumprint(data.head())
The dataset should look something like this:
| source | target |
|-----------------------------------|-------------- ---------------------|
| "Hello, how are you?" | "Bonjour, comment allez-vous ?" |
| "I am learning NLP." | "J /quotesingle.Varapprends le traitement du langage naturel." |
/four.pnum./one.pnum/seven.pnum.8./two.pnum Step /two.pnum: Tokenizing the Dataset
The nextstep isto tokenize thesource andtarget sentencesu sing the tokenizer corresponding to the
pre-trained model. In this example, we will continue using t he ‘MarianMTModel‘ and tokenizer.
/one.pnumfrom transformers import MarianTokenizer
/two.pnum
/three.pnum# Load the tokenizer
/four.pnumtokenizer = MarianTokenizer.from_pretrained( ’Helsinki-NLP/opus-mt-en-fr’ )
/five.pnum
6# Tokenize the source and target texts
/seven.pnumsource_texts = list(data[ ’source’ ])
8target_texts = list(data[ ’target’ ])
/nine.pnum
/one.pnum/zero.pnum# Tokenize the texts
/one.pnum/one.pnumsource_encodings = tokenizer(source_texts, truncation= True, padding=True, return_tensors= ’pt’)
/one.pnum/two.pnumtarget_encodings = tokenizer(target_texts, truncation= True, padding=True, return_tensors= ’pt’)
Here, we tokenize both the source and target texts. The ‘retu rn_tensors=’pt’‘ option converts the
tokenized text into PyTorch tensors, which are required for training the model.
/four.pnum./one.pnum/seven.pnum.8./three.pnum Step /three.pnum: Fine-Tuning the Model
Once we have tokenized the dataset, we can ﬁne-tune the model . Hugging Face’s ‘Trainer‘ API simpli-
ﬁes this process. The ﬁrst step is to deﬁne a ‘Dataset‘ object that wraps our tokenized data.
/one.pnumfrom torch.utils.data import Dataset
/two.pnum

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum8/seven.pnum
/three.pnum# Create a custom dataset class
/four.pnumclass TranslationDataset(Dataset):
/five.pnumdef __init__(self, source_encodings, target_encodings) :
6 self.source_encodings = source_encodings
/seven.pnum self.target_encodings = target_encodings
8
/nine.pnumdef __len__(self):
/one.pnum/zero.pnum return len(self.source_encodings[ ’input_ids’ ])
/one.pnum/one.pnum
/one.pnum/two.pnumdef __getitem__(self, idx):
/one.pnum/three.pnum return {
/one.pnum/four.pnum ’input_ids’ : self.source_encodings[ ’input_ids’ ][idx],
/one.pnum/five.pnum ’attention_mask’ : self.source_encodings[ ’attention_mask’ ][idx],
/one.pnum6 ’labels’ : self.target_encodings[ ’input_ids’ ][idx]
/one.pnum/seven.pnum }
/one.pnum8
/one.pnum/nine.pnum# Create dataset
/two.pnum/zero.pnumtrain_dataset = TranslationDataset(source_encodings, t arget_encodings)
The ‘TranslationDataset‘ class organizes the tokenized in put data and target translations, making
it suitable for the Hugging Face ‘Trainer‘ API.
Now, we can use the ‘Trainer‘ to ﬁne-tune the model:
/one.pnumfrom transformers import MarianMTModel, Trainer, Trainin gArguments
/two.pnum
/three.pnum# Load the MarianMT model
/four.pnummodel = MarianMTModel.from_pretrained( ’Helsinki-NLP/opus-mt-en-fr’ )
/five.pnum
6# Define training arguments
/seven.pnumtraining_args = TrainingArguments(
8output_dir= ’./results’ ,
/nine.pnumper_device_train_batch_size=16,
/one.pnum/zero.pnumnum_train_epochs=3,
/one.pnum/one.pnumsave_steps=500,
/one.pnum/two.pnumsave_total_limit=2,
/one.pnum/three.pnumevaluation_strategy= "epoch"
/one.pnum/four.pnum)
/one.pnum/five.pnum
/one.pnum6# Initialize the Trainer
/one.pnum/seven.pnumtrainer = Trainer(
/one.pnum8model=model,
/one.pnum/nine.pnumargs=training_args,
/two.pnum/zero.pnumtrain_dataset=train_dataset
/two.pnum/one.pnum)
/two.pnum/two.pnum
/two.pnum/three.pnum# Fine-tune the model
/two.pnum/four.pnumtrainer.train()
In this block, we conﬁgure the training process using ‘Train ingArguments‘. Wespecify parameters
such as batch size, the number of epochs, and when to save mode l checkpoints. The ‘Trainer‘ API

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum88
then takes care of the training loop, allowing the model to le arn the translation task using our custom
dataset.
/four.pnum./one.pnum/seven.pnum.8./four.pnum Step /four.pnum: Saving the Fine-Tuned Model
After ﬁne-tuning, we can save the model to reuse it later for t ranslation tasks.
/one.pnum# Save the fine-tuned model and tokenizer
/two.pnummodel.save_pretrained( ’./fine-tuned-marian-en-fr’ )
/three.pnumtokenizer.save_pretrained( ’./fine-tuned-marian-en-fr’ )
This command saves both the model and the tokenizer to a speci ﬁed directory. We can later load
this ﬁne-tuned model for further use, as we did with the pre-t rained model.
/four.pnum./one.pnum/seven.pnum./nine.pnum Evaluating Translation Quality
Once we have a ﬁne-tuned model or even when using pre-trained models, it is important to evaluate
thequality oftranslations. Oneof themostcommon metricsf or evaluating machinetranslationis the
BLEU (Bilingual Evaluation Understudy) score.
/four.pnum./one.pnum/seven.pnum./nine.pnum./one.pnum Step /one.pnum: Installing the sacrebleu Library
To compute the BLEU score [ /nine.pnum/zero.pnum], we will use the ‘sacrebleu‘ library, which is a standard too l for evalu-
ation in machine translation tasks.
/one.pnumpip install sacrebleu
/four.pnum./one.pnum/seven.pnum./nine.pnum./two.pnum Step /two.pnum: Computing the BLEU Score
Once the library is installed, we can evaluate the quality of our translations. Assume we have a list of
reference translations (ground truth) and a list of transla ted outputs from our model.
/one.pnumimport sacrebleu
/two.pnum
/three.pnum# Example list of reference translations and generated tran slations
/four.pnumreferences = [ "Bonjour, comment allez-vous ?" ,"J’apprends le traitement du langage naturel." ]
/five.pnumtranslations = [ "Bonjour, comment allez-vous ?" ,"Je suis en train d’apprendre le NLP." ]
6
/seven.pnum# Calculate BLEU score
8bleu = sacrebleu.corpus_bleu(translations, [references ])
/nine.pnumprint(f"BLEU score: {bleu.score}" )
In this example, the ‘sacrebleu.corpus_bleu()‘ function c omputes the BLEU score between the ref-
erencetranslationsandthemodel-generatedtranslations . AhigherBLEUscore indicatesbettertrans-
lation quality.
/four.pnum./one.pnum/seven.pnum./nine.pnum./three.pnum Limitations of BLEU Score
While the BLEU score is widely used, it has its limitations. F or example, BLEU relies on exact n-gram
matches, which means that valid translations that use synon yms or different phrasing might receive

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum8/nine.pnum
a lower score. Therefore, it is often helpful to combine BLEU with other evaluation metrics such as
METEOR or TER (Translation Error Rate) for a more comprehens ive evaluation.
/four.pnum./one.pnum/seven.pnum./one.pnum/zero.pnum Advanced Decoding Techniques
In addition to beam search, there are other advanced decodin g strategies that can be employed to
improve translation results. Two such techniques are top-k sampling and top-p sampling (nucleus
sampling), which introduce stochasticity into the generat ion process.
/four.pnum./one.pnum/seven.pnum./one.pnum/zero.pnum./one.pnum /one.pnum. Top-k Sampling
Top-k sampling limits the model to selecting from the top-k m ost likely words at each step during
decoding, reducing the risk of selecting low-probability w ords.
/one.pnum# Generate translation using top-k sampling
/two.pnumtranslated_topk = model.generate(**encoded_text, do_sa mple=True, top_k=50)
/three.pnum
/four.pnum# Decode the generated text
/five.pnumtranslated_text_topk = tokenizer.decode(translated_to pk[0], skip_special_tokens=True)
6print(f"Top-k Sampling Translation: {translated_text_topk}" )
In this code, ‘top_k=/five.pnum/zero.pnum‘ restricts the model to selecting fr om the top /five.pnum/zero.pnum candidate words at each
decoding step.
/four.pnum./one.pnum/seven.pnum./one.pnum/zero.pnum./two.pnum /two.pnum. Top-p (Nucleus) Sampling
Top-p sampling, or nucleus sampling, is another approach wh ere the model considers the smallest
possible set of words whose cumulative probability exceeds a threshold ‘p‘.
/one.pnum# Generate translation using top-p sampling
/two.pnumtranslated_topp = model.generate(**encoded_text, do_sa mple=True, top_p=0.9)
/three.pnum
/four.pnum# Decode the generated text
/five.pnumtranslated_text_topp = tokenizer.decode(translated_to pp[0], skip_special_tokens=True)
6print(f"Top-p Sampling Translation: {translated_text_topp}" )
Here, ‘top_p=/zero.pnum./nine.pnum‘ instructs the model to select from the sma llest set of words whose cumulative
probability is /nine.pnum/zero.pnum
Both top-k and top-p sampling introduce more diversity into the translation process, which can be
useful in creative tasks or for generating multiple possibl e translations for ambiguous input.
/four.pnum./one.pnum/seven.pnum./one.pnum/one.pnum Distributed Training for Large Datasets
When working with extremely large datasets or when ﬁne-tuni ng a model on high-resource tasks, dis-
tributed training can be used to accelerate the process. Dis tributed training splits the computation
across multiple GPUs or even multiple machines, reducing tr aining time signiﬁcantly. [ /nine.pnum/one.pnum]

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum/nine.pnum/zero.pnum
/four.pnum./one.pnum/seven.pnum./one.pnum/one.pnum./one.pnum Step /one.pnum: Installing Required Libraries
Hugging Face supports distributed training out of the box us ing PyTorch’s distributed data-parallel
(DDP) and the ‘transformers‘ library’s native support for m ulti-GPU training.
Togetstarted,ensureyouhavethenecessaryenvironmentby installing‘accelerate‘,atoolprovided
by Hugging Face for distributed computing:
/one.pnumpip install accelerate
/four.pnum./one.pnum/seven.pnum./one.pnum/one.pnum./two.pnum Step /two.pnum: Conﬁguring Accelerate for Distributed Training
Once thelibrary is installed, you can conﬁgure ‘accelerate ‘ for distributedtraining. The tool allows you
to set up your environment, choose how many GPUs you want to us e, and manage other aspects of
distributed training.
Run the following command to set up the conﬁguration:
/one.pnumaccelerate config
This will prompt you to answer a series of questions about you r hardware setup (e.g., how many
GPUs you have, if you’re using mixed precision training, etc .). After this, you can use ‘accelerate‘ to
launch your training script across multiple devices.
/four.pnum./one.pnum/seven.pnum./one.pnum/one.pnum./three.pnum Step /three.pnum: Launching Distributed Training
To launch your translation ﬁne-tuning process across multi ple GPUs or machines, you can use the
following command:
/one.pnumaccelerate launch train_translation.py
In this case, ‘train_translation.py‘ would be your ﬁne-tun ing script (as described earlier). Hugging
Face’s ‘Trainer‘ class automatically adapts to distribute d setups, so no changes are required to the
ﬁne-tuning script itself.
/four.pnum./one.pnum/seven.pnum./one.pnum/two.pnum Multilingual Translation
Translationtasksofteninvolvemorethanjusttwolanguage s. Forexample,youmightneedtotranslate
between many different language pairs. Hugging Face provid es several multilingual models that can
handle multiple language pairs using a single model.
/four.pnum./one.pnum/seven.pnum./one.pnum/two.pnum./one.pnum Step /one.pnum: Using MarianMT for Multilingual Translation
MarianMT [ /nine.pnum/two.pnum] is one such model capable of translating between multiple l anguage pairs. Here’s an
example of translating between English and several other la nguages using the same model.
/one.pnumfrom transformers import MarianMTModel, MarianTokenizer
/two.pnum
/three.pnum# Load the MarianMT model for multilingual translation
/four.pnummodel_name = ’Helsinki-NLP/opus-mt-en-ROMANCE’
/five.pnumtokenizer = MarianTokenizer.from_pretrained(model_nam e)
6model = MarianMTModel.from_pretrained(model_name)

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum/nine.pnum/one.pnum
/seven.pnum
8# Example sentences in different Romance languages
/nine.pnumtexts = [
/one.pnum/zero.pnum"Hello, how are you?" ,# English
/one.pnum/one.pnum"Ciao, come stai?" ,# Italian
/one.pnum/two.pnum]
/one.pnum/three.pnum
/one.pnum/four.pnum# Tokenize the input sentences
/one.pnum/five.pnumencoded_texts = tokenizer(texts, return_tensors= "pt", padding=True)
/one.pnum6
/one.pnum/seven.pnum# Generate translations (back to English)
/one.pnum8translated_batch = model.generate(**encoded_texts)
/one.pnum/nine.pnum
/two.pnum/zero.pnum# Decode and print each translation
/two.pnum/one.pnumtranslated_texts = [
/two.pnum/two.pnumtokenizer.decode(t, skip_special_tokens=True)
/two.pnum/three.pnumfor t in translated_batch
/two.pnum/four.pnum]
/two.pnum/five.pnumprint("Translated Texts: " , translated_texts)
In this example, the ‘opus-mt-en-ROMANCE‘ model is capable of translating between English and
Italian. Using multilingual models like this is convenient because it eliminates the need to load sepa-
rate models for each language pair.
/four.pnum./one.pnum/seven.pnum./one.pnum/two.pnum./two.pnum Step /two.pnum: Exploring mBART for Multilingual Translation
Another powerful multilingual model is ‘mBART‘ (Multiling ual BART), which is designed to perform
translation between various languages without needing tas k-speciﬁc ﬁne-tuning for each pair.
Here’s how to use mBART for translation:
/one.pnumfrom transformers import MBartForConditionalGeneration , MBart50Tokenizer
/two.pnum
/three.pnum# Load the mBART model and tokenizer
/four.pnummodel_name = ’facebook/mbart-large-50-many-to-many-mmt’
/five.pnumtokenizer = MBart50Tokenizer.from_pretrained(model_na me)
6model = MBartForConditionalGeneration.from_pretrained (model_name)
/seven.pnum
8# Set the target language
/nine.pnumtokenizer.src_lang = "en_XX"
/one.pnum/zero.pnumtarget_lang = "fr_XX" # Target language set to French
/one.pnum/one.pnum
/one.pnum/two.pnum# Example sentence in English
/one.pnum/three.pnumtext ="Hugging Face is creating the best translation models."
/one.pnum/four.pnum
/one.pnum/five.pnum# Tokenize and generate translation
/one.pnum6encoded_text = tokenizer(text, return_tensors= "pt")
/one.pnum/seven.pnumtranslated_tokens = model.generate(encoded_text, force d_bos_token_id=tokenizer.lang_code_to_id[
target_lang])
/one.pnum8
/one.pnum/nine.pnum# Decode the translation

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum/nine.pnum/two.pnum
/two.pnum/zero.pnumtranslated_text = tokenizer.decode(translated_tokens[ 0], skip_special_tokens=True)
/two.pnum/one.pnumprint(f"Translated to French: {translated_text}" )
Inthisexample,weusethe‘mBART/five.pnum/zero.pnum‘modelformultilingual translation,wherethe‘src_lang‘isEn-
glishandthe‘target_lang‘isFrench. mBARTsupportstrans lationsbetweenmanydifferentlanguages,
making it ideal for tasks involving multiple language pairs .
/four.pnum./one.pnum/seven.pnum./one.pnum/three.pnum Deploying a Translation Model as an API
In real-world applications, you mightwant to exposeyour tr anslation modelas anAPI so thatusersor
systemscansendrequestsandreceive translatedtextinrea ltime. Wecanachievethisusingpopular
web frameworks like Flask.
/four.pnum./one.pnum/seven.pnum./one.pnum/three.pnum./one.pnum Step /one.pnum: Installing Flask
To begin, install Flask if it’s not already installed:
/one.pnumpip install flask
/four.pnum./one.pnum/seven.pnum./one.pnum/three.pnum./two.pnum Step /two.pnum: Creating a Simple Flask API for Translation
Here’s how you can create a basic Flask API that uses Hugging F ace’s translation model to translate
incoming text.
/one.pnumfrom flask import Flask, request, jsonify
/two.pnumfrom transformers import MarianMTModel, MarianTokenizer
/three.pnum
/four.pnum# Initialize Flask app
/five.pnumapp = Flask(__name__)
6
/seven.pnum# Load the MarianMT model for translation (English to French )
8model_name = ’Helsinki-NLP/opus-mt-en-fr’
/nine.pnumtokenizer = MarianTokenizer.from_pretrained(model_nam e)
/one.pnum/zero.pnummodel = MarianMTModel.from_pretrained(model_name)
/one.pnum/one.pnum
/one.pnum/two.pnum# Define the translation route
/one.pnum/three.pnum@app.route( ’/translate’ , methods=[ ’POST’])
/one.pnum/four.pnumdef translate_text():
/one.pnum/five.pnumdata = request.get_json()
/one.pnum6text = data[ ’text’]
/one.pnum/seven.pnum
/one.pnum8# Tokenize the input text
/one.pnum/nine.pnumencoded_text = tokenizer([text], return_tensors= "pt")
/two.pnum/zero.pnum
/two.pnum/one.pnum# Generate the translation
/two.pnum/two.pnumtranslated = model.generate(**encoded_text)
/two.pnum/three.pnum
/two.pnum/four.pnum# Decode the translation
/two.pnum/five.pnumtranslated_text = tokenizer.decode(translated[0], skip _special_tokens=True)

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum/nine.pnum/three.pnum
/two.pnum6
/two.pnum/seven.pnum# Return the translated text as JSON
/two.pnum8return jsonify({ "translated_text" : translated_text})
/two.pnum/nine.pnum
/three.pnum/zero.pnum# Run the app
/three.pnum/one.pnumif __name__ == ’__main__’ :
/three.pnum/two.pnumapp.run(debug=True)
ThissimpleFlaskappcreatesanAPIendpoint‘/translate‘, whichacceptsPOSTrequestswithJSON
data. The input text is tokenized, passed through the Marian MT translation model, and the translated
text is returned in JSON format.
/four.pnum./one.pnum/seven.pnum./one.pnum/three.pnum./three.pnum Step /three.pnum: Testing the Translation API
To test the API, you can use ‘curl‘ or any API testing tool such as Postman. Here’s an example of how
to use ‘curl‘ to test the API:
/one.pnumcurl -X POST http://127.0.0.1:5000/translate -H "Content-Type: application/json" -d’{"text": "
Hello, how are you?"}’
The API will return the translated text in JSON format:
{
"translated_text": "Bonjour, comment allez-vous ?"
}
/four.pnum./one.pnum/seven.pnum./one.pnum/three.pnum./four.pnum Step /four.pnum: Deploying to the Cloud
Once you have your Flask API working locally, you can deploy i t to a cloud platform such as AWS,
Google Cloud, or Heroku for production use. Each platform ha s its own deployment procedures, but
the basic principle remains the same: expose your translati on model as an API that can be accessed
remotely.
/four.pnum./one.pnum/seven.pnum./one.pnum/four.pnum Integrating Translation into Web Applications
Another common application for machine translation is embe dding it directly into web applications.
Using JavaScript and modern web development frameworks, yo u can create a dynamic, real-time
translation experience for users.
/four.pnum./one.pnum/seven.pnum./one.pnum/four.pnum./one.pnum Step /one.pnum: Creating a Front-End Interface
You can use HTML, CSS, and JavaScript to create a simple web in terface where users can input text
and receive translations.
Here’s an example of a basic HTML form that allows users to inp ut text for translation:
/one.pnum<!DOCTYPE html>
/two.pnum<html lang= "en">
/three.pnum<head>
/four.pnum<meta charset= "UTF-8">
/five.pnum<meta name= "viewport" content= "width=device-width, initial-scale=1.0" >

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum/nine.pnum/four.pnum
6<title>Translation App</title>
/seven.pnum</head>
8<body>
/nine.pnum<h1>Translation App</h1>
/one.pnum/zero.pnum<textarea id= "inputText" rows="4"cols="50"placeholder= "Enter text here..." ></textarea>
/one.pnum/one.pnum<br>
/one.pnum/two.pnum<button onclick= "translateText()" >Translate</button>
/one.pnum/three.pnum<p id="outputText" ></p>
/one.pnum/four.pnum
/one.pnum/five.pnum<script>
/one.pnum6 async function translateText() {
/one.pnum/seven.pnum const text = document.getElementById(’inputText’).valu e;
/one.pnum8 const response = await fetch(’http://127.0.0.1:5000/tra nslate’, {
/one.pnum/nine.pnum method: ’POST’,
/two.pnum/zero.pnum headers: {
/two.pnum/one.pnum ’Content-Type’: ’application/json’,
/two.pnum/two.pnum },
/two.pnum/three.pnum body: JSON.stringify({text: text}),
/two.pnum/four.pnum });
/two.pnum/five.pnum const result = await response.json();
/two.pnum6 document.getElementById(’outputText’).innerText = "Translated Text: " + result.
translated_text;
/two.pnum/seven.pnum }
/two.pnum8</script>
/two.pnum/nine.pnum</body>
/three.pnum/zero.pnum</html>
In this simple HTML/JavaScript interface, the user inputs t ext in a text area and clicks the "Trans-
late" button. This triggers an API request to the translatio n Flask API, and the translated text is dis-
played on the page.
/four.pnum./one.pnum/seven.pnum./one.pnum/four.pnum./two.pnum Step /two.pnum: Running the Web Application
To run this web application, ensure that your Flask API is run ning in the background, then open the
HTML ﬁle in a browser. When a user inputs text and clicks the "T ranslate" button, the browser will
send the text to the API and display the translated result.
/four.pnum./one.pnum/seven.pnum./one.pnum/five.pnum Conclusion
Inthissection,wecoveredseveraladvancedtopicsinmachi netranslationusingHuggingFace’sTrans-
formerslibrary. Wediscussedhowtoperformdistributedtr ainingforlargedatasets,multilingualtrans-
lationusingmodelslikeMarianMT andmBART,deploying tran slationmodelsasAPIsusingFlask,and
integratingtranslationintowebapplications. Thesetopi csequipyouwithpracticalknowledgetobuild
and deploy large-scale translation systems for real-world applications.
By leveraging these techniques, you can deploy scalable, ef ﬁcient, and accurate translation solu-
tions tailored to speciﬁc needs and domains.

