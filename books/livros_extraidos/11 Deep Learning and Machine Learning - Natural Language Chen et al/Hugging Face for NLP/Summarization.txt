CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum/nine.pnum/five.pnum
/four.pnum./one.pnum8 Summarization
Summarization is a key task in Natural Language Processing ( NLP) that involves condensing a large
body of text into a shorter, coherent version while preservi ng the main ideas. It can be broadly cate-
gorized into two types: extractive and abstractive summari zation.
•Extractive Summarization : This method selects important sentences directly from the original
text. It essentially "extracts" key phrases or sentences, m aintaining the exact wording of the
original.
•AbstractiveSummarization : Thisapproachgeneratesnewsentencesthatcapturethecor einfor-
mation of theoriginal text, muchlike how humanssummarize c ontent. It can involve rephrasing
and using new words.
With the rise of deep learning, especially transformer mode ls, summarization tasks have been
signiﬁcantly improved. Transformer-based models, such as BART (Bidirectional and Auto-Regressive
Transformers) and T/five.pnum (Text-to-Text Transfer Transformer) , are widely used for this purpose.
Inthissection,wewillfocusonhowtoperformsummarizatio nusingtheHuggingFaceTransform-
erslibrary,awidelyusedframeworkthatprovidesaccessto pre-trainedmodelsforvariousNLPtasks.
We will walk through an example using a pre-trained model to g enerate a summary of a given text.
/four.pnum./one.pnum8./one.pnum Using Hugging Face for Summarization
Hugging Face provides a simple and efﬁcient interface to use state-of-the-art models for summariza-
tion. We’llusethe‘transformers‘library,whichincludes severalpre-trainedmodelsreadytobeusedfor
summarization tasks. In this example, we will utilize the ‘p ipeline‘ function to create a summarization
pipeline.
First, ensure you have the necessary libraries installed. Y ou can install them using the following
command:
/one.pnumpip install transformers
/four.pnum./one.pnum8./one.pnum./one.pnum Step-by-Step Guide to Summarization Using Hugging Face
We will now demonstrate how to perform summarization using t he Hugging Face ‘pipeline‘ and a pre-
trained model like BART. The steps are as follows:
/one.pnum.Import necessary libraries : We’ll start by importing the necessary libraries.
/two.pnum.Initializethesummarizationpipeline : Wewillcreateasummarizationpipelineusingapre-traine d
model.
/three.pnum.Provide input text : The text that needs to be summarized.
/four.pnum.Generate the summary : Finally, we will use the pipeline to generate a summary of th e text.
Here is the Python code for summarizing text:

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum/nine.pnum6
/one.pnum# Step 1: Import necessary libraries
/two.pnumfrom transformers import pipeline
/three.pnum
/four.pnum# Step 2: Initialize the summarization pipeline
/five.pnumsummarizer = pipeline( "summarization" , model= "facebook/bart-large-cnn" )
6
/seven.pnum# Step 3: Provide input text
8text ="""
/nine.pnumNatural Language Processing (NLP) is a sub-field of artific ial intelligence (AI) focused on the
interaction between computers and humans through natural l anguage.
/one.pnum/zero.pnumThe ultimate objective of NLP is to enable computers to under stand, interpret, and generate human
languages in a way that is valuable.
/one.pnum/one.pnumBy utilizing NLP, machines can read text, hear speech, inter pret it, and even gauge sentiments. As
AI and machine learning continue to grow,
/one.pnum/two.pnumNLP has become a crucial aspect of technology development, e specially in areas such as chatbots,
translators, and voice recognition systems.
/one.pnum/three.pnum"""
/one.pnum/four.pnum
/one.pnum/five.pnum# Step 4: Generate the summary
/one.pnum6summary = summarizer(text, max_length=50, min_length=25 , do_sample=False)
/one.pnum/seven.pnum
/one.pnum8# Print the summary
/one.pnum/nine.pnumprint(summary[0][ ’summary_text’ ])
/four.pnum./one.pnum8./one.pnum./two.pnum Explanation of the Code
•Step /one.pnum: We import the ‘pipeline‘ function from the ‘transformers‘ library. This function helps to
quickly create a pipeline for different tasks, including su mmarization.
•Step/two.pnum: The‘pipeline‘functionisusedtoinitializeasummarizat ionpipeline.Wespecifythemodel
‘facebook/bart-large-cnn‘,whichisapre-trainedBARTmo deldesignedforsummarizationtasks.
•Step /three.pnum: We deﬁne the input text that we want to summarize. This text c an be a long document,
a news article, or any other form of content.
•Step /four.pnum: We call the ‘summarizer‘ object to generate a summary of the input text. We specify
parameters like ‘max_length‘ (the maximum length of the sum mary) and ‘min_length‘ (the mini-
mumlengthofthesummary)tocontrolthesizeoftheoutput. S etting‘do_sample=False‘ensures
deterministic results, meaning the output will be the same e ach time.
/four.pnum./one.pnum8./one.pnum./three.pnum Output Example
For the provided text, the model might generate a summary suc h as the following:
"NLP enables computers to understand and generate human lan guages, helping develop technologies like chatbots, 
This summary is a concise version of the original text, captu ring the key points.

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum/nine.pnum/seven.pnum
/four.pnum./one.pnum8./two.pnum Customizing the Summarization Pipeline
You can customize the summarization pipeline by adjusting p arameters such as the length of the
summary. For instance, you can increase the ‘max_length‘ to allow for a longer summary or reduce
the ‘min_length‘ to create a more concise version. Here’s an example where we modify the length
constraints:
/one.pnumsummary = summarizer(text, max_length=80, min_length=30 , do_sample=False)
Inthisexample,weallowthemodeltogenerateasummaryofup to8/zero.pnumwords, whichcouldcapture
more details from the original text.
/four.pnum./one.pnum8./three.pnum Conclusion
Inthissection,weexploredhowtouseapre-trainedtransfo rmermodeltoperformtextsummarization
withtheHuggingFacelibrary. Summarizationisanimportan ttaskinNLP,andtransformer-basedmod-
els like BART make it much easier to generate meaningful summ aries from large texts. By following
the steps outlined above, you can summarize text with minima l effort and high accuracy.
For further customization, Hugging Face also allows you to ﬁ ne-tune these models on speciﬁc
datasets, providing even more control over the summarizati on process.
/four.pnum./one.pnum8./four.pnum Fine-tuning a Pre-trained Model for Summarization
While using pre-trained models like BART or T/five.pnum is effective, there are cases where you might want
to ﬁne-tune these models on speciﬁc datasets for improved pe rformance on domain-speciﬁc tasks.
Fine-tuning involves training a pre-trained model on a smal ler, task-speciﬁc dataset, which helps the
model adapt to particular language patterns or styles.
In this section, we will walk through the basic steps to ﬁne-t une a pre-trained transformer model
for summarization tasks using the Hugging Face library.
/four.pnum./one.pnum8./four.pnum./one.pnum Step /one.pnum: Prepare the Dataset
To ﬁne-tune a model, you will need a dataset with text-summar y pairs. Popular datasets used for
summarization include:
•CNN/DailyMail [/nine.pnum/three.pnum]: A dataset of news articles with human-written summaries.
•XSum: Another news dataset with single-sentence summaries.
•SAMSum [/nine.pnum/four.pnum]: A dataset of human conversations and their summaries.
For demonstration, let’s assume we are working with the CNN/ DailyMail dataset. You can easily
load this dataset using the ‘datasets‘ library from Hugging Face:
/one.pnumfrom datasets import load_dataset
/two.pnum
/three.pnum# Load the CNN/DailyMail dataset
/four.pnumdataset = load_dataset( "cnn_dailymail" ,"3.0.0")
Thisdatasetcontainsmultipleﬁelds,butweareparticular lyinterestedinthe‘article‘and‘highlights‘
ﬁelds. The ‘article‘ is the long-form text, while the ‘highl ights‘ serve as the summary.

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum/nine.pnum8
/four.pnum./one.pnum8./four.pnum./two.pnum Step /two.pnum: Tokenize the Data
Transformer models require tokenized inputs. The Hugging F ace tokenizer will convert the raw text
into tokens that the model can understand. Here’s how we can t okenize the dataset:
/one.pnumfrom transformers import AutoTokenizer
/two.pnum
/three.pnum# Initialize the tokenizer for the BART model
/four.pnumtokenizer = AutoTokenizer.from_pretrained( "facebook/bart-large-cnn" )
/five.pnum
6# Tokenize the dataset
/seven.pnumdef tokenize_data(example):
8return tokenizer(example[ ’article’ ], truncation=True, padding= ’max_length’ , max_length=1024)
/nine.pnum
/one.pnum/zero.pnum# Apply the tokenization to the dataset
/one.pnum/one.pnumtokenized_dataset = dataset.map(tokenize_data, batched =True)
In this example, we use the ‘AutoTokenizer‘ class to automat ically load the appropriate tokenizer
fortheBARTmodel. Wealsolimitthelengthoftheinputartic lesto/one.pnum/zero.pnum/two.pnum/four.pnumtokenstoﬁtwithinthemodel’s
capacity.
/four.pnum./one.pnum8./four.pnum./three.pnum Step /three.pnum: Set up the Data Collator
When ﬁne-tuning a model, we often need to dynamically pad the input data to the same length. Hug-
ging Face provides a ‘DataCollatorForSeq/two.pnumSeq‘ class that h elps with this:
/one.pnumfrom transformers import DataCollatorForSeq2Seq
/two.pnum
/three.pnum# Set up the data collator
/four.pnumdata_collator = DataCollatorForSeq2Seq(tokenizer=toke nizer, model= "facebook/bart-large-cnn" )
This data collator will handle padding of sequences during t raining.
/four.pnum./one.pnum8./four.pnum./four.pnum Step /four.pnum: Initialize the Model
Next, we initialize the pre-trained BART model. In this case , we load the ‘facebook/bart-large-cnn‘
model and set it up for sequence-to-sequence tasks (like sum marization):
/one.pnumfrom transformers import AutoModelForSeq2SeqLM
/two.pnum
/three.pnum# Load the pre-trained BART model
/four.pnummodel = AutoModelForSeq2SeqLM.from_pretrained( "facebook/bart-large-cnn" )
/four.pnum./one.pnum8./four.pnum./five.pnum Step /five.pnum: Train the Model
We are now ready to train the model. Hugging Face provides the ‘Trainer‘ API, which simpliﬁes the
training process. Here’s how you can set up and start the trai ning:
/one.pnumfrom transformers import Trainer, TrainingArguments
/two.pnum
/three.pnum# Set training arguments

CHAPTER /four.pnum. HUGGING FACE FOR NLP /one.pnum/nine.pnum/nine.pnum
/four.pnumtraining_args = TrainingArguments(
/five.pnumoutput_dir= "./results" ,
6evaluation_strategy= "epoch",
/seven.pnumlearning_rate=2e-5,
8per_device_train_batch_size=2,
/nine.pnumper_device_eval_batch_size=2,
/one.pnum/zero.pnumnum_train_epochs=3,
/one.pnum/one.pnumweight_decay=0.01,
/one.pnum/two.pnum)
/one.pnum/three.pnum
/one.pnum/four.pnum# Initialize the Trainer
/one.pnum/five.pnumtrainer = Trainer(
/one.pnum6model=model,
/one.pnum/seven.pnumargs=training_args,
/one.pnum8train_dataset=tokenized_dataset[ ’train’],
/one.pnum/nine.pnumeval_dataset=tokenized_dataset[ ’validation’ ],
/two.pnum/zero.pnumdata_collator=data_collator,
/two.pnum/one.pnum)
/two.pnum/two.pnum
/two.pnum/three.pnum# Start training
/two.pnum/four.pnumtrainer.train()
Explanation of Training Parameters :
•output_dir : This is where the model’s checkpoints and logs will be saved .
•evaluation_strategy : This speciﬁes whento evaluate the model (in thiscase, at th eend of every
epoch).
•learning_rate : The learning rate for the optimizer.
•per_device_train_batch_size andper_device_eval_batch_size : Thesedeﬁnethebatchsizesdur-
ing training and evaluation, respectively.
•num_train_epochs : The number of epochs to train the model.
•weight_decay : This helps in regularizing the model to prevent overﬁtting .
/four.pnum./one.pnum8./four.pnum.6 Step 6: Evaluate the Model
Afterthemodelistrained, it’simportantto evaluateitspe rformanceonthevalidationdataset. Wecan
do this using the same ‘Trainer‘ object by calling the ‘evalu ate()‘ method:
/one.pnum# Evaluate the model
/two.pnumeval_results = trainer.evaluate()
/three.pnum
/four.pnumprint(f"Evaluation results: {eval_results}" )
Thiswilloutputmetricslikelossandaccuracy,helpingusu nderstandhowwellthemodelperforms
on the unseen validation data.

CHAPTER /four.pnum. HUGGING FACE FOR NLP /two.pnum/zero.pnum/zero.pnum
/four.pnum./one.pnum8./four.pnum./seven.pnum Step /seven.pnum: Generate Summaries
Once the model is ﬁne-tuned, you can use it to generate summar ies from new articles:
/one.pnum# Example article for summarization
/two.pnumarticle = """
/three.pnumThe field of natural language processing (NLP) has seen sign ificant progress in recent years.
/four.pnumFrom the introduction of deep learning models like transfor mers to advancements in tasks such as
translation, summarization, and text generation,
/five.pnumNLP continues to evolve rapidly.
6"""
/seven.pnum
8# Generate a summary using the fine-tuned model
/nine.pnuminputs = tokenizer(article, return_tensors= "pt", truncation=True, padding= "max_length" , max_length
=1024)
/one.pnum/zero.pnum
/one.pnum/one.pnumsummary_ids = model.generate(inputs[ ’input_ids’ ], max_length=150, min_length=40, length_penalty
=2.0, num_beams=4, early_stopping=True)
/one.pnum/two.pnum
/one.pnum/three.pnum# Decode the summary
/one.pnum/four.pnumsummary = tokenizer.decode(summary_ids[0], skip_specia l_tokens=True)
/one.pnum/five.pnum
/one.pnum6print(f"Generated summary: {summary}" )
In this code, we use the ‘generate()‘ function to produce a su mmary from an input article. Pa-
rameters like ‘max_length‘ and ‘min_length‘ help control t he length of the generated summary, while
‘num_beams‘ speciﬁes the number of beams for beam search (a d ecoding strategy that improves
generation quality) [ /nine.pnum/five.pnum].
/four.pnum./one.pnum8./five.pnum Conclusion
In this section, we have covered how to ﬁne-tune a pre-traine d transformer model for summarization
tasks. Fine-tuning allows models to adapt to speciﬁc domain s, improving summarization quality on
task-speciﬁc datasets. We used the BART model as an example a nd demonstrated the end-to-end
process of preparing data, tokenizing inputs, training the model, and generating summaries.
By ﬁne-tuning models and adjusting parameters, you can achi eve better summarization perfor-
mance tailored to your needs. This process is highly ﬂexible , allowing for customization at various
stages, from dataset preparation to training strategies.
/four.pnum./one.pnum8.6 Advanced Techniques in Summarization
As we have seen, ﬁne-tuning a pre-trained transformer model can signiﬁcantly improve the quality
of generated summaries. However, there are several advance d techniques that can further enhance
summarization performance. In this section, we will discus s some of these techniques, including
beam search, length penalty, and sequence-level optimizat ion methods.

CHAPTER /four.pnum. HUGGING FACE FOR NLP /two.pnum/zero.pnum/one.pnum
/four.pnum./one.pnum8.6./one.pnum Beam Search for Better Decoding
When generating summaries, the default method used by most m odels is greedy decoding, which
simply picks the token with the highest probability at each s tep. While this method is fast, it often
leads to suboptimal summaries, as it doesn’t explore altern ative sequences.
Beam search is a more advanced decoding method that explores multiple hy potheses at each
step, maintaining a set of the most promising sequences (bea ms). Instead of just picking the most
probable word at each step, beam search keeps track of a prede termined number of top sequences.
By the end of the process, the best sequence across all beams i s selected as the ﬁnal output.
Here is how you can implement beam search in Hugging Face:
/one.pnum# Generate summary using beam search
/two.pnumsummary_ids = model.generate(
/three.pnuminputs[’input_ids’ ],
/four.pnummax_length=150,
/five.pnummin_length=40,
6num_beams=5, # Using beam search with 5 beams
/seven.pnumlength_penalty=2.0,
8early_stopping=True
/nine.pnum)
Inthiscode,weset‘num_beams=/five.pnum‘,meaningthemodelwillex plore/five.pnumdifferentpossiblesequences
before selecting the bestone. Increasing the numberof beam sgenerally leads to better results, but it
also increases the computational cost.
/four.pnum./one.pnum8.6./two.pnum Length Penalty
Anotherimportanttechniquewhengeneratingsummariesisc ontrollingthelengthoftheoutput. Often,
summarization models might produce output that is either to o short or too long. To manage this, we
can use a length penalty , which adjusts the likelihood of generating longer or short er sequences.
In the Hugging Face ‘generate()‘ method, the ‘length_penal ty‘ parameter can be used to penalize
longer sequences:
/one.pnum# Generate summary with length penalty
/two.pnumsummary_ids = model.generate(
/three.pnuminputs[’input_ids’ ],
/four.pnummax_length=150,
/five.pnummin_length=40,
6num_beams=4,
/seven.pnumlength_penalty=2.0, # Penalty for generating longer sequences
8early_stopping=True
/nine.pnum)
Alengthpenaltygreaterthan/one.pnumwillpenalizelongsequences ,encouragingshortersummaries. Con-
versely, setting a penalty less than /one.pnum will encourage the mod el to generate longer summaries.
/four.pnum./one.pnum8.6./three.pnum No Repeat N-Gram Constraint
A common issuewithgenerated summariesis thattheymay cont ain repetitivephrasesor sentences.
To combat this, you can apply the no repeat n-gram constraint, which ensures that the same n-gram

CHAPTER /four.pnum. HUGGING FACE FOR NLP /two.pnum/zero.pnum/two.pnum
(a sequence of n tokens) does not appear more than once in the g enerated output. This can be par-
ticularly useful in preventing repetitive outputs like "Th e model is the best" multiple times in a single
summary.
You can implement this using the ‘no_repeat_ngram_size‘ pa rameter in Hugging Face:
/one.pnum# Generate summary with no-repeat n-gram constraint
/two.pnumsummary_ids = model.generate(
/three.pnuminputs[’input_ids’ ],
/four.pnummax_length=150,
/five.pnummin_length=40,
6num_beams=4,
/seven.pnumno_repeat_ngram_size=3, # Avoid repeating trigrams
8early_stopping=True
/nine.pnum)
In this example, setting ‘no_repeat_ngram_size=/three.pnum‘ ensure s that no /three.pnum-token sequence (trigram) is
repeated within the summary. This helps produce more varied and natural-sounding summaries.
/four.pnum./one.pnum8.6./four.pnum Controlling Summary Style with Temperature and Top-K Sa mpling
If you want more control over the creativity of the summaries , you can adjust the temperature and
top-k sampling parameters. These methods introduce randomness into the to ken selection process,
which can lead to more diverse and creative outputs.
•Temperature : This parameter controls the randomness of predictions by s caling the logits (the
outputprobabilitiesofthemodel). Ahightemperature(e.g .,/one.pnum./five.pnum)makesthemodelmorerandom,
while a lower temperature (e.g., /zero.pnum./seven.pnum) makes it more determin istic.
•Top-K Sampling : Instead of considering all possible tokens at each step, to p-k sampling only
considersthetop kmostlikely tokens. Thishelpsreducethelikelihoodofsele ctingveryimprob-
able tokens.
Here is an example of how to use these techniques in text gener ation:
/one.pnum# Generate summary using temperature and top-k sampling
/two.pnumsummary_ids = model.generate(
/three.pnuminputs[’input_ids’ ],
/four.pnummax_length=150,
/five.pnummin_length=40,
6num_beams=1, # No beam search for diversity
/seven.pnumtemperature=0.8, # Control randomness
8top_k=50, # Consider only the top 50 tokens
/nine.pnumearly_stopping=True
/one.pnum/zero.pnum)
In this case, setting temperature=0.8 introduces some diversity in the token selection process,
whiletop_k=50 limits the model to selecting from the top /five.pnum/zero.pnum most likely toke ns at each step.
/four.pnum./one.pnum8./seven.pnum Evaluation Metrics for Summarization
Whenﬁne-tuning or evaluating summarization models, it’s i mportant to assess the quality of the gen-
erated summaries. Several evaluation metrics are commonly used in the summarization domain:

CHAPTER /four.pnum. HUGGING FACE FOR NLP /two.pnum/zero.pnum/three.pnum
•ROUGE (Recall-Oriented Understudy for Gisting Evaluation) [/nine.pnum6]: This is the most widely used
metricforevaluatingsummarizationmodels. ROUGEmeasure stheoverlapbetweenthen-grams
in the generated summary and the reference summary. Common v ariants include ROUGE-/one.pnum (un-
igrams), ROUGE-/two.pnum (bigrams), and ROUGE-L (longest common su bsequence).
•BLEU (Bilingual Evaluation Understudy) : Although primarily used in machine translation, BLEU
can also be used to evaluate summarization. It compares n-gr ams in the generated summary
with reference summaries, focusing on precision.
•METEOR (Metric for Evaluation of Translation with Explicit ORdering) [/nine.pnum/seven.pnum]: Another metric pri-
marilydesignedformachinetranslationbutusefulforsumm arization. Itincorporatessynonymy
matching, which allows for better evaluation of semantical ly equivalent but differently worded
summaries.
/four.pnum./one.pnum8./seven.pnum./one.pnum Using ROUGE for Evaluation
Let’s now demonstrate how to compute the ROUGE score using th e ‘datasets‘ library from Hugging
Face, which provides a built-in ‘rouge‘ metric:
/one.pnumfrom datasets import load_metric
/two.pnum
/three.pnum# Load the ROUGE metric
/four.pnumrouge = load_metric( "rouge")
/five.pnum
6# Example reference and generated summaries
/seven.pnumreference = "Natural Language Processing involves the interaction bet ween computers and humans
using natural language."
8generated_summary = "NLP is the interaction between computers and human languag e."
/nine.pnum
/one.pnum/zero.pnum# Compute the ROUGE score
/one.pnum/one.pnumresults = rouge.compute(predictions=[generated_summar y], references=[reference])
/one.pnum/two.pnum
/one.pnum/three.pnum# Print ROUGE scores
/one.pnum/four.pnumprint(results)
This code will output the ROUGE-/one.pnum, ROUGE-/two.pnum, and ROUGE-L scor es, which you can use to evaluate
the quality of the generated summary compared to the referen ce.
/four.pnum./one.pnum8.8 Future Directions in Summarization
While transformer-based models have signiﬁcantly advance d the state of summarization, there are
several areas of active research that aim to further improve summarization capabilities:
•Long-Document Summarization : Current models struggle with very long documents due to
memory and computation constraints. New architectures suc h as Longformer and BigBird aim
to address these limitations by enabling transformers to pr ocess longer sequences efﬁciently.
•Controllable Summarization : Researchers are working on models that allow users to contr ol
various aspects of the summary, such as its length, style, or content focus. Controllable sum-
marization allows for more customizable and user-speciﬁc s ummaries.

