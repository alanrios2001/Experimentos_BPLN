4CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
CHAPTER
2Words and Tokens
User: I need some help, that much seems certain.
ELIZA: WHAT WOULD IT MEAN TO YOU IF YOU GOT SOME HELP
User: Perhaps I could learn to get along with my mother.
ELIZA: TELL ME MORE ABOUT YOUR FAMILY
User: My mother takes care of me.
ELIZA: WHO ELSE IN YOU FAMILY TAKES CARE OF YOU
User: My father.
ELIZA: YOUR FATHER
User: You are like my father in some ways.Weizenbaum (1966)
The dialogue above is from ELIZA , an early natural language processing system ELIZA
that could carry on a limited conversation with a user by imitating the responses of
a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple
program that uses pattern matching on words to recognize phrases like ‚ÄúI need X‚Äù
and change the words into suitable outputs like ‚ÄúWhat would it mean to you if you
got X?‚Äù. ELIZA‚Äôs mimicry of human conversation, while very crude by modern
standards, was remarkably successful: many people who interacted with ELIZA
came to believe that it really understood them. As a result, this work led researchers
to Ô¨Årst think about the impacts of chatbots on their users (Weizenbaum, 1976).
Of course modern chatbots don‚Äôt use the simple pattern-based mimicry that
ELIZA pioneered. Yet the pattern-based approach to words instantiated in ELIZA
is still relevant today in the context of tokenization , the task of separating out or tokenization
tokenizing words and word parts from running text. Tokenization, the Ô¨Årst step in
modern NLP, includes pattern-based approaches that date back to ELIZA.
To understand tokenization we Ô¨Årst need to ask: What is a word? Is uma word?
What about New York ? Is the nature of words similar across languages? Some
languages, like Vietnamese or Cantonese, have very short words while others, like
Turkish, have very long words. We also need to think about how to represent words
in terms of characters . We‚Äôll introduce Unicode , the modern system for represent-
ing characters, and the UTF-8 text encoding. And we‚Äôll introduce the morpheme ,
the meaningful subpart of words (like the morpheme -erin the word longer )
The standard way to tokenize text is to use the input characters to guide us.
So once we‚Äôve understand the possible subparts of words, we‚Äôll introduce the stan-
dard Byte-Pair Encoding (BPE ) algorithm that automatically breaks up input text BPE
into tokens. This algorithm uses simple statistics of letter sequences to induce a
vocabulary of subword tokens. All tokenization systems also depend on regular
expressions as a processing step. The regular expression is a language for formallyregular
expressions
specifying and manipulating text strings, an important tool in all modern NLP sys-
tems. We‚Äôll introduce regular expressions and show examples of their use
Finally, we‚Äôll introduce a metric called edit distance that measures how similar
two words or strings are based on the number of edits (insertions, deletions, substi-
tutions) it takes to change one string into the other. Edit distance plays a role in NLP
whenever we need compare two words or strings, for example in the crucial word
error rate metric for automatic speech recognition.

2.1 ‚Ä¢ W ORDS 5
2.1 Words
How many words are in the following sentence?
They picnicked by the pool, then lay back on the grass and
looked at the stars.
This sentence has 16 words if we don‚Äôt count punctuation as words, 18 if we
count punctuation. Whether we treat period (‚Äú .‚Äù), comma (‚Äú ,‚Äù), and so on as words
depends on the task. Punctuation is critical for Ô¨Ånding boundaries of things (com-
mas, periods, colons) and for identifying some aspects of meaning (question marks,
exclamation marks, quotation marks). Large language models generally count punc-
tuation as separate words.
Spoken language introduces other complications with regard to deÔ¨Åning words.
What about this utterance from a spoken conversation? ( Utterance is the technical utterance
linguistic term for the spoken correlate of a sentence).
I do uh main- mainly business data processing
This utterance has two kinds of disÔ¨Çuencies . The broken-off word main- is disÔ¨Çuency
called a fragment . Words like uhandumare called Ô¨Ållers orÔ¨Ålled pauses . Should fragment
Ô¨Ålled pause we consider these to be words? Again, it depends on the application. If we are
building a speech transcription system, we might want to eventually strip out the
disÔ¨Çuencies. But we also sometimes keep disÔ¨Çuencies around. DisÔ¨Çuencies like uh
orumare actually helpful in speech recognition in predicting the upcoming word,
because they may signal that the speaker is restarting the clause or idea, and so for
speech recognition they are treated as regular words. Because different people use
different disÔ¨Çuencies they can also be a cue to speaker identiÔ¨Åcation. In fact Clark
and Fox Tree (2002) showed that uhandumhave different meanings in English.
What do you think they are?
Perhaps most important, in thinking about what is a word, we need to distinguish
two ways of talking about words that will be useful throughout the book. Word types word type
are the number of distinct words in a corpus; if the set of words in the vocabulary
isV, the number of types is the vocabulary size|V|. Word instances are the total word instance
number Nof running words.1If we ignore punctuation, the picnic sentence has 14
types and 16 instances:
They picnicked by the pool, then lay back on the grass and
looked at the stars.
We still have decisions to make! For example, should we consider a capitalized
string (like They ) and one that is uncapitalized (like they) to be the same word type?
The answer is that it depends on the task! They andtheymight be lumped together as
the same type in some tasks where we care less about the formatting, while for other
tasks, capitalization is a useful feature and is retained. Sometimes we keep around
two versions of a particular NLP model, one with capitalization and one without
capitalization.
So far we have been talking about orthographic words : words based on our
English writing system. But there are many other possible ways to deÔ¨Åne words.
For example, while orthographically I‚Äômis one word, grammatically it functions as
two words: the subject pronoun Iand the verb ‚Äôm, short for am.
1In earlier tradition, and occasionally still, you might see word instances referred to as word tokens , but
we now try to reserve the word token instead to mean the output of subword tokenization algorithms.

6CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
Corpus Types =|V|Instances = N
Shakespeare 31 thousand 884 thousand
Brown corpus 38 thousand 1 million
Switchboard telephone conversations 20 thousand 2.4 million
COCA 2 million 440 million
Google n-grams 13 million 1 trillion
Figure 2.1 Rough numbers of wordform types and instances for some English language
corpora. The largest, the Google n-grams corpus, contains 13 million types, but this count
only includes types appearing 40 or more times, so the true number would be much larger.
The distinctions get even harder to make once we start to think about other lan-
guages. For example the writing systems of languages like Chinese, Japanese, and
Thai simply don‚Äôt have orthographic words at all! That is, they don‚Äôt use spaces to
mark potential word-boundaries. In Chinese, for example, words are composed of
characters (called hanzi in Chinese). Each character generally represents a single hanzi
unit of meaning (called a morpheme , introduced below) and is pronounceable as a
single syllable. Words are about 2.4 characters long on average. But since Chinese
has no orthographic words, deciding what counts as a word in Chinese is complex.
For example, consider the following sentence:
(2.1)ÂßöÊòéËøõÂÖ•ÊÄªÂÜ≥Ëµõ y¬¥ao m ¬¥ƒ±ng j`ƒ±n r`u zÀáong ju ¬¥e s`ai
‚ÄúYao Ming reaches the Ô¨Ånals‚Äù
As Chen et al. (2017b) point out, this could be treated as 3 words (a deÔ¨Ånition of
words called the ‚ÄòChinese Treebank‚Äô deÔ¨Ånition, in which Chinese names (family
name followed by personal names) are treated as a single word):
(2.2)ÂßöÊòé
YaoMingËøõÂÖ•
reachesÊÄªÂÜ≥Ëµõ
Ô¨Ånals
But the same sentence could be treated as 5 words (‚ÄòPeking University‚Äô standard),
in which names are separated into their own units and some adjectives appear as
distinct words:
(2.3)Âßö
YaoÊòé
MingËøõÂÖ•
reachesÊÄª
overallÂÜ≥Ëµõ
Ô¨Ånals
Finally, it is possible in Chinese simply to ignore words altogether and use characters
as the basic elements, treating the sentence as a series of 7 characters, which works
pretty well for Chinese since characters are at a reasonable semantic level for most
applications (Li et al., 2019):
(2.4)Âßö
YaoÊòé
MingËøõ
enterÂÖ•
enterÊÄª
overallÂÜ≥
decisionËµõ
game
But that method doesn‚Äôt work for Japanese and Thai, where the individual character
is too small a unit.
These issues with deÔ¨Åning words makes it hard to use words as the basis for
tokenizing text in NLP across languages.
But there‚Äôs another problem with words. There are too many of them!!! How
many words are there in English? When we speak about the number of words in
the language, we are generally referring to word types. Fig. 2.1 shows the rough
numbers of types and instances computed from some English corpora.
You will notice that the larger the corpora we look at, the more word types we
Ô¨Ånd! That suggests that there is not a clear answer to how many words there are;
the answer keeps growing as we see more data! We can see this fact mathematically

2.1 ‚Ä¢ W ORDS 7
because the relationship between the number of types |V|and number of instances
Nis called Herdan‚Äôs Law (Herdan, 1960) or Heaps‚Äô Law (Heaps, 1978) after its Herdan‚Äôs Law
Heaps‚Äô Law discoverers (in linguistics and information retrieval respectively). It is shown in
Eq. 2.5, where kandŒ≤are positive constants, and 0 <Œ≤<1.
|V|=kNŒ≤(2.5)
The value of Œ≤depends on the corpus size and the genre; numbers from 0.44 to 0.56
or even higher have often been reported. Roughly we can say that the vocabulary
size for a text goes up a little faster than the square root of its length in words.
There are also variants of the law, which capture the fact that we can distinguish
roughly two classes of words. One is function words , the grammatical words like function words
English aandof, that tend not to grow indeÔ¨Ånitely (a language tends to have a Ô¨Åxed
number of these). The other is content words : nouns, adjectives and verbs that tend content words
to have meanings about people and places and events. Nouns, and especially partic-
ular nouns like names and technical terms do tend to grow indeÔ¨Ånitely. So models
that are sensitive to this difference between function words and content words have
one value of Œ≤for the initial part of the corpus where all words are still appearing,
and then a second Œ≤afterwords for when only the content words are still appearing.
Fig. 2.2 shows an example from Tria et al. (2018) showing two values of Œ≤for Heaps
law computed on the Gutenberg corpus of books.
Entropy 2018 ,20, 752 4 of 19
Figure 2. Growth of the number of distinct words computed on the Gutenberg corpus of texts [ 15].
The position of texts in the corpus is chosen at random. In this case g'0.44. Similar behaviours are
observed in many other systems.
2.3. Zipf‚Äôs vs. Heaps‚Äô Laws
In this section we compare the two laws just observed, Zipf‚Äôs law for the frequencies of occurrence
of the elements in a system and Heaps‚Äô law for their temporal appearance. It has often been claimed that
Heaps‚Äô and Zipf‚Äôs law are trivially related and that one can derive Heaps‚Äôs law once the Zipf‚Äôs is known.
This is not true in general. It turns out to be true only under the speciÔ¨Åc hypothesis of random-sampling
as follows. Suppose the existence of a strict power-law behaviour of the frequency-rank distribution,
f(R)‚á†R a, and construct a sequence of elements by randomly sampling from this Zipf distribution
f(R). Through this procedure, one recovers a Heaps‚Äô law with the functional form D(t)‚á†tg[23,24]
with g=1/a. In order to do that we need to consider the correct expression for f(R)that includes the
normalisation factor, whose expression can be derived through the following approximated integral:
ZRmax
1f(ÀúR)dÀúR=1 . (3)
Let us now distinguish the two cases. For a6=1 one has
f(R)=1 a
R1 amax 1R a. (4)
while for a=1 one obtains:
f(R)=1
logRmaxR 1. (5)
When a>1, one can neglect the term R1 a
maxin Equation ( 4), and when a<1, one can write
R1 a
max 1'R1 a
max.
Figure 2.2 V ocabulary size as a function of text length, computed on the Gutenberg corpus
of publicly available books. Figure from Tria et al. (2018).
The fact that words grow without end leads to a problem for any computational
model. No matter how big our vocabulary, we will never have a vocabulary that
captures all the possible words that might occur! That means that our computational
model will constantly see unknown words : words that it has never seen before.
This is a huge problem for machine learning models.
Because of these two problems (Ô¨Årst, that many languages don‚Äôt have ortho-
graphic words, and deÔ¨Åning them post-hoc is challenging and second, that the num-
ber of words grows without bound), language models and other NLP models don‚Äôt
tend to use words as their unit of processing. Instead, they use smaller units called
subwords that can be recombined to model new words that our model has never
seen before. To think about deÔ¨Åning subwords, we Ô¨Årst need to talk about units that
are smaller than words; morphemes andcharacters .

8CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
2.2 Morphemes: Parts of Words
Words have parts. At the level of characters, this is obvious. The word catsis com-
posed of four characters, ‚Äòc‚Äô, ‚Äòa‚Äô, ‚Äòt‚Äô, ‚Äòs‚Äô. But this is also true at a more subtle level:
words have components that themselves have coherent meanings. These compo-
nents are called morphemes , and the study of morphemes is called morphology . A morphology
morpheme is a minimal meaning-bearing unit in a language. So, for example, the morpheme
word foxconsists of one morpheme (the morpheme fox) while the word catsconsists
of two: the morpheme catand the morpheme -sthat indicates plural.
Here‚Äôs a sentence in English segmented into morphemes with hyphens:
(2.6) Doc work-ed care-ful-ly wash-ing the glass-es
As we mentioned above, in Chinese, conveniently, the writing system is set up
so that each character mainly describes a morpheme. Here‚Äôs a sentence in Mandarin
Chinese with each morpheme character glossed, followed by the translation:
(2.7)Ê¢Ö
plumÂπ≤
dryËèú
vegetableÁî®
useÊ∏Ö
clearÊ∞¥
waterÊ≥°
soakËΩØ
softÔºå
,Êçû
removeÂá∫
outÂêé
afterÔºå
,Ê≤•
dripÂπ≤
dry
Âàá
chopÁ¢é
fragment
Soak the preserved vegetable in water until soft, remove, drain, and chop
We generally distinguish two broad classes of morphemes: roots ‚Äîthe central root
morpheme of the word, supplying the main meaning‚Äîand afÔ¨Åxes ‚Äîadding ‚Äúad- afÔ¨Åx
ditional‚Äù meanings of various kinds. In the English example above, for the word
worked ,work is a root and -edis an afÔ¨Åx; similarly for glasses ,glass is a root and
-esan afÔ¨Åx.
AfÔ¨Åxes themselves fall into two classes, or more correctly a continuum between
two poles. At one end, inÔ¨Çectional morphemes are grammatical morphemes thatinÔ¨Çectional
morphemes
tend to play a syntactic role, such as marking agreement. For example, English has
the inÔ¨Çectional morpheme -s(or-es) for marking the plural on nouns and the inÔ¨Çec-
tional morpheme -edfor marking the past tense on verbs. InÔ¨Çectional morphemes
tend to be productive and often obligatory and their meanings tend to be predictable.
Derivational morphemes are more idiosyncratic in their application and meaning.derivational
morphemes
Usually they apply only to a speciÔ¨Åc subclass of words and result in a word of a dif-
ferent grammatical class than the root, often with a meaning hard to predict exactly.
In the example above, the word care (a noun) can be combined with the derivational
afÔ¨Åx -fullto produce an adjective ( careful ), and another derivational afÔ¨Åx -lyto result
in an adverb ( carefully ).
There is another class of morphemes: clitics . A clitic is a morpheme that acts clitic
syntactically like a word but is reduced in form and attached (phonologically and
sometimes orthographically) to another word. For example the English morpheme
‚Äôvein the word I‚Äôveis a clitic; it has the grammatical meaning of the word have , but
in form in cannot appear alone (you can‚Äôt just say the sentence ‚Äú‚Äôve‚Äù). The English
possessive morpheme ‚Äôsin the phrase the teacher‚Äôs book is a clitic. French deÔ¨Ånite
article l‚Äôin the word l‚Äôopera is a clitic, as are prepositions in Arabic like b‚Äòby/with‚Äô
and conjunctions like w‚Äòand‚Äô.
The study of how languages vary in their morphology, i.e., how words break
up into their parts, is called morphological typology . While morphologies of lan-morphological
typology
guages can differ along many dimensions, two dimensions are particularly relevant
for computational word tokenization.

2.2 ‚Ä¢ M ORPHEMES : PARTS OF WORDS 9
The Ô¨Årst dimension is the number of morphemes per word . In some languages,
like Vietnamese and Cantonese, each word on average has just over one morpheme.
We call languages at this end of the scale isolating languages. For example each isolating
word in the following Cantonese sentence has one morpheme (and one syllable):
(2.8) keoi5
hewaa6
saycyun4
entiregwok3
countryzeoi3
mostdaai6
biggaan1
buildinguk1
househai6
isni1
thisgaan1
building
‚ÄúHe said the biggest house in the country was this one‚Äù
Alternatively, in languages like Koryak, a Chukotko-Kamchatkan language spo-
ken in the northern part of the Kamchatka peninsula in Russia, a single word may
have very many morphemes, corresponding to a whole sentence in English (Arkadiev,
2020; Kurebito, 2017). We call languages toward this end of the scale synthetic lan- synthetic
guages, and the very end of the scale polysynthetic languages. polysynthetic
(2.9) t-@-nk‚Äôe-mej N-@-jetem @-nni-k
1SG.S-E-midnight-big-E-yurt.cover-E-sew-1SG.S[PFV]
‚ÄúI sewed a lot of yurt covers in the middle of a night. ‚Äù
(Koryak, Chukotko-Kamchatkan, Russia; Kurebito (2017, 844))
Fig. 2.3 shows an early computation of morphemes per words on a few languages
by the linguistic typologist Joseph Greenberg (1960).
AnalyticSyntheticFarsiGreenlandic(Inuit)SanskritSwahiliYakutOld EnglishEnglishVietnamese1.11.51.72.12.22.52.63.7Morphemes per WordPolysynthetic
Figure 2.3 An early estimate of morphemes per word by Joseph Greenberg (1960).
The second dimension is the degree to which morphemes are easily segmentable,
ranging from agglutinative languages like Turkish, in which morphemes have rel- agglutinative
atively clean boundaries, to fusion languages like Russian, in which a single afÔ¨Åx fusion
may conÔ¨Çate multiple morphemes, like -om in the word stolom (table- SG-INSTR -
DECL 1), which fuses the distinct morphological categories instrumental, singular,
and Ô¨Årst declension.
The English -ssufÔ¨Åx in She read sthe article is an example of fusion, since the
sufÔ¨Åx means both third person singular but also means present tense, and there‚Äôs no
way to divide up the meaning to different parts of the -s.
Although we have loosely talked about these properties (analytic, polysynthetic,
fusional, agglutinative) as if they are properties of languages, in fact languages can
make use of different morphological systems so it would be more accurate to talk
about these as general tendencies.
Nonetheless, the fact morphemes can be hard to deÔ¨Åne, and that many languages
can have complex morphemes that aren‚Äôt easy to break up into pieces makes it very
difÔ¨Åcult to use morphemes as a standard for tokenization cross-lingually.

10 CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
2.3 Unicode
Another option we could consider for tokenization is the level of the individual char-
acter. How do we even represent characters across languages and writing system?
TheUnicode standard is a method for representing text written using any character Unicode
in any script of the languages of the world (including dead languages like Sumerian
cuneiform, and invented languages like Klingon).
Let‚Äôs start with a brief historical note about an English-speciÔ¨Åc subset of Unicode
(technically called ‚ÄòBasic Latin‚Äô in Unicode, and commonly referred to as ASCII).
Starting in the 1960s, the Latin characters used to write English (like the ones used
in this sentence), were represented with a code called ASCII (American Standard ASCII
Code for Information Interchange). ASCII represented each character with a single
byte. A byte can represent 256 different characters, but ASCII only used 127 of
them; the high-order bit of ASCII bytes is always set to 0. (Actually it only used 95
of them and the rest were control codes for an obsolete machine called a teletype).
Here‚Äôs a few ASCII characters with their representation in hex and decimal:
Ch Hex Dec Ch Hex Dec Ch Hex Dec Ch Hex Dec
< 3C 60@ 40 64 ...\ 5C 92‚Äò 60 96
= 3D 61A 41 65 ...[ 5D 93a 61 97
> 3E 62B 42 66 ...ÀÜ 5E 94b 62 98
? 3F 63C 43 67 ..._ 5F 95c 63 99
Figure 2.4 Some selected ASCII codes for some English letters, with the codes shown both
in hexadecimal and decimal.
But ASCII is of course insufÔ¨Åcient since there are lots of other characters in the
world‚Äôs writing systems! Even for scripts that use Latin characters, there are many
more than the 95 in ASCII. For example, this Spanish phrase (meaning ‚ÄúSir, replied
Sancho‚Äù) has two non-ASCII characters, Àún and ¬¥o:
(2.10) Se Àúnor- respondi ¬¥o Sancho-
And lots of languages aren‚Äôt based on Latin characters at all! The Devanagari Devanagari
script is used for 120 languages (including Hindi, Marathi, Nepali, Sindhi, and San-
skrit). Here‚Äôs a Devanagari example from the Hindi text of the Universal Declaration
of Human Rights:
Chinese has about 100,000 Chinese characters in Unicode (including overlap-
ping and non-overlapping variants used in Chinese, Japanese, Korean, and Viet-
namese, collectively referred to as CJKV).
All in all there are more than 150,000 characters and 168 different scripts sup-
ported in Unicode 16.0. Even though many scripts from around the world have
yet to be added to Unicode, there are so many there, from scripts used by mod-
ern languages (Chinese, Arabic, Hindi, Cherokee, Ethiopic, Khmer, N‚ÄôKo, Turkish,
Spanish) to scripts of ancient languages (Cuneiform, Ugaritic, Egyptian Hieroglyph,
Pahlavi), as well as mathematical symbols, emojis, currency symbols, and more.
2.3.1 Code Points
How does it work? Unicode assigns a unique id, called a code point , for each one code point

2.3 ‚Ä¢ U NICODE 11
of these 150,000 characters.
The code point is an abstract representation of the character, and each code point
is represented by a number, traditionally written in hexadecimal, from number 0
through 0x10FFFF (which is 1,114,111 decimal). Having over a million code points
means there is a lot of room for new characters. It is traditional to represent these
code points with the preÔ¨Åx ‚ÄúU+‚Äù (which just means ‚Äúthe following is a Unicode hex
representation of a code point‚Äù). So the code point for the character aisU+0061
which is the same as 0x0061 . (Note that Unicode was designed to be backwards
compatible with ASCII, which means that the Ô¨Årst 127 code points, including the
code fora, are identical with ASCII.) Here are some sample code points; some (but
not all) come with descriptions:
U+0061 aLATIN SMALL LETTER A
U+0062 bLATIN SMALL LETTER B
U+0063 cLATIN SMALL LETTER C
U+00F9 ` uLATIN SMALL LETTER U WITH GRAVE
U+00FA ¬¥ uLATIN SMALL LETTER U WITH ACUTE
U+00FB ÀÜ uLATIN SMALL LETTER U WITH CIRCUMFLEX
U+00FC ¬® uLATIN SMALL LETTER U WITH DIAERESIS
U+8FDB Ëøõ
U+8FDC Ëøú
U+8FDD Ëøù
U+8FDE Ëøû
U+1F600
üÄé
5/23/25, 5:26 PMx.html
Ô¨Åle:///Users/jurafsky/x.html1/1 GRINNING FACE
U+1F00E
üÄé
5/23/25, 5:26 PMx.html
Ô¨Åle:///Users/jurafsky/x.html1/1 MAHJONG TILE EIGHT OF CHARACTERS
Note that a code point does not speciÔ¨Åy the glyph , the visual representation glyph
of a character. Glyphs are stored in fonts . The code point U+0061 is an abstract
representation of a. There can be an indeÔ¨Ånite number of visual representations,
for example in different fonts like Times Roman (a) or Courier ( a), or different font
styles like boldface ( a) or italic ( a). But all of them are represented by the same code
pointU+0061 .
2.3.2 UTF-8 Encoding
While the code point (the unique id) is the abstract Unicode representation of the
character, we don‚Äôt just stick that id in a text Ô¨Åle.
Instead, whenever we need to represent a character in a text string, we write an
encoding of the character. There are many different possible encoding methods, but encoding
the encoding method called UTF-8 is by far the most frequent (for example almost
the entire web is encoded in UTF-8).
Let‚Äôs talk about encodings. The Unicode representation of the word hello con-
sists of the following sequence of 5 code points:
U+0068 U+0065 U+006C U+006C U+006F
We can imagine a very simple encoding method: just write the code point id in
a Ô¨Åle. Since there are more than 1 million characters, 16 bits (2 bytes) isn‚Äôt enough,
so we‚Äôll need to use 4 bytes (32 bit) to capture the 21 bits we need to represent 1.1
million characters. (We could Ô¨Åt it in 3 bytes but it‚Äôs inconvenient to use multiples
of 3 for bytes.)
With this 4-byte representation the word hello would be encoded as the follow-
ing set of bytes:

12 CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
00 00 00 68 00 00 00 65 00 00 00 6C 00 00 00 6C 00 00 00 6F
But we don‚Äôt use this encoding (which is technically called UTF-32) because it
makes every Ô¨Åle 4 times longer than it would have been in ASCII, making Ô¨Åles really
big and full of zeros. Also those zeros cause another problem: it turns out that having
any byte that is completely zero messes things up for backwards compatibility for
ASCII-based systems that historically used a 0 byte as an end-of-string marker.
Instead, the most common encoding standard is UTF-8 (Unicode Transforma- UTF-8
tion Format 8), which represents characters efÔ¨Åciently (using fewer bytes on av-
erage) by writing some characters using fewer bytes and some using more bytes.
UTF-8 is thus a variable-length encoding .variable-length
encoding
For some characters (the Ô¨Årst 127 code points, i.e. the set of ASCII characters),
UTF-8 encodes them as a single byte, so the UTF-8 encoding of hello is :
68 65 6C 6C 6F
This conveniently means that Ô¨Åles encoded in ASCII are also valid UTF-8 en-
codings!
But UTF-8 is a variable length encoding, meaning that code points ‚â•128 are
encoded as a sequence of two, three, or four bytes. Each of these bytes are between
128 and 255, so they won‚Äôt be confused with ASCII, and each byte indicates in the
Ô¨Årst few bits whether it‚Äôs a 2-byte, 3-byte, or 4-byte encoding.
Code Points UTF-8 Encoding
From - To Bit Value Byte 1 Byte 2 Byte 3 Byte 4
U+0000-U+007F 0xxxxxxx xxxxxxxx
U+0080-U+07FF 00000yyy yyxxxxxx 110yyyyy 10xxxxxx
U+0800-U+FFFF zzzzyyyy yyxxxxxx 1110zzzz 10yyyyyy 10xxxxxx
U+010000-U+10FFFF 000uuuuu zzzzyyyy yyxxxxxx 11110uuu 10uuzzzz 10yyyyyy 10xxxxxx
Figure 2.5 Mapping from Unicode code point to the variable length UTF-8 encoding. For a given code point
in the From-To range, the bit value in column 2 is packed into 1, 2, 3, or 4 bytes. Figure adapted from Unicode
16.0 Core Spec Chapter 3 Table 3-6.
Fig. 2.5 shows how this mapping occurs. For example these rules explain how
the character Àún, which has code point U+00F1, or bit sequence 00000000 11110001 ,
(where blue indicates the sequence yyyyy and red the sequence xxxxxx) is encoded
into to the two-byte bit sequence 11000011 10110001 or0xC3B1 . As a result of
these rules, the Ô¨Årst 127 characters (ASCII) are mapped to one byte, most remain-
ing characters in European, Middle Eastern, and African scripts map to two bytes,
most Chinese, Japanese, and Korean characters map to three bytes, and rarer CJKV
characters and emojis and some symbols map to 4 bytes.
UTF-8 has a number of advantages. It‚Äôs relatively efÔ¨Åcient, using fewer bytes for
commonly-encountered characters, it doesn‚Äôt use zero bytes (except when literally
representing the NULL character which is U+0000), it‚Äôs backwards compatible with
ASCII, and it‚Äôs self-synchronizing, meaning that if a Ô¨Åle is corrupted, it‚Äôs always
possible to Ô¨Ånd the start of the next or prior character just by moving up to 3 bytes
left or right.
Unicode and Python: Starting with Python 3, all Python strings are stored in-
ternally as Unicode, each string a sequence of Unicode code points. Thus string
functions and regular expressions all apply natively to code points. For example,
functions like len() of a string return its length in characters, i.e., code points, not
its length in bytes.
When reading or writing from a Ô¨Åle, however, the code points need to be encoded
and decoding using a method like UTF-8. That is, every Ô¨Åle is encoded in some

2.4 ‚Ä¢ S UBWORD TOKENIZATION : BYTE-PAIRENCODING 13
encoding. If it‚Äôs not UTF-8, it‚Äôs an older encoding method like ASCII or Latin-1
(iso8859 1). There is no such thing as a text Ô¨Åle without an encoding. The encoding
method is speciÔ¨Åed in Python when opening a Ô¨Åle for reading and writing.
2.4 Subword Tokenization: Byte-Pair Encoding
Tokenization , the Ô¨Årst stage of natural language processing, is the process of seg- tokenization
menting the running input text into tokens . tokens
We‚Äôve seen three candidates for tokens: words, morphemes and characters. But
each has problems as a unit. Words and morphemes seem approximately at the right
level for NLP processing, since they tend to have consistent meanings, but they are
challenging to deÔ¨Åne formally. Characters are clearer to deÔ¨Åne, but seem too small
a unit to choose for tokens.
In this section we introduce what we do in practice for NLP: use a data-driven
approach to deÔ¨Åne tokens that will generally result in units about the size of mor-
phemes or words, but occasionally use units as small as characters.
Why tokenize the input? One reason is that converting an input to a deterministic
Ô¨Åxed set of units means that different algorithms and systems can agree on simple
questions. For example, How long is this text? (How many units are in it?). Or:
Isdon‚Äôt orNew York one token or two? Standardizing is thus essential for repli-
cability in NLP experiments, and many algorithms that we introduce in this book
(like the perplexity metric for language models) assume that all texts have a Ô¨Åxed
tokenization.
Tokenization algorithms that include smaller tokens for morphemes and letters
also eliminate the problem of unknown words . What are these? As we will see
in the next chapter, NLP algorithms often learn some facts about language from
one corpus (a training corpus) and then use these facts to make decisions about a
separate testcorpus and its language. Thus if our training corpus contains, say the
words low,new, and newer , but not lower , then if the word lower appears in our test
corpus, our system will not know what to do with it.
To deal with this unknown word problem, modern tokenizers automatically in-
duce sets of tokens that include tokens smaller than words, called subwords . Sub- subwords
words can be arbitrary substrings, or they can be meaning-bearing units like the
morphemes -estor-er. In modern tokenization schemes, many tokens are words,
but other tokens are frequently occurring morphemes or other subwords like -er.
Every unseen word can thus be represented by some sequence of known subword
units. For example, if we had happened not to ever see the word lower , when it ap-
pears we could segment it successfully into lowanderwhich we had already seen.
In the worst case, a really unusual word (perhaps an acronym like GRPO ) could be
tokenized as a sequence of individual letters if necessary.
Two tokenization algorithms are widely used in modern language models: byte-
pair encoding (BPE) (Sennrich et al., 2016), and unigram language modeling
(ULM) (Kudo, 2018).2In this section we introduce the byte-pair encoding orBPE BPE
algorithm (Sennrich et al., 2016; Gage, 1994); see Fig. 2.6.
Like most tokenization schemes, the BPE algorithm has two parts: a trainer ,
and an encoder . In general in the token training phase we take a raw training corpus
2TheSentencePiece library includes implementations of both of these (Kudo and Richardson, 2018a),
and people sometimes use the name SentencePiece to simply mean ULM tokenization.

14 CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
(usually roughly pre-separated into words, for example by whitespace) and induce
a vocabulary, a set of tokens. Then a token encoder take a raw test sentence and
encodes it into the tokens in the vocabulary that were learned in training.
2.4.1 BPE training
TheBPE training algorithm iteratively merges frequent neighboring tokens to create
longer and longer tokens. The algorithm begins with a vocabulary that is just the
set of all individual characters. It then examines the training corpus, and Ô¨Ånds the
two characters that are most frequently adjacent. Imagine our original corpus is 10
characters long, using a vocabulary of 5 characters, {A,B,C,D,E}:
A B D C A B E C A B
The most frequent neighboring pair of characters is ‚Äú A B‚Äù so we merge those,
add a new merged token ‚Äò AB‚Äô to the vocabulary, and replace every adjacent ‚Äò A‚Äô ‚ÄòB‚Äô
in the corpus with the new ‚Äò AB‚Äô:
AB D C AB E C AB
Now we have a vocabulary of 6 possible tokens {A,B,C,D,E,AB}, and the
corpus has length 7. And now the most frequent pair of tokens is ‚Äú C AB ‚Äù, so we
merge those, leading to a vocabulary with 7 tokens {A,B,C,D,E,AB,CAB}, and the
corpus has length 5.
AB D CAB F CAB
The algorithm continues to count and merge, creating new longer and longer
character strings, until kmerges have been done creating knovel tokens; kis thus a
parameter of the algorithm. The resulting vocabulary consists of the original set of
characters plus knew symbols. That‚Äôs the core of the algorithm.
The only additional complication is that in practice, instead of running on the
raw sequence of characters, the algorithm is usually run only inside words. That is,
the algorithm does not merge across word boundaries. To do this, the input corpus
is often Ô¨Årst separated at white space and punctuation (using the regular expressions
that we deÔ¨Åne later in the chapter). This gives a starting set of strings, each corre-
sponding to the characters of a word, (with the white space usually attached to the
start of the word), together with the counts of the words. Then while counts come
from a corpus, merges are only allowed within the strings.
Let‚Äôs see how the full algorithm thus works on this tiny synthetic corpus, where
we‚Äôve explicitly marked the spaces between words:3
(2.11)setnewnewrenewresetrenew
First, we‚Äôll break up the corpus into words, with leading whitespace, together
with their counts; no merges will be allowed to go beyond these word boundaries.
The result looks like the following list of 4 words and a starting vocabulary of 7
characters:
corpus vocabulary
2n e w , e, n, r, s, t, w
2r e n e w
1s e t
1r e s e t
3Yes, we realize this isn‚Äôt a particularly likely or exciting sentence.

2.4 ‚Ä¢ S UBWORD TOKENIZATION : BYTE-PAIRENCODING 15
The BPE training algorithm Ô¨Årst counts all pairs of adjacent symbols: the most
frequent is the pair n e because it occurs in new (frequency of 2) and renew (fre-
quency of 2) for a total of 4 occurrences. We then merge these symbols, treating ne
as one symbol, and count again:
corpus vocabulary
2ne w , e, n, r, s, t, w, ne
2r e ne w
1s e t
1r e s e t
Now the most frequent pair is ne w (total count=4), which we merge.
corpus vocabulary
2new , e, n, r, s, t, w, ne, new
2r e new
1s e t
1r e s e t
Nextr(total count of 3) get merged to r, and then r e (total count 3) gets
merged to re. The system has essentially induced that there is a word-initial preÔ¨Åx
re-:
corpus vocabulary
2new , e, n, r, s, t, w, ne, new, r,re
2re new
1s e t
1re s e t
If we continue, the next merges are:
merge current vocabulary
(, new) , e, n, r, s, t, w, ne, new, r,re,new
(re, new) , e, n, r, s, t, w, ne, new, r,re,new,renew
(s, e) , e, n, r, s, t, w, ne, new, r,re,new,renew, se
(se, t) , e, n, r, s, t, w, ne, new, r,re,new,renew, se, set
function BYTE-PAIR ENCODING (strings C, number of merges k)returns vocab V
V‚Üêall unique characters in C # initial set of tokens is characters
fori= 1tokdo # merge tokens ktimes
tL,tR‚ÜêMost frequent pair of adjacent tokens in C
tNEW‚ÜêtL+tR # make new token by concatenating
V‚ÜêV+tNEW # update the vocabulary
Replace each occurrence of tL,tRinCwith tNEW # and update the corpus
return V
Figure 2.6 The training part of the BPE algorithm for taking a corpus broken up into in-
dividual characters or bytes, and learning a vocabulary by iteratively merging tokens. Figure
adapted from Bostrom and Durrett (2020).
2.4.2 BPE encoder
Once we‚Äôve learned our vocabulary, the BPE encoder is used to tokenize a test
sentence. The encoder just runs on the test data the merges we have learned from

16 CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
the training data. It runs them greedily, in the order we learned them. (Thus the
frequencies in the test data don‚Äôt play a role, just the frequencies in the training
data). So Ô¨Årst we segment each test sentence word into characters. Then we apply
the Ô¨Årst rule: replace every instance of n ein the test corpus with ne, and then the
second rule: replace every instance of ne w in the test corpus with new, and so on.
By the end of course many of the merges simple recreated words in the training
set. But the merges also created knowledge of morphemes like the re-preÔ¨Åx (that
might appear in perhaps unseen combinations like revisit orrearrange ), or the
morpheme new without an initial space (hence word-internal) that might appear at
the start of sentences or in words unseen in training like anew .
Of course in real settings BPE is run with tens of thousands of merges on a very
large input corpus, to produce vocabulary sizes of 50,000, 100,000, or even 200,000
tokens. The result is that most words can be represented as single tokens, and only
the rarer words (and unknown words) will have to be represented by multiple tokens.
At least for English. For multilingual systems, the tokens can be dominated by
English, leaving fewer tokens for other languages, as we‚Äôll discuss below.
2.4.3 BPE in practice
The example above just showed simple BPE learning from sequences of ASCII
bytes. How does BPE work with Unicode input? We normally run BPE on the
individual bytes of UTF-8-encoded text. That is, we take a Unicode representations
of text as a series of code points, encode it in bytes using UTF-8, and we treat each of
these individual bytes as the input to BPE. Thus BPE likely begins by rediscovering
the 2-byte and common 3-byte sequences that UTF-8 uses to encode various code
points. Again, running BPE only inside presegmented words helps avoid problems.
Because there are only 256 possible values of a byte, there will be no unknown to-
kens, although it‚Äôs possible that BPE will learn some illegal UTF-8 sequences across
character boundaries. These will be very rare, and can be eliminated with a Ô¨Ålter.
Let‚Äôs see some examples of the industrial application of the BPE tokenizer used
in large systems like OpenAI GPT4o. This tokenizer has 200K tokens, which is a
comparatively large number. We can use Tat Dat Duong‚Äôs Tiktokenizer visualizer
(https://tiktokenizer.vercel.app/ ) to see the number of tokens in a given
sentence. For example here‚Äôs the tokenization of a nonsense sentence we made up;
the visualizer uses a center dot to indicate a space:
The visualization shows colors to separate out words, but of course the true out-
put of the tokenizer is simply a sequence of unique token ids. (In case you‚Äôre in-
terested, they were the following 13 tokens: 11865, 8923, 11, 31211, 6177, 23919,
885, 220, 19427, 7633, 18887, 147065, 0)
Notice that most words are their own token, usually including the leading space.
Clitics like ‚Äôsare segmented off when they appear on proper nouns like Jane , but
are counted as part of a word for frequent words like she‚Äôs . Numbers tend to be
segmented into chunks of 3 digits. And some words (like anyhow ) are segmented
differently if they appear capitalized sentence-initially (two tokens, Any andhow),
then if they appear after a space, lower case (one token anyhow ).
Some of these are related to preprocessing steps. As we mentioned brieÔ¨Çy above,
language models usually create their tokens in a pretokenization stage that Ô¨Årst seg- pretokenization
ments the input using regular expressions, for example breaking the input at spaces
and punctuation, stripping off clitics, and breaking numbers into sets of 3 digits.

2.5 ‚Ä¢ R ULE-BASED TOKENIZATION 17
We‚Äôll see how to use regular expressions in Section 2.7.
It‚Äôs possible to change this pretokenization to allow BPE tokens to span multiple
words. For example the SuperBPE algorithm Ô¨Årst induces regular BPE subword SuperBPE
tokens by enforcing pretokenization. It then runs a second stage of BPE allowing
merges across spaces and punctuation. The result is a large set of tokens that can be
more efÔ¨Åcient. See Fig. 2.7.
Preprint
Figure 1: SuperBPE tokenizers encode text much more efÔ¨Åciently than BPE, and the
gap grows with larger vocabulary size. Encoding efÔ¨Åciency ( y-axis) is measured with
bytes-per-token , the number of bytes encoded per token on average over a large corpus of text.
In the above text with 40 bytes, SuperBPE uses 7 tokens and BPE uses 13, so the methods‚Äô
efÔ¨Åciencies are 40 /7=5.7 and 40 /13=3.1 bytes-per-token, respectively. In the graph,
the encoding efÔ¨Åciency of BPE plateaus early due to exhausting the valuable whitespace-
delimited words in the training data. In fact, it is bounded above by the gray dotted line,
which shows the maximum achievable encoding efÔ¨Åciency with BPE, if every whitespace-
delimited word were in the vocabulary. On the other hand, SuperBPE has dramatically
better encoding efÔ¨Åciency that continues to improve with increased vocabulary size, as
it can continue to add common word sequences to treat as tokens to the vocabulary. The
different gradient lines show different transition points from learning subword to superword
tokens, which always gives an immediate improvement. SuperBPE also has better encoding
efÔ¨Åciency than a naive variant of BPE that does not use whitespace pretokenization at all.
performing well on these languages. Including multi-word tokens promises to be beneÔ¨Åcial
in several ways: it can lead to shorter token sequences, lowering the computational costs of
LM training and inference, and may also offer representational advantages by segmenting
text into more semantically cohesive units ( Salehi et al. ,2015;Otani et al. ,2020;Hofmann
et al. ,2021).
In this work, we introduce a superword tokenization algorithm that produces a vocabulary of
both subword and ‚Äúsuperword‚Äù tokens, which we use to refer to tokens that bridge more
than one word. Our method, SuperBPE , introduces a pretokenization curriculum to the popu-
lar byte-pair encoding (BPE) algorithm ( Sennrich et al. ,2016): whitespace pretokenization is
initially used to enforce learning of subword tokens only (as done in conventional BPE), but
is disabled in a second stage, where the tokenizer transitions to learning superword tokens.
Notably, SuperBPE tokenizers scale much better with vocabulary size‚Äîwhile BPE quickly
hits a point of diminishing returns and begins adding increasingly rare subwords to the
vocabulary, SuperBPE can continue to discover common word sequences to treat as single
tokens and improve encoding efÔ¨Åciency (see Figure 1 ).
In our main experiments, we pretrain English LMs at 8B scale from scratch. When Ô¨Åxing the
model size, vocabulary size, and training compute‚Äîvarying only the algorithm for learning
the vocabulary‚Äîwe Ô¨Ånd that models trained with SuperBPE tokenizers consistently and
signiÔ¨Åcantly improve over counterparts trained with a BPE tokenizer, while also being 27‚Äì
33% more efÔ¨Åcient at inference time . Our best SuperBPE model achieves an average +4.0%
2
Figure 2.7 The SuperBPE algorithm creating larger tokens by allowing a second stage of
merging across spaces. Figure from Liu et al. (2025).
Many of the tokenizers used in practice for large language models are multilin-
gual, trained on many languages. But because the training data for large language
models is vastly dominated by English text, these multilingual BPE tokenizers tend
to use most of the tokens for English, leaving fewer of them for other languages. The
result is that they do a better job of tokenizing English, and the other languages tend
to get their words split up into shorter tokens. For example let‚Äôs look at a Spanish
sentence from a recipe for plantains, together with an English translation.
The English has 18 tokens; each of the 14 words is a token (none of the words
are split into multiple tokens):
By contrast, the original 16 words in Spanish have been encoded into 33 tokens,
a much larger number. Notice that many basic words have been broken into pieces.
For example hondo , ‚Äòdeep‚Äô, has been segmented into handondo . Similarly for
jugo , ‚Äòjuice‚Äô,nuez , ‚Äònut‚Äô and jenjibre ‚Äòginger‚Äô):
Spanish is not a particularly low-resource language; this oversegmenting can be
even more serious in lower resource languages, often down to individual characters.
Oversegmenting into these tiny tokens can cause various problems for the down-
stream processing of the language. As will become more clear once we introduce
transformer models in Chapter 8, such fragmentation can lead to poor representa-
tions of meaning, the need for longer contexts, and higher costs to train models
(Rust et al., 2021; Ahia et al., 2023).
2.5 Rule-based tokenization
While data-based tokenization like BPE is the most common way of doing tokeniza-
tion, there are also situations where we want to constrain our tokens to be words and
not subwords. This might be useful if we are running parsing algorithms for English
where the parser might need grammatical words as input. Or it can be useful for
any linguistic application where we have some a prior deÔ¨Ånition of the token that we

18 CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
are interested in studying. Or it can be useful for social science applications where
orthographic words are useful domains of study.
In rule-based tokenization, we pre-deÔ¨Åne a standard and implement rules to im-
plement that kind of tokenization. Let‚Äôs explore this for English word tokenization.
We have some desiderata for English. We often want to break off punctua-
tion as a separate token; commas are a useful piece of information for parsers,
and periods help indicate sentence boundaries. But we‚Äôll often want to keep the
punctuation that occurs word internally, in examples like m.p.h. ,Ph.D. ,AT&T , and
cap‚Äôn . Special characters and numbers will need to be kept in prices ($45.55) and
dates (01/02/06 ); we don‚Äôt want to segment that price into separate tokens of ‚Äú45‚Äù
and ‚Äú55‚Äù. And there are URLs ( https://www.stanford.edu ), Twitter hashtags
(#nlproc ), or email addresses ( someone@cs.colorado.edu ).
Number expressions introduce complications; in addition to appearing at word
boundaries, commas appear inside numbers in English, every three digits: 555,500.50 .
Tokenization differs by language; languages like Spanish, French, and German, for
example, use a comma to mark the decimal point, and spaces (or sometimes periods)
where English puts commas, for example, 555 500,50 .
A rule-based tokenizer can also be used to expand clitic contractions that are clitic
marked by apostrophes, converting what‚Äôre to the two tokens what are , andwe‚Äôre
towe are . A clitic is a part of a word that can‚Äôt stand on its own, and can only oc-
cur when it is attached to another word. Such contractions occur in other alphabetic
languages, including French pronouns ( j‚Äôai and articles l‚Äôhomme ).
Depending on the application, tokenization algorithms may also tokenize mul-
tiword expressions like New York orrock ‚Äôn‚Äô roll as a single token, which re-
quires a multiword expression dictionary of some sort. Rule-based tokenization is
thus intimately tied up with named entity recognition , the task of detecting names,
dates, and organizations (Chapter 17).
One commonly used tokenization standard is known as the Penn Treebank to-
kenization standard, used for the parsed corpora (treebanks) released by the Lin-Penn Treebank
tokenization
guistic Data Consortium (LDC), the source of many useful datasets. This standard
separates out clitics ( doesn‚Äôt becomes does plus n‚Äôt), keeps hyphenated words to-
gether, and separates out all punctuation (to save space we‚Äôre showing visible spaces
‚Äò‚Äô between tokens, although newlines is a more common output):
Input :"The San Francisco-based restaurant," they said,
"doesn‚Äôt charge $10".
Output :"TheSanFrancisco-based restaurant ,"theysaid,
"doesn‚Äôtcharge $10".
In practice, since tokenization is run before any other language processing, it
needs to be very fast. For rule-based word tokenization we generally use deter-
ministic algorithms based on regular expressions compiled into efÔ¨Åcient Ô¨Ånite state
automata. For example, Fig. 2.8 shows a basic regular expression that can be used
to tokenize English with the nltk.regexp tokenize function of the Python-based
Natural Language Toolkit (NLTK) (Bird et al. 2009; https://www.nltk.org ).
Carefully designed deterministic algorithms can deal with the ambiguities that
arise, such as the fact that the apostrophe needs to be tokenized differently when used
as a genitive marker (as in the book‚Äôs cover ), a quotative as in ‚ÄòThe other class‚Äô, she
said, or in clitics like they‚Äôre .

2.6 ‚Ä¢ C ORPORA 19
>>> text = ‚ÄôThat U.S.A. poster-print costs $12.40...‚Äô
>>> pattern = r‚Äô‚Äô‚Äô(?x) # set flag to allow verbose regexps
... (?:[A-Z]\.)+ # abbreviations, e.g. U.S.A.
... | \w+(?:-\w+)* # words with optional internal hyphens
... | \$?\d+(?:\.\d+)?%? # currency, percentages, e.g. $12.40, 82%
... | \.\.\. # ellipsis
... | [][.,;"‚Äô?():_‚Äò-] # these are separate tokens; includes ], [
... ‚Äô‚Äô‚Äô
>>> nltk.regexp_tokenize(text, pattern)
[‚ÄôThat‚Äô, ‚ÄôU.S.A.‚Äô, ‚Äôposter-print‚Äô, ‚Äôcosts‚Äô, ‚Äô$12.40‚Äô, ‚Äô...‚Äô]
Figure 2.8 A Python trace of regular expression tokenization in the NLTK Python-based
natural language processing toolkit (Bird et al., 2009), commented for readability; the (?x)
verbose Ô¨Çag tells Python to strip comments and whitespace. Figure from Chapter 3 of Bird
et al. (2009).
2.5.1 Sentence Segmentation
Rule-based segmentation is commonly used for another kind of tokenization pro-
cess: the sentence. Sentence segmentation is a step that is can be optionally appliedsentence
segmentation
in text processing. It is especially important when applying NLP algorithms to tasks
of detecting structure, like parse structure.
Sentence segmentation depends on the language and the genre. The most useful
cues for segmenting a text into sentences in English written text tend to be punc-
tuation, like periods, question marks, and exclamation points. Question marks and
exclamation points are relatively unambiguous markers of sentence boundaries, and
simple rules can segment sentences when they appear.
The period character ‚Äú.‚Äù, on the other hand, is ambiguous between a sentence
boundary marker and a marker of abbreviations like Dr.orInc. The previous sen-
tence that you just read showed an even more complex case of this ambiguity, in
which the Ô¨Ånal period of Inc.marked both an abbreviation and the sentence bound-
ary marker. For this reason, sentence tokenization and word tokenization can be
addressed jointly.
Many English sentence tokenization methods work by Ô¨Årst deciding (often based
on deterministic rules, but sometimes via machine learning) whether a period is part
of the word or is a sentence-boundary marker. An abbreviation dictionary can help
determine whether the period is part of a commonly used abbreviation; the dictio-
naries can be hand-built or machine-learned (Kiss and Strunk, 2006), as can the Ô¨Ånal
sentence splitter. In the Stanford CoreNLP toolkit (Manning et al., 2014), for exam-
ple sentence splitting is rule-based, a deterministic consequence of tokenization; a
sentence ends when a sentence-ending punctuation (., !, or ?) is not already grouped
with other characters into a token (such as for an abbreviation or number), optionally
followed by additional Ô¨Ånal quotes or brackets.
2.6 Corpora
Words don‚Äôt appear out of nowhere. Any particular piece of text that we study
is produced by one or more speciÔ¨Åc speakers or writers, in a speciÔ¨Åc dialect of a
speciÔ¨Åc language, at a speciÔ¨Åc time, in a speciÔ¨Åc place, for a speciÔ¨Åc function.

20 CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
Perhaps the most important dimension of variation is the language. NLP algo-
rithms are most useful when they apply across many languages. The world has 7097
languages at the time of this writing, according to the online Ethnologue catalog
(Simons and Fennig, 2018). It is important to test algorithms on more than one lan-
guage, and particularly on languages with different properties; by contrast there is
an unfortunate current tendency for NLP algorithms to be developed or tested just on
English (Bender, 2019). Even when algorithms are developed beyond English, they
tend to be developed for the ofÔ¨Åcial languages of large industrialized nations (Chi-
nese, Spanish, Japanese, German etc.), but we don‚Äôt want to limit tools to just these
few languages. Furthermore, most languages also have multiple varieties, often spo-
ken in different regions or by different social groups. Thus, for example, if we‚Äôre
processing text that uses features of African American English ( AAE ) or African AAE
American Vernacular English (AA VE)‚Äîthe variations of English that can be used
by millions of people in African American communities (King 2020)‚Äîwe must use
NLP tools that function with features of those varieties. Twitter posts might use fea-
tures often used by speakers of African American English, such as constructions like
iont(I don‚Äôt in Mainstream American English ( MAE )), or talmbout corresponding MAE
to MAE talking about , both examples that inÔ¨Çuence word segmentation (Blodgett
et al. 2016, Jones 2015).
It‚Äôs also quite common for speakers or writers to use multiple languages in a sin-
gle utterance, a phenomenon called code switching . Code switching is enormously code switching
common across the world; here are examples showing Spanish and (transliterated)
Hindi code switching with English (Solorio et al. 2014, Jurgens et al. 2017):
(2.12) Por primera vez veo a @username actually being hateful! it was beautiful:)
[For the Ô¨Årst time I get to see @username actually being hateful! it was
beautiful:) ]
(2.13) dost tha or ra- hega ... dont wory ... but dherya rakhe
[‚Äúhe was and will remain a friend ... don‚Äôt worry ... but have faith‚Äù]
Another dimension of variation is the genre. The text that our algorithms must
process might come from newswire, Ô¨Åction or non-Ô¨Åction books, scientiÔ¨Åc articles,
Wikipedia, or religious texts. It might come from spoken genres like telephone
conversations, business meetings, police body-worn cameras, medical interviews,
or transcripts of television shows or movies. It might come from work situations
like doctors‚Äô notes, legal text, or parliamentary or congressional proceedings.
Text also reÔ¨Çects the demographic characteristics of the writer (or speaker): their
age, gender, race, socioeconomic class can all inÔ¨Çuence the linguistic properties of
the text we are processing.
And Ô¨Ånally, time matters too. Language changes over time, and for some lan-
guages we have good corpora of texts from different historical periods.
Because language is so situated, when developing computational models for lan-
guage processing from a corpus, it‚Äôs important to consider who produced the lan-
guage, in what context, for what purpose. How can a user of a dataset know all these
details? The best way is for the corpus creator to build a datasheet (Gebru et al., datasheet
2020) or data statement (Bender et al., 2021) for each corpus. A datasheet speciÔ¨Åes
properties of a dataset like:
Motivation : Why was the corpus collected, by whom, and who funded it?
Situation : When and in what situation was the text written/spoken? For example,
was there a task? Was the language originally spoken conversation, edited
text, social media communication, monologue vs. dialogue?

2.7 ‚Ä¢ R EGULAR EXPRESSIONS 21
Language variety : What language (including dialect/region) was the corpus in?
Speaker demographics : What was, e.g., the age or gender of the text‚Äôs authors?
Collection process : How big is the data? If it is a subsample how was it sampled?
Was the data collected with consent? How was the data pre-processed, and
what metadata is available?
Annotation process : What are the annotations, what are the demographics of the
annotators, how were they trained, how was the data annotated?
Distribution : Are there copyright or other intellectual property restrictions?
2.7 Regular Expressions
One of the most useful tools for text processing in computer science is the regular
expression (orregex ), a language for specifying text strings. Regexes are used inregular
expression
every computer language, in text processing tools like Unix grep , and in editors
like vim or Emacs. And they play an important role in the pre-tokenization step
for tokenization algorithms like BPE. Formally, a regular expression is an algebraic
notation for characterizing a set of strings. Practically, we can use a regex to search
for a string in a text and to specify how to change the string, both of which are key
to tokenization.
We use regular expressions to search for a pattern in astring which can be a string
single line or a longer text. For example, the Python function
re.search(pattern,string)
scans through the string and returns the Ô¨Årst match inside it for the pattern . In the
following examples we generally highlight the exact string that matches the regular
expression and show only the Ô¨Årst match. We‚Äôll use Python syntax, expressing the
regex as a raw string delimited by double quotes: r"regex" . Raw strings treat
backslashes as literal characters, which will be important since many regex patterns
we‚Äôll introduce use backslashes.
Regular expressions come in different variants, so using an online regex tester
can help make sure your regex does what you think it‚Äôs doing.
2.7.1 Character Disjunction: The Square Bracket
The simplest kind of regular expression is a sequence of simple characters. The pat-
ternr"Buttercup" matches the substring Buttercup in any string (like the string
I‚Äôm called little Buttercup ). But often we need to use special characters.
For example, we might want to match either some character or another. For exam-
ple, regular expressions are generally case sensitive :r"s" matches a lower case s
but not an upper case S. To match both sandSwe can use the character disjunc-
tion operator, the square braces [and]. The string of characters inside the bracescharacter
disjunction
speciÔ¨Åes a disjunction of characters to match. For example, Fig. 2.9 shows that the
patternr"[mM]" matches patterns containing either morM.
Pattern Match String
r"[mM]ary" Mary or mary ‚ÄúMary Ann stopped by Mona‚Äôs‚Äù
r"[abc]" ‚Äòa‚Äô, ‚Äòb‚Äô, or‚Äòc‚Äô ‚ÄúIn uomini, in solda ti‚Äù
r"[1234567890]" any one digit ‚Äúplenty of 7 to 5‚Äù
Figure 2.9 The use of the brackets []to specify a disjunction of characters.

22 CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
The regular expression r"[1234567890]" speciÔ¨Åes any single digit. This can
get awkward (imagine typing r"[ABCDEFGHIJKLMNOPQRSTUVWXYZ]" to mean an
uppercase letter) so the brackets can also be used with a dash ( -) to specify any one
character in a range . The pattern r"[2-5]" speciÔ¨Åes any one of the characters 2,3, range
4, or5. The pattern r"[b-g]" speciÔ¨Åes one of the characters b,c,d,e,f, org. Some
other examples are shown in Fig. 2.10.
Regex Match Example Patterns Matched
r"[A-Z]" an upper case letter ‚Äúwe should call it ‚ÄòD renched Blossoms‚Äô ‚Äù
r"[a-z]" a lower case letter ‚Äúmy beans were impatient to be hoed!‚Äù
r"[0-9]" a single digit ‚ÄúChapter 1 : Down the Rabbit Hole‚Äù
Figure 2.10 The use of the brackets []plus the dash -to specify a range.
The square braces can also be used to specify what a single character cannot be,
by use of the caret ÀÜ. If the caret ÀÜis the Ô¨Årst symbol after the open square brace
[, the resulting pattern is negated. For example, the pattern r"[ÀÜa]" matches any
single character (including special characters) except a. This is only true when the
caret is the Ô¨Årst symbol after the open square brace. If it occurs anywhere else, it
usually stands for a caret; Fig. 2.11 shows some examples.
Regex Match (single characters) Example Patterns Matched
r"[ÀÜA-Z]" not an upper case letter ‚ÄúOyfn pripetchik‚Äù
r"[ÀÜSs]" neither ‚ÄòS‚Äô nor ‚Äòs‚Äô ‚ÄúIhave no exquisite reason for‚Äôt‚Äù
r"[ÀÜ.]" not a period ‚Äúour resident Djinn‚Äù
r"[eÀÜ]" either ‚Äòe‚Äô or ‚Äò ÀÜ‚Äô ‚Äúlook up ÀÜ now‚Äù
r"aÀÜb" the pattern ‚Äò aÀÜb‚Äô ‚Äúlook up aÀÜ b now‚Äù
Figure 2.11 The caret ÀÜfor negation or just to mean ÀÜ. See below re: the backslash for escaping the period.
2.7.2 Counting, Optionality, and Wildcards
How can we talk about optional elements, like an optional sif we want to match both
koala andkoalas ? We can‚Äôt use the square brackets, because while they allow us to
say ‚Äús or S‚Äù, they don‚Äôt allow us to say ‚Äús or nothing‚Äù. For this we use the question
markr"?" , which means ‚Äúthe preceding character or nothing‚Äù,. So r"colou?r"
matches both color andcolour , andr"koala?" matcheskoala orkoalas .
There‚Äôs another way to talk about elements that may or may not occur. Consider
the language of certain sheep, which consists of strings that look like the following:
baa!
baaa!
baaaa!
. . .
This sheep language consists of strings with a b, followed by at least two (and
arbitrarily more) a‚Äôs, followed by an exclamation point. To represent this language,
we‚Äôll use a useful operator that is represented by the asterisk or *, called the Kleene
*(generally pronounced ‚Äúcleany star‚Äù). The Kleene star means ‚Äúzero or more oc- Kleene *
currences of the immediately previous character or regular expression‚Äù. So r"a*"
means ‚Äúany string of zero or more as‚Äù.
Couldr"ba*" represent the sheep language? It will correctly match baor
baaaaaa , but there‚Äôs a problem! It will also match b, with noa, orbawith only one

2.7 ‚Ä¢ R EGULAR EXPRESSIONS 23
a. That‚Äôs because Kleene star means ‚Äúzero or more occurrences‚Äù. Instead, for the
sheep language we‚Äôll want r"baaa*" , meaning bfollowed by aafollowed by zero
or more additional as. More complex patterns can also be repeated. So r"[ab]*"
means ‚Äúzero or more a‚Äôs or b‚Äôs‚Äù (not ‚Äúzero or more right square braces‚Äù). This will
match strings like aaaa orababab orbbbb , as well as the empty string. For speci-
fying an integer (a string of digits) we can use r"[0-9][0-9]*" . (Why isn‚Äôt it just
r"[0-9]*" ?)
There is a slightly shorter way to specify ‚Äúat least one‚Äù of some character: the
Kleene + , which means ‚Äúone or more occurrences of the immediately preceding Kleene +
character or regular expression‚Äù. So r"[0-9]+" is the normal way to specify ‚Äúa
sequence of digits‚Äù, and we could also specify the sheep language as r"baa+!" .
Besides the Kleene * and Kleene + we can also use explicit numbers as coun-
ters, by enclosing them in curly brackets. The operator r"{3}" means ‚Äúexactly 3
occurrences of the previous character or expression‚Äù. So r"ax{10}z" will match a
followed by exactly 10 x‚Äôs followed by z.
An important special character is the period (r"." ), awildcard expression that period
matches any single character ( except a newline).
The wildcard is often used together with the Kleene star to mean ‚Äúany string
of characters‚Äù. For example, suppose we want to Ô¨Ånd any line in which a particu-
lar word, for example, rose , appears twice. We can specify this with the regular
expression r"rose.*rose" , meaning two roses, with a sequence of zero or more
characters (of any kind) between them. Fig. 2.12 summarizes.
Regex Match
* zero or more occurrences of the previous char or expression
+ one or more occurrences of the previous char or expression
? zero or one occurrence of the previous char or expression
{n} exactly noccurrences of the previous char or expression
. any single char
.* any string of zero or more chars
Figure 2.12 Counting and wildcards.
2.7.3 Anchors and Boundaries
Anchors are special characters that anchor regular expressions to particular places anchors
in a string. The most common anchors are the caret ÀÜand the dollar sign $. The
caretÀÜmatches the start of a line. The pattern r"ÀÜThe" matches the word Theonly
at the start of a line. Thus, the caret ÀÜhas three uses: to match the start of a line,
to indicate a negation inside of square brackets, and just to mean a caret. (What are
the contexts that allow the system to know which function a given caret is supposed
to have?) The dollar sign $matches the end of a line. So the pattern ‚ê£$is a useful
pattern for matching a space at the end of a line, and r"ÀÜThe dog\.$" matches a
line that contains only the phrase The dog. with a Ô¨Ånal period.
Note that we have to use the backslash in the prior example since we want
the.to mean ‚Äúperiod‚Äù and not the wildcard. By contrast, the regular expression
r"ÀÜThe dog.$" would match The dog. but alsoThe dog! andThe dogo . As
we‚Äôll discuss below, all the special characters we‚Äôve deÔ¨Åned so far ( * + ? . [
]) need to be backslashed when we mean to use them literally.
There are other anchors: \bmatches a word boundary, and \Bmatches a non
word-boundary. Thus, r"\bthe\b" matches the word thebut not the word other .

24 CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
Regex Match
ÀÜ start of line
$ end of line
\b word boundary
\B non-word boundary
Figure 2.13 Anchors in regular expressions.
A ‚Äúword‚Äù for the purposes of a regex is deÔ¨Åned (based on words in programming
languages) as a sequence of digits, underscores, or letters. Thus r"\b99\b" will
match the string 99inThere are 99 bottles of beer on the wall (because
99 follows a space) but not 99inThere are 299 bottles of beer on the
wall (since 99 follows a number). But it will match 99in$99(since99follows a
dollar sign ($), which is not a digit, underscore, or letter).
Note that all these anchors and boundary operators technically match the empty
string, meaning that they don‚Äôt eat up any characters of the string. The carat in the
patternr"ÀÜThe" matches the start of "The" but doesn‚Äôt actually advance over the
Ô¨Årst character T. And the pattern r"the\b the matchesthe the ; the\bis aware
of the fact that the space is a boundary, but it matches the empty string right before
the space, not the space, so that the space character is available to be matched.
2.7.4 Disjunction, Grouping, and Precedence
Suppose we need to search for texts about pets; perhaps we are particularly interested
in cats and dogs. In such a case, we might want to search for either the string
cator the string dog. Since we can‚Äôt use the square brackets to search for ‚Äúcat or
dog‚Äù (why wouldn‚Äôt r"[catdog]" do the right thing?), we need a new operator,
thedisjunction operator, also called the pipe symbol|. The pattern r"cat|dog" disjunction
matches either the string cator the string dog.
Sometimes we need to use this disjunction operator in the midst of a larger se-
quence. For example, suppose I want to search for mentions of pet Ô¨Åsh. How can
I specify both guppy andguppies ? We cannot simply say r"guppy|ies" , because
that would match only the strings guppy andies. This is because sequences like
guppy take precedence over the disjunction operator |. To make the disjunction precedence
operator apply only to a speciÔ¨Åc pattern, we need to use the parenthesis operators (
and). Enclosing a pattern in parentheses makes it act like a single character for the
purposes of neighboring operators like the pipe |and the Kleene *. So the pattern
r"gupp(y|ies)" would specify that we meant the disjunction only to apply to the
sufÔ¨Åxesyandies.
The parenthesis operator (is also useful when we are using counters like the
Kleene*. Unlike the |operator, the Kleene *operator applies by default only to
a single character, not to a whole sequence. Suppose we want to match repeated
instances of a string. Perhaps we have a line that has column labels of the form
Column 1 Column 2 Column 3 . The expression r"Column‚ê£[0-9]+‚ê£*" will not
match any number of columns; instead, it will match a single column followed by
any number of spaces! The star here applies only to the space ‚ê£that precedes it,
not to the whole sequence. With the parentheses, we could write the expression
r"(Column‚ê£[0-9]+‚ê£+)*" to match the word Column , followed by a number and
optional spaces, the whole pattern repeated zero or more times.
This idea that one operator may take precedence over another, requiring us to
sometimes use parentheses to specify what we mean, is formalized by the operator
precedence hierarchy for regular expressions. The following table gives the orderoperator
precedence

2.7 ‚Ä¢ R EGULAR EXPRESSIONS 25
of operator precedence, from highest precedence to lowest precedence.
Parenthesis ()
Counters * + ? {}
Sequences and anchors the ÀÜmy end$
Disjunction |
Thus, because counters have a higher precedence than sequences,
r"the*" matches theeeee but not thethe . Because sequences have a higher prece-
dence than disjunction, r"the|any" matches theoranybut not thany ortheny .
Patterns can be ambiguous in another way. Consider the expression r"[a-z]*"
when matching against the text once upon a time . Sincer"[a-z]*r" matches zero
or more letters, this expression could match nothing, or just the Ô¨Årst letter o,on,onc,
oronce . In these cases regular expressions always match the largest string they can;
we say that patterns are greedy , expanding to cover as much of a string as they can. greedy
There are, however, ways to enforce non-greedy matching, using another mean- non-greedy
ing of the ?qualiÔ¨Åer. The operator *?is a Kleene star that matches as little text as *?
possible. The operator +?is a Kleene plus that matches as little text as possible. +?
2.7.5 A Simple Example
Suppose we wanted to write a regex to Ô¨Ånd cases of the English article the. A simple
(but incorrect) pattern might be:
r"the" (2.14)
One problem is that this pattern will miss the word when it begins a sentence and
hence is capitalized (i.e., The). This might lead us to the following pattern:
r"[tT]he" (2.15)
But we will still overgeneralize, incorrectly return texts with theembedded in other
words (e.g., other orthere ). So we need to specify that we want instances with a
word boundary on both sides:
r"\b[tT]he\b" (2.16)
The simple process we just went through was based on Ô¨Åxing two kinds of errors:
false positives , strings that we incorrectly matched like other orthere , and false false positives
negatives , strings that we incorrectly missed, like The. Addressing these two kinds false negatives
of errors comes up again and again in language processing. Reducing the overall
error rate for an application thus involves two antagonistic efforts:
‚Ä¢ Increasing precision (minimizing false positives)
‚Ä¢ Increasing recall (minimizing false negatives)
We‚Äôll come back to precision and recall with more precise deÔ¨Ånitions in Chapter 4.
2.7.6 More Operators
Figure 2.14 shows some useful aliases for common ranges:
Finally, certain special characters are referred to by special notation based on the
backslash ( \) (see Fig. 2.15). The most common of these are the newline character newline
\nand the tabcharacter\t.

26 CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
Regex Expansion Match First Matches
\d [0-9] any digit Party‚ê£of‚ê£5
\D [ÀÜ0-9] any non-digit Blue‚ê£moon
\w [a-zA-Z0-9_] any alphanumeric/underscore Daiyu
\W [ÀÜ\w] a non-alphanumeric !!!!
\s [‚ê£\r\t\n\f] whitespace (space, tab) inConcord
\S [ÀÜ\s] Non-whitespace in‚ê£Concord
Figure 2.14 Aliases for common sets of characters.
How do we refer to characters that are special themselves (like .,*,-,[, and
\) when we mean them literally, not in their special usage? That is, if we are trying
to match a period, or a star, or a bracket or paren? To get the literal meaning of a
special character, we need to precede them with a backslash, (i.e., r"\." ,r"\*" ,
r"\[" , andr"\\" ).
Regex Match First Patterns Matched
\* an asterisk ‚Äú*‚Äù ‚ÄúK* A*P*L*A*N‚Äù
\. a period ‚Äú.‚Äù ‚ÄúDr. Livingston, I presume‚Äù
\? a question mark ‚ÄúWhy don‚Äôt they come and lend a hand? ‚Äù
\n a newline
\t a tab
Figure 2.15 Some characters that need to be escaped (via backslash).
2.7.7 Substitutions and Capture Groups
An important use of regular expressions is in substitutions , where we want to re- substitution
place one string with another. Regular expression can help us specify the string to
be replaced as well as the replacement. In Python we use the function re.sub()
(similar functions exist in other languages and environments).
re.sub (pattern, repl, string) takes three arguments: a pattern to search for, a
replacement to replace it with, and a string in which to do the search and replacing
We could for example change every instance of cherry toapricot instring :
re.sub(r"cherry", r"apricot", string)
Or we could convert to upper case all the instances of a particular name:
re.sub(r"janet", r"Janet", string)
More often, however, the substitution depends in a more complex way on the
string that matched the pattern . For example, suppose we have a document in
which all the dates are in US format (mm/dd/yyyy) and we want to change them
into the format used in the EU and many other regions: (dd-mm-yyyy). The pat-
ternr"\d{2}/\d{2}/\d{4}" will match a date. But how do we specify in the
replacement that we want to swap the date and month values?
The tool in regular expression for this is the capture group . A capture group capture group
uses parentheses to capture ( store ) the values that we matched in the search, so we
can reuse them in the replacement. We put a set of parentheses around the part of
thepattern we want to capture, and it will get stored in a numbered group (groups
are numbered from left to right). Then in the repl, we refer back to that group with
a number command.
Consider the following expression:

2.7 ‚Ä¢ R EGULAR EXPRESSIONS 27
re.sub(r"(\d{2})/(\d{2})/(\d{4})", r"\2-\1-\3", string)}
We‚Äôve put parentheses (and)around the two month digits, the two day digits,
and the four year digits, thus storing the Ô¨Årst 2 digits in group 1, the second 2 digits
in group 2, and the Ô¨Ånal digits in group 3. Then in the replstring, we use number
operators \1,\2, and\3, to refer back to the Ô¨Årst, second, and third registers. The
result would take a string like
The date is 10/15/2011
and convert it to
The date is 15-10-2011
Capture groups can be useful even if we are not doing substitutions. For example
we can use them to Ô¨Ånd repetitions, something we often need in text processing. For
example, to Ô¨Ånd a repeated word in a string, we can use this pattern which searches
for a word, captures it in a group, and then refers back to it after whitespace:
r"\b([A-Za-z]+)\s+\1\b"
Parentheses thus have a double function in regular expressions; they are used to
group terms for specifying the order in which operators should apply, and they are
used to capture the match. Occasionally we need parentheses for grouping, but don‚Äôt
want to capture the resulting pattern. In that case we use a non-capturing group ,non-capturing
group
which is speciÔ¨Åed by putting the special commands ?:after the open parenthesis,
in the form (?: pattern ) . Non-capture groups are usually used when we are
trying to capture only part of a long or complex pattern. Perhaps we are matching
a sequence of dates ( \d\d/\d\d/\d\d\d\d ) separated by spaces and we want to
extract only the 15th one. We need to use parenthesis in order to use the counting
operator on the Ô¨Årst 14, but we don‚Äôt want to store all the useless information. The
following pattern only stores the 15th date in group 1:
r"(?:\d\d/\d\d/\d\d\d\d\s+){14}(\d\d/\d\d/\d\d\d\d)" (2.17)
Substitutions and capture groups are also useful for implementing historically
important chatbots like ELIZA (Weizenbaum, 1966). Recall that ELIZA simulates
a Rogerian psychologist by carrying on conversations like the following:
User 2: They‚Äôre always bugging us about something or other.
ELIZA 2:CAN YOU THINK OF A SPECIFIC EXAMPLE
User 3: Well, my boyfriend made me come here.
ELIZA 3:YOUR BOYFRIEND MADE YOU COME HERE
User 4: He says I‚Äôm depressed much of the time.
ELIZA 4:I AM SORRY TO HEAR YOU ARE DEPRESSED
ELIZA works by having a series or cascade of regex substitutions each of which
matches and changes some part of the input lines. After the input is uppercased,
substitutions change all instances of MYtoYOUR , andI‚ÄôM toYOU ARE , and so on.
That way when ELIZA repeats back part of the user utterance, it will seem to be
referring correctly to the user. The next set of substitutions matches and replaces
other patterns in the input, turning the input into a complete response. Here are
some examples:
re.sub(r".* YOU ARE (DEPRESSED|SAD) .*",r"I AM SORRY TO HEAR YOU ARE \1",input)
re.sub(r".* YOU ARE (DEPRESSED|SAD) .*",r"WHY DO YOU THINK YOU ARE \1",input)
re.sub(r".* ALWAYS .*",r"CAN YOU THINK OF A SPECIFIC EXAMPLE",input)

28 CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
2.7.8 Lookahead Assertions
Finally, there will be times when we need to predict the future: look ahead in the
text to see if some pattern matches, but not yet advance the pointer we always keep
to where we are in the text, so that we can then deal with the pattern if it occurs, but
if it doesn‚Äôt we can check for something else instead.
These lookahead assertions make use of the (?syntax that we saw in the previ- lookahead
ous section for non-capture groups. The operator (?= pattern) is true ifpattern
occurs, but is zero-width , i.e. the match pointer doesn‚Äôt advance, just as we saw zero-width
with anchors and boundary markers like \b. The operator (?! pattern) only re-
turns true if a pattern does not match, but again is zero-width and doesn‚Äôt advance
the pointer. Negative lookahead is commonly used when we are parsing some com-
plex pattern but want to rule out a special case. For example suppose we want to
capture the Ô¨Årst word on the line, but only if it doesn‚Äôt start with the letter T. We can
use negative lookahead to do this:
r"ÀÜ(?![tT])(\w+)\b" (2.18)
The Ô¨Årst negative lookahead says that the line must not start with a torT, but
matches the empty string, not moving the match pointer. Then the capture group
captures the Ô¨Årst word.
2.8 Simple Unix Tools for Word Tokenization
For English it is possible to do simple naive word tokenization and frequency com-
putation in a single Unix command-line. As Church (1994) points out, this can be
useful when we need quick information about a text corpus. We‚Äôll make use of some
Unix commands: tr, used to systematically change particular characters in the in-
put;sort , which sorts input lines in alphabetical order; and uniq , which collapses
and counts adjacent identical lines.
For example let‚Äôs begin with the ‚Äòcomplete words‚Äô of Shakespeare in one Ô¨Åle,
sh.txt . We can use trto tokenize the words by changing every sequence of non-
alphabetic characters to a newline (‚ÄôA-Za-z‚Äô means alphabetic and the -c option
complements to non-alphabet, so together they mean to change every non-alphabetic
character into a newline. The -s (‚Äòsqueeze‚Äô) option is used to replace the result
of multiple consecutive changes into a single output, so a series of non-alphabetic
characters in a row would all be ‚Äòsqueezed‚Äô into a single newline):
tr -sc ‚ÄôA-Za-z‚Äô ‚Äô\n‚Äô < sh.txt
The output of this command will be:
THE
SONNETS
by
William
Shakespeare
From
fairest
creatures
...

2.9 ‚Ä¢ M INIMUM EDITDISTANCE 29
Now that there is one word per line, we can sort the lines, and pass them to uniq
-cwhich will collapse and count them:
tr -sc ‚ÄôA-Za-z‚Äô ‚Äô\n‚Äô < sh.txt | sort | uniq -c
with the following output:
1945 A
72 AARON
19 ABBESS
25 Aaron
6 Abate
1 Abates
...
Alternatively, we can collapse all the upper case to lower case:
tr -sc ‚ÄôA-Za-z‚Äô ‚Äô\n‚Äô < sh.txt | tr A-Z a-z | sort | uniq -c
whose output is
14725 a
97 aaron
1 abaissiez
10 abandon
2 abandoned
2 abase
1 abash
14 abate
...
Now we can sort again to Ô¨Ånd the frequent words. The -noption tosort means
to sort numerically rather than alphabetically, and the -roption means to sort in
reverse order (highest-to-lowest):
tr -sc ‚ÄôA-Za-z‚Äô ‚Äô\n‚Äô < sh.txt | tr A-Z a-z | sort | uniq -c | sort -n -r
The results show that the most frequent words in Shakespeare, as in any other
corpus, are the short function words like articles, pronouns, prepositions:
27378 the
26084 and
22538 i
19771 to
17481 of
14725 a
13826 you
...
Unix tools of this sort can be very handy in building quick word count statistics
for any corpus in English. For anything more complex, we generally turn to the
more sophisticated tokenization algorithms we‚Äôve discussed above.
2.9 Minimum Edit Distance
We often need a way to compare how similar two words or strings are. As we‚Äôll
see in later chapters, this comes up most commonly in tasks like automatic speech

30 CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
recognition or machine translation, where we want to know how similar the sequence
of words is to some reference sequence of words.
Edit distance gives us a way to quantify these intuitions about string similarity.
More formally, the minimum edit distance between two strings is deÔ¨Åned as theminimum edit
distance
minimum number of editing operations (operations like insertion, deletion, substitu-
tion) needed to transform one string into another. In this section we‚Äôll introduce edit
distance for single words, but the algorithm applies equally to entire strings.
The gap between intention andexecution , for example, is 5 (delete an i, substi-
tuteeforn, substitute xfort, insertc, substitute uforn). It‚Äôs much easier to see
this by looking at the most important visualization for string distances, an alignment alignment
between the two strings, shown in Fig. 2.16. Given two sequences, an alignment is
a correspondence between substrings of the two sequences. Thus, we say Ialigns
with the empty string, NwithE, and so on. Beneath the aligned strings is another
representation; a series of symbols expressing an operation list for converting the
top string into the bottom string: dfor deletion, sfor substitution, ifor insertion.
INTE*NTION
||||||||||
*EXECUTION
d s s i s
Figure 2.16 Representing the minimum edit distance between two strings as an alignment .
The Ô¨Ånal row gives the operation list for converting the top string into the bottom string: d for
deletion, s for substitution, i for insertion.
We can also assign a particular cost or weight to each of these operations. The
Levenshtein distance between two sequences is the simplest weighting factor in
which each of the three operations has a cost of 1 (Levenshtein, 1966)‚Äîwe assume
that the substitution of a letter for itself, for example, tfort, has zero cost. The Lev-
enshtein distance between intention andexecution is 5. Levenshtein also proposed
an alternative version of his metric in which each insertion or deletion has a cost of
1 and substitutions are not allowed. (This is equivalent to allowing substitution, but
giving each substitution a cost of 2 since any substitution can be represented by one
insertion and one deletion). Using this version, the Levenshtein distance between
intention andexecution is 8.
2.9.1 The Minimum Edit Distance Algorithm
How do we Ô¨Ånd the minimum edit distance? We can think of this as a search task, in
which we are searching for the shortest path‚Äîa sequence of edits‚Äîfrom one string
to another.
n t e n t i o ni n t e c n t i o ni n x e n t i o ndelinssubsti n t e n t i o n
Figure 2.17 Finding the edit distance viewed as a search problem
The space of all possible edits is enormous, so we can‚Äôt search naively. However,
lots of distinct edit paths will end up in the same state (string), so rather than recom-

2.9 ‚Ä¢ M INIMUM EDITDISTANCE 31
puting all those paths, we could just remember the shortest path to a state each time
we saw it. We can do this by using dynamic programming . Dynamic programmingdynamic
programming
is the name for a class of algorithms, Ô¨Årst introduced by Bellman (1957), that apply
a table-driven method to solve problems by combining solutions to subproblems.
Some of the most commonly used algorithms in natural language processing make
use of dynamic programming, such as the Viterbi algorithm (Chapter 17) and the
CKY algorithm for parsing (Chapter 18).
The intuition of a dynamic programming problem is that a large problem can
be solved by properly combining the solutions to various subproblems. Consider
the shortest path of transformed words that represents the minimum edit distance
between the strings intention andexecution shown in Fig. 2.18.
n t e n t i o ni n t e n t i o n
e t e n t i o n
e x e n t i o n
e x e n u t i o n
e x e c u t i o ndelete i
substitute n by e
substitute t by x
insert u
substitute n by c
Figure 2.18 Path from intention toexecution .
Imagine some string (perhaps it is exention ) that is in this optimal path (whatever
it is). The intuition of dynamic programming is that if exention is in the optimal
operation list, then the optimal sequence must also include the optimal path from
intention toexention . Why? If there were a shorter path from intention toexention ,
then we could use it instead, resulting in a shorter overall path, and the optimal
sequence wouldn‚Äôt be optimal, thus leading to a contradiction.
The minimum edit distance algorithm was named by Wagner and Fischerminimum edit
distance
algorithm(1974) but independently discovered by many people (see the Historical Notes sec-
tion of Chapter 17).
Let‚Äôs Ô¨Årst deÔ¨Åne the minimum edit distance between two strings. Given two
strings, the source string Xof length n, and target string Yof length m, we‚Äôll deÔ¨Åne
D[i,j]as the edit distance between X[1..i]andY[1..j], i.e., the Ô¨Årst icharacters of X
and the Ô¨Årst jcharacters of Y. The edit distance between XandYis thus D[n,m].
We‚Äôll use dynamic programming to compute D[n,m]bottom up, combining so-
lutions to subproblems. In the base case, with a source substring of length ibut an
empty target string, going from icharacters to 0 requires ideletes. With a target
substring of length jbut an empty source going from 0 characters to jcharacters
requires jinserts. Having computed D[i,j]for small i,jwe then compute larger
D[i,j]based on previously computed smaller values. The value of D[i,j]is com-
puted by taking the minimum of the three possible paths through the matrix which
arrive there:
D[i,j] =minÔ£±
Ô£≤
Ô£≥D[i‚àí1,j]+del-cost (source [i])
D[i,j‚àí1]+ins-cost (target [j])
D[i‚àí1,j‚àí1]+sub-cost (source [i],target [j])(2.19)
We mentioned above two versions of Levenshtein distance, one in which substitu-
tions cost 1 and one in which substitutions cost 2 (i.e., are equivalent to an insertion
plus a deletion). Let‚Äôs here use that second version of Levenshtein distance in which

32 CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
the insertions and deletions each have a cost of 1 (ins-cost( ¬∑) = del-cost(¬∑) = 1), and
substitutions have a cost of 2 (except substitution of identical letters has zero cost).
Under this version of Levenshtein, the computation for D[i,j]becomes:
D[i,j] =minÔ£±
Ô£¥Ô£¥Ô£≤
Ô£¥Ô£¥Ô£≥D[i‚àí1,j]+1
D[i,j‚àí1]+1
D[i‚àí1,j‚àí1]+{2; if source [i]Ã∏=target [j]
0; if source [i] =target [j](2.20)
The algorithm is summarized in Fig. 2.19; Fig. 2.20 shows the results of applying
the algorithm to the distance between intention andexecution with the version of
Levenshtein in Eq. 2.20.
function MIN-EDIT-DISTANCE (source ,target )returns min-distance
n‚ÜêLENGTH (source )
m‚ÜêLENGTH (target )
Create a distance matrix D[n+1,m+1]
#Initialization: the zeroth row and column is the distance from the empty string
D[0,0] = 0
foreach row ifrom 1tondo
D[i,0]‚ÜêD[i-1,0] + del-cost (source [i])
foreach column jfrom 1tomdo
D[0,j]‚ÜêD[0,j-1] + ins-cost (target [j])
#Recurrence relation:
foreach row ifrom 1tondo
foreach column jfrom 1tomdo
D[i, j]‚ÜêMIN(D[i‚àí1,j] +del-cost (source [i]),
D[i‚àí1,j‚àí1] + sub-cost (source [i],target [j]),
D[i,j‚àí1] + ins-cost (target [j]))
#Termination
return D[n,m]
Figure 2.19 The minimum edit distance algorithm, an example of the class of dynamic
programming algorithms. The various costs can either be Ô¨Åxed (e.g., ‚àÄx,ins-cost (x) =1)
or can be speciÔ¨Åc to the letter (to model the fact that some letters are more likely to be in-
serted than others). We assume that there is no cost for substituting a letter for itself (i.e.,
sub-cost (x,x) =0).
Alignment Knowing the minimum edit distance is useful for algorithms like Ô¨Ånd-
ing potential spelling error corrections. But the edit distance algorithm is important
in another way; with a small change, it can also provide the minimum cost align-
ment between two strings. Aligning two strings is useful throughout speech and
language processing. In speech recognition, minimum edit distance alignment is
used to compute the word error rate (Chapter 15). Alignment plays a role in ma-
chine translation, in which sentences in a parallel corpus (a corpus with a text in two
languages) need to be matched to each other.
To extend the edit distance algorithm to produce an alignment, we can start by
visualizing an alignment as a path through the edit distance matrix. Figure 2.21
shows this path with boldfaced cells. Each boldfaced cell represents an alignment
of a pair of letters in the two strings. If two boldfaced cells occur in the same row,

2.9 ‚Ä¢ M INIMUM EDITDISTANCE 33
Src\Tar # e x e c u t i o n
# 0 1 2 3 4 5 6 7 8 9
i 1 2 3 4 5 6 7 6 7 8
n 2 3 4 5 6 7 8 7 8 7
t 3 4 5 6 7 8 7 8 9 8
e 4 3 4 5 6 7 8 9 10 9
n 5 4 5 6 7 8 9 10 11 10
t 6 5 6 7 8 9 8 9 10 11
i 7 6 7 8 9 10 9 8 9 10
o 8 7 8 9 10 11 10 9 8 9
n 9 8 9 10 11 12 11 10 9 8
Figure 2.20 Computation of minimum edit distance between intention andexecution with
the algorithm of Fig. 2.19, using Levenshtein distance with cost of 1 for insertions or dele-
tions, 2 for substitutions.
there will be an insertion in going from the source to the target; two boldfaced cells
in the same column indicate a deletion.
Figure 2.21 also shows the intuition of how to compute this alignment path. The
computation proceeds in two steps. In the Ô¨Årst step, we augment the minimum edit
distance algorithm to store backpointers in each cell. The backpointer from a cell
points to the previous cell (or cells) that we came from in entering the current cell.
We‚Äôve shown a schematic of these backpointers in Fig. 2.21. Some cells have mul-
tiple backpointers because the minimum extension could have come from multiple
previous cells. In the second step, we perform a backtrace . In a backtrace, we start backtrace
from the last cell (at the Ô¨Ånal row and column), and follow the pointers back through
the dynamic programming matrix. Each complete path between the Ô¨Ånal cell and the
initial cell is a minimum distance alignment. Exercise 2.7 asks you to modify the
minimum edit distance algorithm to store the pointers and compute the backtrace to
output an alignment.
# e x e c u t i o n
# 0‚Üê1‚Üê2‚Üê3‚Üê4‚Üê5‚Üê6‚Üê7‚Üê8‚Üê9
i‚Üë1‚Üñ‚Üê‚Üë 2‚Üñ‚Üê‚Üë 3‚Üñ‚Üê‚Üë 4‚Üñ‚Üê‚Üë 5‚Üñ‚Üê‚Üë 6‚Üñ‚Üê‚Üë 7‚Üñ6‚Üê7‚Üê8
n‚Üë2‚Üñ‚Üê‚Üë 3‚Üñ‚Üê‚Üë 4‚Üñ‚Üê‚Üë 5‚Üñ‚Üê‚Üë 6‚Üñ‚Üê‚Üë 7‚Üñ‚Üê‚Üë 8‚Üë7‚Üñ‚Üê‚Üë 8‚Üñ7
t‚Üë3‚Üñ‚Üê‚Üë 4‚Üñ‚Üê‚Üë 5‚Üñ‚Üê‚Üë 6‚Üñ‚Üê‚Üë 7‚Üñ‚Üê‚Üë 8‚Üñ7‚Üê‚Üë8‚Üñ‚Üê‚Üë 9‚Üë8
e‚Üë4‚Üñ3‚Üê4‚Üñ‚Üê5‚Üê6‚Üê7‚Üê‚Üë8‚Üñ‚Üê‚Üë 9‚Üñ‚Üê‚Üë 10‚Üë9
n‚Üë5‚Üë4‚Üñ‚Üê‚Üë 5‚Üñ‚Üê‚Üë 6‚Üñ‚Üê‚Üë 7‚Üñ‚Üê‚Üë 8‚Üñ‚Üê‚Üë 9‚Üñ‚Üê‚Üë 10‚Üñ‚Üê‚Üë 11‚Üñ‚Üë10
t‚Üë6‚Üë5‚Üñ‚Üê‚Üë 6‚Üñ‚Üê‚Üë 7‚Üñ‚Üê‚Üë 8‚Üñ‚Üê‚Üë 9‚Üñ8‚Üê9‚Üê10‚Üê‚Üë11
i‚Üë7‚Üë6‚Üñ‚Üê‚Üë 7‚Üñ‚Üê‚Üë 8‚Üñ‚Üê‚Üë 9‚Üñ‚Üê‚Üë 10‚Üë9‚Üñ8‚Üê9‚Üê10
o‚Üë8‚Üë7‚Üñ‚Üê‚Üë 8‚Üñ‚Üê‚Üë 9‚Üñ‚Üê‚Üë 10‚Üñ‚Üê‚Üë 11‚Üë10‚Üë9‚Üñ8‚Üê9
n‚Üë9‚Üë8‚Üñ‚Üê‚Üë 9‚Üñ‚Üê‚Üë 10‚Üñ‚Üê‚Üë 11‚Üñ‚Üê‚Üë 12‚Üë11‚Üë10‚Üë9‚Üñ8
Figure 2.21 When entering a value in each cell, we mark which of the three neighboring
cells we came from with up to three arrows. After the table is full we compute an alignment
(minimum edit path) by using a backtrace , starting at the 8in the lower-right corner and
following the arrows back. The sequence of bold cells represents one possible minimum
cost alignment between the two strings, again using Levenshtein distance with cost of 1 for
insertions or deletions, 2 for substitutions. Diagram design after GusÔ¨Åeld (1997).
While we worked our example with simple Levenshtein distance, the algorithm
in Fig. 2.19 allows arbitrary weights on the operations. For spelling correction, for
example, substitutions are more likely to happen between letters that are next to

34 CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
each other on the keyboard. The Viterbi algorithm is a probabilistic extension of
minimum edit distance. Instead of computing the ‚Äúminimum edit distance‚Äù between
two strings, Viterbi computes the ‚Äúmaximum probability alignment‚Äù of one string
with another. We‚Äôll discuss this more in Chapter 17.
2.10 Summary
This chapter introduced the fundamental concepts of tokens and tokenization in lan-
guage processing. We discussed the linguistic levels of words, morphemes, and
characters, introduced Unicode code points and the UTF-8 encoding, introduced
theBPE algorithm for tokenization, and introduced the regular expression and the
minimum edit distance algorithm for comparing strings. Here‚Äôs a summary of the
main points we covered about these ideas:
‚Ä¢ Words and morphemes are useful units of representation, but difÔ¨Åcult to deÔ¨Åne
formally.
‚Ä¢Unicode is a system for representing characters in the many scripts used to
write the languages of the world.
‚Ä¢ Each character is represented internally with a unique id called a code point ,
and can be encoded in a Ô¨Åle via encoding methods like UTF-8 , which is a
variable-length encoding.
‚Ä¢Byte-Pair Encoding orBPE is the standard way to induce tokens in a data-
driven way. It is the Ô¨Årst step in most large language models.
‚Ä¢BPE tokens are often roughly word or morpheme-sized, although they can be
as small as single characters.
‚Ä¢ The regular expression language is a powerful tool for pattern-matching.
‚Ä¢ Basic operations in regular expressions include disjunction of symbols ( [],
|),counters (*,+, and{n,m} ),anchors (ÀÜ,$), capture groups ( (,)), and
substitutions.
‚Ä¢ The minimum edit distance between two strings is the minimum number of
operations it takes to edit one into the other. Minimum edit distance can be
computed by dynamic programming , which also results in an alignment of
the two strings.
Historical Notes
For more on Herdan‚Äôs law and Heaps‚Äô Law, see Herdan (1960, p. 28), Heaps (1978),
Egghe (2007) and Baayen (2001);
Unicode drew on ASCII and ISO character encoding standards. Early drafts
were worked out in discussions between engineers from Xerox and Apple. An early
draft standard was published in 1988, with a more formal release of the Unicode
Stanford in 1991. What became UTF-8 began with ISO drafts in 1989, with various
extensions. The self-synchronizing aspects were famously outlined on a placemat in
a New Jersey dinner in 1992 by Ken Thompson.
Word tokenization and other text normalization algorithms have been applied
since the beginning of the Ô¨Åeld. This include stemming, like the widely used stem-
mer of Lovins (1968), and applications to the digital humanities like those of by
Packard (1973), who built an afÔ¨Åx-stripping morphological parser for Ancient Greek.

EXERCISES 35
BPE, originally a text compression method proposed by Gage (1994), was applied
to subword tokenization in the context of early neural machine translation by Sen-
nrich et al. (2016). It was then taken up in OpenAI‚Äôs GPT-2 (Radford et al., 2019)
as the default tokenization method, and also included in the open-source Sentence-
Piece library (Kudo and Richardson, 2018b). There is a nice a public implemen-
tation,minbpe ,https://github.com/karpathy/minbpe , by Andrej Karpathy,
who also has a popular lecture introducing BPE ( https://www.youtube.com/
watch?v=zduSFxRajkE ).
Kleene 1951; 1956 Ô¨Årst deÔ¨Åned regular expressions and the Ô¨Ånite automaton,
based on the McCulloch-Pitts neuron. Ken Thompson was one of the Ô¨Årst to build
regular expressions compilers into editors for text searching (Thompson, 1968). His
editor edincluded a command ‚Äúg/regular expression/p‚Äù, or Global Regular Expres-
sion Print, which later became the Unix grep utility.
NLTK is an essential tool that offers both useful Python libraries ( https://
www.nltk.org ) and textbook descriptions (Bird et al., 2009) of many algorithms
including text normalization and corpus interfaces.
For more on edit distance, see GusÔ¨Åeld (1997). Our example measuring the edit
distance from ‚Äòintention‚Äô to ‚Äòexecution‚Äô was adapted from Kruskal (1983). There are
various publicly available packages to compute edit distance, including Unix diff
and the NIST sclite program (NIST, 2005).
In his autobiography Bellman (1984) explains how he originally came up with
the term dynamic programming :
‚Äú...The 1950s were not good years for mathematical research. [the]
Secretary of Defense ...had a pathological fear and hatred of the word,
research... I decided therefore to use the word, ‚Äúprogramming‚Äù. I
wanted to get across the idea that this was dynamic, this was multi-
stage... I thought, let‚Äôs ... take a word that has an absolutely precise
meaning, namely dynamic... it‚Äôs impossible to use the word, dynamic,
in a pejorative sense. Try thinking of some combination that will pos-
sibly give it a pejorative meaning. It‚Äôs impossible. Thus, I thought
dynamic programming was a good name. It was something not even a
Congressman could object to.‚Äù
Exercises
2.1 Write regular expressions for the following languages.
1. the set of all alphabetic strings;
2. the set of all lower case alphabetic strings ending in a b;
3. the set of all strings from the alphabet a,bsuch that each ais immedi-
ately preceded by and immediately followed by a b;
2.2 Write regular expressions for the following languages. By ‚Äúword‚Äù, we mean
an alphabetic string separated from other words by whitespace, any relevant
punctuation, line breaks, and so forth.
1. the set of all strings with two consecutive repeated words (e.g., ‚ÄúHum-
bert Humbert‚Äù and ‚Äúthe the‚Äù but not ‚Äúthe bug‚Äù or ‚Äúthe big bug‚Äù);
2. all strings that start at the beginning of the line with an integer and that
end at the end of the line with a word;

36 CHAPTER 2 ‚Ä¢ W ORDS AND TOKENS
3. all strings that have both the word grotto and the word raven in them
(but not, e.g., words like grottos that merely contain the word grotto );
4. write a pattern that places the Ô¨Årst word of an English sentence in a
register. Deal with punctuation.
2.3 Implement an ELIZA-like program, using substitutions such as those described
on page 27. You might want to choose a different domain than a Rogerian psy-
chologist, although keep in mind that you would need a domain in which your
program can legitimately engage in a lot of simple repetition.
2.4 Compute the edit distance (using insertion cost 1, deletion cost 1, substitution
cost 1) of ‚Äúleda‚Äù to ‚Äúdeal‚Äù. Show your work (using the edit distance grid).
2.5 Figure out whether drive is closer to brief or to divers and what the edit dis-
tance is to each. You may use any version of distance that you like.
2.6 Now implement a minimum edit distance algorithm and use your hand-computed
results to check your code.
2.7 Augment the minimum edit distance algorithm to output an alignment; you
will need to store pointers and add a stage to compute the backtrace.

