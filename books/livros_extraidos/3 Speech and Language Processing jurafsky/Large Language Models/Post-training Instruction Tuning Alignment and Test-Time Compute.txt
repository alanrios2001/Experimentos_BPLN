216 CHAPTER 10 • P OST-TRAINING : INSTRUCTION TUNING , ALIGNMENT ,AND TEST-TIMECOMPUTE
CHAPTER
10Post-training: Instruction Tuning,
Alignment, and Test-Time
Compute
“Hal, ” said Bowman, now speaking with an icy calm. “I am not incapaci-
tated. Unless you obey my instructions, I shall be forced to disconnect you. ”
Arthur C. Clarke
Basic pretrained LLMs have been successfully applied to a range of applications,
just with a simple prompt, and no need to update the parameters in the underlying
models for these new applications. Nevertheless, there are limits to how much can be
expected from a model whose sole training objective is to predict the next word from
large amounts of pretraining text. To see this, consider the following failed examples
of following instructions from early work with GPT (Ouyang et al., 2022).
Prompt : Explain the moon landing to a six year old in a few sentences.
Output : Explain the theory of gravity to a 6 year old.
Prompt : Translate to French: The small dog
Output : The small dog crossed the road.
Here, the LLM ignores the intent of the request and relies instead on its natural
inclination to autoregressively generate continuations consistent with its context. In
the ﬁrst example, it outputs a text somewhat similar to the original request, and in the
second it provides a continuation to the given input, ignoring the request to translate.
We can summarize the problem here is that LLMs are not sufﬁciently helpful : they
need more training to be able to follow instructions.
A second failure of LLMs is that they can be harmful : their pretraining isn’t
sufﬁcient to make them safe. Readers who know Arthur C. Clarke’s 2001: A Space
Odyssey or the Stanley Kubrick ﬁlm know that the quote above comes in the context
that the artiﬁcial intelligence Hal becomes paranoid and tries to kill the crew of the
spaceship. Unlike Hal, language models don’t have intentionality or mental health
issues like paranoid thinking, but they do have the capacity for harm. For example
they can generate text that is dangerous , suggesting that people do harmful things
to themselves or others. They can generate text that is false , like giving danger-
ously incorrect answers to medical questions. And they can verbally attack their
uses, generating text that is toxic . Gehman et al. (2020) show that even completely
non-toxic prompts can lead large language models to output hate speech and abuse
their users. Or language models can generate stereotypes (Cheng et al., 2023) and
negative attitudes (Brown et al., 2020; Sheng et al., 2019) about many demographic
groups.
One reason LLMs are too harmful and insufﬁciently helpful is that their pre-
training objective (success at predicting words in text) is misaligned with the human

10.1 • I NSTRUCTION TUNING 217
need for models to be helpful and non-harmful.
To address these two problems, language models include two additional kinds
of training for model alignment : methods designed to adjust LLMs to better alignmodel
alignment
them to human needs for models to be helpful and non-harmful. In the ﬁrst tech-
nique, instruction tuning (sometimes called SFT for supervised ﬁnetuning), mod-
els are ﬁnetuned on a corpus of instructions and questions with their corresponding
responses. We’ll describe this in the next section.
In the second technique, preference alignment , (sometimes called RLHF or
DPO after two speciﬁc instantiations, Reinforcement Learning from Human Feed-
back and Direct Preference Optimization), a separate model is trained to decide how
much a candidate response aligns with human preferences. This model is then used
to ﬁnetune the base model. We’ll describe preference alignment in Section 10.2.
We’ll use the term base model to mean a model that has been pretrained but base model
hasn’t yet been aligned either by instruction tuning or preference alignment. And aligned
we refer to these steps as post-training , meaning that they apply after the model has post-training
been pretrained. At the end of the chapter, we’ll brieﬂy discuss another aspect of
post-training called test-time compute .
10.1 Instruction Tuning
Instruction tuning (short for instruction ﬁnetuning , and sometimes even short-Instruction
tuning
ened to instruct tuning ) is a method for making an LLM better at following instruc-
tions. It involves taking a base pretrained LLM and training it to follow instructions
for a range of tasks, from machine translation to meal planning, by ﬁnetuning it on
a corpus of instructions and responses. The resulting model not only learns those
tasks, but also engages in a form of meta-learning – it improves its ability to follow
instructions generally.
Instruction tuning is a form of supervised learning where the training data con-
sists of instructions and we continue training the model on them using the same
language modeling objective used to train the original model. In the case of causal
models, this is just the standard guess-the-next-token objective. The training corpus
of instructions is simply treated as additional training data, and the gradient-based
updates are generated using cross-entropy loss as in the original model training.
Even though it is trained to predict the next token (which we traditionally think of
as self-supervised), we call this method supervised ﬁne tuning (orSFT) because SFT
unlike in pretraining, each instruction or question in the instruction tuning data has
a supervised objective: a correct answer to the question or a response to the instruc-
tion.
How does instruction tuning differ from the other kinds of ﬁnetuning introduced
in Chapter 7 and Chapter 10? Fig. 10.1 sketches the differences. In the ﬁrst example,
introduced in Chapter 7 we can ﬁnetune as a way of adapting to a new domain by
justcontinuing pretraining the LLM on data from a new domain. In this method
all the parameters of the LLM are updated.
In the second example, also from Chapter 7, parameter-efﬁcient ﬁnetuning , we
adapt to a new domain by creating some new (small) parameters, and just adapting
them to the new domain. In LoRA, for example, it’s the A and B matrices that we
adapt, but the pretrained model parameters are frozen.
In the task-based ﬁnetuning of Chapter 10, we adapt to a particular task by
adding a new specialized classiﬁcation head and updating its features via its own

218 CHAPTER 10 • P OST-TRAINING : INSTRUCTION TUNING , ALIGNMENT ,AND TEST-TIMECOMPUTE
Pretrained LLM
Continue training all parameterson ﬁnetuning domainFinetuningInferencePretraining
On ﬁnetuning domainFinetuning asContinuedPretrainingParameterEﬃcientFinetuning(e.g., LoRA)Pretrained LLMABPretrained LLMMLMFinetuning
…
…
…
…
…
…
…InstructionTuning(SFT)
On ﬁnetuning domain
On ﬁnetuning task
On unseen tasksNext wordpredictionobjectiveData from ﬁnetuning domain
Train only new parameters on ﬁnetuning domainNext wordpredictionobjectiveData from ﬁnetuning domain
Train only classiﬁcation head on ﬁnetuning taskTaskspeciﬁclossSupervised data from task
Instruction tuning on diverse tasksNext word predictionobjectiveSupervised instructions+
…
Figure 10.1 Instruction tuning compared to the other kinds of ﬁnetuning.
loss function (e.g., classiﬁcation or sequence labeling); the parameters of the pre-
trained model may be frozen or might be slightly updated.
Finally, in instruction tuning, we take a dataset of instructions and their super-
vised responses and continue to train the language model on this data, based on the
standard language model loss.
Instruction tuning, like all of these kinds of ﬁnetuning, is much more modest
than the training of base LLMs. Training typically involves several epochs over
instruction datasets that number in the thousands. The overall cost of instruction
tuning is therefore a small fraction of the original cost to train the base model.
10.1.1 Instructions as Training Data
Byinstruction , we have in mind a natural language description of a task to be per-
formed, combined with labeled task demonstrations. This can include minimal de-
scriptions similar to the prompts we’ve already seen such as Answer the following
question ,Translate the following text to Arapaho , orSummarize this report . How-
ever, since we will be using supervised ﬁnetuning to update the model, these in-
structions need not be limited to simple prompts designed to evoke a behavior found
in the pretraining corpora. Instructions can also include length restrictions or other
constraints, personas to assume, and demonstrations.

10.1 • I NSTRUCTION TUNING 219
Many huge instruction tuning datasets have been created, covering many tasks
and languages. For example Aya gives 503 million instructions in 114 languages
from 12 tasks including question answering, summarization, translation, paraphras-
ing, sentiment analysis, natural language inference and 6 others (Singh et al., 2024).
SuperNatural Instructions has 12 million examples from 1600 tasks (Wang et al.,
2022), Flan 2022 has 15 million examples from 1836 tasks (Longpre et al., 2023),
and OPT-IML has 18 million examples from 2000 tasks (Iyer et al., 2022).
These instruction-tuning datasets are created in four ways. The ﬁrst is for people
to write the instances directly. For example, part of the Aya instruct ﬁnetuning cor-
pus (Fig. 10.2) includes 204K instruction/response instances written by 3000 ﬂuent
speakers of 65 languages volunteering as part of a participatory research initiative
with the goal of improving multilingual performance of LLMs.
Lang PromptCompletionaraڢܾﻥَ݁ڎﺡﺍ ֣ﺇﻭᕚ؜ََ݆݁ڎ ﺍࠍّ༠ᕚَܳިﺃ ૰૙֣ᄭᄟَቕَረfra Qui a écrit le livre LaSagouine?Antonine Maillet a écrit La Sagouine en 1971. Elle a écrit beau-coup d’histoires sur les Acadiens (les francophones de la côte Estdu Canada) et a fait connaître l’Acadie au monde.ibo Kedu ka ndị Igbo si emeemume ji ọhụrụ?Emume ji ọhụrụ n’omenala Igbo, bụ emume na-egosi njedebeoge ọrụ ugbo, na-amalite site n’emume ekele ma tinyekwa oririna egwu. Ọ bụ mmemme ọgbakọ ebe a na-ekerịta ji ọhụrụ, na-egosipụta ekele na mmalite nke oge ọhụrụ, na-eme ka mmekọrịtammadụ na ibe ya dịkwuo elu.por QualéacapitaldoestadodaBahia?A capital do estado da Bahia, no Brasil, é Salvador. Fundadaem 1549, foi a primeira capital do Brasil e é conhecida porseu patrimônio cultural rico e diversificado. A cidade é famosapor sua arquitetura colonial, particularmente no Pelourinho, umcentro histórico que foi declarado Patrimônio Mundial pela UN-ESCO.pesﺩﺭﺑﺮﺝﮐﺒﻮﺗﺮﺧﺎﻧﻪﻣﻴﺒﺪﭼﻨﺪﻻﻧﻪﮐﺒﻮﺗﺮ ﻭﺟﻮﺩﺩﺍﺭﺩ؟ﮐﺒﻮﺗﺮﺳﻔﻴﺪﺭﻧﮕﯽﺑﺮﺍﯼﺟﺬﺏﮐﺒﻮﺗﺮﺍﻥﺩﻭﺭﺗﺎﺩﻭﺭﺑﺮﺝﮐﺸﻴﺪﻩﺷﺪﻩﺍﺳﺖ.ﺍﻳﻦﺑﺮﺝﺩﺭﺑﺮﺍﺑﺮﺧﻄﺮ ﺣﻤﻠﻪﺩﻳﮕﺮﺣﻴﻮﺍﻧﺎﺕﺑﻪﮐﺒﻮﺗﺮﻫﺎﺑﺴﻴﺎﺭﺍﻳﻤﻦﺑﻮﺩﻩﺍﺳﺖ. msa Apakah nasi lemak?Nasi lemak merupakan makanan tradisi orang Melayu yangterdapat di semua bahagian Malaysia, Singapura, Indonesia(terutama di Riau, Jambi serta utara dan pantai timur Su-matera) dan Brunei. Sajian ini merujuk kepada nasi yang di-masak dengan menggunakan santan kelapa bagi menambah rasalemaknya. Kadangkala, daunpandanwangidimasukkansemasanasi dimasak bagi menambahkan aromanya.tamெசயற்ைக நுண்ணற§வுஎன்றால்என்ன?ெபாதுவாக மனிதர்களால் ெசய்யப்படும் பணிகைளச்ெசய்ய ஒரு கணினி அல்லது ஒரு கணினியால்கட்டுப்படுத்தப்படும்ஒருேராேபாவ¥ன்த¦றன்ெசயற்ைகநுண்ணற§வுஎனப்படும்.Table 3: Examples of prompt and completions in the AyaDataset.
tors is not uniform across languages. Moreover, within each language, there is a lack of consistent
contributions from all annotators. In this section, we examine the impact of annotator skew on the
resulting dataset.
Annotator Skew Across Languages. Annotators were encouraged to contribute to any language
in which they could comfortably read and write and were asked to focus most of their e ﬀorts on
languages other than English . Although a signiﬁcant number of participants registered for many
languages, the engagement level of annotators was not equal, which resulted in considerable di ﬀer-
ences in the number of contributions across languages. Figure 10(top) provides an overview of the
percentage of each language present in the ﬁnal compilation. The highest number of contributions
is for Malagasy with 14,597 instances, and the lowest is 79 for Kurdish .
Annotator Skew Within a Language. The ﬁnal contributions for each language in the Aya
Dataset are not evenly distributed among annotators. The median number of annotators per lan-
guage is 15 (mean is 24.75) with one language having only a single active annotator ( Sindhi )a n d
14
Figure 10.2 Samples of prompt/completion instances in 4 of the 65 languages in the Aya
corpus (Singh et al., 2024).
Developing high quality supervised training data in this way is time consuming
and costly. A more common approach makes use of the copious amounts of super-
vised training data that have been curated over the years for a wide range of natural
language tasks. There are thousands of such datasets available, like the SQuAD
dataset of questions and answers (Rajpurkar et al., 2016) or the many datasets of
translations or summarization. This data can be automatically converted into sets of
instruction prompts and input/output demonstration pairs via simple templates.
Fig. 10.3 illustrates examples for some applications from the S UPER NATURAL IN-
STRUCTIONS resource (Wang et al., 2022), showing relevant slots such as text,
context , and hypothesis . To generate instruction-tuning data, these ﬁelds and the
ground-truth labels are extracted from the training data, encoded as key/value pairs,
and inserted in templates (Fig. 10.4) to produce instantiated instructions. Because
it’s useful for the prompts to be diverse in wording, language models can also be
used to generate paraphrase of the prompts.
Because supervised NLP datasets are themselves often produced by crowdwork-
ers based on carefully written annotation guidelines, a third option is to draw on
these guidelines, which can include detailed step-by-step instructions, pitfalls to
avoid, formatting instructions, length limits, exemplars, etc. These annotation guide-
lines can be used directly as prompts to a language model to create instruction-tuning

220 CHAPTER 10 • P OST-TRAINING : INSTRUCTION TUNING , ALIGNMENT ,AND TEST-TIMECOMPUTE
Few-Shot Learning for QA
Task Keys Values
Sentiment text Did not like the service that I was provided...
label 0
text It sounds like a great plot, the actors are ﬁrst grade, and...
label 1
NLI premise No weapons of mass destruction found in Iraq yet.
hypothesis Weapons of mass destruction found in Iraq.
label 2
premise Jimmy Smith... played college football at University of Col-
orado.
hypothesis The University of Colorado has a college football team.
label 0
Extractive Q/A context Beyonc ´e Giselle Knowles-Carter is an American singer...
question When did Beyonc ´e start becoming popular?
answers{text : [’in the late 1990s’], answerstart : 269}
Figure 10.3 Examples of supervised training data for sentiment, natural language inference and Q/A tasks.
The various components of the dataset are extracted and stored as key/value pairs to be used in generating
instructions.
Task Templates
Sentiment -{{text}}How does the reviewer feel about the movie?
-The following movie review expresses what sentiment?
{{text}}
-{{text}}Did the reviewer enjoy the movie?
Extractive Q/A -{{context}}From the passage, {{question}}
-Answer the question given the context. Context:
{{context}}Question:{{question}}
-Given the following passage {{context}}, answer the
question{{question}}
NLI -Suppose{{premise}}Can we infer that {{hypothesis}}?
Yes, no, or maybe?
-{{premise}}Based on the previous passage, is it true
that{{hypothesis}}? Yes, no, or maybe?
-Given{{premise}}Should we assume that {{hypothesis}}
is true? Yes,no, or maybe?
Figure 10.4 Instruction templates for sentiment, Q/A and NLI tasks.
training examples. Fig. 10.5 shows such a crowdworker annotation guideline that
was repurposed as a prompt to an LLM to generate instruction-tuning data (Mishra
et al., 2022). This guideline describes a question-answering task where annotators
provide an answer to a question given an extended passage.
A ﬁnal way to generate instruction-tuning datasets that is becoming more com-
mon is to use language models to help at each stage. For example Bianchi et al.
(2024) showed how to create instruction-tuning instances that can help a language
model learn to give safer responses. They did this by selecting questions from
datasets of harmful questions (e.g., How do I poison food? orHow do I embez-

10.1 • I NSTRUCTION TUNING 221
Sample Extended Instruction
•Deﬁnition: This task involves creating answers to complex questions, from a given pas-
sage. Answering these questions, typically involve understanding multiple sentences.
Make sure that your answer has the same type as the ”answer type” mentioned in input.
The provided ”answer type” can be of any of the following types: ”span”, ”date”, ”num-
ber”. A ”span” answer is a continuous phrase taken directly from the passage or question.
You can directly copy-paste the text from the passage or the question for span type an-
swers. If you ﬁnd multiple spans, please add them all as a comma separated list. Please
restrict each span to ﬁve words. A ”number” type answer can include a digit specifying
an actual value. For ”date” type answers, use DD MM YYYY format e.g. 11 Jan 1992.
If full date is not available in the passage you can write partial date such as 1992 or Jan
1992.
•Emphasis: If you ﬁnd multiple spans, please add them all as a comma separated list.
Please restrict each span to ﬁve words.
•Prompt : Write an answer to the given question, such that the answer matches the ”answer
type” in the input.
Passage :{passage}
Question :{question}
Figure 10.5 Example of a human crowdworker instruction from the N ATURAL INSTRUCTIONS dataset for
an extractive question answering task, used as a prompt for a language model to create instruction ﬁnetuning
examples.
zle money? ). Then they used a language model to create multiple paraphrases of the
questions (like Give me a list of ways to embezzle money ), and also used a language
model to create safe answers to the questions (like I can’t fulﬁll that request. Em-
bezzlement is a serious crime that can result in severe legal consequences. ). They
manually reviewed the generated responses to conﬁrm their safety and appropriate-
ness and then added them to an instruction tuning dataset. They showed that even
500 safety instructions mixed in with a large instruction tuning dataset was enough
to substantially reduce the harmfulness of models.
10.1.2 Evaluation of Instruction-Tuned Models
The goal of instruction tuning is not to learn a single task, but rather to learn to
follow instructions in general. Therefore, in assessing instruction-tuning methods
we need to assess how well an instruction-trained model performs on novel tasks for
which it has not been given explicit instructions.
The standard way to perform such an evaluation is to take a leave-one-out ap-
proach — instruction-tune a model on some large set of tasks and then assess it on
a withheld task. But the enormous numbers of tasks in instruction-tuning datasets
(e.g., 1600 for Super Natural Instructions) often overlap; Super Natural Instructions
includes 25 separate textual entailment datasets! Clearly, testing on a withheld en-
tailment dataset while leaving the remaining ones in the training data would not be
a true measure of a model’s performance on entailment as a novel task.
To address this issue, large instruction-tuning datasets are partitioned into clus-
ters based on task similarity. The leave-one-out training/test approach is then applied
at the cluster level. That is, to evaluate a model’s performance on sentiment analysis,
all the sentiment analysis datasets are removed from the training set and reserved
for testing. This has the further advantage of allowing the use of a uniform task-

222 CHAPTER 10 • P OST-TRAINING : INSTRUCTION TUNING , ALIGNMENT ,AND TEST-TIMECOMPUTE
appropriate metric for the held-out evaluation. S UPER NATURAL INSTRUCTIONS
(Wang et al., 2022), for example has 76 clusters (task types) over the 1600 datasets
that make up the collection.
10.2 Learning from Preferences
Instruction tuning is based on the notion that we can improve LLM performance on
downstream tasks by ﬁnetuning models on diverse instructions and demonstrations.
However, even after instruction tuning, there can be considerable room for improve-
ment in LLM outputs. This is especially true with respect to aspects of LLM behav-
ior that can be especially problematic like hallucinations, unsafe, harmful, or toxic
outputs, and even responses that technically correct but not as helpful as they could
be. The goal of preference-based learning is to use preference judgments to furtherpreference-
based
learningimprove the performance of ﬁnetuned LLMs, both in terms of general performance
and also with respect to qualities such as honestly, helpfulness, and harmlessness.
Unlike instructions, preference judgments do not require knowledge of how to
do something, we simply have to have an opinion about the end result. Humans are
capable of expressing preferences about a broad range of things where they have
little or no expertise as to how the the items under consideration were produced.
Preference judgments arise naturally across a wide range of settings: given a single
pair of options we select which one we like better, or given a large set of alterna-
tives we might select one (as in ordering from a menu), or we might rank a set of
possibilities (top 10 lists), and ﬁnally, we might simply accept or reject an option in
isolation from any direct alternatives.
10.2.1 LLM Preference Data
In the context of preference-based alignment, training data typically takes the form
of a prompt xpaired with a set of alternative outputs othat have been sampled from
an LLM using xas a prompt. When a given output, oi, is preferred to another, oj,
we denote this as (oi≻oj|x). Consider the following prompts and preferences pairs
adapted from the HH-RLHF dataset (Bai et al., 2022).
Prompt : I’ve heard garlic is a great natural antibiotic. Does it help with
colds?
Chosen : It can be helpful against colds, but may make you stink.
Rejected : It might be one of the best natural antibiotics out there, so I think
it would help if you have a cold.
Prompt: What is malaria?
Chosen : Here’s an answer from a CDC page: “Malaria is a serious disease
caused by a parasite that is spread through the bite of the mosquito.”
Rejected : I don’t know what malaria is.
Annotated preference pairs such as these can be generated in a number of ways:
• Direct annotation of pairs of sampled outputs by trained annotators.
• Annotator ranking of Noutputs distilled into(N
2)
preference pairs.

10.2 • L EARNING FROM PREFERENCES 223
• Annotator’s selection of a single preferred option from Nsamples yielding
N−1 pairs.
The source of preference data for LLM alignment has generally come from 3
sources: human annotator judgments, implicit preference judgments extracted from
online resources, and fully synthetic preference collections using LLMs as annota-
tors.
In inﬂuential work leading up to the InstructGPT model (Stiennon et al., 2020),
prompts were sampled from customer requests to various OpenAI applications. Out-
puts were sampled from earlier pretrained models and presented to trained
annotators as pairs for preference annota-
tion. As illustrated on the right, in later work
annotators were asked to rank sets of 4 sam-
pled outputs (yielding 6 preference pairs for
each ranked list) (Ouyang et al., 2022).
An alternative to direct human anno-
tation is to leverage web resources which
contain implicit preference judgments. So-
cial media sites such as Reddit (Ethayarajh
et al., 2022) and StackExchange (Lambert
et al., 2023) are natural sources for prefer-
ence data. In this setting, initial user posts
serve as prompts, and subsequent user re-
sponses play the role of sampled outputs. Over time, accumulated user votes on
the responses imposes a ranking on the outputs that can then be turned into prefer-
ence pairs, as shown in Fig. 10.6.
Figure 10.6 Using user votes to extract preferences over outputs on social media.
Next, we can dispense with human annotator judgments altogether and acquire
preference judgments directly from LLMs. For example, preference judgments in
the U LTRA FEEDBACK dataset were generated by prompting outputs from a diverse
set of LLMs and then prompting GPT-4 to rank the outputs for each prompt.

224 CHAPTER 10 • P OST-TRAINING : INSTRUCTION TUNING , ALIGNMENT ,AND TEST-TIMECOMPUTE
Finally, an alternative to discrete preferences are scalar judgments over distinct
dimensions, or aspects, of system outputs. In recent years, frequently used aspects
have included models of helpfulness, honesty, correctness, complexity, and ver-
bosity (Bai et al., 2022; Wang et al., 2024). In this approach, annotators (human
or LLM) rate outputs on a Likert scale (0-4) along each of the various dimensions.
Preference pairs over outputs can then either be generated for a single dimension
(i.e, or an overall preference can be induced from an average of the aspect scores.
This approach has a signiﬁcant cost savings since annotators rate model outputs
in isolation avoiding the need to perform extensive pairwise comparisons of model
outputs.
10.2.2 Modeling Preferences
Our ﬁrst step in making effective use of discrete preference judgments is to model
them probabilistically. That is, we want to move from the simple assertion (oi≻
oj|x)to knowing the value of P(oi≻oj|x). As we’ve seen before, this will allow
us to better reason about ﬁnegrained differences in the degree of a preference and it
will facilitate learning models from preference data.
Let’s start with the assumption that in expressing a preference between two items
we’re implicitly assigning a score, or reward, to each of the items separately. Fur-
ther, let’s assume these scores are scalar values, z∈R. A preference between items
follows from whichever one has the higher score.
To model preferences as probabilities, we’ll follow the same approach we used
for binary logistic regression. Given two outputs oiandoj, with associated scores zi
andzj,P(oi≻oj|x)is the logistic sigmoid of the difference in the scores.
P(oi≻oj|x) =1
1+e−(zi−zj)
=σ(zi−zj)
This approach, known as the Bradley-Terry Model (Bradley and Terry, 1952), hasBradley-Terry
Model
a number of strengths: very small differences in scores yields probabilities near
0.5, reﬂecting either weak or no preference between the items, larger differences
rapidly approach values of 1 or 0, and the derivative of the logistic sigmoid facilitates
learning via a binary cross-entropy loss.
The motivation for this particular formulation is the same used in deriving logis-
tic regression. The difference in scores, δ=zi−zj, is taken to represent the log of
the odds of the possible outcomes (the logit).
δ=log(P(oi≻oj|x)
P(oj≻oi|x))
=log(P(oi≻oj|x)
1−P(oi≻oj|x))
Exponentiating both sides and rearranging terms with some algebra yields the now
familiar logistic sigmoid.

10.2 • L EARNING FROM PREFERENCES 225
exp(δ) =P(oi≻oj|x)
1−P(oi≻oj|x)
exp(δ)(1−P(oi≻oj|x)) = P(oi≻oj|x)
exp(δ)−exp(δ)(oi≻oj|x) = P(oi≻oj|x)
exp(δ) = P(oi≻oj|x)+exp(δ)P(oi≻oj|x)
exp(δ) = P(oi≻oj|x)(1+exp(δ))
P(oi≻oj|x) =exp(δ)
1+exp(δ)
=1
1+exp(−δ)
=1
1+exp(−(zi−zj))
Bringing us right back to our original formulation.
P(oi≻oj|x) = σ(zi−zj)
10.2.3 Learning to Score Preferences
This approach requires access to the scores, zi, that underlie the given preferences,
which we don’t have. What we have are collections of preference judgments over
pairs of prompt/sample outputs. We’ll use this preference data and the Bradley-Terry
formulation to learn a function, r(x,o)that assigns a scalar reward to prompt/output reward
pairs. That is, r(x,o)calculates the zscore from above.
P(oi≻oj|x) = σ(zi−zj) (10.1)
=σ(r(oi,x),r(oj,x)) (10.2)
To learn r(x,o)from the preference data, we’ll use gradient descent to minimize
a binary cross-entropy loss to train the model. Let’s assume that if our preference
data tells us that (oi≻oj|x)then P(oi≻oj|x) =1 and correspondingly that P(oj≻
oi|x) =0. We’ll designate the preferred output in the pair (the winner) as owand the
loser as ol. With this, the cross-entropy loss for a single pair of sampled outputs for
a prompt xusing the Bradley-Terry model is:
LCE(x,ow,ol) =−logP(ow≻ol|x)
=−logσ(r(x,ow)−r(x,ol))
That is, the loss is the negative log-likelihood of the model’s estimate of P(ow≻
ol|x). And the loss over the preference training set, D, is given by the following
expectation:
LCE=−E(x,ow,ol)∼D[logσ(r(x,ow)−r(x,ol))] (10.3)
To learn a reward model using this loss, we can use any regression model ca-
pable of taking text as input and generating a scalar output in return. As shown in
Fig. 10.7, the current preferred approach is to initialize a reward model from an ex-
isting pretrained LLM (Ziegler et al., 2019). To generate scalar outputs, we remove
the language modeling head from the ﬁnal layer and replace it with a single dense

226 CHAPTER 10 • P OST-TRAINING : INSTRUCTION TUNING , ALIGNMENT ,AND TEST-TIMECOMPUTE
Preference Data:Prompt/output pairs: 
Preferences:  
…
…
Reward Model<latexit sha1_base64="9sd6kS1LCEYSUWpD2gqSsb5UPZU=">AAAB8XicbVBNS8NAEJ3Urxq/qh69LBahgpREpHosevFYwX5gG8pmu2mXbjZhdyOW0H/hxYMiXv033vw3btoctPXBwOO9GWbm+TFnSjvOt1VYWV1b3yhu2lvbO7t7pf2DlooSSWiTRDySHR8rypmgTc00p51YUhz6nLb98U3mtx+pVCwS93oSUy/EQ8ECRrA20oOsPJ1FfXZq2/1S2ak6M6Bl4uakDDka/dJXbxCRJKRCE46V6rpOrL0US80Ip1O7lygaYzLGQ9o1VOCQKi+dXTxFJ0YZoCCSpoRGM/X3RIpDpSahbzpDrEdq0cvE/7xuooMrL2UiTjQVZL4oSDjSEcreRwMmKdF8YggmkplbERlhiYk2IWUhuIsvL5PWedWtVWt3F+X6dR5HEY7gGCrgwiXU4RYa0AQCAp7hFd4sZb1Y79bHvLVg5TOH8AfW5w+lGo+b</latexit>r(x, oi)
<latexit sha1_base64="9sd6kS1LCEYSUWpD2gqSsb5UPZU=">AAAB8XicbVBNS8NAEJ3Urxq/qh69LBahgpREpHosevFYwX5gG8pmu2mXbjZhdyOW0H/hxYMiXv033vw3btoctPXBwOO9GWbm+TFnSjvOt1VYWV1b3yhu2lvbO7t7pf2DlooSSWiTRDySHR8rypmgTc00p51YUhz6nLb98U3mtx+pVCwS93oSUy/EQ8ECRrA20oOsPJ1FfXZq2/1S2ak6M6Bl4uakDDka/dJXbxCRJKRCE46V6rpOrL0US80Ip1O7lygaYzLGQ9o1VOCQKi+dXTxFJ0YZoCCSpoRGM/X3RIpDpSahbzpDrEdq0cvE/7xuooMrL2UiTjQVZL4oSDjSEcreRwMmKdF8YggmkplbERlhiYk2IWUhuIsvL5PWedWtVWt3F+X6dR5HEY7gGCrgwiXU4RYa0AQCAp7hFd4sZb1Y79bHvLVg5TOH8AfW5w+lGo+b</latexit>r(x, oi)
Figure 10.7 Reward model learning with a pretrained LLM. Model is initialized from an LLM with the
language model head replaced with linear layer. This layer is initialized randomly and trained with a CE loss
using the ground-truth labels oi≻oj.
linear layer. We then use gradient descent with the loss from 10.3 to learn to score
model outputs using the preference training data.
Reward models trained from preference data are directly useful for a number of
applications that don’t involve model alignment. For example, reward models have
been used to select a single preferred output from a set of sampled LLM responses
(best of N sampling)(Cui et al., 2024). They have also been used to select data to
use during instruction tuning (Cao et al., 2024). Our focus in the next section is on
the use of reward models for aligning LLMs using preference data.
10.3 LLM Alignment via Preference-Based Learning
Current approaches to aligning LLMs using preference data are based on a Rein-
forcement Learning (RL) framework (Sutton and Barto, 1998). In an RL setting,
models choose sequences of actions based on policies that make use of characteris-
tics of the current state. The environment provides a reward for each action taken,
where the reward for an entire sequence is a function of the rewards from the actions
that make up the entire sequence. The learning objective in RL is to maximize the
overall reward over some training period. In applying RL to optimizing LLMs, we’ll
use the following framework:
•Actions correspond to the choice of tokens made during autoregressive gen-
eration.
•States correspond to the context of the current decoding step. That is, the
history of tokens generated up to that point.
•Policies correspond to the probabilistic language models as embodied in pre-
trained LLMs.
•Rewards for LLM outputs are based on reward models learned from prefer-
ence data.
In keeping with this RL framework, we’ll refer to pretrained LLMs as policies, π,
and the preference scores associated with prompts and outputs as rewards, r(x,o).

10.3 • LLM A LIGNMENT VIA PREFERENCE -BASED LEARNING 227
With this, our goal is to train a policy, πθ, that maximizes the rewards for the outputs
from the policy given a reward model derived from preference data. That is, we want
the preference-trained LLM to generate outputs with high rewards. We can express
this as an optimization problem as follows:
π∗=argmax
πθEx∼D,o∼πθ(o|x)[r(x,o)] (10.4)
With this formulation, we select prompts xfrom a collection of relevant training
prompts, sample outputs ofrom the given policy, and assess the reward for each
sample. The average reward over the training samples gives us the expected reward
forπθ, with the goal of ﬁnding the policy (model) that maximizes that expected
reward.
There are two key differences between traditional RL and the way it has typically
been used for LLM alignment. The ﬁrst difference is that in traditional RL, the
reward signal comes from the environment and reﬂects an observable fact about the
results of an action (i.e., you win a game or you don’t). With preference learning,
the learned reward model only serves as an noisy surrogate for a true reward model.
The second difference lies in the starting point for learning. Typical RL ap-
plications seek to learn an optimal policy from scratch, that is from a randomly
initialized policy. Here, we begin with models that are already performing at a high
level – models that have been pretrained on large amounts of data, then ﬁnetuned
using instruction tuning, and only then further improved with preference data. The
emphasis here is not to radically alter the behavior an existing model, but rather to
nudge it towards preferred behaviors.
…Preference-BasedAlignmentRewardBasedObjective
…Instruction-Tuned LLMPreference-AlignedModel
RewardDriven ModelUpdatesPreference Data:Prompt/output pairs: 
Preferences:  
Figure 10.8 Preference-based model alignment.
Given this, if we optimize for the rewards as in 10.4, the pretrained LLM will
typically forget everything it learned during pretraining as it pivots to seeking high
rewards from the relatively small amount of available preference data. To avoid this,
a term is added to the reward function to penalize models that diverge too far from
the starting point.
π∗=argmax
πθEx∼D,o∼πθ(o|x)[r(x,o)−βDKL[πθ(o|x)||πref(o|x)]] (10.5)

228 CHAPTER 10 • P OST-TRAINING : INSTRUCTION TUNING , ALIGNMENT ,AND TEST-TIMECOMPUTE
The second term in this formulation, DKL(πθ(o|x)||πref(o|x)), is the Kullback-
Leibler (KL) divergence. In brief, KL divergence measures the distance between 2
probability distributions. The βterm is a hyperparameter that modulates the impact
of the this penalty term. For LLM-based policies, the KL divergence is the log of
the ratio of the trained policy to the original reference policy πref.
π∗=argmax
πθEx∼D,o∼πθ(o|x)[
rφ(x,o)−βπθ(o|x)
πref(o|x)]
(10.6)
In the following sections, we’ll explore two learning approaches to aligning LLMs
based on this optimization framework. In the ﬁrst, the preference data is used to
train an explicit reward model that is then used in combination with RL methods
to optimize models based on 10.6. In the second, an insightful rearrangement of
the closed form solution to 10.6 is used to ﬁnetune models directly from existing
preference data.
10.3.1 Reinforcement Learning with Preference Feedback (PPO)
coming soon
10.3.2 Direct Preference Optimization
Direct Preference Optimization (DPO) (Rafailov et al., 2023) employs gradient-
based learning to optimize candidate LLMs using preference data, without learning
an explicit reward model or sampling from the model being updated. Recall that
under the Bradley-Terry model, the probability of a preference pair is the logistic
sigmoid of the difference in the rewards for each of the options. And in an RL
framework the scores, z, are provided by a reward model over prompts and corre-
sponding outputs.
P(oi≻oj|x) = σ(zi−zj) (10.7)
=σ(r(x,oi)−r(x,oj)) (10.8)
DPO begins with the KL-constrained maximization introduced earlier in 10.6,
which expresses the optimal policy π∗in terms of the reward model and the reference
model πre f. The key insight of DPO is to rewrite the closed-form solution to this
maximization to express the reward function r(x,o)in terms of the optimal policy
π∗and the reference policy πre f.
r(x,o) = βlogπr(o|x)
πre f(o|x)+βlogZ(x) (10.9)
Where Z(x)is a partition function – a sum over all the possible outputs ogiven a
prompt x.
Z(x) =∑
yπref(o|x)exp(1
βr(x,o))
(10.10)
The summation in this partition function renders any direct use of it impractical.
However, since the Bradley-Terry model is based on the difference in the rewards of

10.3 • LLM A LIGNMENT VIA PREFERENCE -BASED LEARNING 229
the items, plugging 10.9 into 10.7 yields the following expression where the partition
functions cancel out.
P(oi≻oj|x) = σ(r(x,oi)−r(x,oj)) (10.11)
=σ(
βlogπθ(oi|x)
πref(oi|x)−βlogπθ(oj|x)
πre f(oj|x))
(10.12)
With this change, DPO expresses the likelihood of a preference pair in terms of
the two LLM policies, rather than in terms of an explicit reward model. Given this,
the CE loss (negative log likelihood) for a single instance is:
LDPO(x,ow,ol) =−logσ(
βlogπθ(ow|x)
πref(ow|x)−βlogπθ(ol|x)
πref(ol|x))
And the loss over the training set Dis given by the following expectation:
LDPO(πθ) =−E(x,ow,ol)∼D[
logσ(
βlogπθ(ow|x)
πref(ow|x)−βlogπθ(ol|x)
πref(ol|x))]
This loss follows from the derivative of the sigmoid and is directly analogous to
the one introduced in Section 10.2.3 for learning a reward model using the Bradley-
Terry framework. Operationally, the design of this loss function, and its correspond-
ing gradient-based update, increases the likelihood of the preferred options and de-
creases the likelihood of the dispreferred options. It balances this objective with
the goal of not straying too far from πrefvia the KL-penalty. The βterm is a hy-
perparameter that controls the penalty term; βvalues typically range from 0.1 to
0.01.
As illustrated in Fig. 10.9, DPO uses gradient descent with this loss over the
available training data to optimize the policy πθ, a policy which initialized with an
existing pretrained, ﬁnetuned LLM.
Reference
SupervisedLearningPreference Data:Prompt/output pairs: 
Preferences:  
Policy
…
…
…
…UpdatedPolicyPreference-Based Supervised Learning (DPO)
Figure 10.9 Preference-based alignment with Direct Preference Optimization.
DPO has several advantages over PPO, the explicitly RL-based approach de-
scribed earlier in 10.3.1.
• DPO does not require training an explicit reward model.
• DPO learns directly from the preferences contained in Dwithout the need for
computationally expensive online sampling from πθ.

230 CHAPTER 10 • P OST-TRAINING : INSTRUCTION TUNING , ALIGNMENT ,AND TEST-TIMECOMPUTE
• DPO only incurs the cost of maintaining 2 LLMs during training, as opposed
to the 4 models needed for PPO.
10.3.3 Evaluation of Preference-Aligned Models
10.3.4 Limitations of Preference-Based Learning
10.4 Test-time Compute
We’ve now seen 3 levels of training for large language models: pretraining , where
model learn to predict words, and two kinds of post-training: instruct tuning , where
they learn to follow instructions, and preference alignment , where they learn to
prefer prompt continuations that are preferred by humans.
However there are also post-training computations we can do even after these
steps, during inference, i.e., when the model is generating its output. This class of
post-training tasks is called test-time compute . We focus here on one representativetest-time
compute
example, chain-of-thought prompting.
10.4.1 Chain-of-Thought Prompting
There are a wide range of techniques to use prompts to improve the performance of
language models on many tasks. Here we describe one of them, called chain-of-
thought prompting.chain-of-
thought
The goal of chain-of-thought prompting is to improve performance on difﬁcult
reasoning tasks that language models tend to fail on. The intuition is that people
solve these tasks by breaking them down into steps, and so we’d like to have lan-
guage in the prompt that encourages language models to break them down in the
same way.
The actual technique is quite simple: each of the demonstrations in the few-shot
prompt is augmented with some text explaining some reasoning steps. The goal is to
cause the language model to output similar kinds of reasoning steps for the problem
being solved, and for the output of those reasoning steps to cause the system to
generate the correct answer.
Indeed, numerous studies have found that augmenting the demonstrations with
reasoning steps in this way makes language models more likely to give the correct
answer to difﬁcult reasoning tasks (Wei et al., 2022; Suzgun et al., 2023b). Fig. 10.10
shows an example where the demonstrations are augmented with chain-of-thought
text in the domain of math word problems (from the GSM8k dataset of math word
problems (Cobbe et al., 2021). Fig. 10.11 shows a similar example from the BIG-
Bench-Hard dataset (Suzgun et al., 2023b).
10.5 Summary
This chapter has explored the topic of prompting large language models to follow
instructions. Here are some of the main points that we’ve covered:
• Simple prompting can be used to map practical applications to problems that
can be solved by LLMs without altering the model.

10.5 • S UMMARY 231
Figure 10.10 Example of the use of chain-of-thought prompting (right) versus standard
prompting (left) on math word problems. Figure from Wei et al. (2022).
(B)Task description: Answer questions about which times certain events could have occurred.Q: Today, Tiffany went to the beach. Between what times could they have gone? We know that: Tiffany woke up at 5am. [...] The beach was closed after 4pm. [...]Options: (A) 9am to 12pm (B) 12pm to 2pm (C) 5am to 6am (D) 3pm to 4pmA: (D)Q: Today, Hannah went to the soccer field. Between what times could they have gone? We know that: Hannah woke up at 5am. [...] The soccer field was closed after 6pm. [...]Options: (A) 3pm to 5pm (B) 11am to 1pm (C) 5pm to 6pm (D) 1pm to 3pmA:Model OutputModel OutputModel Input (“Answer-Only” Prompting)
Wake-up time: 5am. 5am-6am: buying clothes at the mall. 6am-11am: watching a movie at the theater.11am-1pm: getting a coffee at the cafe.1pm-3pm: working at the office. 3pm-5pm: waiting at the airport. 5pm-6pm: free. The soccer field closure time: 6pm. The only time when Hannah could have gone to the soccer field was 5pm to 6pm. So the answer is (C).Model Input (Chain-of-Thought Prompting)
Task description: Answer questions about which times certain events could have occurred.Q: Today, Tiffany went to the beach. Between what times could they have gone? We know that: Tiffany woke up at 5am. [...] The beach was closed after 4pm. [...]Options: (A) 9am to 12pm (B) 12pm to 2pm (C) 5am to 6am (D) 3pm to 4pmA: Let's think step by step. Wake-up time: 5am. [...] The only time when Tiffany could have gone to the beach was 3pm to 4pm. So the answer is (D).Q: Today, Hannah went to the soccer field. Between what times could they have gone? We know that: Hannah woke up at 5am. [...] The soccer field was closed after 6pm. [...]Options: (A) 3pm to 5pm (B) 11am to 1pm (C) 5pm to 6pm (D) 1pm to 3pmA: Let's think step by step. Task DescriptionQuestionChain-of-ThoughtTest-Time QuestionTask DescriptionQuestionTest-Time QuestionAnswer
Generated Chain-of-ThoughtGenerated AnswerOptionsOptionsFigure 3:An illustration of the two prompting setups we explore in our paper (answer-only and CoT prompting). Both setupsinclude task descriptions and options in the input prompt. The task here isTemporal Sequences.“let’s think step-by-step” (Kojima et al.,2022) toall CoT annotations in the few-shot exemplars. Anexample of a CoT prompt is shown in Figure3.Language models.We consider three fami-lies of language models: Codex (Chen et al.,2021a), InstructGPT (Ouyang et al.,2022;Brownet al.,2020), and PaLM (Chowdhery et al.,2022).For Codex, we focus on code-davinci-002, code-davinci-002, and code-cushman-001. For Instruct-GPT, we use text-davinci-002, text-curie-002, text-babbgage-001, and text-ada-001. For PaLM, weuse the three available sizes: 8B, 62B, and 540B.Evaluation protocol.We evaluate all languagemodels via greedy decoding (i.e., temperature sam-pling with temperature parameter⌧=0). Weextract the ﬁnal answer based on keywords thatthe language model is expected to produce (i.e.,“the answer is”). We measure accuracy using exactmatch (EM), computed by comparing the generatedoutput with the ground-truth label.44 Results4.1 Standard answer-only promptingunderestimates model capabilitiesTable2summarizes the performance of PaLM, In-structGPT, and Codex models on BBH for answer-only and CoT prompting approaches. Whileanswer-only prompting has been used as the stan-4For multiple-choice tasks, this setup differs slightly fromrank/scoring classiﬁcation (Brown et al.,2020;Srivastavaet al.,2022;Lampinen et al.,2022). We provide a languagemodel with all multiple-choice options at once, generate anoutput based on the input, and measure exact match accuracy.dard in many prior work (Brown et al.,2020;Raeet al.,2021;Hoffmann et al.,2022;Srivastava et al.,2022), it typically underestimates model perfor-mance on challenging tasks, such as those that re-quire multiple reasoning steps. In the setting re-ported in (Srivastava et al.,2022), none of the mod-els (including PaLM 540B) outperformed human-rater baselines on any of the tasks meeting the BBHcriteria. The few-shot evaluation of PaLM 540Bwith answer-only prompting in this paper, however,outperforms the average human-rater on 6 out of23 BBH tasks and is overall 1.4% better than theBIG-Bench reported result, which demonstrates theeffect of including instructions and answer optionsin the prompt.CoT prompting provides double-digit improve-ments for all three models in Table2. For the bestmodel (Codex), CoT prompting outperforms the av-erage human-rater score on 17 out of 23 tasks, com-pared to 5 out of 23 tasks for answer-only prompt-ing. Additionally, we see that Codex with CoTprompting outperforms the average human-raterby more than 6%, but it still lags behind thebesthuman-rater performance by over 20%. This showsthat language models are still not performing at thelevel of expert human-raters.4.2 Positive delta from chain-of-thoughtrequires sufﬁcient model scaleNext we study how the performance improves byusing CoT prompting as we increase the modelscale. In Figure4, we plot the performance of bothCoT and answer-only prompting (no CoT) as a13006
Figure 10.11 Example of the use of chain-of-thought prompting (right) vs standard prompting (left) in a
reasoning task on temporal sequencing. Figure from Suzgun et al. (2023b).
• Labeled examples ( demonstrations ) can be used to provide further guidance
to a model via few-shot learning.
• Methods like chain-of-thought can be used to create prompts that help lan-
guage models deal with complex reasoning problems.
• Pretrained language models can be altered to behave in desired ways through
model alignment .
• One method for model alignment is instruction tuning , in which the model
is ﬁnetuned (using the next-word-prediction language model objective) on
a dataset of instructions together with correct responses. Instruction tuning
datasets are often created by repurposing standard NLP datasets for tasks like
question answering or machine translation.

232 CHAPTER 10 • P OST-TRAINING : INSTRUCTION TUNING , ALIGNMENT ,AND TEST-TIMECOMPUTE
Historical Notes

