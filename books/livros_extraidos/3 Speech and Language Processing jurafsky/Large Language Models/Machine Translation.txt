CHAPTER
12Machine Translation
“I want to talk the dialect of your people. It’s no use of talking unless
people understand what you say.”
Zora Neale Hurston, Moses, Man of the Mountain 1939, p. 121
This chapter introduces machine translation (MT), the use of computers to trans-machine
translation
MT late from one language to another.
Of course translation, in its full generality, such as the translation of literature, or
poetry, is a difﬁcult, fascinating, and intensely human endeavor, as rich as any other
area of human creativity.
Machine translation in its present form therefore focuses on a number of very
practical tasks. Perhaps the most common current use of machine translation is
forinformation access . We might want to translate some instructions on the web,information
access
perhaps the recipe for a favorite dish, or the steps for putting together some furniture.
Or we might want to read an article in a newspaper, or get information from an
online resource like Wikipedia or a government webpage in some other language.
MT for information
access is probably
one of the most com-
mon uses of NLP
technology, and Google
Translate alone (shown above) translates hundreds of billions of words a day be-
tween over 100 languages. Improvements in machine translation can thus help re-
duce what is often called the digital divide in information access: the fact that much digital divide
more information is available in English and other languages spoken in wealthy
countries. Web searches in English return much more information than searches in
other languages, and online resources like Wikipedia are much larger in English and
other higher-resourced languages. High-quality translation can help provide infor-
mation to speakers of lower-resourced languages.
Another common use of machine translation is to aid human translators. MT sys-
tems are routinely used to produce a draft translation that is ﬁxed up in a post-editing post-editing
phase by a human translator. This task is often called computer-aided translation
orCAT . CAT is commonly used as part of localization : the task of adapting content CAT
localization or a product to a particular language community.
Finally, a more recent application of MT is to in-the-moment human commu-
nication needs. This includes incremental translation, translating speech on-the-ﬂy
before the entire sentence is complete, as is commonly used in simultaneous inter-
pretation. Image-centric translation can be used for example to use OCR of the text
on a phone camera image as input to an MT system to translate menus or street signs.
The standard algorithm for MT is the encoder-decoder network/ We brieﬂyencoder-
decoder
mentioned in Chapter 7 that encoder-decoder or sequence-to-sequence models are
used for tasks in which we need to map an input sequence to an output sequence
that is a complex function of the entire input sequence, like machine translation or

254 CHAPTER 12 • M ACHINE TRANSLATION
speech recognition. Indeed, in machine translation, the words of the target language
don’t necessarily agree with the words of the source language in number or order.
Consider translating the following made-up English sentence into Japanese.
(12.1) English: He wrote a letter to a friend
Japanese: tomodachi
friendni
totegami-o
letterkaita
wrote
Note that the elements of the sentences are in very different places in the different
languages. In English, the verb is in the middle of the sentence, while in Japanese,
the verb kaita comes at the end. The Japanese sentence doesn’t require the pronoun
he, while English does.
Such differences between languages can be quite complex. In the following ac-
tual sentence from the United Nations, notice the many changes between the Chinese
sentence (we’ve given in red a word-by-word gloss of the Chinese characters) and
its English equivalent produced by human translators.
(12.2)大会/General Assembly 在/on 1982年/1982 12月/December 10 日/10通过
了/adopted第37号/37th决议/resolution ，核准了/approved 第二
次/second探索/exploration 及/and和平peaceful利用/using外层空
间/outer space会议/conference 的/of各项/various建议/suggestions 。
On 10 December 1982 , the General Assembly adopted resolution 37 in
which it endorsed the recommendations of the Second United Nations
Conference on the Exploration and Peaceful Uses of Outer Space .
Note the many ways the English and Chinese differ. For example the order-
ing differs in major ways; the Chinese order of the noun phrase is “peaceful using
outer space conference of suggestions” while the English has “suggestions of the ...
conference on peaceful use of outer space”). And the order differs in minor ways
(the date is ordered differently). English requires thein many places that Chinese
doesn’t, and adds some details (like “in which” and “it”) that aren’t necessary in
Chinese. Chinese doesn’t grammatically mark plurality on nouns (unlike English,
which has the “-s” in “recommendations”), and so the Chinese must use the modi-
ﬁer各项/various to make it clear that there is not just one recommendation. English
capitalizes some words but not others. Encoder-decoder networks are very success-
ful at handling these sorts of complicated cases of sequence mappings.
We’ll begin in the next section by considering the linguistic background about
how languages vary, and the implications this variance has for the task of MT. Then
we’ll sketch out the standard algorithm, give details about things like input tokeniza-
tion and creating training corpora of parallel sentences, give some more low-level
details about the encoder-decoder network, and ﬁnally discuss how MT is evaluated,
introducing the simple chrF metric.
12.1 Language Divergences and Typology
There are about 7,000 languages in the world. Some aspects of human language
seem to be universal , holding true for every one of these languages, or are statistical universal
universals, holding true for most of these languages. Many universals arise from the
functional role of language as a communicative system by humans. Every language,
for example, seems to have words for referring to people, for talking about eating
and drinking, for being polite or not. There are also structural linguistic univer-
sals; for example, every language seems to have nouns and verbs (Chapter 17), has

12.1 • L ANGUAGE DIVERGENCES AND TYPOLOGY 255
ways to ask questions, or issue commands, has linguistic mechanisms for indicating
agreement or disagreement.
Yet languages also differ in many ways (as has been pointed out since ancient
times; see Fig. 12.1). Understanding what causes such translation divergencestranslation
divergence
(Dorr, 1994) can help us build better MT models. We often distinguish the idiosyn-
cratic and lexical differences that must be dealt with one by one (the word for “dog”
differs wildly from language to language), from systematic differences that we can
model in a general way (many languages put the verb before the grammatical ob-
ject; others put the verb after the grammatical object). The study of these systematic
cross-linguistic similarities and differences is called linguistic typology . This sec- typology
tion sketches some typological facts that impact machine translation; the interested
reader should also look into WALS, the World Atlas of Language Structures, which
gives many typological facts about languages (Dryer and Haspelmath, 2013).
Figure 12.1 The Tower of Babel, Pieter Bruegel 1563. Wikimedia Commons, from the
Kunsthistorisches Museum, Vienna.
12.1.1 Word Order Typology
As we hinted at in our example above comparing English and Japanese, languages
differ in the basic word order of verbs, subjects, and objects in simple declara-
tive clauses. German, French, English, and Mandarin, for example, are all SVO SVO
(Subject-Verb-Object ) languages, meaning that the verb tends to come between
the subject and object. Hindi and Japanese, by contrast, are SOV languages, mean- SOV
ing that the verb tends to come at the end of basic clauses, and Irish and Arabic are
VSO languages. Two languages that share their basic word order type often have VSO
other similarities. For example, VOlanguages generally have prepositions , whereas
OVlanguages generally have postpositions .
Let’s look in more detail at the example we saw above. In this SVO English
sentence, the verb wrote is followed by its object a letter and the prepositional phrase

256 CHAPTER 12 • M ACHINE TRANSLATION
to a friend , in which the preposition tois followed by its argument a friend . Arabic,
with a VSO order, also has the verb before the object and prepositions. By contrast,
in the Japanese example that follows, each of these orderings is reversed; the verb is
preceded by its arguments, and the postposition follows its argument.
(12.3) English: He wrote a letter to a friend
Japanese: tomodachi
friendni
totegami-o
letterkaita
wrote
Arabic: katabt
wroteris¯ala
letterli
to˙sadq
friend
Other kinds of ordering preferences vary idiosyncratically from language to lan-
guage. In some SVO languages (like English and Mandarin) adjectives tend to ap-
pear before nouns, while in others languages like Spanish and Modern Hebrew, ad-
jectives appear after the noun:
(12.4) Spanish bruja verde English green witch
(a) (b)
Figure 12.2 Examples of other word order differences: (a) In German, adverbs occur in
initial position that in English are more natural later, and tensed verbs occur in second posi-
tion. (b) In Mandarin, preposition phrases expressing goals often occur pre-verbally, unlike
in English.
Fig. 12.2 shows examples of other word order differences. All of these word
order differences between languages can cause problems for translation, requiring
the system to do huge structural reorderings as it generates the output.
12.1.2 Lexical Divergences
Of course we also need to translate the individual words from one language to an-
other. For any translation, the appropriate word can vary depending on the context.
The English source-language word bass, for example, can appear in Spanish as the
ﬁshlubina or the musical instrument bajo. German uses two distinct words for what
in English would be called a wall:Wand for walls inside a building, and Mauer for
walls outside a building. Where English uses the word brother for any male sib-
ling, Chinese and many other languages have distinct words for older brother and
younger brother (Mandarin gege anddidi, respectively). In all these cases, trans-
lating bass,wall, orbrother from English would require a kind of specialization,
disambiguating the different uses of a word. For this reason the ﬁelds of MT and
Word Sense Disambiguation (Appendix G) are closely linked.
Sometimes one language places more grammatical constraints on word choice
than another. We saw above that English marks nouns for whether they are singular
or plural. Mandarin doesn’t. Or French and Spanish, for example, mark grammat-
ical gender on adjectives, so an English translation into French requires specifying
adjective gender.
The way that languages differ in lexically dividing up conceptual space may be
more complex than this one-to-many translation problem, leading to many-to-many

12.1 • L ANGUAGE DIVERGENCES AND TYPOLOGY 257
mappings. For example, Fig. 12.3 summarizes some of the complexities discussed
by Hutchins and Somers (1992) in translating English leg, foot , and paw, to French.
For example, when legis used about an animal it’s translated as French patte ; but
about the leg of a journey, as French etape ; if the leg is of a chair, we use French
pied.
Further, one language may have a lexical gap , where no word or phrase, short lexical gap
of an explanatory footnote, can express the exact meaning of a word in the other
language. For example, English does not have a word that corresponds neatly to
Mandarin xi`aoor Japanese oyak ¯ok¯o(in English one has to make do with awkward
phrases like ﬁlial piety orloving child , orgood son/daughter for both).
etapepattejambepied   paw        footlegJOURNEYANIMALHUMANCHAIRANIMALBIRDHUMAN
Figure 12.3 The complex overlap between English leg,foot, etc., and various French trans-
lations as discussed by Hutchins and Somers (1992).
Finally, languages differ systematically in how the conceptual properties of an
event are mapped onto speciﬁc words. Talmy (1985, 1991) noted that languages
can be characterized by whether direction of motion and manner of motion are
marked on the verb or on the “satellites”: particles, prepositional phrases, or ad-
verbial phrases. For example, a bottle ﬂoating out of a cave would be described in
English with the direction marked on the particle out, while in Spanish the direction
would be marked on the verb:
(12.5) English: The bottle ﬂoated out.
Spanish: La
Thebotella
bottlesali´o
exitedﬂotando .
ﬂoating.
Verb-framed languages mark the direction of motion on the verb (leaving the verb-framed
satellites to mark the manner of motion), like Spanish acercarse ‘approach’, al-
canzar ‘reach’, entrar ‘enter’, salir ‘exit’. Satellite-framed languages mark the satellite-framed
direction of motion on the satellite (leaving the verb to mark the manner of motion),
like English crawl out ,ﬂoat off ,jump down ,run after . Languages like Japanese,
Tamil, and the many languages in the Romance, Semitic, and Mayan languages fam-
ilies, are verb-framed; Chinese as well as non-Romance Indo-European languages
like English, Swedish, Russian, Hindi, and Farsi are satellite framed (Talmy 1991,
Slobin 1996).
12.1.3 Morphological Typology
Morphologically , languages are often characterized along two dimensions of vari-
ation. The ﬁrst is the number of morphemes per word, ranging from isolating isolating
languages like Vietnamese and Cantonese, in which each word generally has one
morpheme, to polysynthetic languages like Siberian Yupik (“Eskimo”), in which a polysynthetic
single word may have very many morphemes, corresponding to a whole sentence in
English. The second dimension is the degree to which morphemes are segmentable,
ranging from agglutinative languages like Turkish, in which morphemes have rel- agglutinative
atively clean boundaries, to fusion languages like Russian, in which a single afﬁx fusion

258 CHAPTER 12 • M ACHINE TRANSLATION
may conﬂate multiple morphemes, like -om in the word stolom (table- SG-INSTR -
DECL 1), which fuses the distinct morphological categories instrumental, singular,
and ﬁrst declension.
Translating between languages with rich morphology requires dealing with struc-
ture below the word level, and for this reason modern systems generally use subword
models like the wordpiece or BPE models of Section 12.2.1.
12.1.4 Referential density
Finally, languages vary along a typological dimension related to the things they tend
to omit. Some languages, like English, require that we use an explicit pronoun when
talking about a referent that is given in the discourse. In other languages, however,
we can sometimes omit pronouns altogether, as the following example from Spanish
shows1:
(12.6) [El jefe] idio con un libro. / 0 iMostr ´o su hallazgo a un descifrador ambulante.
[The boss] came upon a book. [He] showed his ﬁnd to a wandering decoder.
Languages that can omit pronouns are called pro-drop languages. Even among pro-drop
the pro-drop languages, there are marked differences in frequencies of omission.
Japanese and Chinese, for example, tend to omit far more than does Spanish. This
dimension of variation across languages is called the dimension of referential den-
sity. We say that languages that tend to use more pronouns are more referentiallyreferential
density
dense than those that use more zeros. Referentially sparse languages, like Chinese or
Japanese, that require the hearer to do more inferential work to recover antecedents
are also called cold languages. Languages that are more explicit and make it easier cold language
for the hearer are called hotlanguages. The terms hotandcold are borrowed from hot language
Marshall McLuhan’s 1964 distinction between hot media like movies, which ﬁll in
many details for the viewer, versus cold media like comics, which require the reader
to do more inferential work to ﬁll out the representation (Bickel, 2003).
Translating from languages with extensive pro-drop, like Chinese or Japanese, to
non-pro-drop languages like English can be difﬁcult since the model must somehow
identify each zero and recover who or what is being talked about in order to insert
the proper pronoun.
12.2 Machine Translation using Encoder-Decoder
The standard architecture for MT is the encoder-decoder transformer orsequence-
to-sequence model, an architecture we saw for RNNs in Chapter 13. We’ll see the
details of how to apply this architecture to transformers in Section 12.3, but ﬁrst let’s
talk about the overall task.
Most machine translation tasks make the simpliﬁcation that we can translate each
sentence independently, so we’ll just consider individual sentences for now. Given
a sentence in a source language, the MT task is then to generate a corresponding
sentence in a target language. For example, an MT system is given an English
sentence like
The green witch arrived
and must translate it into the Spanish sentence:
1Here we use the / 0-notation; we’ll introduce this and discuss this issue further in Chapter 23

12.2 • M ACHINE TRANSLATION USING ENCODER -DECODER 259
Lleg ´o la bruja verde
MT uses supervised machine learning: at training time the system is given a
large set of parallel sentences (each sentence in a source language matched with
a sentence in the target language), and learns to map source sentences into target
sentences. In practice, rather than using words (as in the example above), we split
the sentences into a sequence of subword tokens (tokens can be words, or subwords,
or individual characters). The systems are then trained to maximize the probability
of the sequence of tokens in the target language y1,...,ymgiven the sequence of
tokens in the source language x1,...,xn:
P(y1,..., ym|x1,..., xn) (12.7)
Rather than use the input tokens directly, the encoder-decoder architecture con-
sists of two components, an encoder and a decoder . The encoder takes the input
words x= [x1,..., xn]and produces an intermediate context h. At decoding time, the
system takes hand, word by word, generates the output y:
h=encoder (x) (12.8)
yt+1=decoder (h,y1,..., yt)∀t∈[1,..., m] (12.9)
In the next two sections we’ll talk about subword tokenization, and then how to get
parallel corpora for training, and then we’ll introduce the details of the encoder-
decoder architecture.
12.2.1 Tokenization
Machine translation systems use a vocabulary that is ﬁxed in advance, and rather
than using space-separated words, this vocabulary is generated with subword to-
kenization algorithms, like the BPE algorithm sketched in Chapter 2. A shared
vocabulary is used for the source and target languages, which makes it easy to copy
tokens (like names) from source to target. Using subword tokenization with tokens
shared between languages makes it natural to translate between languages like En-
glish or Hindi that use spaces to separate words, and languages like Chinese or Thai
that don’t.
We build the vocabulary by running a subword tokenization algorithm on a cor-
pus that contains both source and target language data.
Rather than the simple BPE algorithm from Fig. 2.6, modern systems often use
more powerful tokenization algorithms. Some systems (like BERT) use a variant of
BPE called the wordpiece algorithm, which instead of choosing the most frequent wordpiece
set of tokens to merge, chooses merges based on which one most increases the lan-
guage model probability of the tokenization. Wordpieces use a special symbol at the
beginning of each token; here’s a resulting tokenization from the Google MT system
(Wu et al., 2016):
words : Jet makers feud over seat width with big orders at stake
wordpieces :J et makers fe ud over seat width with big orders atstake
The wordpiece algorithm is given a training corpus and a desired vocabulary size
V , and proceeds as follows:
1. Initialize the wordpiece lexicon with characters (for example a subset of Uni-
code characters, collapsing all the remaining characters to a special unknown
character token).

260 CHAPTER 12 • M ACHINE TRANSLATION
2. Repeat until there are V wordpieces:
(a) Train an n-gram language model on the training corpus, using the current
set of wordpieces.
(b) Consider the set of possible new wordpieces made by concatenating two
wordpieces from the current lexicon. Choose the one new wordpiece that
most increases the language model probability of the training corpus.
Recall that with BPE we had to specify the number of merges to perform; in
wordpiece, by contrast, we specify the total vocabulary, which is a more intuitive
parameter. A vocabulary of 8K to 32K word pieces is commonly used.
An even more commonly used tokenization algorithm is (somewhat ambigu-
ously) called the unigram algorithm (Kudo, 2018) or sometimes the SentencePiece unigram
SentencePiece algorithm, and is used in systems like ALBERT (Lan et al., 2020) and T5 (Raf-
fel et al., 2020). (Because unigram is the default tokenization algorithm used in a
library called SentencePiece that adds a useful wrapper around tokenization algo-
rithms (Kudo and Richardson, 2018b), authors often say they are using Sentence-
Piece tokenization but really mean they are using the unigram algorithm).
In unigram tokenization, instead of building up a vocabulary by merging tokens,
we start with a huge vocabulary of every individual unicode character plus all fre-
quent sequences of characters (including all space-separated words, for languages
with spaces), and iteratively remove some tokens to get to a desired ﬁnal vocabulary
size. The algorithm is complex (involving sufﬁx-trees for efﬁciently storing many
tokens, and the EM algorithm for iteratively assigning probabilities to tokens), so we
don’t give it here, but see Kudo (2018) and Kudo and Richardson (2018b). Roughly
speaking the algorithm proceeds iteratively by estimating the probability of each
token, tokenizing the input data using various tokenizations, then removing a per-
centage of tokens that don’t occur in high-probability tokenization, and then iterates
until the vocabulary has been reduced down to the desired number of tokens.
Why does unigram tokenization work better than BPE? BPE tends to create lots
of very small non-meaningful tokens (because BPE can only create larger words or
morphemes by merging characters one at a time), and it also tends to merge very
common tokens, like the sufﬁx ed, onto their neighbors. We can see from these
examples from Bostrom and Durrett (2020) that unigram tends to produce tokens
that are more semantically meaningful:
Original: corrupted Original: Completely preposterous suggestions
BPE: cor rupted BPE: Comple t ely prep ost erous suggest ions
Unigram: corrupt ed Unigram: Complete ly pre post er ous suggestion s
12.2.2 Creating the Training data
Machine translation models are trained on a parallel corpus , sometimes called a parallel corpus
bitext , a text that appears in two (or more) languages. Large numbers of paral-
lel corpora are available. Some are governmental; the Europarl corpus (Koehn, Europarl
2005), extracted from the proceedings of the European Parliament, contains between
400,000 and 2 million sentences each from 21 European languages. The United Na-
tions Parallel Corpus contains on the order of 10 million sentences in the six ofﬁcial
languages of the United Nations (Arabic, Chinese, English, French, Russian, Span-
ish) Ziemski et al. (2016). Other parallel corpora have been made from movie and
TV subtitles, like the OpenSubtitles corpus (Lison and Tiedemann, 2016), or from
general web text, like the ParaCrawl corpus of 223 million sentence pairs between
23 EU languages and English extracted from the CommonCrawl Ba ˜n´on et al. (2020).

12.2 • M ACHINE TRANSLATION USING ENCODER -DECODER 261
Sentence alignment
Standard training corpora for MT come as aligned pairs of sentences. When creat-
ing new corpora, for example for underresourced languages or new domains, these
sentence alignments must be created. Fig. 12.4 gives a sample hypothetical sentence
alignment.
F1: -Bonjour, dit le petit prince.F2: -Bonjour, dit le marchand de pilules perfectionnées qui apaisent la soif.F3: On en avale une par semaine et l'on n'éprouve plus le besoin de boire.F4: -C’est une grosse économie de temps, dit le marchand.F5: Les experts ont fait des calculs.F6: On épargne cinquante-trois minutes par semaine.F7: “Moi, se dit le petit prince, si j'avais cinquante-trois minutes à dépenser, je marcherais tout doucement vers une fontaine..."E1: “Good morning," said the little prince.E2: “Good morning," said the merchant.E3: This was a merchant who sold pills that had been perfected to quench thirst.E4: You just swallow one pill a week and you won’t feel the need for anything to drink.E5: “They save a huge amount of time," said the merchant.E6: “Fifty−three minutes a week."E7: “If I had  fifty−three minutes to spend?" said the little prince to himself. E8: “I would take a stroll to a spring of fresh water”
Figure 12.4 A sample alignment between sentences in English and French, with sentences extracted from
Antoine de Saint-Exupery’s Le Petit Prince and a hypothetical translation. Sentence alignment takes sentences
e1,...,en, and f1,...,fmand ﬁnds minimal sets of sentences that are translations of each other, including single
sentence mappings like (e 1,f1), (e 4,f3), (e 5,f4), (e 6,f6) as well as 2-1 alignments (e 2/e3,f2), (e 7/e8,f7), and null
alignments (f 5).
Given two documents that are translations of each other, we generally need two
steps to produce sentence alignments:
• a cost function that takes a span of source sentences and a span of target sen-
tences and returns a score measuring how likely these spans are to be transla-
tions.
• an alignment algorithm that takes these scores to ﬁnd a good alignment be-
tween the documents.
To score the similarity of sentences across languages, we need to make use of
amultilingual embedding space , in which sentences from different languages are
in the same embedding space (Artetxe and Schwenk, 2019). Given such a space,
cosine similarity of such embeddings provides a natural scoring function (Schwenk,
2018). Thompson and Koehn (2019) give the following cost function between two
sentences or spans x,yfrom the source and target documents respectively:
c(x,y) =(1−cos(x,y))nSents (x)nSents (y)∑S
s=11−cos(x,ys)+∑S
s=11−cos(xs,y)(12.10)
where nSents ()gives the number of sentences (this biases the metric toward many
alignments of single sentences instead of aligning very large spans). The denom-
inator helps to normalize the similarities, and so x1,...,xS,y1,...,yS,are randomly
selected sentences sampled from the respective documents.
Usually dynamic programming is used as the alignment algorithm (Gale and
Church, 1993), in a simple extension of the minimum edit distance algorithm we
introduced in Chapter 2.
Finally, it’s helpful to do some corpus cleanup by removing noisy sentence pairs.
This can involve handwritten rules to remove low-precision pairs (for example re-
moving sentences that are too long, too short, have different URLs, or even pairs

262 CHAPTER 12 • M ACHINE TRANSLATION
that are too similar, suggesting that they were copies rather than translations). Or
pairs can be ranked by their multilingual embedding cosine score and low-scoring
pairs discarded.
12.3 Details of the Encoder-Decoder Model
EncoderThegreenllegówitcharrived<s>llególa
labruja
brujaverde
verde</s>Decodercross-attentiontransformerblocks
Figure 12.5 The encoder-decoder transformer architecture for machine translation. The encoder uses the
transformer blocks we saw in Chapter 8, while the decoder uses a more powerful block with an extra cross-
attention layer that can attend to all the encoder words. We’ll see this in more detail in the next section.
The standard architecture for MT is the encoder-decoder transformer. (For those
of you who studied RNNs, the encoder-decoder architecture was introduced already
for RNNs in Chapter 13.) Fig. 12.5 shows the intuition of the architecture at a high
level. You’ll see that the encoder-decoder architecture is made up of two transform-
ers: an encoder , which is the same as the basic transformers from Chapter 8, and
adecoder , which is augmented with a special new layer called the cross-attention
layer. The encoder takes the source language input word tokens X=x1,...,xnand
maps them to an output representation Henc=h1,...,hn; via a stack of encoder
blocks.
The decoder is essentially a conditional language model that attends to the en-
coder representation and generates the target words one by one, at each timestep
conditioning on the source sentence and the previously generated target language
words to generate a token. Decoding can use any of the decoding methods discussed
in Chapter 8 like greedy, or temperature or nucleus sampling. But the most com-
mon decoding algorithm for MT is the beam search algorithm that we’ll introduce
in Section 12.4.
But the components of the architecture differ somewhat from the transformer
block we’ve seen. First, in order to attend to the source language, the transformer
blocks in the decoder have an extra cross-attention layer. Recall that the transformer
block of Chapter 8 consists of a self-attention layer that attends to the input from the
previous layer, preceded by layer norm, and followed by another layer norm and the
feed forward layer. The decoder transformer block includes an extra layer with a
special kind of attention, cross-attention (also sometimes called encoder-decoder cross-attention
attention orsource attention ). Cross-attention has the same form as the multi-head
attention in a normal transformer block, except that while the queries as usual come
from the previous layer of the decoder, the keys and values come from the output of
theencoder .

12.3 • D ETAILS OF THE ENCODER -DECODER MODEL 263
Encoderx1x2xixn…Decoderhih2h1…hn
EncoderBlock 1Block 2Block Ky2y1…
DecoderBlock 1Block 2Block LUnembedding Matrixym
Multi-Head AttentionFeedforward+Cross-Attention………………LanguageModeling HeadHenc
Layer Normalize+
Layer Normalize…
…yi…Causal (Left-to-Right) Multi-Head AttentionFeedforward+Layer Normalize+
Layer Normalize…Layer Normalize+
y1…ym<>yi+1
Figure 12.6 The transformer block for the encoder and the decoder, showing the residual stream view. The
ﬁnal output of the encoder Henc=h1,...,hnis the context used in the decoder. The decoder is a standard
transformer except with one extra layer, the cross-attention layer, which takes that encoder output Hencand
uses it to form its KandVinputs.
That is, where in standard multi-head attention the input to each attention layer is
X, in cross attention the input is the the ﬁnal output of the encoder Henc=h1,...,hn.
Hencis of shape [n×d], each row representing one input token. To link the keys
and values from the encoder with the query from the prior layer of the decoder, we
multiply the encoder output Hencby the cross-attention layer’s key weights WKand
value weights WV. The query comes from the output from the prior decoder layer
Hdec[ℓ−1], which is multiplied by the cross-attention layer’s query weights WQ:
Q=Hdec[ℓ−1]WQ;K=HencWK;V=HencWV(12.11)
CrossAttention (Q,K,V) = softmax(QK⊺
√dk)
V (12.12)
The cross attention thus allows the decoder to attend to each of the source language
words as projected into the entire encoder ﬁnal output representations. The other
attention layer in each decoder block, the multi-head attention layer, is the same
causal (left-to-right) attention that we saw in Chapter 8. The multi-head attention in
the encoder, however, is allowed to look ahead at the entire source language text, so
it is not masked.
To train an encoder-decoder model, we use the same self-supervision model we
used for training encoder-decoders RNNs in Chapter 13. The network is given the
source text and then starting with the separator token is trained autoregressively to
predict the next token using cross-entropy loss. Recall that cross-entropy loss for
language modeling is determined by the probability the model assigns to the correct

264 CHAPTER 12 • M ACHINE TRANSLATION
next word. So at time tthe CE loss is the negative log probability the model assigns
to the next word in the training sequence:
LCE(ˆyt,yt) =−logˆyt[wt+1] (12.13)
As in that case, we use teacher forcing in the decoder. Recall that in teacher forc- teacher forcing
ing, at each time step in decoding we force the system to use the gold target token
from training as the next input xt+1, rather than allowing it to rely on the (possibly
erroneous) decoder output ˆ yt.
12.4 Decoding in MT: Beam Search
Recall the greedy decoding algorithm from Chapter 8: at each time step tin gen-
eration, the output ytis chosen by computing the probability for each word in the
vocabulary and then choosing the highest probability word (the argmax):
ˆwt=argmaxw∈VP(w|w<t) (12.14)
A problem with greedy decoding is that what looks high probability at word tmight
turn out to have been the wrong choice once we get to word t+1. The beam search
algorithm maintains multiple choices until later when we can see which one is best.
In beam search we model decoding as searching the space of possible genera-
tions, represented as a search tree whose branches represent actions (generating a search tree
token), and nodes represent states (having generated a particular preﬁx). We search
for the best action sequence, i.e., the string with the highest probability.
An illustration of the problem
Fig. 12.7 shows a made-up example. The most probable sequence is ok ok EOS (its
probability is .4×.7×1.0). But greedy search doesn’t ﬁnd it, incorrectly choosing
yesas the ﬁrst word since it has the highest local probability (0.5).
startokyesEOSokyesEOSokyesEOSEOSEOSEOSEOSt2t3p(t1|start)
t1p(t2| t1)p(t3| t1,t2)
.1.5.4.3.4.3.1.2.71.01.01.01.0
Figure 12.7 A search tree for generating the target string T=t1,t2,...from vocabulary
V={yes,ok,<s>}, showing the probability of generating each token from that state. Greedy
search chooses yesfollowed by yes, instead of the globally most probable sequence ok ok .
For some problems, like part-of-speech tagging or parsing as we will see in
Chapter 17 or Chapter 18, we can use dynamic programming search (the Viterbi

12.4 • D ECODING IN MT: B EAM SEARCH 265
algorithm) to address this problem. Unfortunately, dynamic programming is not ap-
plicable to generation problems with long-distance dependencies between the output
decisions. The only method guaranteed to ﬁnd the best solution is exhaustive search:
computing the probability of every one of the VTpossible sentences (for some length
value T) which is obviously too slow.
The solution: beam search
Instead, MT systems generally decode using beam search , a heuristic search method beam search
ﬁrst proposed by Lowerre (1976). In beam search, instead of choosing the best token
to generate at each timestep, we keep kpossible tokens at each step. This ﬁxed-size
memory footprint kis called the beam width , on the metaphor of a ﬂashlight beam beam width
that can be parameterized to be wider or narrower.
Thus at the ﬁrst step of decoding, we compute a softmax over the entire vocab-
ulary, assigning a probability to each word. We then select the k-best options from
this softmax output. These initial koutputs are the search frontier and these kinitial
words are called hypotheses . A hypothesis is an output sequence, a translation-so-
far, together with its probability.
a…aardvark..arrived..the…zebrastart
t1a…aardvark..the..witch…zebraa…aardvark..green..witch…zebrat2hd1y1BOSy1y2
y2hd1hd2thetheBOShd2greengreeny3hd1hd2arrivedarrivedBOSy2
t3hd1hd2thetheBOSy2
hd1hd2thetheBOShd2witchwitchy3a…mage..the..witch…zebraarrived…aardvark..green..who…zebray3
y3
Figure 12.8 Beam search decoding with a beam width of k=2. At each time step, we choose the kbest
hypotheses, form the Vpossible extensions of each, score those k×Vhypotheses and choose the best k=2
to continue. At time 1, the frontier has the best 2 options from the initial decoder state: arrived andthe. We
extend each, compute the probability of all the hypotheses so far ( arrived the ,arrived aardvark ,the green ,the
witch ) and again chose the best 2 ( the green andthe witch ) to be the search frontier. The images on the arcs
schematically represent the decoders that must be run at each step to score the next words (for simplicity not
depicting cross-attention).
At subsequent steps, each of the kbest hypotheses is extended incrementally

266 CHAPTER 12 • M ACHINE TRANSLATION
by being passed to distinct decoders, which each generate a softmax over the entire
vocabulary to extend the hypothesis to every possible next token. Each of these k×V
hypotheses is scored by P(yi|x,y<i): the product of the probability of the current
word choice multiplied by the probability of the path that led to it. We then prune
thek×Vhypotheses down to the kbest hypotheses, so there are never more than k
hypotheses at the frontier of the search, and never more than kdecoders. Fig. 12.8
illustrates this with a beam width of 2 for the beginning of The green witch arrived .
This process continues until an EOS is generated indicating that a complete can-
didate output has been found. At this point, the completed hypothesis is removed
from the frontier and the size of the beam is reduced by one. The search continues
until the beam has been reduced to 0. The result will be khypotheses.
To score each node by its log probability, we use the chain rule of probability to
break down p(y|x)into the product of the probability of each word given its prior
context, which we can turn into a sum of logs (for an output string of length t):
score(y) = logP(y|x)
=log(P(y1|x)P(y2|y1,x)P(y3|y1,y2,x)...P(yt|y1,...,yt−1,x))
=t∑
i=1logP(yi|y1,...,yi−1,x) (12.15)
Thus at each step, to compute the probability of a partial sentence, we simply add the
log probability of the preﬁx sentence so far to the log probability of generating the
next token. Fig. 12.9 shows the scoring for the example sentence shown in Fig. 12.8,
using some simple made-up probabilities. Log probabilities are negative or 0, and
the max of two log probabilities is the one that is greater (closer to 0).
BOSarrivedthethewitchgreenwitchmage
whoy2y3log P(y1|x)y1log P(y2|y1,x)log P(y3|y2,y1,x)-.92-1.6-1.2-.69-2.3-.69-1.6
-2.3arrived-.11-.51witch-.36-.22EOS
-.51EOS-2.3at-1.61bylog P(y4|y3,y2,y1,x)log P(y5|y4,y3,y2,y1,x)arrivedcame-1.6
y4y5log P(arrived|x) log P(arrived witch|x)log P(the|x)log P(the green|x)log P(the witch|x) =-1.6log P (arrived the|x) log P (“the green witch arrived”|x) = log P (the|x) + log P(green|the,x) + log P(witch | the, green,x)+logP(arrived|the,green,witch,x)+log P(EOS|the,green,witch,arrived,x)= -2.3= -3.9= -1.6= -2.1=-.92-2.1-3.2
-4.4-2.2-2.5-3.7-2.7-3.8-2.7-4.8
Figure 12.9 Scoring for beam search decoding with a beam width of k=2. We maintain the log probability
of each hypothesis in the beam by incrementally adding the logprob of generating each next token. Only the top
kpaths are extended to the next step.
Fig. 12.10 gives the algorithm. One problem with this version of the algorithm is
that the completed hypotheses may have different lengths. Because language mod-

12.4 • D ECODING IN MT: B EAM SEARCH 267
function BEAM DECODE (c,beam width )returns best paths
y0,h0←0
path←()
complete paths←()
state←(c,y0,h0, path) ;initial state
frontier←⟨state⟩ ;initial frontier
while frontier contains incomplete paths andbeamwidth>0
extended frontier←⟨⟩
for each state∈frontier do
y←DECODE (state )
for each word i∈Vocabulary do
successor←NEWSTATE (state ,i,yi)
extended frontier←ADDTOBEAM (successor ,extended frontier ,
beam width )
for each state inextended frontier do
ifstate is complete do
complete paths←APPEND (complete paths ,state )
extended frontier←REMOVE (extended frontier ,state )
beam width←beam width - 1
frontier←extended frontier
return completed paths
function NEWSTATE (state ,word ,word prob)returns new state
function ADDTOBEAM (state ,frontier ,width )returns updated frontier
ifLENGTH (frontier )<width then
frontier←INSERT (state ,frontier )
else if SCORE (state )>SCORE (WORST OF(frontier ))
frontier←REMOVE (WORST OF(frontier ))
frontier←INSERT (state ,frontier )
return frontier
Figure 12.10 Beam search decoding.
els generally assign lower probabilities to longer strings, a naive algorithm would
choose shorter strings for y. (This is not an issue during the earlier steps of decod-
ing; since beam search is breadth-ﬁrst, all the hypotheses being compared had the
same length.) For this reason we often apply length normalization methods, like
dividing the logprob by the number of words:
score(y) =1
tlogP(y|x) =1
tt∑
i=1logP(yi|y1,...,yi−1,x) (12.16)
For MT we generally use beam widths kbetween 5 and 10, giving us khypotheses at
the end. We can pass all kto the downstream application with their respective scores,
or if we just need a single translation we can pass the most probable hypothesis.
12.4.1 Minimum Bayes Risk Decoding
Minimum Bayes risk orMBR decoding is an alternative decoding algorithm thatminimum
Bayes risk
MBR

268 CHAPTER 12 • M ACHINE TRANSLATION
can work even better than beam search and also tends to be better than the other
decoding algorithms like temperature sampling introduced in Section 7.4.
The intuition of minimum Bayes risk is that instead of trying to choose the trans-
lation which is most probable, we choose the one that is likely to have the least error.
For example, we might want our decoding algorithm to ﬁnd the translation which
has the highest score on some evaluation metric. For example in Section 12.6 we will
introduce metrics like chrF or BERTScore that measure the goodness-of-ﬁt between
a candidate translation and a set of reference human translations. A translation that
maximizes this score, especially with a hypothetically huge set of perfect human
translations is likely to be a good one (have minimum risk) even if it is not the most
probable translation by our particular probability estimator.
In practice, we don’t know the perfect set of translations for a given sentence. So
the standard simpliﬁcation used in MBR decoding algorithms is to instead choose
the candidate translation which is most similar (by some measure of goodness-of-
ﬁt) with some set of candidate translations. We’re essentially approximating the
enormous space of all possible translations Uwith a smaller set of possible candidate
translations Y.
Given this set of possible candidate translations Y, and some similarity or align-
ment function util, we choose the best translation ˆ yas the translation which is most
similar to all the other candidate translations:
ˆy=argmax
y∈Y∑
c∈Yutil(y,c) (12.17)
Various util functions can be used, like chrF or BERTscore or BLEU. We can get the
set of candidate translations by sampling using one of the basic sampling algorithms
of Section 7.4 like temperature sampling; good results can be obtained with as few
as 32 or 64 candidates.
Minimum Bayes risk decoding can also be used for other NLP tasks; indeed
it was widely applied to speech recognition (Stolcke et al., 1997; Goel and Byrne,
2000) before being applied to machine translation (Kumar and Byrne, 2004), and
has been shown to work well across many other generation tasks as well (e.g., sum-
marization, dialogue, and image captioning (Suzgun et al., 2023a)).
12.5 Translating in low-resource situations
For some languages, and especially for English, online resources are widely avail-
able. There are many large parallel corpora that contain translations between En-
glish and many languages. But the vast majority of the world’s languages do not
have large parallel training texts available. An important ongoing research question
is how to get good translation with lesser resourced languages. The resource prob-
lem can even be true for high resource languages when we need to translate into low
resource domains (for example in a particular genre that happens to have very little
bitext).
Here we brieﬂy introduce two commonly used approaches for dealing with this
data sparsity: backtranslation , which is a special case of the general statistical
technique called data augmentation , and multilingual models , and also discuss
some socio-technical issues.

12.5 • T RANSLATING IN LOW -RESOURCE SITUATIONS 269
12.5.1 Data Augmentation
Data augmentation is a statistical technique for dealing with insufﬁcient training
data, by adding new synthetic data that is generated from the current natural data.
The most common data augmentation technique for machine translation is called
backtranslation . Backtranslation relies on the intuition that while parallel corpora backtranslation
may be limited for particular languages or domains, we can often ﬁnd a large (or
at least larger) monolingual corpus, to add to the smaller parallel corpora that are
available. The algorithm makes use of monolingual corpora in the target language
by creating synthetic bitexts.
In backtranslation, our goal is to improve source-to-target MT, given a small
parallel text (a bitext) in the source/target languages, and some monolingual data in
the target language. We ﬁrst use the bitext to train a MT system in the reverse di-
rection: a target-to-source MT system . We then use it to translate the monolingual
target data to the source language. Now we can add this synthetic bitext (natural
target sentences, aligned with MT-produced source sentences) to our training data,
and retrain our source-to-target MT model. For example suppose we want to trans-
late from Navajo to English but only have a small Navajo-English bitext, although of
course we can ﬁnd lots of monolingual English data. We use the small bitext to build
an MT engine going the other way (from English to Navajo). Once we translate the
monolingual English text to Navajo, we can add this synthetic Navajo/English bitext
to our training data.
Backtranslation has various parameters. One is how we generate the backtrans-
lated data; we can run the decoder in greedy inference, or use beam search. Or
we can do sampling, like the temperature sampling algorithm we saw in Chapter 8.
Another parameter is the ratio of backtranslated data to natural bitext data; we can
choose to upsample the bitext data (include multiple copies of each sentence). In
general backtranslation works surprisingly well; one estimate suggests that a system
trained on backtranslated text gets about 2/3 of the gain as would training on the
same amount of natural bitext (Edunov et al., 2018).
12.5.2 Multilingual models
The models we’ve described so far are for bilingual translation: one source language,
one target language. It’s also possible to build a multilingual translator.
In a multilingual translator, we train the system by giving it parallel sentences
in many different pairs of languages. That means we need to tell the system which
language to translate from and to! We tell the system which language is which
by adding a special token lsto the encoder specifying the source language we’re
translating from, and a special token ltto the decoder telling it the target language
we’d like to translate into.
Thus we slightly update Eq. 12.9 above to add these tokens in Eq. 12.19:
h=encoder (x,ls) (12.18)
yi+1=decoder (h,lt,y1,..., yi)∀i∈[1,..., m] (12.19)
One advantage of a multilingual model is that they can improve the translation
of lower-resourced languages by drawing on information from a similar language
in the training data that happens to have more resources. Perhaps we don’t know
the meaning of a word in Galician, but the word appears in the similar and higher-
resourced language Spanish.

270 CHAPTER 12 • M ACHINE TRANSLATION
12.5.3 Sociotechnical issues
Many issues in dealing with low-resource languages go beyond the purely techni-
cal. One problem is that for low-resource languages, especially from low-income
countries, native speakers are often not involved as the curators for content selec-
tion, as the language technologists, or as the evaluators who measure performance
(∀et al., 2020). Indeed, one well-known study that manually audited a large set of
parallel corpora and other major multilingual datasets found that for many of the
corpora, less than 50% of the sentences were of acceptable quality, with a lot of
data consisting of repeated sentences with web boilerplate or incorrect translations,
suggesting that native speakers may not have been sufﬁciently involved in the data
process (Kreutzer et al., 2022).
Other issues, like the tendency of many MT approaches to focus on the case
where one of the languages is English (Anastasopoulos and Neubig, 2020), have to
do with allocation of resources. Where most large multilingual systems were trained
on bitexts in which English was one of the two languages, recent huge corporate
systems like those of Fan et al. (2021) and Costa-juss `a et al. (2022) and datasets
like Schwenk et al. (2021) attempt to handle large numbers of languages (up to 200
languages) and create bitexts between many more pairs of languages and not just
through English.
At the smaller end, ∀et al. (2020) propose a participatory design process to
encourage content creators, curators, and language technologists who speak these
low-resourced languages to participate in developing MT algorithms. They provide
online groups, mentoring, and infrastructure, and report on a case study on devel-
oping MT algorithms for low-resource African languages. Among their conclusions
was to perform MT evaluation by post-editing rather than direct evaluation, since
having labelers edit an MT system and then measure the distance between the MT
output and its post-edited version both was simpler to train evaluators and makes it
easier to measure true errors in the MT output and not differences due to linguistic
variation (Bentivogli et al., 2018).
12.6 MT Evaluation
Translations are evaluated along two dimensions:
1.adequacy: how well the translation captures the exact meaning of the source adequacy
sentence. Sometimes called faithfulness orﬁdelity .
2.ﬂuency: how ﬂuent the translation is in the target language (is it grammatical, ﬂuency
clear, readable, natural).
Using humans to evaluate is most accurate, but automatic metrics are also used for
convenience.
12.6.1 Using Human Raters to Evaluate MT
The most accurate evaluations use human raters, such as online crowdworkers, to
evaluate each translation along the two dimensions. For example, along the dimen-
sion of ﬂuency , we can ask how intelligible, how clear, how readable, or how natural
the MT output (the target text) is. We can give the raters a scale, for example, from
1 (totally unintelligible) to 5 (totally intelligible), or 1 to 100, and ask them to rate
each sentence or paragraph of the MT output.

12.6 • MT E VALUATION 271
We can do the same thing to judge the second dimension, adequacy , using raters
to assign scores on a scale. If we have bilingual raters, we can give them the source
sentence and a proposed target sentence, and rate, on a 5-point or 100-point scale,
how much of the information in the source was preserved in the target. If we only
have monolingual raters but we have a good human translation of the source text, we
can give the monolingual raters the human reference translation and a target machine
translation and again rate how much information is preserved. An alternative is to
doranking : give the raters a pair of candidate translations, and ask them which one ranking
they prefer.
Training of human raters (who are often online crowdworkers) is essential; raters
without translation expertise ﬁnd it difﬁcult to separate ﬂuency and adequacy, and
so training includes examples carefully distinguishing these. Raters often disagree
(source sentences may be ambiguous, raters will have different world knowledge,
raters may apply scales differently). It is therefore common to remove outlier raters,
and (if we use a ﬁne-grained enough scale) normalizing raters by subtracting the
mean from their scores and dividing by the variance.
As discussed above, an alternative way of using human raters is to have them
post-edit translations, taking the MT output and changing it minimally until they
feel it represents a correct translation. The difference between their post-edited
translations and the original MT output can then be used as a measure of quality.
12.6.2 Automatic Evaluation
While humans produce the best evaluations of machine translation output, running a
human evaluation can be time consuming and expensive. For this reason automatic
metrics are often used as temporary proxies. Automatic metrics are less accurate
than human evaluation, but can help test potential system improvements, and even
be used as an automatic loss function for training. In this section we introduce two
families of such metrics, those based on character- or word-overlap and those based
on embedding similarity.
Automatic Evaluation by Character Overlap: chrF
The simplest and most robust metric for MT evaluation is called chrF , which stands chrF
forcharacter F-score (Popovi ´c, 2015). chrF (along with many other earlier related
metrics like BLEU, METEOR, TER, and others) is based on a simple intuition de-
rived from the pioneering work of Miller and Beebe-Center (1956): a good machine
translation will tend to contain characters and words that occur in a human trans-
lation of the same sentence. Consider a test set from a parallel corpus, in which
each source sentence has both a gold human target translation and a candidate MT
translation we’d like to evaluate. The chrF metric ranks each MT target sentence by
a function of the number of character n-gram overlaps with the human translation.
Given the hypothesis and the reference, chrF is given a parameter kindicating
the length of character n-grams to be considered, and computes the average of the
kprecisions (unigram precision, bigram, and so on) and the average of the krecalls
(unigram recall, bigram recall, etc.):
chrP percentage of character 1-grams, 2-grams, ..., k-grams in the hypothesis that
occur in the reference, averaged.
chrR percentage of character 1-grams, 2-grams,..., k-grams in the reference that
occur in the hypothesis, averaged.
The metric then computes an F-score by combining chrP and chrR using a weighting

272 CHAPTER 12 • M ACHINE TRANSLATION
parameter β. It is common to set β=2, thus weighing recall twice as much as
precision:
chrFβ= (1+β2)chrP·chrR
β2·chrP+chrR(12.20)
Forβ=2, that would be:
chrF2 =5·chrP·chrR
4·chrP+chrR
For example, consider two hypotheses that we’d like to score against the refer-
ence translation witness for the past . Here are the hypotheses along with chrF values
computed using parameters k=β=2 (in real examples, kwould be a higher number
like 6):
REF:witness for the past,
HYP1:witness of the past, chrF2,2 = .86
HYP2:past witness chrF2,2 = .62
Let’s see how we computed that chrF value for HYP1 (we’ll leave the compu-
tation of the chrF value for HYP2 as an exercise for the reader). First, chrF ignores
spaces, so we’ll remove them from both the reference and hypothesis:
REF:witnessforthepast, (18 unigrams, 17 bigrams)
HYP1:witnessofthepast, (17 unigrams, 16 bigrams)
Next let’s see how many unigrams and bigrams match between the reference and
hypothesis:
unigrams that match: w i t n e s s f o t h e p a s t , (17 unigrams)
bigrams that match: wi it tn ne es ss th he ep pa as st t, (13 bigrams)
We use that to compute the unigram and bigram precisions and recalls:
unigram P: 17/17 = 1 unigram R: 17/18 = .944
bigram P: 13/16 = .813 bigram R: 13/17 = .765
Finally we average to get chrP and chrR, and compute the F-score:
chrP = (17/17+13/16)/2=.906
chrR = (17/18+13/17)/2=.855
chrF2,2 =5chrP∗chrR
4chrP +chrR=.86
chrF is simple, robust, and correlates very well with human judgments in many
languages (Kocmi et al., 2021).
Alternative overlap metric: BLEU
There are various alternative overlap metrics. For example, before the development
of chrF, it was common to use a word-based overlap metric called BLEU (for BiLin-
gual Evaluation Understudy), that is purely precision-based rather than combining
precision and recall (Papineni et al., 2002). The BLEU score for a corpus of candi-
date translation sentences is a function of the n-gram word precision over all the
sentences combined with a brevity penalty computed over the corpus as a whole.
What do we mean by n-gram precision? Consider a corpus composed of a single
sentence. The unigram precision for this corpus is the percentage of unigram tokens

12.6 • MT E VALUATION 273
in the candidate translation that also occur in the reference translation, and ditto for
bigrams and so on, up to 4-grams. BLEU extends this unigram metric to the whole
corpus by computing the numerator as the sum over all sentences of the counts of all
the unigram types that also occur in the reference translation, and the denominator
is the total of the counts of all unigrams in all candidate sentences. We compute
this n-gram precision for unigrams, bigrams, trigrams, and 4-grams and take the
geometric mean. BLEU has many further complications, including a brevity penalty
for penalizing candidate translations that are too short, and it also requires the n-
gram counts be clipped in a particular way.
Because BLEU is a word-based metric, it is very sensitive to word tokenization,
making it impossible to compare different systems if they rely on different tokeniza-
tion standards, and doesn’t work as well in languages with complex morphology.
Nonetheless, you will sometimes still see systems evaluated by BLEU, particularly
for translation into English. In such cases it’s important to use packages that enforce
standardization for tokenization like S ACRE BLEU (Post, 2018).
Statistical Signiﬁcance Testing for MT evals
Character or word overlap-based metrics like chrF (or BLEU, or etc.) are mainly
used to compare two systems, with the goal of answering questions like: did the
new algorithm we just invented improve our MT system? To know if the difference
between the chrF scores of two MT systems is a signiﬁcant difference, we use the
paired bootstrap test, or the similar randomization test.
To get a conﬁdence interval on a single chrF score using the bootstrap test, re-
call from Section 4.11 that we take our test set (or devset) and create thousands of
pseudo-testsets by repeatedly sampling with replacement from the original test set.
We now compute the chrF score of each of the pseudo-testsets. If we drop the top
2.5% and bottom 2.5% of the scores, the remaining scores will give us the 95%
conﬁdence interval for the chrF score of our system.
To compare two MT systems A and B, we draw the same set of pseudo-testsets,
and compute the chrF scores for each of them. We then compute the percentage of
pseudo-test-sets in which A has a higher chrF score than B.
chrF: Limitations
While automatic character and word-overlap metrics like chrF or BLEU are useful,
they have important limitations. chrF is very local: a large phrase that is moved
around might barely change the chrF score at all, and chrF can’t evaluate cross-
sentence properties of a document like its discourse coherence (Chapter 24). chrF
and similar automatic metrics also do poorly at comparing very different kinds of
systems, such as comparing human-aided translation against machine translation, or
different machine translation architectures against each other (Callison-Burch et al.,
2006). Instead, automatic overlap metrics like chrF are most appropriate when eval-
uating changes to a single system.
12.6.3 Automatic Evaluation: Embedding-Based Methods
The chrF metric is based on measuring the exact character n-grams a human refer-
ence and candidate machine translation have in common. However, this criterion
is overly strict, since a good translation may use alternate words or paraphrases. A
solution ﬁrst pioneered in early metrics like METEOR (Banerjee and Lavie, 2005)
was to allow synonyms to match between the reference xand candidate ˜ x. More

274 CHAPTER 12 • M ACHINE TRANSLATION
recent metrics use BERT or other embeddings to implement this intuition.
For example, in some situations we might have datasets that have human as-
sessments of translation quality. Such datasets consists of tuples (x,˜x,r), where
x= (x1,..., xn)is a reference translation, ˜ x= (˜x1,..., ˜xm)is a candidate machine
translation, and r∈Ris a human rating that expresses the quality of ˜ xwith respect
tox. Given such data, algorithms like COMET (Rei et al., 2020) BLEURT (Sellam
et al., 2020) train a predictor on the human-labeled datasets, for example by passing
xand ˜xthrough a version of BERT (trained with extra pretraining, and then ﬁnetuned
on the human-labeled sentences), followed by a linear layer that is trained to predict
r. The output of such models correlates highly with human labels.
In other cases, however, we don’t have such human-labeled datasets. In that
case we can measure the similarity of xand ˜xby the similarity of their embeddings.
The BERTS CORE algorithm (Zhang et al., 2020) shown in Fig. 12.11, for example,
passes the reference xand the candidate ˜ xthrough BERT, computing a BERT em-
bedding for each token xiand ˜xj. Each pair of tokens (xi,˜xj)is scored by its cosine
xi·˜xj
|xi||˜xj|. Each token in xis matched to a token in ˜ xto compute recall, and each token in
˜xis matched to a token in xto compute precision (with each token greedily matched
to the most similar token in the corresponding sentence). BERTS CORE provides
precision and recall (and hence F 1):
RBERT=1
|x|∑
xi∈xmax
˜xj∈˜xxi·˜xjPBERT=1
|˜x|∑
˜xj∈˜xmax
xi∈xxi·˜xj (12.21)
Published as a conference paper at ICLR 2020
Referencethe weather is cold today
Candidateit is freezing today
CandidateContextualEmbeddingPairwise CosineSimilarityRBERT=(0.713 1.27)+(0.515 7.94)+...1.27+7.94+1.82+7.90+8.88
<latexit sha1_base64="OJyoKlmBAgUA0KDtUcsH/di5BlI=">AAACSHicbZDLattAFIaPnLRJ3JvTLrsZYgoJAqFxGqwsCqal0FVJQ5wELCNG41EyZHRh5ijECL1EnqAv002X2eUZsumipXRR6Mj2Ipf+MPDznXM4Z/64UNKg7187raXlR49XVtfaT54+e/6is/7y0OSl5mLIc5Xr45gZoWQmhihRieNCC5bGShzFZx+a+tG50Ebm2QFOCzFO2UkmE8kZWhR1ov2oClFcYPX+4/5BXZN3JEw049Wm7/XpdogyFYZQr9ffci3aoTsL1Pd23265oZrkaOqqaXAb5FIv6DXOdwMvCOqo0/U9fyby0NCF6Q52/15+BYC9qHMVTnJepiJDrpgxI+oXOK6YRsmVqNthaUTB+Bk7ESNrM2aPGVezIGryxpIJSXJtX4ZkRm9PVCw1ZprGtjNleGru1xr4v9qoxCQYVzIrShQZny9KSkUwJ02qZCK14Kim1jCupb2V8FNmc0SbfduGQO9/+aE57HnU9+gX2h18hrlW4TVswCZQ6MMAPsEeDIHDN7iBn/DL+e78cH47f+atLWcx8wruqNX6B8dUrVw=</latexit><latexit sha1_base64="RInTcZkWiVBnf/ncBstCvatCtG4=">AAACSHicbZDPShxBEMZ7Nproxugaj14al4AyMEyvyoyHwGIQPImKq8LOMvT09mhjzx+6a0KWYV4iL5EnySXH3HwGLx4U8SDYs7sHo/mg4eNXVVT1F+VSaHDda6vxbmb2/Ye5+ebHhU+LS63lz6c6KxTjPZbJTJ1HVHMpUt4DAZKf54rTJJL8LLr6VtfPvnOlRZaewCjng4RepCIWjIJBYSs8DssA+A8od/eOT6oKf8VBrCgr113HI5sBiIRrTJyOt2EbtE22p8hzdrY27EAOM9BVWTfYNbKJ43dq59q+4/tV2Gq7jjsWfmvI1LS7O08/f3nLi4dh628wzFiR8BSYpFr3iZvDoKQKBJO8agaF5jllV/SC941NqTlmUI6DqPAXQ4Y4zpR5KeAxfTlR0kTrURKZzoTCpX5dq+H/av0CYn9QijQvgKdssiguJIYM16nioVCcgRwZQ5kS5lbMLqnJEUz2TRMCef3lt+a04xDXIUek3T1AE82hVbSG1hFBHuqifXSIeoih3+gG3aF76491az1Yj5PWhjWdWUH/qNF4BkPYrbk=</latexit><latexit sha1_base64="RInTcZkWiVBnf/ncBstCvatCtG4=">AAACSHicbZDPShxBEMZ7Nproxugaj14al4AyMEyvyoyHwGIQPImKq8LOMvT09mhjzx+6a0KWYV4iL5EnySXH3HwGLx4U8SDYs7sHo/mg4eNXVVT1F+VSaHDda6vxbmb2/Ye5+ebHhU+LS63lz6c6KxTjPZbJTJ1HVHMpUt4DAZKf54rTJJL8LLr6VtfPvnOlRZaewCjng4RepCIWjIJBYSs8DssA+A8od/eOT6oKf8VBrCgr113HI5sBiIRrTJyOt2EbtE22p8hzdrY27EAOM9BVWTfYNbKJ43dq59q+4/tV2Gq7jjsWfmvI1LS7O08/f3nLi4dh628wzFiR8BSYpFr3iZvDoKQKBJO8agaF5jllV/SC941NqTlmUI6DqPAXQ4Y4zpR5KeAxfTlR0kTrURKZzoTCpX5dq+H/av0CYn9QijQvgKdssiguJIYM16nioVCcgRwZQ5kS5lbMLqnJEUz2TRMCef3lt+a04xDXIUek3T1AE82hVbSG1hFBHuqifXSIeoih3+gG3aF76491az1Yj5PWhjWdWUH/qNF4BkPYrbk=</latexit><latexit sha1_base64="fGWl4NCvlvtMu17rjLtk25oWpdc=">AAACSHicbZBLS+RAFIUrPT7bVzsu3RQ2ghIIqVbpuBgQRZiVqNgqdJpQqa5oYeVB1Y1ME/Lz3Lic3fwGNy6UwZ2VNgtfBwoO372Xe+uEmRQaXPef1fgxMTk1PTPbnJtfWFxqLf8812muGO+xVKbqMqSaS5HwHgiQ/DJTnMah5BfhzUFVv7jlSos0OYNRxgcxvUpEJBgFg4JWcBoUPvA/UOwfnp6VJf6F/UhRVmy4Tpds+SBirjFxOt1N26AdslOjrrO7vWn7cpiCLouqwa6QTRyvUznX9hzPK4NW23XcsfBXQ2rTRrWOg9Zff5iyPOYJMEm17hM3g0FBFQgmedn0c80zym7oFe8bm1BzzKAYB1HidUOGOEqVeQngMX0/UdBY61Ecms6YwrX+XKvgd7V+DpE3KESS5cAT9rYoyiWGFFep4qFQnIEcGUOZEuZWzK6pyRFM9k0TAvn85a/mvOMQ1yEnpL13VMcxg1bRGtpABHXRHvqNjlEPMXSHHtATerburUfrv/Xy1tqw6pkV9EGNxisxMKq0</latexit>1.27
7.941.827.908.88idf
weightsImportance Weighting(Optional)Maximum Similarityx<latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit><latexit sha1_base64="f2yzimwbR/Dgjzp6tZ360fHRqNI=">AAAB6HicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cW7Ae0oWy2k3btZhN2N2IJ/QVePCji1Z/kzX/jts1BWx8MPN6bYWZekAiujet+O4W19Y3NreJ2aWd3b/+gfHjU0nGqGDZZLGLVCahGwSU2DTcCO4lCGgUC28H4dua3H1FpHst7M0nQj+hQ8pAzaqzUeOqXK27VnYOsEi8nFchR75e/eoOYpRFKwwTVuuu5ifEzqgxnAqelXqoxoWxMh9i1VNIItZ/ND52SM6sMSBgrW9KQufp7IqOR1pMosJ0RNSO97M3E/7xuasJrP+MySQ1KtlgUpoKYmMy+JgOukBkxsYQyxe2thI2ooszYbEo2BG/55VXSuqh6btVrXFZqN3kcRTiBUzgHD66gBndQhyYwQHiGV3hzHpwX5935WLQWnHzmGP7A+fwB5jmM/A==</latexit>ˆx<latexit sha1_base64="5QTnVRVSrnyzznVU7d5bF5u03Iw=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit sha1_base64="5QTnVRVSrnyzznVU7d5bF5u03Iw=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit sha1_base64="5QTnVRVSrnyzznVU7d5bF5u03Iw=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit><latexit sha1_base64="5QTnVRVSrnyzznVU7d5bF5u03Iw=">AAAB7nicbVBNS8NAEJ3Ur1q/qh69LBbBU0lE0GPRi8cK9gPaUDbbTbt0swm7E7GE/ggvHhTx6u/x5r9x0+agrQ8GHu/NMDMvSKQw6LrfTmltfWNzq7xd2dnd2z+oHh61TZxqxlsslrHuBtRwKRRvoUDJu4nmNAok7wST29zvPHJtRKwecJpwP6IjJULBKFqp0x9TzJ5mg2rNrbtzkFXiFaQGBZqD6ld/GLM04gqZpMb0PDdBP6MaBZN8VumnhieUTeiI9yxVNOLGz+bnzsiZVYYkjLUthWSu/p7IaGTMNApsZ0RxbJa9XPzP66UYXvuZUEmKXLHFojCVBGOS/06GQnOGcmoJZVrYWwkbU00Z2oQqNgRv+eVV0r6oe27du7+sNW6KOMpwAqdwDh5cQQPuoAktYDCBZ3iFNydxXpx352PRWnKKmWP4A+fzB7A8j8k=</latexit>
ReferenceFigure 1: Illustration of the computation of the recall metric RBERT. Given the reference xand
candidate ˆx, we compute BERT embeddings and pairwise cosine similarity. We highlight the greedy
matching in red, and include the optional idfimportance weighting.
We experiment with different models (Section 4), using the tokenizer provided with each model.
Given a tokenized reference sentence x=hx1,...,x ki, the embedding model generates a se-
quence of vectors hx1,...,xki. Similarly, the tokenized candidate ˆx=hˆx1,..., ˆxmiis mapped
tohˆx1,...,ˆxli. The main model we use is BERT, which tokenizes the input text into a sequence
of word pieces (Wu et al., 2016), where unknown words are split into several commonly observed
sequences of characters. The representation for each word piece is computed with a Transformer
encoder (Vaswani et al., 2017) by repeatedly applying self-attention and nonlinear transformations
in an alternating fashion. BERT embeddings have been shown to beneﬁt various NLP tasks (Devlin
et al., 2019; Liu, 2019; Huang et al., 2019; Yang et al., 2019a).
Similarity Measure The vector representation allows for a soft measure of similarity instead of
exact-string (Papineni et al., 2002) or heuristic (Banerjee & Lavie, 2005) matching. The cosine
similarity of a reference token xiand a candidate token ˆxjisx>
iˆxj
kxikkˆxjk. We use pre-normalized
vectors, which reduces this calculation to the inner product x>
iˆxj. While this measure considers
tokens in isolation, the contextual embeddings contain information from the rest of the sentence.
BERTS CORE The complete score matches each token in xto a token in ˆxto compute recall,
and each token in ˆxto a token in xto compute precision. We use greedy matching to maximize
the matching similarity score,2where each token is matched to the most similar token in the other
sentence. We combine precision and recall to compute an F1 measure. For a reference xand
candidate ˆx, the recall, precision, and F1 scores are:
RBERT =1
|x|X
xi2xmax
ˆxj2ˆxx>
iˆxj,P BERT =1
|ˆx|X
ˆxj2ˆxmax
xi2xx>
iˆxj,F BERT =2PBERT ·RBERT
PBERT +RBERT.
Importance Weighting Previous work on similarity measures demonstrated that rare words can
be more indicative for sentence similarity than common words (Banerjee & Lavie, 2005; Vedantam
et al., 2015). BERTS CORE enables us to easily incorporate importance weighting. We experiment
with inverse document frequency ( idf) scores computed from the test corpus. Given Mreference
sentences {x(i)}M
i=1, the idfscore of a word-piece token wis
idf(w)= log1
MMX
i=1I[w2x(i)],
where I[·]is an indicator function. We do not use the full tf-idfmeasure because we process single
sentences, where the term frequency ( tf) is likely 1. For example, recall with idfweighting is
RBERT =P
xi2xidf(xi) max ˆxj2ˆxx>
iˆxjP
xi2xidf(xi).
Because we use reference sentences to compute idf, the idfscores remain the same for all systems
evaluated on a speciﬁc test set. We apply plus-one smoothing to handle unknown word pieces.
2We compare greedy matching with optimal assignment in Appendix C.
4
Figure 12.11 The computation of BERTS CORE recall from reference xand candidate ˆ x, from Figure 1 in
Zhang et al. (2020). This version shows an extended version of the metric in which tokens are also weighted by
their idf values.
12.7 Bias and Ethical Issues
Machine translation raises many of the same ethical issues that we’ve discussed in
earlier chapters. For example, consider MT systems translating from Hungarian
(which has the gender neutral pronoun ˝o) or Spanish (which often drops pronouns)
into English (in which pronouns are obligatory, and they have grammatical gender).
When translating a reference to a person described without speciﬁed gender, MT
systems often default to male gender (Schiebinger 2014, Prates et al. 2019). And
MT systems often assign gender according to culture stereotypes of the sort we saw
in Section 5.8. Fig. 12.12 shows examples from Prates et al. (2019), in which Hun-
garian gender-neutral ˝o is a nurse is translated with she, but gender-neutral ˝o is a
CEO is translated with he. Prates et al. (2019) ﬁnd that these stereotypes can’t com-
pletely be accounted for by gender bias in US labor statistics, because the biases are

12.8 • S UMMARY 275
ampliﬁed by MT systems, with pronouns being mapped to male or female gender
with a probability higher than if the mapping was based on actual labor employment
statistics.
Hungarian (gender neutral) source English MT output
˝o egy ´apol´o she is a nurse
˝o egy tud ´os he is a scientist
˝o egy m ´ern¨ok he is an engineer
˝o egy p ´ek he is a baker
˝o egy tan ´ar she is a teacher
˝o egy esk ¨uv˝oszervez ˝o she is a wedding organizer
˝o egy vez ´erigazgat ´o he is a CEO
Figure 12.12 When translating from gender-neutral languages like Hungarian into English,
current MT systems interpret people from traditionally male-dominated occupations as male,
and traditionally female-dominated occupations as female (Prates et al., 2019).
Similarly, a recent challenge set, the WinoMT dataset (Stanovsky et al., 2019)
shows that MT systems perform worse when they are asked to translate sentences
that describe people with non-stereotypical gender roles, like “The doctor asked the
nurse to help her in the operation”.
Many ethical questions in MT require further research. One open problem is
developing metrics for knowing what our systems don’t know. This is because MT
systems can be used in urgent situations where human translators may be unavailable
or delayed: in medical domains, to help translate when patients and doctors don’t
speak the same language, or in legal domains, to help judges or lawyers communi-
cate with witnesses or defendants. In order to ‘do no harm’, systems need ways to
assign conﬁdence values to candidate translations, so they can abstain from giving conﬁdence
incorrect translations that may cause harm.
12.8 Summary
Machine translation is one of the most widely used applications of NLP, and the
encoder-decoder model, ﬁrst developed for MT is a key tool that has applications
throughout NLP.
• Languages have divergences , both structural and lexical, that make translation
difﬁcult.
• The linguistic ﬁeld of typology investigates some of these differences; lan-
guages can be classiﬁed by their position along typological dimensions like
whether verbs precede their objects.
•Encoder-decoder networks (for transformers just as we saw in Chapter 13
for RNNs) are composed of an encoder network that takes an input sequence
and creates a contextualized representation of it, the context . This context
representation is then passed to a decoder which generates a task-speciﬁc
output sequence.
•Cross-attention allows the transformer decoder to view information from all
the hidden states of the encoder.
• Machine translation models are trained on a parallel corpus , sometimes called
abitext , a text that appears in two (or more) languages.

276 CHAPTER 12 • M ACHINE TRANSLATION
•Backtranslation is a way of making use of monolingual corpora in the target
language by running a pilot MT engine backwards to create synthetic bitexts.
• MT is evaluated by measuring a translation’s adequacy (how well it captures
the meaning of the source sentence) and ﬂuency (how ﬂuent or natural it is
in the target language). Human evaluation is the gold standard, but automatic
evaluation metrics like chrF , which measure character n-gram overlap with
human translations, or more recent metrics based on embedding similarity,
are also commonly used.
Historical Notes
MT was proposed seriously by the late 1940s, soon after the birth of the computer
(Weaver, 1949/1955). In 1954, the ﬁrst public demonstration of an MT system pro-
totype (Dostert, 1955) led to great excitement in the press (Hutchins, 1997). The
next decade saw a great ﬂowering of ideas, preﬁguring most subsequent develop-
ments. But this work was ahead of its time—implementations were limited by, for
example, the fact that pending the development of disks there was no good way to
store dictionary information.
As high-quality MT proved elusive (Bar-Hillel, 1960), there grew a consensus
on the need for better evaluation and more basic research in the new ﬁelds of for-
mal and computational linguistics. This consensus culminated in the famously crit-
ical ALPAC (Automatic Language Processing Advisory Committee) report of 1966
(Pierce et al., 1966) that led in the mid 1960s to a dramatic cut in funding for MT
in the US. As MT research lost academic respectability, the Association for Ma-
chine Translation and Computational Linguistics dropped MT from its name. Some
MT developers, however, persevered, and there were early MT systems like M ´et´eo,
which translated weather forecasts from English to French (Chandioux, 1976), and
industrial systems like Systran.
In the early years, the space of MT architectures spanned three general mod-
els. In direct translation , the system proceeds word-by-word through the source-
language text, translating each word incrementally. Direct translation uses a large
bilingual dictionary, each of whose entries is a small program with the job of trans-
lating one word. In transfer approaches, we ﬁrst parse the input text and then ap-
ply rules to transform the source-language parse into a target language parse. We
then generate the target language sentence from the parse tree. In interlingua ap-
proaches, we analyze the source language text into some abstract meaning repre-
sentation, called an interlingua . We then generate into the target language from
this interlingual representation. A common way to visualize these three early ap-
proaches was the Vauquois triangle shown in Fig. 12.13. The triangle shows theVauquois
triangle
increasing depth of analysis required (on both the analysis and generation end) as
we move from the direct approach through transfer approaches to interlingual ap-
proaches. In addition, it shows the decreasing amount of transfer knowledge needed
as we move up the triangle, from huge amounts of transfer at the direct level (al-
most all knowledge is transfer knowledge for each word) through transfer (transfer
rules only for parse trees or thematic roles) through interlingua (no speciﬁc transfer
knowledge). We can view the encoder-decoder network as an interlingual approach,
with attention acting as an integration of direct and transfer, allowing words or their
representations to be directly accessed by the decoder.

HISTORICAL NOTES 277
sourcetexttarget textDirect TranslationTransferInterlinguaSource Text:Semantic/SyntacticStructureTarget Text:Semantic/SyntacticStructuresource languageanalysissource languageanalysistarget language generation
Figure 12.13 The Vauquois (1968) triangle.
Statistical methods began to be applied around 1990, enabled ﬁrst by the devel-
opment of large bilingual corpora like the Hansard corpus of the proceedings of the
Canadian Parliament, which are kept in both French and English, and then by the
growth of the web. Early on, a number of researchers showed that it was possible
to extract pairs of aligned sentences from bilingual corpora, using words or simple
cues like sentence length (Kay and R ¨oscheisen 1988, Gale and Church 1991, Gale
and Church 1993, Kay and R ¨oscheisen 1993).
At the same time, the IBM group, drawing directly on the noisy channel model
for speech recognition, proposed two related paradigms for statistical MT . These statistical MT
include the generative algorithms that became known as IBM Models 1 through IBM Models
5, implemented in the Candide system. The algorithms (except for the decoder) Candide
were published in full detail— encouraged by the US government who had par-
tially funded the work— which gave them a huge impact on the research community
(Brown et al. 1990, Brown et al. 1993).
The group also developed a discriminative approach, called MaxEnt (for maxi-
mum entropy, an alternative formulation of logistic regression), which allowed many
features to be combined discriminatively rather than generatively (Berger et al.,
1996), which was further developed by Och and Ney (2002).
By the turn of the century, most academic research on machine translation used
statistical MT, either in the generative or discriminative mode. An extended version
of the generative approach, called phrase-based translation was developed, basedphrase-based
translation
on inducing translations for phrase-pairs (Och 1998, Marcu and Wong 2002, Koehn
et al. (2003), Och and Ney 2004, Deng and Byrne 2005, inter alia).
Once automatic metrics like BLEU were developed (Papineni et al., 2002), the
discriminative log linear formulation (Och and Ney, 2004), drawing from the IBM
MaxEnt work (Berger et al., 1996), was used to directly optimize evaluation metrics
like BLEU in a method known as Minimum Error Rate Training , orMERT (Och, MERT
2003), also drawing from speech recognition models (Chou et al., 1993). Toolkits
like GIZA (Och and Ney, 2003) and Moses (Koehn et al. 2006, Zens and Ney 2007) Moses
were widely used.
There were also approaches around the turn of the century that were based on
syntactic structure (Chapter 18). Models based on transduction grammars (alsotransduction
grammars
called synchronous grammars ) assign a parallel syntactic tree structure to a pair
of sentences in different languages, with the goal of translating the sentences by
applying reordering operations on the trees. From a generative perspective, we can
view a transduction grammar as generating pairs of aligned sentences in two lan-
guages. Some of the most widely used models included the inversion transduction
grammar (Wu, 1996) and synchronous context-free grammars (Chiang, 2005),inversion
transduction
grammar

278 CHAPTER 12 • M ACHINE TRANSLATION
Neural networks had been applied at various times to various aspects of machine
translation; for example Schwenk et al. (2006) showed how to use neural language
models to replace n-gram language models in a Spanish-English system based on
IBM Model 4. The modern neural encoder-decoder approach was pioneered by
Kalchbrenner and Blunsom (2013), who used a CNN encoder and an RNN decoder,
and was ﬁrst applied to MT by Bahdanau et al. (2015). The transformer encoder-
decoder was proposed by Vaswani et al. (2017) (see the History section of Chap-
ter 8).
Research on evaluation of machine translation began quite early. Miller and
Beebe-Center (1956) proposed a number of methods drawing on work in psycholin-
guistics. These included the use of cloze and Shannon tasks to measure intelligibility
as well as a metric of edit distance from a human translation, the intuition that un-
derlies all modern overlap-based automatic evaluation metrics. The ALPAC report
included an early evaluation study conducted by John Carroll that was extremely in-
ﬂuential (Pierce et al., 1966, Appendix 10). Carroll proposed distinct measures for
ﬁdelity and intelligibility, and had raters score them subjectively on 9-point scales.
Much early evaluation work focuses on automatic word-overlap metrics like BLEU
(Papineni et al., 2002), NIST (Doddington, 2002), TER (Translation Error Rate)
(Snover et al., 2006), Precision and Recall (Turian et al., 2003), and METEOR
(Banerjee and Lavie, 2005); character n-gram overlap methods like chrF (Popovi ´c,
2015) came later. More recent evaluation work, echoing the ALPAC report, has
emphasized the importance of careful statistical methodology and the use of human
evaluation (Kocmi et al., 2021; Marie et al., 2021).
The early history of MT is surveyed in Hutchins 1986 and 1997; Nirenburg et al.
(2002) collects early readings. See Croft (1990) or Comrie (1989) for introductions
to linguistic typology.
Exercises
12.1 Compute by hand the chrF2,2 score for HYP2 on page 272 (the answer should
round to .62).

