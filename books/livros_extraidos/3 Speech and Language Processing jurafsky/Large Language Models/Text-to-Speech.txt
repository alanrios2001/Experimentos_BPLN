CHAPTER
16Text-to-Speech
‚ÄúWords mean more than what is set down on paper. It takes the human voice to
infuse them with shades of deeper meaning.‚Äù
Maya Angelou, I Know Why the Caged Bird Sings
The task of mapping from text to speech is a task with an even longer history than
speech to text. In Vienna in 1769, Wolfgang von Kempelen built for the Empress
Maria Theresa the famous Mechanical Turk, a chess-playing automaton consisting
of a wooden box Ô¨Ålled with gears, behind which sat a robot mannequin who played
chess by moving pieces with his mechanical arm. The Turk toured Europe and the
Americas for decades, defeating Napoleon Bonaparte and even playing Charles Bab-
bage. The Mechanical Turk might have been one of the early successes of artiÔ¨Åcial
intelligence were it not for the fact that it was, alas, a hoax, powered by a human
chess player hidden inside the box.
What is less well known is that von Kempelen, an extraordinarily
proliÔ¨Åc inventor, also built between
1769 and 1790 what was deÔ¨Ånitely
not a hoax: the Ô¨Årst full-sentence
speech synthesizer, shown partially to
the right. His device consisted of a
bellows to simulate the lungs, a rub-
ber mouthpiece and a nose aperture, a
reed to simulate the vocal folds, var-
ious whistles for the fricatives, and a
small auxiliary bellows to provide the puff of air for plosives. By moving levers
with both hands to open and close apertures, and adjusting the Ô¨Çexible leather ‚Äúvo-
cal tract‚Äù, an operator could produce different consonants and vowels.
More than two centuries later, we no longer build our synthesizers out of wood
and leather, nor do we need human operators. The modern task of text-to-speech or text-to-speech
TTS , also called speech synthesis , is exactly the reverse of ASR; to map text: TTS
speech
synthesis It‚Äôs time for lunch!
to an acoustic waveform:
TTS has a wide variety of applications. It is used in spoken language models
that interact with people, for reading text out loud, for games, and to produce speech
for sufferers of neurological disorders, like the late astrophysicist Steven Hawking
after he lost the use of his voice because of ALS.
In this chapter we introduce an algorithm for TTS that, like the ASR algorithms
of the prior chapter, are trained on enormous amounts of speech datasets. We‚Äôll also
brieÔ¨Çy touch on other speech applications.

362 CHAPTER 16 ‚Ä¢ T EXT-TO-SPEECH
16.1 TTS overview
The task of text-to-speech is to generate a speech waveform that corresponds to a
desired text, using in a particular voice speciÔ¨Åed by the user.
Historically TTS was done by collecting hundreds of hours of speech from a
single talker in a lab and training a large system on it. The resulting TTS system only
worked in one voice; if you wanted a second voice, you went back and collected data
from a second talker.
The modern method is instead to train a speaker-independent synthesizer on tens
of thousands of hours of speech from thousands of talkers. To create speech in a new
voice unseen in training, we use a very small amount of speech from the desired
talker to guide the creation of the voice. So the input to a modern TTS system is a
text prompt and perhaps 3 seconds of speech from the voice we‚Äôd like to generate
the speech in. This TTS task is called zero-shot TTS because the desired voice may zero-shot TTS
never have been seen in training.
The way modern TTS systems address this task is to use language modeling, and
in particular conditional generation. The intuition is to take an enormous dataset of
speech, and use an audio tokenizer based on an audio codec to induce discrete au-
dio tokens from that speech that represent the speech. Then we can train a language
model whose vocabulary includes both speech tokens and text tokens.
We train this language model to take as input two sequences, a text transcript
and a small sample of speech from the desired talker, to tokenize both the text and
the speech into discrete tokens, and then to conditionally generate discrete samples
of the speech corresponding to the text string, in the desired voice.
At inference time we prompt this language model with a tokenized text string and
a sample of the desired voice (tokenized by the codec into discrete audio tokens) and
conditionally generate to produce the desired audio tokens. Then these tokens can
be converted into a waveform.
IEEE TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL. 33, 2025 705
Neural Codec Language Models are Zero-Shot Text
to Speech Synthesizers
Sanyuan Chen,C h e n g y iW a n g ,Y uW u, Ziqiang Zhang, Long Zhou, Shujie Liu, Member, IEEE , Zhuo Chen,
Yanqing Liu, Huaming Wang, Jinyu Li,F e l l o w ,I E E E , Lei He, Sheng Zhao, and Furu WeiAbstract ‚ÄîWe introduce a language modeling approach for text
to speech synthesis (TTS). SpeciÔ¨Åcally, we train a neural codec
language model (called V ALL-E) using discrete codes derived from
an off-the-shelf neural audio codec model, and regard TTS as a
conditional language modeling task rather than continuous signal
regression as in previous work. During the pre-training stage, we
scale up the TTS training data to 50 k hours of English speech which
is hundreds of times larger than existing systems. V ALL-E emerges
in-context learning capability and can be used to synthesize high-
quality personalized speech with only a 3-second enrolled recording
of an unseen speaker as a prompt. Experiment results show that
V ALL-E signiÔ¨Åcantly outperforms the state-of-the-art zero-shot
TTS system in terms of speech naturalness and speaker similarity.
In addition, we Ô¨Ånd V ALL-E could preserve the speaker‚Äôs emotion
and acoustic environment from the prompt in synthesis.
Index Terms ‚ÄîZero-shot text to speech synthesis, speech
generation, voice cloning, language modeling, pre-training, in-
Context learning.
I. INTRODUCTION
THE last decade has yielded dramatic breakthroughs in
speech synthesis through the development of neural net-
works and end-to-end modeling. Currently, cascaded text to
speech (TTS) systems usually leverage a pipeline with an
acoustic model and a vocoder using mel spectrograms as the
intermediate representations [1],[2],[3]. While advanced TTS
systems can synthesize high-quality speech from single or mul-
tiple speakers, it still requires high-quality clean data from
the recording studio [4],[5]. Large-scale data crawled from
the Internet cannot meet the requirement, and always lead to
performance degradation. Because the training data is relatively
small, current TTS systems still suffer from poor generalization
capability. Speaker similarity and speech naturalness drop dra-
matically for unseen speakers in the zero-shot scenario. To tackle
the zero-shot TTS problem, existing work leverages speaker
Received 1 March 2024; revised 18 September 2024; accepted 10 January
2025. Date of publication 15 January 2025; date of current version 10 February
2025. The associate editor coordinating the review of this article and approving
it for publication was Dr. Raul Fernandez. (Sanyuan Chen, Chengyi Wang, and
Yu Wu contributed equally to this work.) (Corresponding author: Shujie Liu.)
Sanyuan Chen is with the Computer Science and Technology, Harbin Institute
of Technology, Harbin 150001, China.
Chengyi Wang is with the Nankai University, Tianjin 300071, China.
Ziqiang Zhang is with the University of Science and Technology of China,
Hefei 230026, China.
Yu Wu, Long Zhou, Shujie Liu, Zhuo Chen, Yanqing Liu, Huaming Wang,
Jinyu Li, Lei He, Sheng Zhao, and Furu Wei are with the Microsoft Corporation,
Redmond, WA 98052, US (e-mail: shujliu@microsoft.com).
Digital Object IdentiÔ¨Åer 10.1109/TASLPRO.2025.3530270
Fig. 1. The overview of V ALL-E. Unlike the previous pipeline (e.g., text ‚Üí
mel-spectrogram ‚Üíwaveform), the pipeline of V ALL-E is text ‚Üídiscrete code
‚Üíwaveform. V ALL-E generates the discrete audio codec codes based on text
input and acoustic code prompt, corresponding to the target content and the
speaker‚Äôs voice.
adaptation and speaker encoding methods, requiring additional
Ô¨Åne-tuning, complex pre-designed features, or heavy structure
engineering [6],[7],[8],[9].
Instead of designing a complex and speciÔ¨Åc network for this
problem, the ultimate solution is to train a model with large and
diverse data as much as possible, motivated by the success in the
Ô¨Åeld of text generation [10],[11].R e c e n ty e a r sh a v ew i t n e s s e d
notable performance improvement for data increase in the text
language model, from 16 GB of uncompressed text [12],t o
160 GB [13],t o5 7 0G B [10],a n dÔ¨Å n a l l y ,a r o u n d1 T B [11].
Transferring this success to the Ô¨Åeld of speech synthesis, we in-
troduce V ALL-E, the Ô¨Årst language model based TTS framework
leveraging the large, diverse, and multi-speaker speech data. As
shown in Fig. 1,t os y n t h e s i z ep e r s o n a l i z e ds p e e c h( e . g . ,z e r o -
shot TTS), V ALL-E generates the corresponding acoustic codes
based on the acoustic codes of the 3-second enrolled recording
and the text input, which constrain the speaker and content
information respectively. Finally, the generated acoustic codes
are used to synthesize the Ô¨Ånal waveform with the corresponding
neural codec decoder [14].T h eu t i l i z a t i o no fd i s c r e t ea c o u s t i c
codes derived from an audio codec model allows us to treat TTS
as conditional codec language modeling. This approach enables
us to employ advanced prompting-based large-model techniques
(as in GPTs [10])f o rt h ez e r o - s h o tT T Sp r o b l e m .
We train VALL-E with Libriheavy corpus [15], a labeled ver-
sion of Librilight corpus [16]that consists 50 k hours of English
speech derived from open-source audio books with over 7000
unique speakers. Compared to previous TTS training datasets,
2998-4173 ¬© 2025 IEEE. All rights reserved, including rights for text and data mining, and training of artiÔ¨Åcial intelligence and similar technologi es.
Personal use is permitted, but republication/redistribution requires IEEE permission. See https://www.ieee.org/publications/rights/index. html for more information.
Authorized licensed use limited to: Stanford University Libraries. Downloaded on August 18,2025 at 16:03:17 UTC from IEEE Xplore.  Restrictions apply. 
Figure 16.1 V ALL-E architecture for personalized TTS (Ô¨Ågure from Chen et al. (2025)).
Fig. 16.1 from Chen et al. (2025) shows the intuition for one such TTS system,
called V ALL-E . V ALL-E is trained on 60K hours of English speech, from over 7000 V ALL-E
unique talkers. Systems like V ALL-E have 2 components:
1. The audio tokenizer , generally based on an audio codec , a system we‚Äôll de-

16.2 ‚Ä¢ U SING A CODEC TO LEARN DISCRETE AUDIO TOKENS 363
scribe in the next section. Codecs have three parts: an encoder (that turns
speech into embedding vectors), a quantizer (that turns the embeddings into
discrete tokens) and decoders (that turns the discrete tokens back into speech).
2. The 2-stage conditional language model that can generate audio tokens cor-
responding to the desired text. We‚Äôll sketch this in Section 16.3.
16.2 Using a codec to learn discrete audio tokens
Modern TTS systems are based around converting the waveform into a sequence of
discrete audio tokens. This idea of manipulating discrete audio tokens is also useful
for other speech-enabled systems like spoken language models , which take text
or speech input and can generate text or speech output to solve tasks like speech-
to-speech translation, diarization, or spoken question answering. Having discrete
tokens means that we can make use of language model technology, since language
models are specialized for sequences of discrete tokens. Audio tokenizers are thus
an important component of the modern speech toolkit.
The standard way to learn audio tokens is from a neural audio codec (the word codec
is formed from coder/decoder ). Historically a codec was a hardware device that
digitized analog symbols. More generally we use the word to mean a mechanism
for encoding analog speech signals into a digitized compressed representation that
can be efÔ¨Åciently stored and sent. Codecs are still used for compression, but for TTS
and also for spoken language models, we employ them for converting speech into
discrete tokens.
Of course the digital representation of speech we described in Chapter 14 is al-
ready discrete. For example 16 kHz speech stored in 16-bit format could be thought
of as a series of 216= 65,536 symbols, with 16,000 of those symbols per second of
speech. But a system that generates 16,000 symbols per second makes the speech
signal too long to be feasibly processed by a language model, especially one based
on transformers with their inefÔ¨Åcient quadratic attention. Instead we want symbols
that represent longer chunks of speech, perhaps something on the order of a few
hundred tokens a second.
xEncoderDecoderztzqtQuantizationqt,1 qt,2  ‚Ä¶ qt,Ncx^
Figure 16.2 Standard architecture of an audio tokenizer performing inference, Ô¨Ågure
adapted from Mousavi et al. (2025). An input waveform xis encoded (generally using a series
of downsampling convolution networks) into a series of embeddings zt. Each embedding is
then passed through a quantizer to produce a series of quantized tokens qt. To regenerate the
speech signal, the quantized tokens are re-mapped back to a vector zqtand then encoded (usu-
ally using a series of upsampling convolution networks) back to a waveform. We‚Äôll discuss
how the architecture is trained in Section 16.2.4.
Fig. 16.2 adapted from Mousavi et al. (2025). shows the standard architecture
of an audio tokenizer. Audio tokenizers take as input an audio waveform, and are

364 CHAPTER 16 ‚Ä¢ T EXT-TO-SPEECH
trained to recreate the same audio waveform out, via an intermediate representation
consisting of discrete tokens created by vector quantization.
Audio tokenizers have three stages:
1. an encoder maps the acoustic waveform, a series of Tvalues x=x1,x2,...,xT,
to a sequence of œÑembeddings z=z1,z2,...,zœÑ.œÑis typically 100-1000 times
smaller than T.
2. a vector quantizer that takes each embedding ztcorresponding to part of the
waveform, and represents it by a sequence of discrete tokens each taken from
one of the Nccodebooks, qt=qt,1,qt,2,...,qt,Nc. The vector quantizer also
sums the vector codewords from each codebook to create a quantizer output
vector zqt.
3. a decoder that generates a lossy reconstructed waveform span ÀÜxfrom the
quantizer output vector zqt.
Audio tokenizers are generally learned end-to-end, using loss functions that re-
ward a tokenization that allows the system to reconstruct the input waveform.
In the following subsections we‚Äôll go through the components of one particular
tokenizer, the E NCODEC tokenizer of D ¬¥efossez et al. (2023).
16.2.1 The Encoder and Decoder for the ENCODEC model
Conv1D (k=7, n=C)Encoder
EncoderBlock (N=16C, S=8)EncoderBlock (N=8C, S=5)EncoderBlock (N=4C, S=4)EncoderBlock (N=2C, S=2)LSTMEncoderBlock (N, S)Conv1D (K=2S, N=C, Stride=S)Residual Unit (N)Conv1D (k=7, n=D)
Waveform @ 24kHzEmbeddings @ 75Hz‚Ä¶+Residual UnitConv1D (K=3, N=C)Conv1D (K=3, N=C)Conv1D (k=7, n=C)Decoder
DecoderBlock (N=16C, S=8)DecoderBlock (N=4C, S=4)DecoderBlock (N=2C, S=2)Conv1DT (k=7, n=D)
Waveform @ 24kHzEmbeddings @ 75Hz‚Ä¶DecoderBlock (N, S)Conv1DT (K=2S, N=C)Residual UnitDecoderBlock (N=8C, S=5)LSTMDD
Figure 16.3 The encoder and decoder stages of the E NCODEC model. The goal of the encoder is to down-
sample an input waveform by encoded it as a series of embeddings ztat 75Hz, i.e. 75 embeddings a second.
Because the original signal was represented at 24kHz, this is a downsampling of24000
75=320 times. Between
the encoder and decoder is a quantization step producing a lossy embedding zqt. The goal of the decoder is to
take the lossy embedding zqtand upsample it, converting it back to a waveform.
The encoder and decoder of the E NCODEC model (D ¬¥efossez et al., 2023) are
sketched in Fig. 16.3. The goal of the encoder is to downsample a span of waveform
at time t, which is at 24kHz‚Äîone second of speech has 24,000 real values‚Äîto an
embedding representation ztat 75Hz‚Äîone second of audio is represented by 75

16.2 ‚Ä¢ U SING A CODEC TO LEARN DISCRETE AUDIO TOKENS 365
vectors, each of dimensionality D. For the purposes of this explanation, we‚Äôll use
D=256.
This downsampling is accomplished by having a series of encoder blocks that are
made up of convolutional layers with strides larger than 1 that iteratively downsam-
ple the audio, as we discussed at the end of Section 15.2. The convolution blocks are
sketched in Fig. 16.3, and include a long series of convolutions as well as residual
units that add a convolution to the prior input.
The output of the encoder is an embedding ztat time t, 75 of which are produced
per second. This embedding is then quantized (as discussed in the next section),
turning each embedding ztinto a series of Ncdiscrete symbols qt=qt,1,qt,2,...,qt,Nc,
and also turning the series of symbols into a new quantizer output vector zqt. Fi-
nally, the decoder takes the output embedding from the quantizer zqtand generates
a waveform via a symmetric set of convnets that upsample the audio.
In summary, a 24kHz waveform comes through, we encode/downsample it into
a vector ztof dimensionality D=256, quantize it into discrete symbols qt, turn it
back into a vector zqtof dimensionality D=256, and then decode/upsample that
vector back into a waveform at 24kHz.
16.2.2 Vector Quantization
The goal of the vector quantization orVQstep is to turn a series of vectors into avector
quantization
VQ series of discrete symbols.
Historically vector quantization (Gray, 1984) was used to compress a speech
signal, to reduce the bit rate for transmission or storage. To compress a sequence
of vector representations of speech, we turn each vector into an integer, an index
representing a class or cluster. Then instead of transmitting a big vector of Ô¨Çoating
point numbers, we transmit that integer index. At the other end of the transmission,
we reconstitute the vector from the index.
For TTS and other modern speech applications we use vector quantization for a
different reason: because VQ conveniently creates discrete tokens, and those Ô¨Åt well
into the language modeling paradigm, since language models do well at predicting
sequences of discrete tokens.
In practice for the E NCODEC model and other audio tokenizers, we use a power-
ful from of vector quantization called residual vector quantization that we‚Äôll deÔ¨Åne
in the following section. But it will be helpful to Ô¨Årst see the basic VQ algorithm
before we extend it.
Vector quantization has a training phase and an inference phase. We already
introduced the core of the basic VQ training algorithm when we described k-means
clustering of vectors in Section 15.4.3, since k-means clustering is the most common
algorithm used to implement VQ. To review, in VQ training, we run a big set of
speech waveÔ¨Åles through an encoder to generate Nvectors, each one corresponding
to some frame of speech. Then we cluster all these Nvectors into kclusters; kis set
by the designer as a parameter to the algorithm as the number of discrete symbols
we want, generally with k<<N. In the simplest VQ algorithm, we use the iterative
k-means algorithm to learn the clusters. Recall from Section 15.4.3 that k-means
is a two-step algorithm based on iteratively updating a set of kcentroid vectors. A
centroid is the geometric center of a set of a points in n-dimensional space. centroid
The k-means algorithm for clustering starts by assigning a random vector to each
cluster k. Then there are two iterative steps. In the assignment step, given a set of
kcurrent centroids and the entire dataset of vectors, each vector is assigned to the
cluster whose codeword is the closest (by squared Euclidean distance). In the re-

366 CHAPTER 16 ‚Ä¢ T EXT-TO-SPEECH
1.20.1‚Ä¶0.9-557-90.2Encoder256-d vectorto be quantized0
255Similarity
Codebook256-d codewords(vectors)1Cluster #23451024‚Ä¶‚Ä¶3Vector QuantizerOutput: discrete symbol
Figure 16.4 The basic VQ algorithm at inference time, after the codebook has been learned.
The input is a span of speech encoded by the encoder into a vector of dimensionality D=
256. This vector is compared with each codeword (cluster centroid) in the codebook. The
codeword for cluster 3 is most similar, so the VQ outputs 3 as the discrete representation of
this vector.
estimation step, the codeword for each cluster is recomputed by recalculating a new
mean vector. The result is that the clusters and their centroids slowly adjust to the
training space. We iterate back and forth between these two steps until the algorithm
converges.
VQ can also be used as part of end-to-end training, as we will discuss below,
in which case instead of iterative k-means, we instead recompute the means during
minibatch training via online algorithms like exponential moving averages .
At the end of clustering, the cluster index can be used as a discrete symbol. Each
cluster is also associated with a codeword , the vector which is the centroid of all codeword
the vectors in the cluster. We call the list of cluster ids (tokens) together with their
codeword the codebook , and we often call the cluster id the code . codebook
code In inference, when a new vector comes in, we compare it to each vector in the
codebook. Whichever codeword is closest, we assign it to that codeword‚Äôs associated
cluster. Fig. 16.4 shows an intuition of this inference step in the context of speech
encoding:
1. an input speech waveform is encoded into a vector v,
2. this input vector vis compared to each of the 1024 possible codewords in the
codebook,
3.vis found to be most similar to codeword 3,
4. and so the output of VQ is the discrete symbol 3 as a representation of v.
As we will see below, for training the E NCODEC model end-to-end we will need
a way to turn this discrete symbol back into a waveform. For simple VQ we do
that by directly using the codeword for that cluster, passing that codeword to the
decoder for it to reconstruct the waveform. Of course the codeword vector won‚Äôt
exactly match the original vector encoding of the input speech span, especially with
only 1024 possible codewords, but the hope is that it‚Äôs at least close if our codebook
is good, and the decoder will still produce reasonable speech. Nonetheless, more
powerful methods are usually used, as we‚Äôll see in the next section.

16.2 ‚Ä¢ U SING A CODEC TO LEARN DISCRETE AUDIO TOKENS 367
16.2.3 Residual Vector Quantization
In practice, simple VQ doesn‚Äôt produce good enough reconstructions, at least not
with codebook sizes of 1024. 1024 codeword vectors just isn‚Äôt enough to represent
the wide variety of embeddings we get from encoding all possible speech wave-
forms. So what the E NCODEC model (and many other audio tokenization methods)
use instead is a more sophisticated variant called residual vector quantization , orresidual vector
quantization
RVQ . In residual vector quantization, we use multiple codebooks arranged in a kind RVQ
of hierarchy.
Figure 2: The neural audio codec model revisit. Because RVQ is employed, the Ô¨Årst quantizer plays
the most important role in reconstruction, and the impact from others gradually decreases.
we can explicitly control the content in speech synthesis. Another direction is to apply pre-training
to the neural TTS. Chung et al. [2018 ] pre-trains speech decoder in TTS through autoregressive
mel-spectrogram prediction. In Ao et al. [2022 ], the authors propose a uniÔ¨Åed-modal encoder-decoder
framework SpeechT5, which can leverage unlabeled speech and text data to pre-train all components
of TTS model. Tjandra et al. [2019 ] quantizes unlabeled speech into discrete tokens by a VQV AE
model [ van den Oord et al. ,2017 ], and train a model with the token-to-speech sequence. They
demonstrate that the pre-trained model only requires a small amount of real data for Ô¨Åne-tuning. Bai
et al. [2022 ] proposes mask and reconstruction on mel spectrogram and showing better performance
on speech editing and synthesis. Previous TTS pre-training work leverages less than 1K hours of
data, whereas VA L L - E is pre-trained with 60K hours of data. Furthermore, VA L L - E is the Ô¨Årst to
use audio codec codes as intermediate representations, and emerge in-context learning capability in
zero-shot TTS.
3 Background: Speech Quantization
Since audio is typically stored as a sequence of 16-bit integer values, a generative model is required
to output 216=6 5,536probabilities per timestep to synthesize the raw audio. In addition, the audio
sample rate exceeding ten thousand leads to an extraordinarily long sequence length, making it more
intractable for raw audio synthesis. To this end, speech quantization is required to compress integer
values and sequence length. ¬µ-law transformation can quantize each timestep to 256 values and
reconstruct high-quality raw audio. It is widely used in speech generative models, such as WaveNet
[van den Oord et al. ,2016 ], but the inference speed is still slow since the sequence length is not
reduced. Recently, vector quantization is widely applied in self-supervised speech models for feature
extraction, such as vq-wav2vec [ Baevski et al. ,2020a ] and HuBERT [ Hsu et al. ,2021 ]. The following
work [ Lakhotia et al. ,2021 ,Du et al. ,2022 ] shows the codes from self-supervised models can also
reconstruct content, and the inference speed is faster than WaveNet. However, the speaker identity has
been discarded and the reconstruction quality is low [ Borsos et al. ,2022 ]. AudioLM [ Borsos et al. ,
2022 ] trains speech-to-speech language models on both k-means tokens from a self-supervised model
and acoustic tokens from a neural codec model, leading to high-quality speech-to-speech generation.
In this paper, we follow AudioLM [ Borsos et al. ,2022 ] to leverage neural codec models to represent
speech in discrete tokens. To compress audio for network transmission, codec models are able to
encode waveform into discrete acoustic codes and reconstruct high-quality waveform even if the
speaker is unseen in training. Compared to traditional audio codec approaches, the neural-based
codec is signiÔ¨Åcantly better at low bitrates, and we believe the quantized tokens contain sufÔ¨Åcient
information about the speaker and recording conditions. Compared to other quantization methods,
the audio codec shows the following advantages: 1) It contains abundant speaker information and
acoustic information, which could maintain speaker identity in reconstruction compared to HuBERT
codes [ Hsu et al. ,2021 ]. 2) There is an off-the-shelf codec decoder to convert discrete tokens into a
waveform, without the additional efforts on vocoder training like VQ-based methods that operated on
spectrum [ Du et al. ,2022 ]. 3) It could reduce the length of time steps for efÔ¨Åciency to address the
problem in ¬µ-law transformation [ van den Oord et al. ,2016 ].
4
Figure 16.5 Residual VQ (Ô¨Ågure from Chen et al. (2025)). We run VQ on the encoder out-
put embedding to produce a discrete symbol and the corresponding codeword. We then look
at the residual , the difference between the encoder output embedding ztand the codeword
chosen by VQ. We then take a second codebook and run VQ on this residual. We repeat the
process until we have 8 tokens.
The idea is very simple. We run standard VQ with a codebook just as in Fig. 16.4
in the prior section. Then for an input embedding ztwe take the codeword vector
that is produced, let‚Äôs call it zq1tfor the ztas quantiÔ¨Åed by codebook 1, and take the
difference between the two:
residual =zt‚àíz(1)
qt. (16.1)
This residual is the error in the VQ; the part of the original vector that the VQ residual
didn‚Äôt capture. The residual is kind of a rounding error; it‚Äôs as if in VQ we ‚Äòround‚Äô
the vector to the nearest codeword, and that creates some error. So we then take that
residual vector and pass it through another vector quantizer! That gives us a second
codeword that represents the residual part of the vector. We then take the residual
from the second codeword, and do this again. The total result is 8 codewords (the
original codeword and the 7 residuals).
That means for RVQ we represent the original speech span by a sequence of 8
discrete symbols (instead of 1 discrete symbol in basic VQ). Fig. 16.5 shows the
intuition.
What do we do when we want to reconstruct the speech? The method used in
ENCODEC RVQ is again simple: we take the 8 codewords and add them together!
The resulting vector zqtis then passed through the decoder to generate a waveform.
.
16.2.4 Training the ENCODEC model of audio tokens
The E NCODEC model (like similar audio tokenizer models) is trained end to end.
The input is a waveform, a span of speech of perhaps 1 or 10 seconds extracted from
a longer original waveform. The desired output is the same waveform span, since

368 CHAPTER 16 ‚Ä¢ T EXT-TO-SPEECH
the model is a kind of autoencoder that learns to map to itself. The model is trained
to do this reconstruction on large speech datasets like Common V oice (Ardila et al.,
2020) (over 30,000 hours of speech in 133 languages) as well as other audio data
like Audio Set (Gemmeke et al., 2017) (1.7 million 10 sec excerpts from YouTube
videos labeled from a large ontology including natural, animal, and machine sounds,
music, and so on).
xEncoderDecoderztzqtQuantizationqt,1 qt,2  ‚Ä¶ qt,Ncx^ùìõreconstructionùìõVQùìõGAN
Figure 16.6 Architecture of audio tokenizer training, Ô¨Ågure adapted from Mousavi et al.
(2025). The audio tokenizer is trained with a weighted combination of various loss functions,
summarized in the Ô¨Ågure and described below.
The E NCODEC model, like most audio tokenizers, is trained with a number of
loss functions, as suggested in Fig. 16.6. The reconstruction loss Lreconstruction mea-reconstruction
loss
sures how similar the output waveform is to the input waveform, for example by the
sum-squared difference between the original and reconstructed audio:
Lreconstruction (x,ÀÜx) =T‚àë
t=1||xt‚àíÀÜxt||2(16.2)
Similarity can additionally be measured in the frequency domain, by comparing the
original and reconstructed mel-spectrogram, again using sum-squared (L2) distance
or L1 distance or some combination.
Another kind of loss is the adversarial loss LGAN. For this loss we train a adversarial loss
generative adversarial network, a generator and a binary discriminator D, which
is a classiÔ¨Åer to distinguish between the true waveÔ¨Åle xand a generated one. We
want to train the model to fool this discriminator, so the better the discriminator, the
worse our reconstruction must be, and so we use the discriminator‚Äôs success as a loss
function, We can also incorporate various features from the generator.
Finally, we need a loss for the quantizer. This is because having a quantizer in
the middle of end-to-end training causes problems in propagation of the gradient in
the backward pass of training, because the quantization step is not differentiable.
We deal with this problem in two ways. First, we ignore the quantization step in
the backward pass. Instead we copy the gradients from the output of the quantizer
(zqt) back to the input of the quantizer ( zt), a method called the straight-through
estimator (Van Den Oord et al., 2017).
But then we need a method to make sure the code words in the vector quantizer
step get updated during training. One method is to start these off using k-means
clustering of the vectors ztto get an initial clustering. Then we can add to a loss
component, LVQ, which will be a function of the difference between the encoder

16.3 ‚Ä¢ VALL-E: G ENERATING AUDIO WITH 2-STAGE LM 369
output vector ztand the reconstructed vector after the quantization zqt, i.e. the
codeword, summed over all the Nccodebooks and residuals.
LVQ(x,ÀÜx) =T‚àë
t=1Nc‚àë
c=1||z(c)t‚àíz(c)
qt|| (16.3)
The total loss function can then just be a weighted sum of these losses:
L(x,ÀÜx) =Œª1Lreconstruction (x,ÀÜx)+Œª2LGAN(x,ÀÜx)+Œª3LVQ(x,ÀÜx) (16.4)
16.3 V ALL-E: Generating audio with 2-stage LM
As we summarized in the introduction, the structure of TTS systems like V ALL-E
is to take as input a text to be synthesized and a sample of the voice to be used, and
tokenize both, using BPE for the text and an audio codec for the speech. We then
use a language model to conditionally generate discrete audio tokens corresponding
to the text prompt, in the voice of the speech sample.
Autoregressive (AR) TransformerTextAudio PromptCT‚Äô+1,1CT‚Äô+2,1CT,1CT‚Äô+1,2CT‚Äô+1,3CT‚Äô+2,2CT,2CT‚Äô+2,3
xCCT,3CT‚Äô+1,1CT‚Äô+2,1CT,1
xCxCxCCT‚Äô+1,1CT‚Äô+1,2‚Ä¶‚Ä¶‚Ä¶CT,1CT‚Äô+2,1CT‚Äô+2,2CT,2‚Ä¶
Non-AutoregressiveNon-AutoregressiveNon-AutoregressiveOutput code sequence‚Ä¶
Figure 16.7 The 2-stage language modelling approach for V ALL-E, showing the inference
stage for the autoregressive transformer and the Ô¨Årst 3 of the 7 non-autoregressive transform-
ers. The output sequence of discrete audio codes is generated in two stages. First the au-
toregressive LM generates all the codes for the Ô¨Årst quantizer from left to right. Then the
non-autoregressive model is called 7 times to generate the remaining codes conditioned on all
the codes from the preceding quantizer, including conditioning on the codes to the right.
Instead of doing this conditional generation with a single autoregressive lan-
guage model, V ALL-E does the conditional generation in a 2-stage process, using
two distinct language models. This architectural choice is inÔ¨Çuenced by the hierar-
chical nature of the RVQ quantizer that generates the audio tokens. The output of the
Ô¨Årst RVQ quantizer is the most important token to the Ô¨Ånal speech, while the subse-
quent quantizers contribute less and less residual information to the Ô¨Ånal signal. So

370 CHAPTER 16 ‚Ä¢ T EXT-TO-SPEECH
the language model generates the acoustic codes in two stages. First, an autoregres-
sive LM generates the Ô¨Årst-quantizer codes for the entire output sequence, given the
input text and enrolled audio. Then given those codes, a non-autoregressive LM is
run 7 times, each time taking as input the output of the initial autoregressive codes
and the prior non-autoregressive quantizer and thus generating the codes from the
remaining quantizers one by one. Fig. 16.7 shows the intuition for the inference
step.
Now let‚Äôs see the architecture in a bit more detail. For training , we are given
an audio sample yand its tokenized text transcription x= [x0,x1,..., xL]. We use a
pretrained E NCODEC to convert yinto a code matrix C. Let Tbe the number of
downsampled vectors output by E NCODEC , with 8 codes per vector. Then we can
represent the encoder output as
CT√ó8=ENCODEC (y) (16.5)
Here Cis a two-dimensional acoustic code matrix that has T√ó8 entries, where the
columns represent time and the rows represent different quantizers. That is, the row
vector ct,:of the matrix contains the 8 codes for the t-th frame, and the column vector
c:,jcontains the code sequence from the j-th vector quantizer where j‚àà[1,...,8].
Given the text xand audio C, we train the TTS as a conditional code language
model to maximize the likelihood of Cconditioned on x:
L=‚àílogp(C|x)
=‚àílogT‚àè
t=0p(c<t,:,x) (16.6)
CHEN et al.: NEURAL CODEC LANGUAGE MODELS ARE ZERO-SHOT TEXT TO SPEECH SYNTHESIZERS 709
Fig. 3. Training overview of V ALL-E. We regard TTS as a conditional codec language modeling task. We structure V ALL-E as two conditional codec languag e
models in a hierarchical structure. The AR model is used to generate each code of the Ô¨Årst code sequence in an autoregressive manner, while the NAR model is
used to generate each remaining code sequence based on the previous code sequences in a non-autoregressive manner.
B. Hierarchical Structure: AR and NAR Model
As introduced in Section III,t h ec o d e cc o d e sd e r i v e df r o mt h e
neural audio codec model with RVQ exhibit two key properties:
(1) A single speech sample is encoded into multiple code se-
quences with multiple quantizers in the audio codec model. (2)
Ah i e r a r c h i c a ls t r u c t u r ei sp r e s e n tw h e r et h ec o d es e q u e n c ef r o m
the Ô¨Årst quantizer covers most of the acoustic information, while
subsequent code sequences contain the residual acoustic infor-
mation from their predecessors, serving to reÔ¨Åne and augment
the acoustic details.
Inspired by these properties, we design V ALL-E as two
conditional codec language models in a hierarchical structure:
an Autoregressive (AR) codec language model and a Non-
Autoregressive (NAR) codec language model. The AR model
generates sequence of the Ô¨Årst code for each frame in an
autoregressive manner, while the NAR model generates each
remaining code sequence based on the previous code sequences
in a non-autoregressive manner. Both models utilize the same
Transformer architecture with a text embedding layer, a code
embedding layer, and a code prediction layer. We use distinct
embeddings for the code from different codec quantizers and
share the parameters of the code prediction layer with the
parameters of the code embedding layer. Compared to the AR
model, the NAR model also have a code ID embedding layer to
specify the ID of the code sequence to predict. The AR model
and NAR model have different attention mask strategies: the AR
model uses the causal attention strategy and the NAR model uses
the full attention strategy, as shown in the right part of Fig. 3.
The combination of the AR and NAR models offers a balance
between speech quality and inference speed. The AR model con-
tributes to better speech quality with its Ô¨Çexibility for acoustic
sequence length prediction, as it ensures the rate of the generated
speech matches the enrolled recording. This is particularly useful
given the difÔ¨Åculty of training a length predictor for different
speakers whose speaking speeds can vary signiÔ¨Åcantly. On the
other hand, the NAR model enhances inference speed. Once
the acoustic sequence length has been determined by the AR
model, the NAR model can generate codes for all timestepssimultaneously, thus reducing the time complexity from O(T)
toO(1).
C. Training: Conditional Codec Language Modeling
As depicted in Fig. 3,V A L L - Ei st r a i n e du s i n gt h ec o n d i -
tional codec language modeling method. It is noteworthy that
the training of V ALL-E requires only simple utterance-wise
audio-transcription pair data, and no complex data such as
force-alignment information or additional audio clips of the
same speaker for reference. This greatly simpliÔ¨Åes the process of
collecting and processing training data, facilitating scalability.
SpeciÔ¨Åcally, for each audio and corresponding transcription
in the training dataset, we initially utilize the audio codec
encoder and text tokenizer to obtain the codec matrix C=
[c0,:,c1,:,..., cT,:]and the text sequence x=[x0,x1,...,x L],
respectively. These are then used to train the AR model and
the NAR model using the conditional codec language modeling
method.
1) Autoregressive Codec Language Modeling: The AR
model is trained to predict Ô¨Årst code sequence c:,1conditioned
on the text sequence xin an autoregressive manner.
As shown in the lower middle part of Fig. 3, we Ô¨Årst obtain
the text embedding sequence Ex=[ex
0,ex
1,..., ex
L]and the
code embedding sequence Ec=[ec
0,ec
1,..., ec
T]using the text
embedding matrix Wxand the code embedding matrix Wc,
respectively.
ex
i=Wx‚äôxi, (7)
ec
t=Wc‚äôct,1, (8)
where ‚äôdenotes index selection. We concatenate the text em-
bedding sequence and the code embedding sequence, inserting
the embedding of special tokens <eos>and<bos>in be-
tween:
E=Ex‚à•[e<eos>,e<bos>]‚à•Ec, (9)
where ||indicates concatenation. We then separately add the
learnable position embedding to the text embedding sequence
Authorized licensed use limited to: Stanford University Libraries. Downloaded on August 18,2025 at 16:03:17 UTC from IEEE Xplore.  Restrictions apply. 
Figure 16.8 Training procedure for V ALL-E. Given the text prompt, the autoregressive
transformer is Ô¨Årst trained to generate each code of the Ô¨Årst-quantizer code sequence, autore-
gressively The the non-autoregressive transformer generates the rest of the codes. Figure from
Chen et al. (2025).
Fig. 16.8 shows the intuition. On the left, we have an audio sample and its
transcription, and both are tokenized. Then we append an [ EOS] and [ BOS] token
toxand an [ EOS] token to the end of Cand train the autoregressive transformer
to predict the acoustic tokens, starting with c0,1, until [ EOS], and then the non-
autoregressive transformers to Ô¨Åll in the other tokens.
During inference , we are given a text sequence to be spoken as well as y‚Ä≤, an en-
rolled speech sample from some unseen speaker, for which we have the transcription

16.4 ‚Ä¢ TTS E VALUATION 371
transcript (y‚Ä≤). We Ô¨Årst run the codec to get an acoustic code matrix for y‚Ä≤, which will
beCP=C:T‚Ä≤,:= [c0,:,c1,:,...cT‚Ä≤,::]. Next we concatenate the transcription of y‚Ä≤to
the text sequence to be spoken to create the total input text x, which we pass through
a text tokenizer. At this stage we thus have a tokenized text xand a tokenized audio
prompt CP.
Then we generate CT=C>T‚Ä≤,:= [cT‚Ä≤+1,:,...cT,:]conditioned on the text se-
quence xand the prompt CP:
CT=argmax
CTp(CT|CP,x)
=argmax
CTT‚àè
t=T‚Ä≤+1p(ct,:|c<t,:,x) (16.7)
Then the generated tokens CTcan be converted by the E NCODEC decoder into
a waveform. Fig. 16.9 shows the intuition.
710 IEEE TRANSACTIONS ON AUDIO, SPEECH AND LANGUAGE PROCESSING, VOL. 33, 2025
and the code embedding sequence. The AR model is fed with
Eand trained to predict corresponding code sequence with
as p e c i a lt o k e n <eos>appended at the end using a code
prediction layer. Due to the causal attention mask strategy, the
prediction of each token ct,1can only attend to the text sequence
xand the previous codes c<t,1,a sd e m o n s t r a t e di nt h el o w e r
right part of Fig. 3.
Overall, the AR model Œ∏ARis optimized by minimizing the
negative log likelihood of the Ô¨Årst code sequence c:,1condi-
tioned on the text sequence x:
LAR=‚àílogp(c:,1|x;Œ∏AR) (10)
=‚àílogT‚àè
t=0p(ct,1|c<t,1,x;Œ∏AR). (11)
2) Non-Autoregressive Codec Language Modeling: Given
the Ô¨Årst code sequence generated by the AR model, the NAR
model is trained to generate each remaining code sequence
c:,jconditioned on the text sequence xand the preceding
code sequences C:,<jin a non-autoregressive manner, where
j‚àà[2,..., 8].
As we have access to all 8 code sequences of the prompt
during inference, to better model the speaker information of the
prompt, during training, we explicitly split the code matrix C
into an acoustic condition C:T‚Ä≤,:and target code matrix C>T‚Ä≤,:
with a randomly sampled length T‚Ä≤.T h em o d e li st h e no p t i m i z e d
to predict each target code sequence c>T‚Ä≤,jconditioned on the
text sequence x,a l l8c o d es e q u e n c e si nt h ea c o u s t i cc o n d i t i o n
C:T‚Ä≤,:and the preceding code sequences in the target code matrix
C:,<jin a non-autoregressive manner.
As shown in the upper middle part of Fig. 3, we Ô¨Årst obtain the
text embedding sequence Ex=[ex
0,ex
1,..., ex
L]using the text
embedding matrix Wx,a sd e n o t e di n (7).T h e n ,w eo b t a i nt h e
code embedding sequence Ec=[ec
0,ec
1,..., ec
T]by obtaining
all the code embeddings in the acoustic condition C:T‚Ä≤,:and
target code matrix C>T‚Ä≤,<jwith the code embedding matrix
Wc,a n ds u m m i n gt h e ma l o n gw i t ht h ec o d ed i m e n s i o n :
ec
t={‚àë8
k=1Wc‚äôct,k,t ‚â§T‚Ä≤
‚àëj‚àí1
k=1Wc‚äôct,k,t > T‚Ä≤. (12)
Next, we obtain the code ID embedding eidwith the code ID
embedding matrix Wid.
eid=Wid‚äôj. (13)
We concatenate the text embedding sequence, the code em-
bedding sequence, and the code ID embedding, inserting the
embedding of the special token <eos>in the middle:
E=Ex‚à•[e<eos>]‚à•Ec‚à•[e<eos>,eid]. (14)
We then separately add the learnable position embedding to the
text embedding sequence and the code embedding sequence,
similar to the AR model. The NAR model is fed with Eand
trained to predict the corresponding code sequence c:,jusing
ac o d ep r e d i c t i o nl a y e r .W i t ht h ef u l la t t e n t i o nm a s ks t r a t e g y ,
the prediction of each token ct,jcan attend to the entire input
sequence, as depicted in the upper right part of Fig. 3.
Fig. 4. Inference overview of V ALL-E. We perform zero-shot TTS via prompt-
ing the conditional codec language model.
Overall, the NAR model is optimized by minimizing the
negative log likelihood of each j-th target code sequence c>T‚Ä≤,j
conditioned on the text sequence x,a l lt h ec o d es e q u e n c e so f
the acoustic condition C:T‚Ä≤,:and the preceding j‚àí1target code
sequences c>T‚Ä≤,<j.
LNAR=‚àílogp(C>T‚Ä≤,>1|x,C<T‚Ä≤,:,c>T‚Ä≤,1;Œ∏NAR) (15)
=‚àí8‚àë
j=2logp(c>T‚Ä≤,j|x,C<T‚Ä≤,:,C>T‚Ä≤,<j;Œ∏NAR).(16)
In practice, to optimize computational efÔ¨Åciency during training,
we do not calculate the training loss by iterating over all values of
jand aggregating the corresponding losses. Instead, during each
training step, we randomly select a j‚àà[2,..., 8]and optimize
the model using the training loss:
LNAR_step =‚àílogp(c>T‚Ä≤,j|x,C<T‚Ä≤,:,C>T‚Ä≤,<j;Œ∏NAR).(17)
D. Inference: In-Context Learning Via Prompting
In-context learning is a remarkable capability of text-based
language models, enabling them to predict labels for unseen
inputs without requiring additional parameter updates. A TTS
model is considered to possess in-context learning capability if it
can synthesize high-quality speech for unseen speakers without
the need of Ô¨Åne-tuning. However, previous TTS systems have
exhibited limited in-context learning capability, either necessi-
tating additional Ô¨Åne-tuning or suffering signiÔ¨Åcant degradation
in performance for unseen speakers.
For language models, prompting is necessary to enable in-
context learning in the zero-shot scenario. In this work, we
propose performing the zero-shot TTS task via prompting during
inference. As depicted in Fig. 4,g i v e nt h et e x ts e n t e n c ea n d
the enrolled speech sample of the unseen speaker along with
its corresponding transcription, we Ô¨Årst concatenate the speech
transcription and the text sentence. This concatenated text is
encoded into the text sequence xusing the text tokenizer to
serve as the text condition. The speech sample is converted
into a code matrix CP=C:T‚Ä≤,:=[c0,:,c1,:,..., cT‚Ä≤,:]using
the audio codec encoder to serve as the prompt. By prompting
Authorized licensed use limited to: Stanford University Libraries. Downloaded on August 18,2025 at 16:03:17 UTC from IEEE Xplore.  Restrictions apply. 
Figure 16.9 Inference procedure for V ALL-E. Figure from Chen et al. (2025). The tran-
script for the 3 seconds of enrolled speech is Ô¨Årst prepended to the text to be generated, and
both the speech and text are tokenized. Next the autoregressive transformer starts generating
the Ô¨Årst codes ct‚Ä≤+1,1conditioned on the transcript and acoustic prompt.
See Chen et al. (2025) for more details on the transformer components and other
details of training.
16.4 TTS Evaluation
TTS systems are evaluated by humans, by playing an utterance to listeners and ask-
ing them to give a mean opinion score (MOS ), a rating of how good the synthesized MOS
utterances are, usually on a scale from 1‚Äì5. We can then compare systems by com-
paring their MOS scores on the same sentences (using, e.g., paired t-tests to test for
signiÔ¨Åcant differences).

372 CHAPTER 16 ‚Ä¢ T EXT-TO-SPEECH
If we are comparing exactly two systems (perhaps to see if a particular change
actually improved the system), we can also compare using CMOS (Comparative CMOS
MOS). where users give their preference on which of the two utterances is better.
CMOS scores range from -3 (the system is much worse than the reference) to 3 (the
system is better than the reference) Here we play the same sentence synthesized by
two different systems. The human listeners choose which of the two utterances they
like better. We do this for say 50 sentences (presented in random order) and compare
the number of sentences preferred for each system.
Although speech synthesis systems are best evaluated by human listeners, some
automatic metrics can be used to add more information. For example we can run the
output through an ASR system and compute the word error rate (WER) to see how
robust the synthesized output is. Or for measuring how well the voice output of the
TTS system matches the enrolled voice, we can treat the task as if it were speaker
veriÔ¨Åcation, passing the two voices to a speaker veriÔ¨Åcation system and using the
resulting score as a similarity score.
16.5 Other speech tasks
There are a wide variety of other speech-related tasks.
Speaker diarization is the task of determining ‚Äòwho spoke when‚Äô in a longspeaker
diarization
multi-speaker audio recording, marking the start and end of each speaker‚Äôs turns in
the interaction. This can be useful for transcribing meetings, classroom speech, or
medical interactions. Often diarization systems use voice activity detection (V AD) to
Ô¨Ånd segments of continuous speech, extract speaker embedding vectors, and cluster
the vectors to group together segments likely from the same speaker. More recent
work is investigating end-to-end algorithms to map directly from input speech to a
sequence of speaker labels for each frame.
Speaker recognition , is the task of identifying a speaker. We generally distin-speaker
recognition
guish the subtasks of speaker veriÔ¨Åcation , where we make a binary decision (isspeaker
veriÔ¨Åcation
this speaker Xor not?), such as for security when accessing personal information
over the telephone, and speaker identiÔ¨Åcation , where we make a one of Ndecision
trying to match a speaker‚Äôs voice against a database of many speakers.
In the task of language identiÔ¨Åcation , we are given a waveÔ¨Åle and must identifylanguage
identiÔ¨Åcation
which language is being spoken; this is an important part of building multilingual
models, creating datasets, and even plays a role in online systems.
The task of wake word detection is to detect a word or short phrase, usually in wake word
order to wake up a voice-enable assistant like Alexa, Siri, or the Google Assistant.
The goal with wake words is build the detection into small devices at the computing
edge, to maintain privacy by transmitting the least amount of user speech to a cloud-
based server. Thus wake word detectors need to be fast, small footprint software that
can Ô¨Åt into embedded devices. Wake word detectors usually use the same frontend
feature extraction we saw for ASR, often followed by a whole-word classiÔ¨Åer.
16.6 Spoken Language Models
TBD

16.7 ‚Ä¢ S UMMARY 373
16.7 Summary
This chapter introduced the fundamental algorithms of text-to-speech (TTS) .
‚Ä¢ A common modern algorithm for TTS is to use conditional generation with a
language model over audio tokens learned by a codec model.
‚Ä¢ A neural audio codec , short for coder/decoder , is a system that encodes ana-
log speech signals into a digitized, discrete compressed representation for
compression.
‚Ä¢ The discrete symbols that a codec produces as its compressed representation
can be used as discrete codes for language modeling.
‚Ä¢ A codec includes an encoder that uses convnets to downsample speech into
a downsampled embedding, a quantizer that converts the embedding into a
series of discrete tokens, and a decoder that uses convnets to upsample the
tokens/embedding back into a lossy reconstructed waveform.
‚Ä¢Vector Quantization (VQ) is a method for turning a series of vectors into a
series of discrete symbols. This can be done by using k-means clustering, and
then creating a codebook in which each code is represented by a vector at the
centroid of each cluster, called a codeword . Input vector can be assigned the
nearest codeword cluster.
‚Ä¢Residual Vector Quantization (RVQ ) is a hierarchical version of vector
quantization that produces multiple codes for an input vector by Ô¨Årst quantiz-
ing a vector into a codebook, and then quantizing the residual (the difference
between the codeword and the input vector) and then iterating.
‚Ä¢ TTS systems like V ALL-E take a text to be synthesized and a sample of the
voice to be used, tokenize with BPE (text) and an audio codec (speech) and
then use an LM to conditionally generate discrete audio tokens corresponding
to the text prompt, in the voice of the speech sample.
‚Ä¢ TTS is evaluated by playing a sentence to human listeners and having them
give a mean opinion score (MOS) .
Historical Notes
As we noted at the beginning of the chapter, speech synthesis is one of the earliest
Ô¨Åelds of speech and language processing. The 18th century saw a number of physical
models of the articulation process, including the von Kempelen model mentioned
above, as well as the 1773 vowel model of Kratzenstein in Copenhagen using organ
pipes.
The early 1950s saw the development of three early paradigms of waveform
synthesis: formant synthesis, articulatory synthesis, and concatenative synthesis.
Formant synthesizers originally were inspired by attempts to mimic human
speech by generating artiÔ¨Åcial spectrograms. The Haskins Laboratories Pattern
Playback Machine generated a sound wave by painting spectrogram patterns on a
moving transparent belt and using reÔ¨Çectance to Ô¨Ålter the harmonics of a wave-
form (Cooper et al., 1951); other very early formant synthesizers include those of
Lawrence (1953) and Fant (1951). Perhaps the most well-known of the formant
synthesizers were the Klatt formant synthesizer and its successor systems, includ-
ing the MITalk system (Allen et al., 1987) and the Klattalk software used in Digital
Equipment Corporation‚Äôs DECtalk (Klatt, 1982). See Klatt (1975) for details.

374 CHAPTER 16 ‚Ä¢ T EXT-TO-SPEECH
A second early paradigm, concatenative synthesis, seems to have been Ô¨Årst pro-
posed by Harris (1953) at Bell Laboratories; he literally spliced together pieces of
magnetic tape corresponding to phones. Soon afterwards, Peterson et al. (1958) pro-
posed a theoretical model based on diphones, including a database with multiple
copies of each diphone with differing prosody, each labeled with prosodic features
including F0, stress, and duration, and the use of join costs based on F0 and formant
distance between neighboring units. But such diphone synthesis models were not
actually implemented until decades later (Dixon and Maxey 1968, Olive 1977). The
1980s and 1990s saw the invention of unit selection synthesis , based on larger units
of non-uniform length and the use of a target cost, (Sagisaka 1988, Sagisaka et al.
1992, Hunt and Black 1996, Black and Taylor 1994, Syrdal et al. 2000).
A third paradigm, articulatory synthesizers attempt to synthesize speech by
modeling the physics of the vocal tract as an open tube. Representative models
include Stevens et al. (1953), Flanagan et al. (1975), and Fant (1986). See Klatt
(1975) and Flanagan (1972) for more details.
Most early TTS systems used phonemes as input; development of the text anal-
ysis components of TTS came somewhat later, drawing on NLP. Indeed the Ô¨Årst
true text-to-speech system seems to have been the system of Umeda and Teranishi
(Umeda et al. 1968, Teranishi and Umeda 1968, Umeda 1976), which included a
parser that assigned prosodic boundaries, as well as accent and stress.
History of codecs and modern history of neural TTS TBD.
Exercises

