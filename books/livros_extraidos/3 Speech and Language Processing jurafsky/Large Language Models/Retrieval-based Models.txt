CHAPTER
11Information Retrieval and
Retrieval-Augmented Generation
On two occasions I have been asked,—“Pray, Mr. Babbage, if you put into
the machine wrong ﬁgures, will the right answers come out?” ... I am not able
rightly to apprehend the kind of confusion of ideas that could provoke such a
question. Babbage (1864)
People need to know things. So pretty much as soon as there were computers
we were asking them questions. By 1961 there was a system to answer questions
about American baseball statistics like “How many games did the Yankees play
in July?” (Green et al., 1961). Even ﬁctional computers in the 1970s like Deep
Thought, invented by Douglas Adams in The Hitchhiker’s Guide to the Galaxy ,
answered “the Ultimate Question Of Life, The Universe, and Everything”.1And
because so much knowledge is encoded in text, systems were answering questions
at human-level performance even before LLMs: IBM’s Watson system won the TV
game-show Jeopardy! in 2011, surpassing humans at answering questions like:
WILLIAM WILKINSON’S “AN ACCOUNT OF THE PRINCIPALITIES OF WALLACHIA AND MOLDOVIA”INSPIRED THIS AUTHOR’S MOST FAMOUS NOVEL
2
It follows naturally, then, that an important function of large language models is
to ﬁll human information needs by answering people’s questions. And since a lot
of information is online, answering questions is closely related to web information
retrieval, the task performed by search engines. Indeed, the distinction is becom-
ing ever more fuzzy, as modern search engines are integrated with large language
models.
Consider some simple information needs, for example factoid questions thatfactoid
questions
can be answered with facts expressed in short texts like the following:
(11.1) Where is the Louvre Museum located?
(11.2) Where does the energy in a nuclear explosion come from?
(11.3) How to get a script l in latex?
To get an LLM to answer these questions, we can just prompt it! For example a
pretrained LLM that has been instruction-tuned on question answering (Chapter 9)
could directly answer the following question
Where is the Louvre Museum located?
by performing conditional generation given this preﬁx, and take the response as the
answer. This works because large language models have processed a lot of facts in
their pretraining data, including the location of the Louvre, and have encoded this
information in their parameters. Factual knowledge of this type seems to be stored
1The answer was 42, but unfortunately the question was never revealed.
2The answer, of course, is ‘Who is Bram Stoker’, and the novel was Dracula .

234 CHAPTER 11 • R ETRIEVAL -BASED MODELS
in the connections in the very large feedforward layers of transformer models (Geva
et al., 2021; Meng et al., 2022).
Simply prompting an LLM can be a useful approach to answer many factoid
questions. But the fact that knowledge is stored in the feedforward weights of the
LLM leads to a number of problems with prompting as a method for correctly an-
swering factual questions.
The ﬁrst and main problem is that LLMs often give the wrong answer to factual
questions! Large language models hallucinate . A hallucination is a response that is hallucinate
not faithful to the facts of the world. That is, when asked questions, large language
models sometimes make up answers that sound reasonable. For example, Dahl et al.
(2024) found that when asked questions about the legal domain (like about particu-
lar legal cases), large language models hallucinated from 69% to 88% of the time!
LLMs sometimes give incorrect factual responses even when the correct facts are
stored in the parameters; this seems to be caused by the feedforward layers failing
to recall the knowledge stored in their parameters (Jiang et al., 2024).
And it’s not always possible to tell when language models are hallucinating,
partly because LLMs aren’t well- calibrated . In a calibrated system, the conﬁdence calibrated
of a system in the correctness of its answer is highly correlated with the probability
of an answer being correct. So if a calibrated system is wrong, at least it might hedge
its answer or tell us to go check another source. But since language models are not
well-calibrated, they often give a very wrong answer with complete certainty (Zhou
et al., 2024).
A second problem with answering questions with simple prompting methods
is that prompting a large language model to answer from its pretrained parameters
doesn’t allow us to ask questions about proprietary data. We would like to use
language models to answer factual questions about proprietary data like personal
email. Or for the healthcare application we might want to apply a language model to
medical records. Or a company may have internal documents that contain answers
for customer service or internal use. Or legal ﬁrms need to ask questions about legal
discovery from proprietary documents. None of this data (hopefully) was in the
large web-based corpora that large language models are pretrained on.
A ﬁnal issue with using large language models to answer knowledge questions
is that they are static; they were pretrained once, at a particular time. This means
that LLMs cannot answer questions about rapidly changing information (like ques-
tions about something that happened last week) since they won’t have up-to-date
information from after their release data.
One solution to all these problems with simple prompting for answering factual
questions is to give a language model external sources of knowledge, for example
proprietary texts like medical or legal records, personal emails, or corporate docu-
ments, and to use those documents in answering questions. This method is called
retrieval-augmented generation orRAG , and that is the method we will focus on RAG
in this chapter. In RAG we use information retrieval (IR) techniques to retrieveinformation
retrieval
documents that are likely to have information that might help answer the question.
Then we use a large language model to generate an answer given these documents.
Basing our answers on retrieved documents can solve some of the problems with
using simple prompting to answer questions. First, it helps ensure that the answer is
grounded in facts from some curated dataset. And the system can give the user the
answer accompanied by the context of the passage or document the answer came
from. This information can help users have conﬁdence in the accuracy of the answer
(or help them spot when it is wrong!). And these retrieval techniques can be used on

11.1 • I NFORMATION RETRIEVAL 235
any proprietary data we want, such as legal or medical data for those applications.
We’ll begin by introducing information retrieval, the task of choosing the most
relevant document from a document set given a user’s query expressing their infor-
mation need. We’ll see the classic method based on cosines of sparse tf-idf vectors,
modern neural ‘dense’ retrievers based on instead representing queries and docu-
ments neurally with BERT or other language models. We then introduce retriever-
based question answering and the retrieval-augmented generation paradigm.
Finally, we’ll discuss various datasets with questions and answers that can be
used for ﬁnetuning LLMs in instruction tuning and for use as benchmarks for eval-
uation.
11.1 Information Retrieval
Information retrieval orIRis the name of the ﬁeld encompassing the retrieval of allinformation
retrieval
IR manner of media based on user information needs. The resulting IR system is often
called a search engine . Our goal in this section is to give a sufﬁcient overview of IR
to see its application to question answering. Readers with more interest speciﬁcally
in information retrieval should see the Historical Notes section at the end of the
chapter and textbooks like Manning et al. (2008).
The IR task we consider is called ad hoc retrieval , in which a user poses a ad hoc retrieval
query to a retrieval system, which then returns an ordered set of documents from
some collection . Adocument refers to whatever unit of text the system indexes and document
retrieves (web pages, scientiﬁc papers, news articles, or even shorter passages like
paragraphs). A collection refers to a set of documents being used to satisfy user collection
requests. A term refers to a word in a collection, but it may also include phrases. term
Finally, a query represents a user’s information need expressed as a set of terms. query
The high-level architecture of an ad hoc retrieval engine is shown in Fig. 11.1.
DocumentDocumentDocumentDocumentDocumentDocumentQuery ProcessingIndexingSearchDocumentDocumentDocumentDocumentDocumentRanked DocumentsDocumentqueryInvertedIndexqueryvectordocument collection
Figure 11.1 The architecture of an ad hoc IR system.
The basic IR architecture uses the vector space model we introduced in Chap-
ter 5, in which we map queries and document to vectors based on unigram word
counts, and use the cosine similarity between the vectors to rank potential documents
(Salton, 1971). This is thus an example of the bag-of-words model introduced in
Appendix K, since words are considered independently of their positions.
11.1.1 Term weighting and document scoring
Let’s look at the details of how the match between a document and query is scored.

236 CHAPTER 11 • R ETRIEVAL -BASED MODELS
We don’t use raw word counts in IR, instead computing a term weight for each term weight
document word. Two term weighting schemes are common: the tf-idf weighting
introduced in Chapter 5, and a slightly more powerful variant called BM25 . BM25
We’ll reintroduce tf-idf here so readers don’t need to look back at Chapter 5.
Tf-idf (the ‘-’ here is a hyphen, not a minus sign) is the product of two terms, the
term frequency tfand the inverse document frequency idf.
The term frequency tells us how frequent the word is; words that occur more
often in a document are likely to be informative about the document’s contents. We
usually use the log 10of the word frequency, rather than the raw count. The intuition
is that a word appearing 100 times in a document doesn’t make that word 100 times
more likely to be relevant to the meaning of the document. We also need to do
something special with counts of 0, since we can’t take the log of 0.3
tft,d={
1+log10count (t,d) if count (t,d)>0
0 otherwise(11.4)
If we use log weighting, terms which occur 0 times in a document would have tf =0,
1 times in a document tf =1+log10(1) =1+0=1, 10 times in a document tf =
1+log10(10) =2, 100 times tf =1+log10(100) =3, 1000 times tf =4, and so on.
The document frequency dftof a term tis the number of documents it oc-
curs in. Terms that occur in only a few documents are useful for discriminating
those documents from the rest of the collection; terms that occur across the entire
collection aren’t as helpful. The inverse document frequency oridfterm weight
(Sparck Jones, 1972) is deﬁned as:
idft=log10N
dft(11.5)
where Nis the total number of documents in the collection, and df tis the number
of documents in which term toccurs. The fewer documents in which a term occurs,
the higher this weight; the lowest weight of 0 is assigned to terms that occur in every
document.
Here are some idf values for some words in the corpus of Shakespeare plays,
ranging from extremely informative words that occur in only one play like Romeo ,
to those that occur in a few like salad orFalstaff , to those that are very common like
foolor so common as to be completely non-discriminative since they occur in all 37
plays like good orsweet .4
Word df idf
Romeo 1 1.57
salad 2 1.27
Falstaff 4 0.967
forest 12 0.489
battle 21 0.246
wit 34 0.037
fool 36 0.012
good 37 0
sweet 37 0
3We can also use this alternative formulation, which we have used in earlier editions: tf t,d=
log10(count (t,d)+1)
4Sweet was one of Shakespeare’s favorite adjectives, a fact probably related to the increased use of
sugar in European recipes around the turn of the 16th century (Jurafsky, 2014, p. 175).

11.1 • I NFORMATION RETRIEVAL 237
Thetf-idf value for word tin document dis then the product of term frequency
tft,dand IDF:
tf-idf(t,d) =tft,d·idft (11.6)
11.1.2 Document Scoring
We score document dby the cosine of its vector dwith the query vector q:
score(q,d) =cos(q,d) =q·d
|q||d|(11.7)
Another way to think of the cosine computation is as the dot product of unit vectors;
we ﬁrst normalize both the query and document vector to unit vectors, by dividing
by their lengths, and then take the dot product:
score(q,d) =cos(q,d) =q
|q|·d
|d|(11.8)
We can spell out Eq. 11.8, using the tf-idf values and spelling out the dot product as
a sum of products:
score(q,d) =∑
t∈qtf-idf(t,q)√∑
qi∈qtf-idf2(qi,q)·tf-idf(t,d)√∑
di∈dtf-idf2(di,d)(11.9)
Now let’s use Eq. 11.9 to walk through an example of a tiny query against a
collection of 4 nano documents, computing tf-idf values and seeing the rank of the
documents. We’ll assume all words in the following query and documents are down-
cased and punctuation is removed:
Query :sweet love
Doc 1 :Sweet sweet nurse! Love?
Doc 2 :Sweet sorrow
Doc 3 :How sweet is love?
Doc 4 :Nurse!
Fig. 11.2 shows the computation of the tf-idf cosine between the query and Doc-
ument 1, and the query and Document 2. The cosine is the normalized dot product
of tf-idf values, so for the normalization we must need to compute the document
vector lengths|q|,|d1|, and|d2|for the query and the ﬁrst two documents using
Eq. 11.4, Eq. 11.5, Eq. 11.6, and Eq. 11.9 (computations for Documents 3 and 4 are
also needed but are left as an exercise for the reader). The dot product between the
vectors is the sum over dimensions of the product, for each dimension, of the values
of the two tf-idf vectors for that dimension. This product is only non-zero where
both the query and document have non-zero values, so for this example, in which
only sweet andlove have non-zero values in the query, the dot product will be the
sum of the products of those elements of each vector.
Document 1 has a higher cosine with the query (0.747) than Document 2 has
with the query (0.0779), and so the tf-idf cosine model would rank Document 1
above Document 2. This ranking is intuitive given the vector space model, since
Document 1 has both terms including two instances of sweet , while Document 2 is
missing one of the terms. We leave the computation for Documents 3 and 4 as an
exercise for the reader.

238 CHAPTER 11 • R ETRIEVAL -BASED MODELS
Query
word cnt tf df idf tf-idf n’lized = tf-idf/|q|
sweet 1 1 3 0.125 0.125 0.383
nurse 0 0 2 0.301 0 0
love 1 1 2 0.301 0.301 0.924
how 0 0 1 0.602 0 0
sorrow 0 0 1 0.602 0 0
is 0 0 1 0.602 0 0
|q|=√
.1252+.3012=.326
Document 1 Document 2
word cnt tf tf-idf n’lized ×q cnt tf tf-idf n’lized ×q
sweet 2 1.301 0.163 0.357 0.137 1 1.000 0.125 0.203 0.0779
nurse 1 1.000 0.301 0.661 0 0 0 0 0 0
love 1 1.000 0.301 0.661 0.610 0 0 0 0 0
how 0 0 0 0 0 0 0 0 0 0
sorrow 0 0 0 0 0 1 1.000 0.602 0.979 0
is 0 0 0 0 0 0 0 0 0 0
|d1|=√
.1632+.3012+.3012=.456 |d2|=√
.1252+.6022=.615
Cosine:∑of column: 0.747 Cosine:∑of column: 0.0779
Figure 11.2 Computation of tf-idf cosine score between the query and nano-documents 1 (0.747) and 2
(0.0779), using Eq. 11.4, Eq. 11.5, Eq. 11.6 and Eq. 11.9.
In practice, there are many variants and approximations to Eq. 11.9. For exam-
ple, we might choose to simplify processing by removing some terms. To see this,
let’s start by expanding the formula for tf-idf in Eq. 11.9 to explicitly mention the tf
and idf terms from Eq. 11.6:
score(q,d) =∑
t∈qtft,q·idft√∑
qi∈qtf-idf2(qi,q)·tft,d·idft√∑
di∈dtf-idf2(di,d)(11.10)
In one common variant of tf-idf cosine, for example, we drop the idf term for the
document. Eliminating the second copy of the idf term (since the identical term is
already computed for the query) turns out to sometimes result in better performance:
score(q,d) =∑
t∈qtft,q·idft√∑
qi∈qtf-idf2(qi,q)·tft,d·idft√∑
di∈dtf-idf2(di,d)(11.11)
Other variants of tf-idf eliminate various other terms.
A slightly more complex variant in the tf-idf family is the BM25 weighting BM25
scheme (sometimes called Okapi BM25 after the Okapi IR system in which it was
introduced (Robertson et al., 1995)). BM25 adds two parameters: k, a knob that
adjust the balance between term frequency and IDF, and b, which controls the im-
portance of document length normalization. The BM25 score of a document dgiven
a query qis:
∑
t∈qIDF
log(N
dft)weighted tf
tft,d
k(
1−b+b(
|d|
|davg|))
+tft,d(11.12)

11.1 • I NFORMATION RETRIEVAL 239
where|davg|is the length of the average document. When kis 0, BM25 reverts to
no use of term frequency, just a binary selection of terms in the query (plus idf).
A large kresults in raw term frequency (plus idf). branges from 1 (scaling by
document length) to 0 (no length scaling). Manning et al. (2008) suggest reasonable
values are k = [1.2,2] and b = 0.75. Kamphuis et al. (2020) is a useful summary of
the many minor variants of BM25.
Stop words In the past it was common to remove high-frequency words from both
the query and document before representing them. The list of such high-frequency
words to be removed is called a stop list . The intuition is that high-frequency terms stop list
(often function words like the,a,to) carry little semantic weight and may not help
with retrieval, and can also help shrink the inverted index ﬁles we describe below.
The downside of using a stop list is that it makes it difﬁcult to search for phrases
that contain words in the stop list. For example, common stop lists would reduce the
phrase to be or not to be to the phrase not. In modern IR systems, the use of stop lists
is much less common, partly due to improved efﬁciency and partly because much
of their function is already handled by IDF weighting, which downweights function
words that occur in every document. Nonetheless, stop word removal is occasionally
useful in various NLP tasks so is worth keeping in mind.
11.1.3 Inverted Index
In order to compute scores, we need to efﬁciently ﬁnd documents that contain words
in the query. (Any document that contains none of the query terms will have a score
of 0 and can be ignored.) The basic search problem in IR is thus to ﬁnd all documents
d∈Cthat contain a term q∈Q.
The data structure for this task is the inverted index , which we use for mak- inverted index
ing this search efﬁcient, and also conveniently storing useful information like the
document frequency and the count of each term in each document.
An inverted index, given a query term, gives a list of documents that contain the
term. It consists of two parts, a dictionary and the postings . The dictionary is a list postings
of terms (designed to be efﬁciently accessed), each pointing to a postings list for the
term. A postings list is the list of document IDs associated with each term, which
can also contain information like the term frequency or even the exact positions of
terms in the document. The dictionary can also store the document frequency for
each term. For example, a simple inverted index for our 4 sample documents above,
with each word containing its document frequency in {}, and a pointer to a postings
list that contains document IDs and term counts in [], might look like the following:
how{1}→3 [1]
is{1}→3 [1]
love{2}→1 [1]→3 [1]
nurse{2}→1 [1]→4 [1]
sorry{1}→2 [1]
sweet{3}→1 [2]→2 [1]→3 [1]
Given a list of terms in query, we can very efﬁciently get lists of all candidate
documents, together with the information necessary to compute the tf-idf scores we
need.
There are alternatives to the inverted index. For the question-answering domain
of ﬁnding Wikipedia pages to match a user query, Chen et al. (2017a) show that
indexing based on bigrams works better than unigrams, and use efﬁcient hashing
algorithms rather than the inverted index to make the search efﬁcient.

240 CHAPTER 11 • R ETRIEVAL -BASED MODELS
11.1.4 Evaluation of Information-Retrieval Systems
We measure the performance of ranked retrieval systems using the same precision
andrecall metrics we have been using. We make the assumption that each docu-
ment returned by the IR system is either relevant to our purposes or not relevant .
Precision is the fraction of the returned documents that are relevant, and recall is the
fraction of all relevant documents that are returned. More formally, let’s assume a
system returns Tranked documents in response to an information request, a subset
Rof these are relevant, a disjoint subset, N, are the remaining irrelevant documents,
andUdocuments in the collection as a whole are relevant to this request. Precision
and recall are then deﬁned as:
Precision =|R|
|T|Recall =|R|
|U|(11.13)
Unfortunately, these metrics don’t adequately measure the performance of a system
thatranks the documents it returns. If we are comparing the performance of two
ranked retrieval systems, we need a metric that prefers the one that ranks the relevant
documents higher. We need to adapt precision and recall to capture how well a
system does at putting relevant documents higher in the ranking.
Rank Judgment Precision Rank Recall Rank
1 R 1.0 .11
2 N .50 .11
3 R .66 .22
4 N .50 .22
5 R .60 .33
6 R .66 .44
7 N .57 .44
8 R .63 .55
9 N .55 .55
10 N .50 .55
11 R .55 .66
12 N .50 .66
13 N .46 .66
14 N .43 .66
15 R .47 .77
16 N .44 .77
17 N .44 .77
18 R .44 .88
19 N .42 .88
20 N .40 .88
21 N .38 .88
22 N .36 .88
23 N .35 .88
24 N .33 .88
25 R .36 1.0
Figure 11.3 Rank-speciﬁc precision and recall values calculated as we proceed down
through a set of ranked documents (assuming the collection has 9 relevant documents).
Let’s turn to an example. Assume the table in Fig. 11.3 gives rank-speciﬁc pre-
cision and recall values calculated as we proceed down through a set of ranked doc-
uments for a particular query; the precisions are the fraction of relevant documents
seen at a given rank, and recalls the fraction of relevant documents found at the same

11.1 • I NFORMATION RETRIEVAL 241
0.0 0.2 0.4 0.6 0.8 1.0
Recall0.00.20.40.60.81.0Precision
Figure 11.4 The precision recall curve for the data in table 11.3.
rank. The recall measures in this example are based on this query having 9 relevant
documents in the collection as a whole.
Note that recall is non-decreasing; when a relevant document is encountered,
recall increases, and when a non-relevant document is found it remains unchanged.
Precision, on the other hand, jumps up and down, increasing when relevant doc-
uments are found, and decreasing otherwise. The most common way to visualize
precision and recall is to plot precision against recall in a precision-recall curve ,precision-recall
curve
like the one shown in Fig. 11.4 for the data in table 11.3.
Fig. 11.4 shows the values for a single query. But we’ll need to combine values
for all the queries, and in a way that lets us compare one system to another. One way
of doing this is to plot averaged precision values at 11 ﬁxed levels of recall (0 to 100,
in steps of 10). Since we’re not likely to have datapoints at these exact levels, we
useinterpolated precision values for the 11 recall values from the data points we dointerpolated
precision
have. We can accomplish this by choosing the maximum precision value achieved
at any level of recall at or above the one we’re calculating. In other words,
IntPrecision (r) =max
i>=rPrecision (i) (11.14)
This interpolation scheme not only lets us average performance over a set of queries,
but also helps smooth over the irregular precision values in the original data. It is
designed to give systems the beneﬁt of the doubt by assigning the maximum preci-
sion value achieved at higher levels of recall from the one being measured. Fig. 11.5
and Fig. 11.6 show the resulting interpolated data points from our example.
Given curves such as that in Fig. 11.6 we can compare two systems or approaches
by comparing their curves. Clearly, curves that are higher in precision across all
recall values are preferred. However, these curves can also provide insight into the
overall behavior of a system. Systems that are higher in precision toward the left
may favor precision over recall, while systems that are more geared towards recall
will be higher at higher levels of recall (to the right).
A second way to evaluate ranked retrieval is mean average precision (MAP),mean average
precision
which provides a single metric that can be used to compare competing systems or
approaches. In this approach, we again descend through the ranked list of items,
but now we note the precision only at those points where a relevant item has been
encountered (for example at ranks 1, 3, 5, 6 but not 2 or 4 in Fig. 11.3). For a single

242 CHAPTER 11 • R ETRIEVAL -BASED MODELS
Interpolated Precision Recall
1.0 0.0
1.0 .10
.66 .20
.66 .30
.66 .40
.63 .50
.55 .60
.47 .70
.44 .80
.36 .90
.36 1.0
Figure 11.5 Interpolated data points from Fig. 11.3.
Interpolated Precision Recall Curve
00.10.20.30.40.50.60.70.80.91
0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1
RecallPrecision
Figure 11.6 An 11 point interpolated precision-recall curve. Precision at each of the 11
standard recall levels is interpolated for each query from the maximum at any higher level of
recall. The original measured precision recall points are also shown.
query, we average these individual precision measurements over the return set (up
to some ﬁxed cutoff). More formally, if we assume that Rris the set of relevant
documents at or above r, then the average precision (AP) for a single query is
AP=1
|Rr|∑
d∈RrPrecision r(d) (11.15)
where Precision r(d)is the precision measured at the rank at which document dwas
found. For an ensemble of queries Q, we then average over these averages, to get
our ﬁnal MAP measure:
MAP =1
|Q|∑
q∈QAP(q) (11.16)
The MAP for the single query (hence = AP) in Fig. 11.3 is 0.6.

11.2 • I NFORMATION RETRIEVAL WITH DENSE VECTORS 243
11.2 Information Retrieval with Dense Vectors
The classic tf-idf or BM25 algorithms for IR have long been known to have a con-
ceptual ﬂaw: they work only if there is exact overlap of words between the query
and document. In other words, the user posing a query (or asking a question) needs
to guess exactly what words the writer of the answer might have used, an issue called
thevocabulary mismatch problem (Furnas et al., 1987).
The solution to this problem is to use an approach that can handle synonymy:
instead of (sparse) word-count vectors, using (dense) embeddings. This idea was
ﬁrst proposed for retrieval in the last century under the name of Latent Semantic
Indexing approach (Deerwester et al., 1990), but is implemented in modern times
via encoders like BERT.
The most powerful approach is to present both the query and the document to a
single encoder, allowing the transformer self-attention to see all the tokens of both
the query and the document, and thus building a representation that is sensitive to
the meanings of both query and document. Then a linear layer can be put on top of
the [CLS] token to predict a similarity score for the query/document tuple:
z=BERT (q;[SEP] ;d)[CLS]
score(q,d) =softmax (U(z)) (11.17)
This architecture is shown in Fig. 11.7a. Usually the retrieval step is not done on
an entire document. Instead documents are broken up into smaller passages, such
as non-overlapping ﬁxed-length chunks of say 100 tokens, and the retriever encodes
and retrieves these passages rather than entire documents. The query and document
have to be made to ﬁt in the BERT 512-token window, for example by truncating
the query to 64 tokens and truncating the document if necessary so that it, the query,
[CLS], and [SEP] ﬁt in 512 tokens. The BERT system together with the linear layer
Ucan then be ﬁne-tuned for the relevance task by gathering a tuning dataset of
relevant and non-relevant passages.
The problem with the full BERT architecture in Fig. 11.7a is the expense in
computation and time. With this architecture, every time we get a query, we have to
pass every single document in our entire collection through a BERT encoder jointly
with the new query! This enormous use of resources is impractical for real cases.
At the other end of the computational spectrum is a much more efﬁcient archi-
tecture, the bi-encoder . In this architecture we can encode the documents in the
collection only one time by using two separate encoder models, one to encode the
query and one to encode the document. We encode each document, and store all
the encoded document vectors in advance. When a query comes in, we encode just
this query and then use the dot product between the query vector and the precom-
puted document vectors as the score for each candidate document (Fig. 11.7b). For
example, if we used BERT, we would have two encoders BERT Qand BERT Dand
we could represent the query and document as the [CLS] token of the respective
encoders (Karpukhin et al., 2020):
zq=BERT Q(q)[CLS]
zd=BERT D(d)[CLS]
score(q,d) =zq·zd (11.18)
The bi-encoder is much cheaper than a full query/document encoder, but is also
less accurate, since its relevance decision can’t take full advantage of all the possi-

244 CHAPTER 11 • R ETRIEVAL -BASED MODELS
QueryDocument………………[sep]s(q,d)zCLSU
QueryzCLS_QzCLS_D
Document………………•s(q,d)
(a) (b)
Figure 11.7 Two ways to do dense retrieval, illustrated by using lines between layers to schematically rep-
resent self-attention: (a) Use a single encoder to jointly encode query and document and ﬁnetune to produce a
relevance score with a linear layer over the CLS token. This is too compute-expensive to use except in rescoring
(b) Use separate encoders for query and document, and use the dot product between CLS token outputs for the
query and document as the score. This is less compute-expensive, but not as accurate.
ble meaning interactions between all the tokens in the query and the tokens in the
document.
There are numerous approaches that lie in between the full encoder and the bi-
encoder. One intermediate alternative is to use cheaper methods (like BM25) as the
ﬁrst pass relevance ranking for each document, take the top N ranked documents,
and use expensive methods like the full BERT scoring to rerank only the top N
documents rather than the whole set.
Another intermediate approach is the ColBERT approach of Khattab and Za- ColBERT
haria (2020) and Khattab et al. (2021), shown in Fig. 11.8. This method separately
encodes the query and document, but rather than encoding the entire query or doc-
ument into one vector, it separately encodes each of them into contextual represen-
tations for each token. These BERT representations of each document word can be
pre-stored for efﬁciency. The relevance score between a query qand a document dis
a sum of maximum similarity (MaxSim) operators between tokens in qand tokens
ind. Essentially, for each token in q, ColBERT ﬁnds the most contextually simi-
lar token in d, and then sums up these similarities. A relevant document will have
tokens that are contextually very similar to the query.
More formally, a question qis tokenized as [q1,..., qn], prepended with a [CLS]
and a special [Q]token, truncated to N=32 tokens (or padded with [MASK] tokens if
it is shorter), and passed through BERT to get output vectors q= [q1,...,qN]. The
passage dwith tokens [d1,..., dm], is processed similarly, including a [CLS] and
special[D]token. A linear layer is applied on top of dandqto control the output
dimension, so as to keep the vectors small for storage efﬁciency, and vectors are
rescaled to unit length, producing the ﬁnal vector sequences Eq(length N) and Ed
(length m). The ColBERT scoring mechanism is:
score(q,d) =N∑
i=1mmax
j=1Eqi·Edj(11.19)
While the interaction mechanism has no tunable parameters, the ColBERT ar-

11.2 • I NFORMATION RETRIEVAL WITH DENSE VECTORS 245
QueryDocument………………s(q,d)MaxSimMaxSimMaxSim∑
normnormnormnormnormnorm
Figure 11.8 A sketch of the ColBERT algorithm at inference time. The query and docu-
ment are ﬁrst passed through separate BERT encoders. Similarity between query and doc-
ument is computed by summing a soft alignment between the contextual representations of
tokens in the query and the document. Training is end-to-end. (Various details aren’t de-
picted; for example the query is prepended by a [CLS] and[Q:] tokens, and the document
by[CLS] and[D:] tokens). Figure adapted from Khattab and Zaharia (2020).
chitecture still needs to be trained end-to-end to ﬁne-tune the BERT encoders and
train the linear layers (and the special [Q]and[D]embeddings) from scratch. It is
trained on triples⟨q,d+,d−⟩of query q, positive document d+and negative docu-
ment d−to produce a score for each document using Eq. 11.19, optimizing model
parameters using a cross-entropy loss.
All the supervised algorithms (like ColBERT or the full-interaction version of
the BERT algorithm applied for reranking) need training data in the form of queries
together with relevant and irrelevant passages or documents (positive and negative
examples). There are various semi-supervised ways to get labels; some datasets
(like MS MARCO Ranking, Section 11.4) contain gold positive examples. Negative
examples can be sampled randomly from the top-1000 results from some existing
IR system. If datasets don’t have labeled positive examples, iterative methods like
relevance-guided supervision can be used (Khattab et al., 2021) which rely on the
fact that many datasets contain short answer strings. In this method, an existing IR
system is used to harvest examples that do contain short answer strings (the top few
are taken as positives) or don’t contain short answer strings (the top few are taken as
negatives), these are used to train a new retriever, and then the process is iterated.
Efﬁciency is an important issue, since every possible document must be ranked
for its similarity to the query. For sparse word-count vectors, the inverted index
allows this very efﬁciently. For dense vector algorithms ﬁnding the set of dense
document vectors that have the highest dot product with a dense query vector is
an instance of the problem of nearest neighbor search . Modern systems there-
fore make use of approximate nearest neighbor vector search algorithms like Faiss Faiss
(Johnson et al., 2017).

246 CHAPTER 11 • R ETRIEVAL -BASED MODELS
11.3 Answering Questions with RAG
Here we introduce an important paradigm for using LLMs to answer knowledge-
based questions, based on ﬁrst ﬁnding supportive text segments from the web or
another other large collection of documents, and then generating an answer based
on the documents. The method of generating based on retrieved documents is
called retrieval-augmented generation orRAG , and the two components are some-
times called, for historical reasons, the retriever and the reader (Chen et al., 2017a).
Fig. 11.9 sketches out this standard model for answering questions.
Q: When wasthe premiere ofThe Magic Flute?RelevantDocsA:  1791RetrieverIndexed Docs
querydocsLLMpromptReader/Generator
Figure 11.9 Retrieval-based question answering has two stages: retrieval , which returns relevant documents
from the collection, and reading , in which an LLM generates answers given the documents as a prompt.
In the ﬁrst stage of the 2-stage retrieve and read model in Fig. 11.9 we retrieve
relevant passages from a text collection, for example using the dense retrievers of the
previous section. In the second reader stage, we generate the answer via retrieval-
augmented generation . In this method, we take a large pretrained language model,
give it the set of retrieved passages and other text as its prompt, and autoregressively
generate a new answer token by token.
11.3.1 Retrieval-Augmented Generation
The standard reader algorithm is to generate from a large language model, condi-
tioned on the retrieved passages. This method is known as retrieval-augmented
generation , orRAG .retrieval-
augmented
generation
RAG Recall that in simple conditional generation, we can cast the task of question
answering as word prediction by giving a language model a question and a token
likeA:suggesting that an answer should come next:
Q: Who wrote the book ‘‘The Origin of Species"? A:
Then we generate autoregressively conditioned on this text.
More formally, recall that simple autoregressive language modeling computes
the probability of a string from the previous tokens:
p(x1,..., xn) =n∏
i=1p(xi|x<i)
And simple conditional generation for question answering adds a prompt like Q:,
followed by a query q, andA:, all concatenated:
p(x1,..., xn) =n∏
i=1p([Q:] ;q;[A:] ;x<i)

11.4 • Q UESTION ANSWERING DATASETS 247
The advantage of using a large language model is the enormous amount of
knowledge encoded in its parameters from the text it was pretrained on. But as
we mentioned at the start of the chapter, while this kind of simple prompted gener-
ation can work ﬁne for many simple factoid questions, it is not a general solution
for QA, because it leads to hallucination, is unable to show users textual evidence to
support the answer, and is unable to answer questions from proprietary data.
The idea of retrieval-augmented generation is to address these problems by con-
ditioning on the retrieved passages as part of the preﬁx, perhaps with some prompt
text like “Based on these texts, answer this question:”. Let’s suppose we have a
query q, and call the set of retrieved passages based on it R( q). For example, we
could have a prompt like:
Schematic of a RAG Prompt
retrieved passage 1
retrieved passage 2
...
retrieved passage n
Based on these texts, answer this question: Q: Who wrote
the book ‘‘The Origin of Species"? A:
Or more formally,
p(x1,..., xn) =n∏
i=1p(xi|R(q) ; prompt ; [Q:] ;q;[A:] ;x<i)
As with the span-based extraction reader, successfully applying the retrieval-
augmented generation algorithm for QA requires a successful retriever, and often
a two-stage retrieval algorithm is used in which the retrieval is reranked. Some
complex questions may require multi-hop architectures, in which a query is used to multi-hop
retrieve documents, which are then appended to the original query for a second stage
of retrieval. Details of prompt engineering also have to be worked out, like deciding
whether to demarcate passages, for example with [SEP] tokens, and so on. Combi-
nations of private data and public data involving an externally hosted large language
model may lead to privacy concerns that need to be worked out (Arora et al., 2023).
Much research in this area also focuses on ways to more tightly integrate the retrieval
and reader stages.
11.4 Question Answering Datasets
There are scores of question answering datasets, used both for instruction tuning and
for evaluation of the question answering abilities of language models.
We can distinguish the datasets along many dimensions, summarized nicely in
Rogers et al. (2023). One is the original purpose of the questions in the data, whether
they were natural information-seeking questions, or whether they were questions
designed for probing : evaluating or testing systems or humans.

248 CHAPTER 11 • R ETRIEVAL -BASED MODELS
On the natural side there are datasets like Natural Questions (KwiatkowskiNatural
Questions
et al., 2019), a set of anonymized English queries to the Google search engine and
their answers. The answers are created by annotators based on Wikipedia infor-
mation, and include a paragraph-length long answer and a short span answer. For
example the question “When are hops added to the brewing process?” has the short
answer the boiling process and a long answer which is an entire paragraph from the
Wikipedia page on Brewing .
A similar natural question set is the MS MARCO (Microsoft Machine Reading MS MARCO
Comprehension) collection of datasets, including 1 million real anonymized English
questions from Microsoft Bing query logs together with a human generated answer
and 9 million passages (Bajaj et al., 2016), that can be used both to test retrieval
ranking and question answering.
Although many datasets focus on English, natural information-seeking ques-
tion datasets exist in other languages. The DuReader dataset is a Chinese QA
resource based on search engine queries and community QA (He et al., 2018).
TyDi QA dataset contains 204K question-answer pairs from 11 typologically di- TyDi QA
verse languages, including Arabic, Bengali, Kiswahili, Russian, and Thai (Clark
et al., 2020a). In the T YDIQA task, a system is given a question and the passages
from a Wikipedia article and must (a) select the passage containing the answer (or
NULL if no passage contains the answer), and (b) mark the minimal answer span (or
NULL).
On the probing side are datasets like MMLU (Massive Multitask Language Un- MMLU
derstanding), a commonly-used dataset of 15908 knowledge and reasoning ques-
tions in 57 areas including medicine, mathematics, computer science, law, and oth-
ers. MMLU questions are sourced from various exams for humans, such as the US
Graduate Record Exam, Medical Licensing Examination, and Advanced Placement
exams. So the questions don’t represent people’s information needs, but rather are
designed to test human knowledge for academic or licensing purposes. Fig. 11.10
shows some examples, with the correct answers in bold.
Some of the question datasets described above augment each question with pas-
sage(s) from which the answer can be extracted. These datasets were mainly created
for an earlier QA task called reading comprehension in which a model is givenreading
comprehension
a question and a document and is required to extract the answer from the given
document. We sometimes call the task of question answering given one or more
documents (for example via RAG), the open book QA task, while the task of an- open book
swering directly from the LM with no retrieval component at all is the closed book closed book
QA task.5Thus datasets like Natural Questions can be treated as open book if the
solver uses each question’s attached document, or closed book if the documents are
not used, while datasets like MMLU are solely closed book.
Another dimension of variation is the format of the answer: multiple-choice
versus freeform. And of course there are variations in prompting, like whether the
model is just the question (zero-shot) or also given demonstrations of answers to
similar questions (few-shot). MMLU offers both zero-shot and few-shot prompt
options.
5This repurposes the word for types of exams in which students are allowed to ‘open their books’ or
not.

11.5 • E VALUATING QUESTION ANSWERING 249
MMLU examples
College Computer Science
Any set of Boolean operators that is sufﬁcient to represent all Boolean ex-
pressions is said to be complete. Which of the following is NOT complete?
(A) AND, NOT
(B) NOT, OR
(C) AND, OR
(D) NAND
College Physics
The primary source of the Sun’s energy is a series of thermonuclear
reactions in which the energy produced is c2times the mass difference
between
(A) two hydrogen atoms and one helium atom
(B)four hydrogen atoms and one helium atom
(C) six hydrogen atoms and two helium atoms
(D) three helium atoms and one carbon atom
International Law
Which of the following is a treaty-based human rights mechanism?
(A)The UN Human Rights Committee
(B) The UN Human Rights Council
(C) The UN Universal Periodic Review
(D) The UN special mandates
Prehistory
Unlike most other early civilizations, Minoan culture shows little evidence
of
(A) trade.
(B) warfare.
(C) the development of a common religion.
(D)conspicuous consumption by elites.
Figure 11.10 Example problems from MMLU
11.5 Evaluating Question Answering
Three techniques are commonly employed to evaluate question-answering systems,
with the choice depending on the type of question and QA situation. For multiple
choice questions like in MMLU, we report exact match:
Exact match : The % of predicted answers that match the gold answer
exactly.
For questions with free text answers, like Natural Questions, we commonly evalu-
ated with token F1score to roughly measure the partial string overlap between the
answer and the reference answer:
F1score : The average token overlap between predicted and gold an-
swers. Treat the prediction and gold as a bag of tokens, and compute F 1
for each question, then return the average F 1over all questions.

250 CHAPTER 11 • R ETRIEVAL -BASED MODELS
Finally, in some situations QA systems give multiple ranked answers. In such cases
we evaluated using mean reciprocal rank , orMRR (V oorhees, 1999). MRR ismean
reciprocal rank
MRR designed for systems that return a short ranked list of answers or passages for each
test set question, which we can compare against the (human-labeled) correct answer.
First, each test set question is scored with the reciprocal of the rank of the ﬁrst
correct answer. For example if the system returned ﬁve answers to a question but
the ﬁrst three are wrong (so the highest-ranked correct answer is ranked fourth), the
reciprocal rank for that question is1
4. The score for questions that return no correct
answer is 0. The MRR of a system is the average of the scores for each question in
the test set. In some versions of MRR, questions with a score of zero are ignored
in this calculation. More formally, for a system returning ranked answers to each
question in a test set Q, (or in the alternate version, let Qbe the subset of test set
questions that have non-zero scores). MRR is then deﬁned as
MRR =1
|Q||Q|∑
i=11
rank i(11.20)
11.6 Summary
This chapter introduced the tasks of question answering andinformation retrieval .
•Question answering (QA) is the task of answering a user’s questions.
• We focus in this chapter on the task of retrieval-based question answering,
in which the user’s questions are intended to be answered by the material in
some set of documents (which might be the web).
•Information Retrieval (IR) is the task of returning documents to a user based
on their information need as expressed in a query . In ranked retrieval, the
documents are returned in ranked order.
• The match between a query and a document can be done by ﬁrst representing
each of them with a sparse vector that represents the frequencies of words,
weighted by tf-idf orBM25 . Then the similarity can be measured by cosine.
• Documents or queries can instead be represented by dense vectors, by encod-
ing the question and document with an encoder-only model like BERT, and in
that case computing similarity in embedding space.
• The inverted index is a storage mechanism that makes it very efﬁcient to ﬁnd
documents that have a particular word.
• Ranked retrieval is generally evaluated by mean average precision orinter-
polated precision .
• Question answering systems generally use the retriever /reader architecture.
In the retriever stage, an IR system is given a query and returns a set of
documents.
• The reader stage is implemented by retrieval-augmented generation , in
which a large language model is prompted with the query and a set of doc-
uments and then conditionally generates a novel answer.
• QA can be evaluated by exact match with a known answer if only a single
answer is given, with token F 1score for free text answers, or with mean re-
ciprocal rank if a ranked set of answers is given.

HISTORICAL NOTES 251
Historical Notes
Question answering was one of the earliest NLP tasks. By 1961 the BASEBALL
system (Green et al., 1961) answered questions about baseball games like “Where
did the Red Sox play on July 7” by querying a structured database of game infor-
mation. The database was stored as a kind of attribute-value matrix with values for
attributes of each game:
Month = July
Place = Boston
Day = 7
Game Serial No. = 96
(Team = Red Sox, Score = 5)
(Team = Yankees, Score = 3)
Each question was constituency-parsed using the algorithm of Zellig Harris’s
TDAP project at the University of Pennsylvania, essentially a cascade of ﬁnite-state
transducers (see the historical discussion in Joshi and Hopely 1999 and Karttunen
1999). Then in a content analysis phase each word or phrase was associated with a
program that computed parts of its meaning. Thus the phrase ‘Where’ had code to
assign the semantics Place = ? , with the result that the question “Where did the
Red Sox play on July 7” was assigned the meaning
Place = ?
Team = Red Sox
Month = July
Day = 7
The question is then matched against the database to return the answer.
The Protosynthex system of Simmons et al. (1964), given a question, formed a
query from the content words in the question, and then retrieved candidate answer
sentences in the document, ranked by their frequency-weighted term overlap with
the question. The query and each retrieved sentence were then parsed with depen-
dency parsers, and the sentence whose structure best matches the question structure
selected. Thus the question What do worms eat? would match worms eat grass :
both have the subject worms as a dependent of eat, in the version of dependency
grammar used at the time, while birds eat worms hasbirds as the subject:
Whatdowormseat Wormseatgrass Birdseatworms
Simmons (1965) summarizes other early QA systems.
By the 1970s, systems used predicate calculus as the meaning representation
language. The LUNAR system (Woods et al. 1972, Woods 1978) was designed to LUNAR
be a natural language interface to a database of chemical facts about lunar geology. It
could answer questions like Do any samples have greater than 13 percent aluminum
by parsing them into a logical form
(TEST (FOR SOME X16 / (SEQ SAMPLES) : T ; (CONTAIN’ X16
(NPR* X17 / (QUOTE AL203)) (GREATERTHAN 13 PCT))))
By the 1990s question answering shifted to machine learning. Zelle and Mooney
(1996) proposed to treat question answering as a semantic parsing task, by creat-

252 CHAPTER 11 • R ETRIEVAL -BASED MODELS
ing the Prolog-based GEOQUERY dataset of questions about US geography. This
model was extended by Zettlemoyer and Collins (2005) and 2007. By a decade
later, neural models were applied to semantic parsing (Dong and Lapata 2016, Jia
and Liang 2016), and then to knowledge-based question answering by mapping text
to SQL (Iyer et al., 2017).
[TBD: History of IR.]
Meanwhile, a paradigm for answering questions that drew more on information-
retrieval was inﬂuenced by the rise of the web in the 1990s. The U.S. government-
sponsored TREC (Text REtrieval Conference) evaluations, run annually since 1992,
provide a testbed for evaluating information-retrieval tasks and techniques (V oorhees
and Harman, 2005). TREC added an inﬂuential QA track in 1999, which led to a
wide variety of factoid and non-factoid question answering systems competing in
annual evaluations.
At that same time, Hirschman et al. (1999) introduced the idea of using chil-
dren’s reading comprehension tests to evaluate machine text comprehension algo-
rithms. They acquired a corpus of 120 passages with 5 questions each designed for
3rd-6th grade children, built an answer extraction system, and measured how well
the answers given by their system corresponded to the answer key from the test’s
publisher. Their algorithm focused on word overlap as a feature; later algorithms
added named entity features and more complex similarity between the question and
the answer span (Riloff and Thelen 2000, Ng et al. 2000).
The DeepQA component of the Watson Jeopardy! system was a large and so-
phisticated feature-based system developed just before neural systems became com-
mon. It is described in a series of papers in volume 56 of the IBM Journal of Re-
search and Development, e.g., Ferrucci (2012).
Early neural reading comprehension systems drew on the insight common to
early systems that answer ﬁnding should focus on question-passage similarity. Many
of the architectural outlines of these neural systems were laid out in Hermann et al.
(2015), Chen et al. (2017a), and Seo et al. (2017). These systems focused on datasets
like Rajpurkar et al. (2016) and Rajpurkar et al. (2018) and their successors, usually
using separate IR algorithms as input to neural reading comprehension systems. The
paradigm of using dense retrieval with a span-based reader, often with a single end-
to-end architecture, is exempliﬁed by systems like Lee et al. (2019) or Karpukhin
et al. (2020). An important research area with dense retrieval for open-domain QA
is training data: using self-supervised methods to avoid having to label positive and
negative passages (Sachan et al., 2023).
Early work on large language models showed that they stored sufﬁcient knowl-
edge in the pretraining process to answer questions (Petroni et al., 2019; Raffel
et al., 2020; Radford et al., 2019; Roberts et al., 2020), at ﬁrst not competitively
with special-purpose question answerers, but quickly surpassing them. Retrieval-
augmented generation algorithms were ﬁrst introduced as a way to improve lan-
guage modeling word prediction (Khandelwal et al., 2019), but were quickly applied
to question answering (Izacard et al., 2022; Ram et al., 2023; Shi et al., 2023).
Exercises

