CHAPTER
4Logistic Regression and Text
Classiﬁcation
En sus remotas p ´aginas est ´a escrito que los animales se dividen en:
a. pertenecientes al Emperador h. incluidos en esta clasiﬁcaci ´on
b. embalsamados i. que se agitan como locos
c. amaestrados j. innumerables
d. lechones k. dibujados con un pincel ﬁn ´ısimo de pelo de camello
e. sirenas l. etc ´etera
f. fabulosos m. que acaban de romper el jarr ´on
g. perros sueltos n. que de lejos parecen moscas
Borges (1964)
Classiﬁcation lies at the heart of language processing and intelligence. Recog-
nizing a letter, a word, or a face, sorting mail, assigning grades to homeworks; these
are all examples of assigning a category to an input. The challenges of classiﬁcation
were famously highlighted by the fabulist Jorge Luis Borges (1964), who imagined
an ancient mythical encyclopedia that classiﬁed animals into:
(a) those that belong to the Emperor, (b) embalmed ones, (c) those that
are trained, (d) suckling pigs, (e) mermaids, (f) fabulous ones, (g) stray
dogs, (h) those that are included in this classiﬁcation, (i) those that
tremble as if they were mad, (j) innumerable ones, (k) those drawn with
a very ﬁne camel’s hair brush, (l) others, (m) those that have just broken
a ﬂower vase, (n) those that resemble ﬂies from a distance.
Luckily, the classes we use for language processing are easier to deﬁne than
those of Borges. In this chapter we introduce the logistic regression algorithm for
classiﬁcation, and apply it to text categorization , the task of assigning a label ortext
categorization
category to a text or document. We’ll focus on one text categorization task, senti-
ment analysis , the categorization of sentiment , the positive or negative orientationsentiment
analysis
that a writer expresses toward some object. A review of a movie, book, or product
expresses the author’s sentiment toward the product, while an editorial or political
text expresses sentiment toward an action or candidate. Extracting sentiment is thus
relevant for ﬁelds from marketing to politics.
For the binary task of labeling a text as indicating positive or negative stance,
words (like awesome andlove, orawful andridiculously are very informative, as we
can see from these sample extracts from movie/restaurant reviews:
+...awesome caramel sauce and sweet toasty almonds. I love this place!
−...awful pizza and ridiculously overpriced...
There are many text classiﬁcation tasks. In spam detection we assign an email spam detection
to one of the two classes spam ornot-spam .Language id is the task of determin- language id
ing what language a text is written in, while authorship attribution is the task ofauthorship
attribution
determining a text’s author, relevant to both humanistic and forensic analysis.

62 CHAPTER 4 • L OGISTIC REGRESSION
But what makes classiﬁcation so important is that language modeling can also
be viewed as classiﬁcation: each word can be thought of as a class, and so predicting
the next word is classifying the context-so-far into a class for each next word. As
we’ll see, this intuition underlies large language models.
The algorithm for classiﬁcation we introduce in this chapter, logistic regression,
is equally important, in a number of ways. First, logistic regression has a close
relationship with neural networks. As we will see in Chapter 6, a neural network
can be viewed as a series of logistic regression classiﬁers stacked on top of each
other. Second, logistic regression introduces ideas that are fundamental to neural
networks and language models, like the sigmoid andsoftmax functions, the logit , sigmoid
softmax
logitand the key gradient descent algorithm for learning. Finally, logistic regression is
also one of the most important analytic tools in the social and natural sciences.
4.1 Machine learning and classiﬁcation
The goal of classiﬁcation is to take a single input (we call each input an observa-
tion), extract some useful features or properties of the input, and thereby classify observation
the observation into one of a set of discrete classes. We’ll call the input x, and say
that the output comes from a ﬁxed set of output classes Y={y1,y2,...,yM}. Our goal
is return a predicted class ˆ y∈Y. The hatorcircumﬂex notation ˆ yis used to refer to hat
an estimated or predicted value. Sometimes you’ll see the output classes referred to
as the set Cinstead of Y.
For sentiment analysis, the input xmight be a review, or some other text. And
the output set Ymight be the set:
{positive,negative}
or the set
{0,1}
For language id, the input might be a text that we need to know what language it was
written in, and the output set Yis the set of languages, i.e.,
Y={Abkhaz,Ainu,Albanian,Amharic,...Zulu,Zu˜ni}
There are many ways to do classiﬁcation. One method is to use rules handwritten
by humans. For example, we might have a rule like:
If the word ‘‘love’’ appears in x,andit’snotprecededbythe
word‘‘don’t",classifyaspositive
Handwritten rules can be components of modern NLP systems, such as the hand-
written lists of positive and negative words that can be used in sentiment analysis,
as we’ll see below. But rules can be fragile, as situations or data change over time,
and for many tasks there are complex interactions between different features (like
the example of negation with “don’t” in the rule above), so it can be quite hard for
humans to come up with rules that are successful over many situations.
Another method that we will introduce later is to ask a large language model (of
the type we will introduce in Chapter 7) by prompting the model to give a label to
some text. Prompting can be powerful, but again has weaknesses: language models
often hallucinate, and may not be able to explain why they chose the class they did.
For these reasons the most common way to do classiﬁcation is to use super-
vised machine learning . Supervised machine learning is a paradigm in which, insupervised
machine
learning

4.1 • M ACHINE LEARNING AND CLASSIFICATION 63
addition to the input and the set of output classes, we have a labeled training set
and a learning algorithm . We talked about training sets in Chapter 3 as a locus for
computing n-gram statistics. But in supervised machine learning the training set is
labeled , meaning that it contains a set of input observations, each observation asso-
ciated with the correct output (a ‘supervision signal’). We can generally refer to a
training set of minput/output pairs, where each input xis a text, in the case of text
classiﬁcation, and each is hand-labeled with an associated class (the correct label):.
training set:{(x(1),y(1)),(x(2),y(2)),...,(x(m),y(m))} (4.1)
We’ll use superscripts in parentheses to refer to individual observations or instances
in the training set. So for sentiment classiﬁcation, a training set might be a set of
sentences or other texts, each with their correct sentiment label.
Our goal is to learn from this training set a classiﬁer that is capable of mapping
from a new input xto its correct class y∈Y. It does this by learn to ﬁnd features in
these training sentences (perhaps words like “awesome” or “awful”). Probabilistic
classiﬁers are the subset of machine learning classiﬁers that in addition to giving an
answer (which class is this observation in?), additionally will tell us the probability
of the observation being in the class. This full distribution over the classes can be
useful information for downstream decisions; avoiding making discrete decisions
early on can be useful when combining systems.
There are many algorithms for achieving this supervised machine learning task,
(naive Bayes, support vector machines, neural networks, ﬁne-tuned language mod-
els), but logistic regression has the advantages we discussed above and so we’ll
introduce it! Any machine learning classiﬁer thus has four components:
1. A feature representation of the input. For each input observation x(i), this
will be a vector of features [x1,x2,...,xn]. We will generally refer to feature
ifor input x(j)asx(j)
i, sometimes simpliﬁed as xi, but we will also see the
notation fi,fi(x), or, for multiclass classiﬁcation, fi(c,x).
2. A classiﬁcation function that computes ˆ y, the estimated class, via P(y|x). We
will introduce the sigmoid andsoftmax tools for classiﬁcation.
3. An objective function that we want to optimize for learning, usually involv-
ing minimizing a loss function corresponding to error on training examples.
We will introduce the cross-entropy loss function .
4. An algorithm for optimizing the objective function. We introduce the stochas-
tic gradient descent algorithm.
At the highest level, logistic regression, and really any supervised machine learn-
ing classiﬁer, has two phases
training: We train the system (in the case of logistic regression that means train-
ing the weights wandb, introduced below) using stochastic gradient descent
and the cross-entropy loss.
test: Given a test example xwe compute the probability P(yi|x)of each class yi,
and return the higher probability label y=1 ory=0.
Logistic regression can be used to classify an observation into one of two classes
(like ‘positive sentiment’ and ‘negative sentiment’), or into one of many classes.
Because the mathematics for the two-class case is simpler, we’ll ﬁrst describe this
special case of logistic regression in the next few sections, beginning with the sig-
moid function, and then turn to multinomial logistic regression for more than two
classes and the use of the softmax function in Section 4.4.

64 CHAPTER 4 • L OGISTIC REGRESSION
4.2 The sigmoid function
The goal of binary logistic regression is to train a classiﬁer that can make a binary
decision about the class of a new input observation. Here we introduce the sigmoid
classiﬁer that will help us make this decision.
Consider a single input observation x, which we will represent by a vector of
features [x1,x2,...,xn]. (We’ll show sample features in the next subsection.) The
classiﬁer output ycan be 1 (meaning the observation is a member of the class) or
0 (the observation is not a member of the class). We want to know the probability
P(y=1|x)that this observation is a member of the class. So perhaps the decision
is “positive sentiment” versus “negative sentiment”, the features represent counts of
words in a document, P(y=1|x)is the probability that the document has positive
sentiment, and P(y=0|x)is the probability that the document has negative senti-
ment.
Logistic regression solves this task by learning, from a training set, a vector of
weights and a bias term . Each weight wiis a real number, and is associated with one
of the input features xi. The weight wirepresents how important that input feature
is to the classiﬁcation decision, and can be positive (providing evidence that the in-
stance being classiﬁed belongs in the positive class) or negative (providing evidence
that the instance being classiﬁed belongs in the negative class). Thus we might
expect in a sentiment task the word awesome to have a high positive weight, and
abysmal to have a very negative weight. The bias term , also called the intercept , is bias term
intercept another real number that’s added to the weighted inputs.
To make a decision on a test instance—after we’ve learned the weights in training—
the classiﬁer ﬁrst multiplies each xiby its weight wi, sums up the weighted features,
and adds the bias term b. The resulting single number zexpresses the weighted sum
of the evidence for the class.
z=(n∑
i=1wixi)
+b (4.2)
In the rest of the book we’ll represent such sums using the dot product notation dot product
from linear algebra. The dot product of two vectors aandb, written as a·b, is the
sum of the products of the corresponding elements of each vector. (Notice that we
represent vectors using the boldface notation b). Thus the following is an equivalent
formation to Eq. 4.2:
z=w·x+b (4.3)
But note that nothing in Eq. 4.3 forces zto be a legal probability, that is, to lie
between 0 and 1. In fact, since weights are real-valued, the output might even be
negative; zranges from−∞to∞.
To create a probability, we’ll pass zthrough the sigmoid function, σ(z). The sigmoid
sigmoid function (named because it looks like an s) is also called the logistic func-
tion, and gives logistic regression its name. The sigmoid has the following equation,logistic
function
shown graphically in Fig. 4.1:
σ(z) =1
1+e−z=1
1+exp(−z)(4.4)
(For the rest of the book, we’ll use the notation exp (x)to mean ex.) The sigmoid
has a number of advantages; it takes a real-valued number and maps it into the range

4.3 • C LASSIFICATION WITH LOGISTIC REGRESSION 65
Figure 4.1 The sigmoid function σ(z) =1
1+e−ztakes a real value and maps it to the range
(0,1). It is nearly linear around 0 but outlier values get squashed toward 0 or 1.
(0,1), which is just what we want for a probability. Because it is nearly linear around
0 but ﬂattens toward the ends, it tends to squash outlier values toward 0 or 1. And
it’s differentiable, which as we’ll see in Section 4.15 will be handy for learning.
We’re almost there. If we apply the sigmoid to the sum of the weighted features,
we get a number between 0 and 1. To make it a probability, we just need to make
sure that the two cases, P(y=1)andP(y=0), sum to 1. We can do this as follows:
P(y=1) = σ(w·x+b)
=1
1+exp(−(w·x+b))
P(y=0) = 1−σ(w·x+b)
=1−1
1+exp(−(w·x+b))
=exp(−(w·x+b))
1+exp(−(w·x+b))(4.5)
The sigmoid function has the property
1−σ(x) =σ(−x) (4.6)
so we could also have expressed P(y=0)asσ(−(w·x+b)).
Finally, one terminological point. The input to the sigmoid function, the score
z=w·x+bfrom Eq. 4.3, is often called the logit . This is because the logit function logit
is the inverse of the sigmoid. The logit function is the log of the odds ratiop
1−p:
logit(p) =σ−1(p) =lnp
1−p(4.7)
Using the term logit forzis a way of reminding us that by using the sigmoid to turn
z(which ranges from −∞to∞) into a probability, we are implicitly interpreting zas
not just any real-valued number, but as speciﬁcally a log odds.
4.3 Classiﬁcation with Logistic Regression
The sigmoid function from the prior section thus gives us a way to take an instance
xand compute the probability P(y=1|x).
How do we make a decision about which class to apply to a test instance x? For
a given x, we say yes if the probability P(y=1|x)is more than .5, and no otherwise.
We call .5 the decision boundary :decision
boundary

66 CHAPTER 4 • L OGISTIC REGRESSION
decision (x) ={1 if P(y=1|x)>0.5
0 otherwise
Let’s have some examples of applying logistic regression as a classiﬁer for language
tasks.
4.3.1 Sentiment Classiﬁcation
Suppose we are doing binary sentiment classiﬁcation on movie review text, and
we would like to know whether to assign the sentiment class +or−to a review
document doc. We’ll represent each input observation by the 6 features x1...x6of
the input shown in the following table; Fig. 4.2 shows features in a sample mini test
document.
Var Deﬁnition Value in Fig. 4.2
x1 count(positive lexicon words ∈doc) 3
x2 count(negative lexicon words ∈doc) 2
x3{1 if “no”∈doc
0 otherwise1
x4 count (1st and 2nd pronouns ∈doc) 3
x5{
1 if “!”∈doc
0 otherwise0
x6 ln(word+punctuation count of doc ) ln(66) =4.19
 It's hokey . There are virtually no surprises , and the writing is second-rate . So why was it so enjoyable  ? For one thing , the cast is great . Another nice touch is the music . I was overcome with the urge to get off the couch and start dancing .  It sucked me in , and it'll do the same to you  .x1=3x6=4.19x3=1x4=3x5=0x2=2
Figure 4.2 A sample mini test document showing the extracted features in the vector x.
Let’s assume for the moment that we’ve already learned a real-valued weight
for each of these features, and that the 6 weights corresponding to the 6 features
are[2.5,−5.0,−1.2,0.5,2.0,0.7], while b= 0.1. (We’ll discuss in the next section
how the weights are learned.) The weight w1, for example indicates how important
a feature the number of positive lexicon words ( great ,nice,enjoyable , etc.) is to
a positive sentiment decision, while w2tells us the importance of negative lexicon
words. Note that w1=2.5 is positive, while w2=−5.0, meaning that negative words
are negatively associated with a positive sentiment decision, and are about twice as
important as positive words.
Given these 6 features and the input review x,P(+|x)andP(−|x)can be com-

4.3 • C LASSIFICATION WITH LOGISTIC REGRESSION 67
puted using Eq. 4.5:
P(+|x) =P(y=1|x) = σ(w·x+b)
=σ([2.5,−5.0,−1.2,0.5,2.0,0.7]·[3,2,1,3,0,4.19]+0.1)
=σ(.833)
=0.70 (4.8)
P(−|x) =P(y=0|x) = 1−σ(w·x+b)
=0.30
4.3.2 Other classiﬁcation tasks and features
Logistic regression is applied to all sorts of NLP tasks, and any property of the input
can be a feature. Consider the task of period disambiguation : deciding if a periodperiod
disambiguation
is the end of a sentence or part of a word, by classifying each period into one of two
classes, EOS (end-of-sentence) and not-EOS. We might use features like x1below
expressing that the current word is lower case, perhaps with a positive weight. Or a
feature expressing that the current word is in our abbreviations dictionary (“Prof.”),
perhaps with a negative weight. A feature can also express a combination of proper-
ties. For example a period following an upper case word is likely to be an EOS, but
if the word itself is St.and the previous word is capitalized then the period is likely
part of a shortening of the word street following a street name.
x1={1 if “ Case(wi) =Lower”
0 otherwise
x2={1 if “ wi∈AcronymDict”
0 otherwise
x3={1 if “ wi=St. & Case(wi−1) =Upper”
0 otherwise
Designing versus learning features: In classic models, features are designed by
hand by examining the training set with an eye to linguistic intuitions and literature,
supplemented by insights from error analysis on the training set of an early version
of a system. We can also consider feature interactions , complex features that arefeature
interactions
combinations of more primitive features. We saw such a feature for period disam-
biguation above, where a period on the word St.was less likely to be the end of the
sentence if the previous word was capitalized. Features can be created automatically
viafeature templates , abstract speciﬁcations of features. For example a bigramfeature
templates
template for period disambiguation might create a feature for every pair of words
that occurs before a period in the training set. Thus the feature space is sparse, since
we only have to create a feature if that n-gram exists in that position in the training
set. The feature is generally created as a hash from the string descriptions. A user
description of a feature as, “bigram(American breakfast)” is hashed into a unique
integer ithat becomes the feature number fi.
It should be clear from the prior paragraph that designing features by hand re-
quires extensive human effort. For this reason, recent NLP systems avoid hand-
designed features and instead focus on representation learning : ways to learn fea-
tures automatically in an unsupervised way from the input. We’ll introduce methods
for representation learning in Chapter 5 and Chapter 6.
Scaling input features: When different input features have extremely different
ranges of values, it’s common to rescale them so they have comparable ranges. We

68 CHAPTER 4 • L OGISTIC REGRESSION
standardize input values by centering them to result in a zero mean and a standard standardize
deviation of one (this transformation is sometimes called the z-score ). That is, if µiz-score
is the mean of the values of feature xiacross the mobservations in the input dataset,
andσiis the standard deviation of the values of features xiacross the input dataset,
we can replace each feature xiby a new feature x′
icomputed as follows:
µi=1
mm∑
j=1x(j)
i σi=√1
mm∑
j=1(
x(j)
i−µi)2
x′
i=xi−µi
σi(4.9)
Alternatively, we can normalize the input features values to lie between 0 and 1: normalize
x′
i=xi−min(xi)
max(xi)−min(xi)(4.10)
Having input data with comparable range is useful when comparing values across
features. Data scaling is especially important in large neural networks, since it helps
speed up gradient descent.
4.3.3 Processing many examples at once
We’ve shown the equations for logistic regression for a single example. But in prac-
tice we’ll of course want to process an entire test set with many examples. Let’s
suppose we have a test set consisting of mtest examples each of which we’d like
to classify. We’ll continue to use the notation from page 63, in which a superscript
value in parentheses refers to the example index in some set of data (either for train-
ing or for test). So in this case each test example x(i)has a feature vector x(i),
1≤i≤m. (As usual, we’ll represent vectors and matrices in bold.)
One way to compute each output value ˆ y(i)is just to have a for-loop, and compute
each test example one at a time:
foreach x(i)in input [x(1),x(2),...,x(m)]
y(i)=σ(w·x(i)+b) (4.11)
For the ﬁrst 3 test examples, then, we would be separately computing the pre-
dicted ˆ y(i)as follows:
P(y(1)=1|x(1)) = σ(w·x(1)+b)
P(y(2)=1|x(2)) = σ(w·x(2)+b)
P(y(3)=1|x(3)) = σ(w·x(3)+b)
But it turns out that we can slightly modify our original equation Eq. 4.5 to do
this much more efﬁciently. We’ll use matrix arithmetic to assign a class to all the
examples with one matrix operation!
First, we’ll pack all the input feature vectors for each input xinto a single input
matrix X, where each row iis a row vector consisting of the feature vector for in-
put example x(i)(i.e., the vector x(i)). Assuming each example has ffeatures and

4.4 • M ULTINOMIAL LOGISTIC REGRESSION 69
weights, Xwill therefore be a matrix of shape [m×f], as follows:
X=
x(1)
1x(1)
2...x(1)
f
x(2)
1x(2)
2...x(2)
f
x(3)
1x(3)
2...x(3)
f
...
(4.12)
Now if we introduce bas a vector of length mwhich consists of the scalar bias
term brepeated mtimes, b= [b,b,...,b], and ˆ y= [ˆy(1),ˆy(2)...,ˆy(m)]as the vector of
outputs (one scalar ˆ y(i)for each input x(i)and its feature vector x(i)), and represent
the weight vector was a column vector, we can compute all the outputs with a single
matrix multiplication and one addition:
ˆy=σ(Xw+b) (4.13)
You should convince yourself that Eq. 4.13 computes the same thing as our for-loop
in Eq. 4.11. For example ˆ y(1), the ﬁrst entry of the output vector y, will correctly be:
ˆy(1)= [x(1)
1,x(1)
2,...,x(1)
f]·[w1,w2,...,wf]+b (4.14)
Note that we had to reorder Xandwfrom the order they appeared in in Eq. 4.5 to
make the multiplications come out properly. Here is Eq. 4.13 again with the shapes
shown:
ˆy=σ(X w +b)
(m×1) ( m×f) (f×1) (m×1) (4.15)
Modern compilers and compute hardware can compute this matrix operation very
efﬁciently, making the computation much faster, which becomes important when
training or testing on very large datasets.
Note by the way that we could have kept Xandwin the original order (as
ˆy=σ(wX+b)) if we had chosen to deﬁne Xdifferently as a matrix of column
vectors, one vector for each input example, instead of row vectors, and then it would
have shape [f×m]. But we conventionally represent inputs as rows.
4.4 Multinomial logistic regression
Sometimes we need more than two classes. Perhaps we might want to do 3-way
sentiment classiﬁcation (positive, negative, or neutral). Or we could be assigning
some of the labels we will introduce in Chapter 17, like the part of speech of a word
(choosing from 10, 30, or even 50 different parts of speech), or the named entity
type of a phrase (choosing from tags like person, location, organization). Or, for
large language models, we’ll be predicting the next word out of the |V|possible
words in the vocabulary, so it’s |V|-way classiﬁcation.
In such cases we use multinomial logistic regression , also called softmax re-multinomial
logistic
regressiongression (in older NLP literature you will sometimes see the name maxent classi-
ﬁer). In multinomial logistic regression we want to label each observation with a
class kfrom a set of Kclasses, under the stipulation that only one of these classes is
the correct one (sometimes called hard classiﬁcation ; an observation can not be in

70 CHAPTER 4 • L OGISTIC REGRESSION
multiple classes). Let’s use the following representation: the output yfor each input
xwill be a vector of length K. If class cis the correct class, we’ll set yc=1, and
set all the other elements of yto be 0, i.e., yc=1 and yj=0∀j̸=c. A vector like
thisy, with one value=1 and the rest 0, is called a one-hot vector . The job of the
classiﬁer is to produce an estimate vector ˆ y. For each class k, the value ˆ ykwill be
the classiﬁer’s estimate of the probability P(yk=1|x).
4.4.1 Softmax
The multinomial logistic classiﬁer uses a generalization of the sigmoid, called the
softmax function, to compute p(yk=1|x). The softmax function takes a vector softmax
z= [z1,z2,...,zK]ofKarbitrary values and maps them to a probability distribution,
with each value in the range [0,1], and all the values summing to 1. Like the sigmoid,
it is an exponential function.
For a vector zof dimensionality K, the softmax is deﬁned as:
softmax (zi) =exp(zi)∑K
j=1exp(zj)1≤i≤K (4.16)
The softmax of an input vector z= [z1,z2,...,zK]is thus a vector itself:
softmax (z) =[
exp(z1)∑K
i=1exp(zi),exp(z2)∑K
i=1exp(zi),...,exp(zK)∑K
i=1exp(zi)]
(4.17)
The denominator∑K
i=1exp(zi)is used to normalize all the values into probabilities.
Thus for example given a vector:
z= [0.6,1.1,−1.5,1.2,3.2,−1.1]
the resulting (rounded) softmax( z) is
[0.05,0.09,0.01,0.1,0.74,0.01]
Like the sigmoid, the softmax has the property of squashing values toward 0 or 1.
Thus if one of the inputs is larger than the others, it will tend to push its probability
toward 1, and suppress the probabilities of the smaller inputs.
Finally, note that, just as for the sigmoid, we refer to z, the vector of scores that
is the input to the softmax, as logits (see Eq. 4.7).
4.4.2 Applying softmax in logistic regression
When we apply softmax for logistic regression, the input will (just as for the sig-
moid) be the dot product between a weight vector wand an input vector x(plus a
bias). But now we’ll need separate weight vectors wkand bias bkfor each of the K
classes. The probability of each of our output classes ˆ ykcan thus be computed as:
P(yk=1|x) =exp(wk·x+bk)
K∑
j=1exp(wj·x+bj)(4.18)
The form of Eq. 4.18 makes it seem that we would compute each output sep-
arately. Instead, it’s more common to set up the equation for more efﬁcient com-
putation by modern vector processing hardware. We’ll do this by representing the

4.4 • M ULTINOMIAL LOGISTIC REGRESSION 71
set of Kweight vectors as a weight matrix Wand a bias vector b. Each row kof
Wcorresponds to the vector of weights wk.Wthus has shape [K×f], for Kthe
number of output classes and fthe number of input features. The bias vector bhas
one value for each of the Koutput classes. If we represent the weights in this way,
we can compute ˆy, the vector of output probabilities for each of the Kclasses, by a
single elegant equation:
ˆy=softmax (Wx+b) (4.19)
If you work out the matrix arithmetic, you can see that the estimated score of
the ﬁrst output class ˆ y1(before we take the softmax) will correctly turn out to be
w1·x+b1.
One helpful interpretation of the weight matrix Wis to see each row wkas a
prototype of class k. The weight vector wkthat is learned represents the class as prototype
a kind of template. Since two vectors that are more similar to each other have a
higher dot product with each other, the dot product acts as a similarity function.
Logistic regression is thus learning an exemplar representation for each class, such
that incoming vectors are assigned the class kthey are most similar to from the K
classes (Doumbouya et al., 2025).
Fig. 4.3 shows the difference between binary and multinomial logistic regression
by illustrating the weight vector versus weight matrix in the computation of the
output class probabilities.
4.4.3 Features in Multinomial Logistic Regression
Features in multinomial logistic regression act like features in binary logistic regres-
sion, with the difference mentioned above that we’ll need separate weight vectors
and biases for each of the Kclasses. Recall our binary exclamation point feature x5
from page 66:
x5={1 if “!”∈doc
0 otherwise
In binary classiﬁcation a positive weight w5on a feature inﬂuences the classiﬁer
toward y=1 (positive sentiment) and a negative weight inﬂuences it toward y=0
(negative sentiment) with the absolute value indicating how important the feature
is. For multinomial logistic regression, by contrast, with separate weights for each
class, a feature can be evidence for or against each individual class.
In 3-way multiclass sentiment classiﬁcation, for example, we must assign each
document one of the 3 classes +,−, or 0 (neutral). Now a feature related to excla-
mation marks might have a negative weight for 0 documents, and a positive weight
for+or−documents:
Feature Deﬁnition w5,+w5,−w5,0
f5(x){1 if “!”∈doc
0 otherwise3.5 3.1−5.3
Because these feature weights are dependent both on the input text and the output
class, we sometimes make this dependence explicit and represent the features them-
selves as f(x,y): a function of both the input and the class. Using such a notation
f5(x)above could be represented as three features f5(x,+),f5(x,−), and f5(x,0),
each of which has a single weight. We’ll use this kind of notation in our description
of the CRF in Chapter 17.

72 CHAPTER 4 • L OGISTIC REGRESSION
Binary Logistic Regression
w[f ⨉1]Outputsigmoid[1⨉f]Input wordsp(+) = 1- p(-)…y^xyInput featurevector [scalar]positive lexiconwords = 1count of “no” = 0wordcount=3x1x2x3xfdessert   was    greatWeight vector
Multinomial Logistic Regression
W[f⨉1]Outputsoftmax[K⨉f]Input wordsp(+)…y1^y2^y3^xyInput featurevector [K⨉1]positive lexiconwords = 1count of “no” = 0wordcount=3x1x2x3xfdessert   was    greatp(-)p(neut)Weight matrixThese f red weightsare a row of W correspondingto weight vector w3,(= weights for class 3,= a prototype of class 3)
Figure 4.3 Binary versus multinomial logistic regression. Binary logistic regression uses a
single weight vector w, and has a scalar output ˆ y. In multinomial logistic regression we have
Kseparate weight vectors corresponding to the Kclasses, all packed into a single weight
matrix W, and a vector output ˆy. We omit the biases from both ﬁgures for clarity.
4.5 Learning in Logistic Regression
How are the parameters of the model, the weights wand bias b, learned? Logistic
regression is an instance of supervised classiﬁcation in which we know the correct
label y(either 0 or 1) for each observation x. What the system produces via Eq. 4.5
is ˆy, the system’s estimate of the true y. We want to learn parameters (meaning w
andb) that make ˆ yfor each training observation as close as possible to the true y.
This requires two components that we foreshadowed in the introduction to the
chapter. The ﬁrst is a metric for how close the current label ( ˆ y) is to the true gold
label y. Rather than measure similarity, we usually talk about the opposite of this:
thedistance between the system output and the gold output, and we call this distance
thelossfunction or the cost function . In the next section we’ll introduce the loss loss
function that is commonly used for logistic regression and also for neural networks,

4.6 • T HE CROSS -ENTROPY LOSS FUNCTION 73
thecross-entropy loss .
The second thing we need is an optimization algorithm for iteratively updating
the weights so as to minimize this loss function. The standard algorithm for this is
gradient descent ; we’ll introduce the stochastic gradient descent algorithm in the
following section.
We’ll describe these algorithms for the simpler case of binary logistic regres-
sion in the next two sections, and then turn to multinomial logistic regression in
Section 4.8.
4.6 The cross-entropy loss function
We need a loss function that expresses, for an observation x, how close the classiﬁer
output ( ˆ y=σ(w·x+b)) is to the correct output ( y, which is 0 or 1). We’ll call this:
L(ˆy,y) = How much ˆ ydiffers from the true y (4.20)
We do this via a loss function that prefers the correct class labels of the train-
ing examples to be more likely . This is called conditional maximum likelihood
estimation : we choose the parameters w,bthatmaximize the log probability of
the true ylabels in the training data given the observations x. The resulting loss
function is the negative log likelihood loss , generally called the cross-entropy loss .cross-entropy
loss
Let’s derive this loss function, applied to a single observation x. We’d like to
learn weights that maximize the probability of the correct label p(y|x). Since there
are only two discrete outcomes (1 or 0), this is a Bernoulli distribution, and we can
express the probability p(y|x)that our classiﬁer produces for one observation as the
following (keeping in mind that if y=1, Eq. 4.21 simpliﬁes to ˆ y; ify=0, Eq. 4.21
simpliﬁes to 1−ˆy):
p(y|x) = ˆyy(1−ˆy)1−y(4.21)
Now we take the log of both sides. This will turn out to be handy mathematically,
and doesn’t hurt us; whatever values maximize a probability will also maximize the
log of the probability:
logp(y|x) = log[
ˆyy(1−ˆy)1−y]
=ylog ˆy+(1−y)log(1−ˆy) (4.22)
Eq. 4.22 describes a log likelihood that should be maximized. In order to turn this
into a loss function (something that we need to minimize), we’ll just ﬂip the sign on
Eq. 4.22. The result is the cross-entropy loss LCE:
LCE(ˆy,y) =−logp(y|x) =−[ylog ˆy+(1−y)log(1−ˆy)] (4.23)
Finally, we can plug in the deﬁnition of ˆ y=σ(w·x+b):
LCE(ˆy,y) =−[ylogσ(w·x+b)+(1−y)log(1−σ(w·x+b))] (4.24)
Let’s see if this loss function does the right thing for our example from Fig. 4.2. We
want the loss to be smaller if the model’s estimate is close to correct, and bigger if
the model is confused. So ﬁrst let’s suppose the correct gold label for the sentiment
example in Fig. 4.2 is positive, i.e., y=1. In this case our model is doing well, since

74 CHAPTER 4 • L OGISTIC REGRESSION
from Eq. 4.8 it indeed gave the example a higher probability of being positive (.70)
than negative (.30). If we plug σ(w·x+b) =.70 and y=1 into Eq. 4.24, the right
side of the equation drops out, leading to the following loss (we’ll use log to mean
natural log when the base is not speciﬁed):
LCE(ˆy,y) =−[ylogσ(w·x+b)+(1−y)log(1−σ(w·x+b))]
=−[logσ(w·x+b)]
=−log(.70)
= .36
By contrast, let’s pretend instead that the example in Fig. 4.2 was actually negative,
i.e., y=0 (perhaps the reviewer went on to say “But bottom line, the movie is
terrible! I beg you not to see it!”). In this case our model is confused and we’d want
the loss to be higher. Now if we plug y=0 and 1−σ(w·x+b) =.30 from Eq. 4.8
into Eq. 4.24, the left side of the equation drops out:
LCE(ˆy,y) =−[ylogσ(w·x+b)+(1−y)log(1−σ(w·x+b))]
= −[log(1−σ(w·x+b))]
= −log(.30)
= 1.2
Sure enough, the loss for the ﬁrst classiﬁer (.36) is less than the loss for the second
classiﬁer (1.2).
Why does minimizing this negative log probability do what we want? A perfect
classiﬁer would assign probability 1 to the correct outcome ( y=1 or y=0) and
probability 0 to the incorrect outcome. That means if yequals 1, the higher ˆ yis (the
closer it is to 1), the better the classiﬁer; the lower ˆ yis (the closer it is to 0), the
worse the classiﬁer. If yequals 0, instead, the higher 1 −ˆyis (closer to 1), the better
the classiﬁer. The negative log of ˆ y(if the true yequals 1) or 1−ˆy(if the true y
equals 0) is a convenient loss metric since it goes from 0 (negative log of 1, no loss)
to inﬁnity (negative log of 0, inﬁnite loss). This loss function also ensures that as
the probability of the correct answer is maximized, the probability of the incorrect
answer is minimized; since the two sum to one, any increase in the probability of the
correct answer is coming at the expense of the incorrect answer. It’s called the cross-
entropy loss, because Eq. 4.22 is also the formula for the cross-entropy between the
true probability distribution yand our estimated distribution ˆ y.
Now we know what we want to minimize; in the next section, we’ll see how to
ﬁnd the minimum.
4.7 Gradient Descent
Our goal with gradient descent is to ﬁnd the optimal weights: minimize the loss
function we’ve deﬁned for the model. In Eq. 4.25 below, we’ll explicitly represent
the fact that the cross-entropy loss function LCEis parameterized by the weights. In
machine learning in general we refer to the parameters being learned as θ; in the
case of logistic regression θ={w,b}. So the goal is to ﬁnd the set of weights which
minimizes the loss function, averaged over all examples:
ˆθ=argmin
θ1
mm∑
i=1LCE(f(x(i);θ),y(i)) (4.25)

4.7 • G RADIENT DESCENT 75
How shall we ﬁnd the minimum of this (or any) loss function? Gradient descent is
a method that ﬁnds a minimum of a function by ﬁguring out in which direction (in
the space of the parameters θ) the function’s slope is rising the most steeply, and
moving in the opposite direction. The intuition is that if you are hiking in a canyon
and trying to descend most quickly down to the river at the bottom, you might look
around yourself in all directions, ﬁnd the direction where the ground is sloping the
steepest, and walk downhill in that direction.
For logistic regression, this loss function is conveniently convex . A convex func- convex
tion has at most one minimum; there are no local minima to get stuck in, so gradient
descent starting from any point is guaranteed to ﬁnd the minimum. (By contrast,
the loss for multi-layer neural networks is non-convex, and gradient descent may
get stuck in local minima for neural network training and never ﬁnd the global opti-
mum.)
Although the algorithm (and the concept of gradient) are designed for direction
vectors , let’s ﬁrst consider a visualization of the case where the parameter of our
system is just a single scalar w, shown in Fig. 4.4.
Given a random initialization of wat some value w1, and assuming the loss
function Lhappened to have the shape in Fig. 4.4, we need the algorithm to tell us
whether at the next iteration we should move left (making w2smaller than w1) or
right (making w2bigger than w1) to reach the minimum.
wLoss
0w1wminslope of loss at w1 is negative(goal)one stepof gradientdescent
Figure 4.4 The ﬁrst step in iteratively ﬁnding the minimum of this loss function, by moving
win the reverse direction from the slope of the function. Since the slope is negative, we need
to move win a positive direction, to the right. Here superscripts are used for learning steps,
sow1means the initial value of w(which is 0), w2the value at the second step, and so on.
The gradient descent algorithm answers this question by ﬁnding the gradient gradient
of the loss function at the current point and moving in the opposite direction. The
gradient of a function of many variables is a vector pointing in the direction of the
greatest increase in a function. The gradient is a multi-variable generalization of the
slope, so for a function of one variable like the one in Fig. 4.4, we can informally
think of the gradient as the slope. The dotted line in Fig. 4.4 shows the slope of this
hypothetical loss function at point w=w1. You can see that the slope of this dotted
line is negative. Thus to ﬁnd the minimum, gradient descent tells us to go in the
opposite direction: moving win a positive direction.
The magnitude of the amount to move in gradient descent is the value of the
sloped
dwL(f(x;w),y)weighted by a learning rate η. A higher (faster) learning learning rate

76 CHAPTER 4 • L OGISTIC REGRESSION
rate means that we should move wmore on each step. The change we make in our
parameter is the learning rate times the gradient (or the slope, in our single-variable
example):
wt+1=wt−ηd
dwL(f(x;w),y) (4.26)
Now let’s extend the intuition from a function of one scalar variable wto many
variables, because we don’t just want to move left or right, we want to know where
in the N-dimensional space (of the Nparameters that make up θ) we should move.
The gradient is just such a vector; it expresses the directional components of the
sharpest slope along each of those Ndimensions. If we’re just imagining two weight
dimensions (say for one weight wand one bias b), the gradient might be a vector with
two orthogonal components, each of which tells us how much the ground slopes in
thewdimension and in the bdimension. Fig. 4.5 shows a visualization of the value
of a 2-dimensional gradient vector taken at the red point.
In an actual logistic regression, the parameter vector wis much longer than 1 or
2, since the input feature vector xcan be quite long, and we need a weight wifor
each xi. For each dimension/variable wiinw(plus the bias b), the gradient will have
a component that tells us the slope with respect to that variable. In each dimension
wi, we express the slope as a partial derivative∂
∂wiof the loss function. Essentially
we’re asking: “How much would a small change in that variable wiinﬂuence the
total loss function L?”
Formally, then, the gradient of a multi-variable function fis a vector in which
each component expresses the partial derivative of fwith respect to one of the vari-
ables. We’ll use the inverted Greek delta symbol ∇to refer to the gradient, and
represent ˆ yasf(x;θ)to make the dependence on θmore obvious:
∇L(f(x;θ),y) =
∂
∂w1L(f(x;θ),y)
∂
∂w2L(f(x;θ),y)
...
∂
∂wnL(f(x;θ),y)
∂
∂bL(f(x;θ),y)
(4.27)
The ﬁnal equation for updating θbased on the gradient is thus
θt+1=θt−η∇L(f(x;θ),y) (4.28)
Cost(w,b)
wb
Figure 4.5 Visualization of the gradient vector at the red point in two dimensions wand
b, showing a red arrow in the x-y plane pointing in the direction we will go to look for the
minimum: the opposite direction of the gradient (recall that the gradient points in the direction
of increase not decrease).

4.7 • G RADIENT DESCENT 77
4.7.1 The Gradient for Logistic Regression
In order to update θ, we need a deﬁnition for the gradient ∇L(f(x;θ),y). Recall that
for logistic regression, the cross-entropy loss function is:
LCE(ˆy,y) =−[ylogσ(w·x+b)+(1−y)log(1−σ(w·x+b))] (4.29)
It turns out that the derivative of this function for one observation vector xis Eq. 4.30
(the interested reader can see Section 4.15 for the derivation of this equation):
∂LCE(ˆy,y)
∂wj= [σ(w·x+b)−y]xj
= ( ˆy−y)xj (4.30)
You’ll also sometimes see this equation in the equivalent form:
∂LCE(ˆy,y)
∂wj=−(y−ˆy)xj (4.31)
Note in these equations that the gradient with respect to a single weight wjrep-
resents a very intuitive value: the difference between the true yand our estimated
ˆy=σ(w·x+b)for that observation, multiplied by the corresponding input value
xj.
4.7.2 The Stochastic Gradient Descent Algorithm
Stochastic gradient descent is an online algorithm that minimizes the loss function
by computing its gradient after each training example, and nudging θin the right
direction (the opposite direction of the gradient). (An “online algorithm” is one that
processes its input example by example, rather than waiting until it sees the entire
input.) Stochastic gradient descent is called stochastic because it chooses a single
random example at a time; in Section 4.7.4 we’ll discuss other versions of gradient
descent that batch many examples at once. Fig. 4.6 shows the algorithm.
The learning rate ηis ahyperparameter that must be adjusted. If it’s too high, hyperparameter
the learner will take steps that are too large, overshooting the minimum of the loss
function. If it’s too low, the learner will take steps that are too small, and take too
long to get to the minimum. It is common to start with a higher learning rate and then
slowly decrease it, so that it is a function of the iteration kof training; the notation
ηkcan be used to mean the value of the learning rate at iteration k.
We’ll discuss hyperparameters in more detail in Chapter 6, but in short, they are
a special kind of parameter for any machine learning model. Unlike regular param-
eters of a model (weights like wandb), which are learned by the algorithm from
the training set, hyperparameters are special parameters chosen by the algorithm
designer that affect how the algorithm works.
4.7.3 Working through an example
Let’s walk through a single step of the gradient descent algorithm. We’ll use a
simpliﬁed version of the example in Fig. 4.2 as it sees a single observation x, whose
correct value is y=1 (this is a positive review), and with a feature vector x= [x1,x2]
consisting of these two features:
x1=3 (count of positive lexicon words)
x2=2 (count of negative lexicon words)

78 CHAPTER 4 • L OGISTIC REGRESSION
function STOCHASTIC GRADIENT DESCENT (L(),f(),x,y)returns θ
# where: L is the loss function
# f is a function parameterized by θ
# x is the set of training inputs x(1),x(2),...,x(m)
# y is the set of training outputs (labels) y(1),y(2),...,y(m)
θ←0 # (or small random values)
repeat til done # see caption
For each training tuple (x(i),y(i))(in random order)
1. Optional (for reporting): # How are we doing on this tuple?
Compute ˆ y(i)=f(x(i);θ)# What is our estimated output ˆ y?
Compute the loss L(ˆy(i),y(i))# How far off is ˆ y(i)from the true output y(i)?
2.g←∇θL(f(x(i);θ),y(i)) # How should we move θto maximize loss?
3.θ←θ−ηg # Go the other way instead
return θ
Figure 4.6 The stochastic gradient descent algorithm. Step 1 (computing the loss) is used
mainly to report how well we are doing on the current tuple; we don’t need to compute the
loss in order to compute the gradient. The algorithm can terminate when it converges (when
the gradient norm <ϵ), or when progress halts (for example when the loss starts going up on
a held-out set). Weights are initialized to 0 for logistic regression, but to small random values
for neural networks, as we’ll see in Chapter 6.
Let’s assume the initial weights and bias in θ0are all set to 0, and the initial learning
rateηis 0.1:
w1=w2=b=0
η=0.1
The single update step requires that we compute the gradient, multiplied by the
learning rate
θt+1=θt−η∇θL(f(x(i);θ),y(i))
In our mini example there are three parameters, so the gradient vector has 3 dimen-
sions, for w1,w2, and b. We can compute the ﬁrst gradient as follows:
∇w,bL=
∂LCE(ˆy,y)
∂w1∂LCE(ˆy,y)
∂w2∂LCE(ˆy,y)
∂b
=
(σ(w·x+b)−y)x1
(σ(w·x+b)−y)x2
σ(w·x+b)−y
=
(σ(0)−1)x1
(σ(0)−1)x2
σ(0)−1
=
−0.5x1
−0.5x2
−0.5
=
−1.5
−1.0
−0.5

Now that we have a gradient, we compute the new parameter vector θ1by moving
θ0in the opposite direction from the gradient:
θ1=
w1
w2
b
−η
−1.5
−1.0
−0.5
=
.15
.1
.05

So after one step of gradient descent, the weights have shifted to be: w1=.15,
w2=.1, and b=.05.
Note that this observation xhappened to be a positive example. We would expect
that after seeing more negative examples with high counts of negative words, that
the weight w2would shift to have a negative value.

4.7 • G RADIENT DESCENT 79
4.7.4 Mini-batch training
Stochastic gradient descent is called stochastic because it chooses a single random
example at a time, moving the weights so as to improve performance on that single
example. That can result in very choppy movements, so it’s common to compute the
gradient over batches of training instances rather than a single instance.
For example in batch training we compute the gradient over the entire dataset. batch training
By seeing so many examples, batch training offers a superb estimate of which di-
rection to move the weights, at the cost of spending a lot of time processing every
single example in the training set to compute this perfect direction.
A compromise is mini-batch training: we train on a group of mexamples (per- mini-batch
haps 512, or 1024) that is less than the whole dataset. (If mis the size of the dataset,
then we are doing batch gradient descent; if m=1, we are back to doing stochas-
tic gradient descent.) Mini-batch training also has the advantage of computational
efﬁciency. The mini-batches can easily be vectorized, choosing the size of the mini-
batch based on the computational resources. This allows us to process all the exam-
ples in one mini-batch in parallel and then accumulate the loss, something that’s not
possible with individual or batch training.
We just need to deﬁne mini-batch versions of the cross-entropy loss function
we deﬁned in Section 4.6 and the gradient in Section 4.7.1. Let’s extend the cross-
entropy loss for one example from Eq. 4.23 to mini-batches of size m. We’ll continue
to use the notation that x(i)andy(i)mean the ith training features and training label,
respectively. We make the assumption that the training examples are independent:
logp(training labels ) = logm∏
i=1p(y(i)|x(i))
=m∑
i=1logp(y(i)|x(i))
=−m∑
i=1LCE(ˆy(i),y(i)) (4.32)
Now the cost function for the mini-batch of mexamples is the average loss for each
example:
Cost(ˆy,y) =1
mm∑
i=1LCE(ˆy(i),y(i))
=−1
mm∑
i=1y(i)logσ(w·x(i)+b)+(1−y(i))log(
1−σ(w·x(i)+b))
(4.33)
The mini-batch gradient is the average of the individual gradients from Eq. 4.30:
∂Cost(ˆy,y)
∂wj=1
mm∑
i=1[
σ(w·x(i)+b)−y(i)]
x(i)
j(4.34)
Instead of using the sum notation, we can more efﬁciently compute the gradient
in its matrix form, following the vectorization we saw on page 69, where we have
a matrix Xof size [m×f]representing the minputs in the batch, and a vector yof
size[m×1]representing the correct outputs:

80 CHAPTER 4 • L OGISTIC REGRESSION
∂Cost(ˆy,y)
∂w=1
m(ˆy−y)⊺X
=1
m(σ(Xw+b)−y)⊺X (4.35)
4.8 Learning in Multinomial Logistic Regression
The loss function for multinomial logistic regression generalizes the loss function
for binary logistic regression from 2 to Kclasses. Recall that that the cross-entropy
loss for binary logistic regression (repeated from Eq. 4.23) is:
LCE(ˆy,y) =−logp(y|x) =−[ylog ˆy+(1−y)log(1−ˆy)] (4.36)
The loss function for multinomial logistic regression generalizes the two terms in
Eq. 4.36 (one that is non-zero when y=1 and one that is non-zero when y=0) to
Kterms. As we mentioned above, for multinomial regression we’ll represent both y
andˆyas vectors. The true label yis a vector with Kelements, each corresponding
to a class, with yc=1 if the correct class is c, with all other elements of ybeing 0.
And our classiﬁer will produce an estimate vector with Kelements ˆy, each element
ˆykof which represents the estimated probability p(yk=1|x).
The loss function for a single example x, generalizing from binary logistic re-
gression, is the sum of the logs of the Koutput classes, each weighted by the indi-
cator function yk(Eq. 4.37). This turns out to be just the negative log probability of
the correct class c(Eq. 4.38):
LCE(ˆy,y) =−K∑
k=1yklog ˆyk (4.37)
=−log ˆyc,(where cis the correct class) (4.38)
=−log ˆp(yc=1|x)(where cis the correct class)
=−logexp(wc·x+bc)∑K
j=1exp(wj·x+bj)(cis the correct class) (4.39)
How did we get from Eq. 4.37 to Eq. 4.38? Because only one class (let’s call it c) is
the correct one, the vector ytakes the value 1 only for this value of k, i.e., has yc=1
andyj=0∀j̸=c. That means the terms in the sum in Eq. 4.37 will all be 0 except
for the term corresponding to the true class c. Hence the cross-entropy loss is simply
the log of the output probability corresponding to the correct class, and we therefore
also call Eq. 4.38 the negative log likelihood loss .negative log
likelihood loss
Of course for gradient descent we don’t need the loss, we need its gradient. The
gradient for a single example turns out to be very similar to the gradient for binary
logistic regression, (ˆy−y)x, that we saw in Eq. 4.30. Let’s consider one piece of the
gradient, the derivative for a single weight. For each class k, the weight of the ith
element of input xiswk,i. What is the partial derivative of the loss with respect to
wk,i? This derivative turns out to be just the difference between the true value for the
class k(which is either 1 or 0) and the probability the classiﬁer outputs for class k,
weighted by the value of the input xicorresponding to the ith element of the weight

4.9 • E VALUATION : PRECISION , RECALL , F- MEASURE 81
vector for class k:
∂LCE
∂wk,i=−(yk−ˆyk)xi
=−(yk−p(yk=1|x))xi
=−(
yk−exp(wk·x+bk)∑K
j=1exp(wj·x+bj))
xi (4.40)
We’ll return to this case of the gradient for softmax regression when we introduce
neural networks in Chapter 6, and at that time we’ll also discuss the derivation of
this gradient in equations Eq. 6.35–Eq. 6.43.
4.9 Evaluation: Precision, Recall, F-measure
To introduce the methods for evaluating text classiﬁcation, let’s ﬁrst consider some
simple binary detection tasks. For example, in spam detection, our goal is to label
every text as being in the spam category (“positive”) or not in the spam category
(“negative”). For each item (email document) we therefore need to know whether
our system called it spam or not. We also need to know whether the email is actually
spam or not, i.e. the human-deﬁned labels for each document that we are trying to
match. We will refer to these human labels as the gold labels . gold labels
Or imagine you’re the CEO of the Delicious Pie Company and you need to know
what people are saying about your pies on social media, so you build a system that
detects tweets concerning Delicious Pie. Here the positive class is tweets about
Delicious Pie and the negative class is all other tweets.
In both cases, we need a metric for knowing how well our spam detector (or
pie-tweet-detector) is doing. To evaluate any system for detecting things, we start
by building a confusion matrix like the one shown in Fig. 4.7. A confusion matrixconfusion
matrix
is a table for visualizing how an algorithm performs with respect to the human gold
labels, using two dimensions (system output and gold labels), and each cell labeling
a set of possible outcomes. In the spam detection case, for example, true positives
are documents that are indeed spam (indicated by human-created gold labels) that
our system correctly said were spam. False negatives are documents that are indeed
spam but our system incorrectly labeled as non-spam.
To the bottom right of the table is the equation for accuracy , which asks what
percentage of all the observations (for the spam or pie examples that means all emails
or tweets) our system labeled correctly. Although accuracy might seem a natural
metric, we generally don’t use it for text classiﬁcation tasks. That’s because accuracy
doesn’t work well when the classes are unbalanced (as indeed they are with spam,
which is a large majority of email, or with tweets, which are mainly not about pie).
To make this more explicit, imagine that we looked at a million tweets, and
let’s say that only 100 of them are discussing their love (or hatred) for our pie,
while the other 999,900 are tweets about something completely unrelated. Imagine a
simple classiﬁer that stupidly classiﬁed every tweet as “not about pie”. This classiﬁer
would have 999,900 true negatives and only 100 false negatives for an accuracy of
999,900/1,000,000 or 99.99%! What an amazing accuracy level! Surely we should
be happy with this classiﬁer? But of course this fabulous ‘no pie’ classiﬁer would
be completely useless, since it wouldn’t ﬁnd a single one of the customer comments
we are looking for. In other words, accuracy is not a good metric when the goal is

82 CHAPTER 4 • L OGISTIC REGRESSION
true positivefalse negativefalse positivetrue negativegold positivegold negativesystempositivesystemnegativegold standard labelssystemoutputlabelsrecall = tptp+fnprecision = tptp+fpaccuracy = tp+tntp+fp+tn+fn
Figure 4.7 A confusion matrix for visualizing how well a binary classiﬁcation system per-
forms against gold standard labels.
to discover something that is rare, or at least not completely balanced in frequency,
which is a very common situation in the world.
That’s why instead of accuracy we generally turn to two other metrics shown in
Fig. 4.7: precision andrecall .Precision measures the percentage of the items that precision
the system detected (i.e., the system labeled as positive) that are in fact positive (i.e.,
are positive according to the human gold labels). Precision is deﬁned as
Precision =true positives
true positives + false positives
Recall measures the percentage of items actually present in the input that were recall
correctly identiﬁed by the system. Recall is deﬁned as
Recall =true positives
true positives + false negatives
Precision and recall will help solve the problem with the useless “nothing is
pie” classiﬁer. This classiﬁer, despite having a fabulous accuracy of 99.99%, has
a terrible recall of 0 (since there are no true positives, and 100 false negatives, the
recall is 0/100). You should convince yourself that the precision at ﬁnding relevant
tweets is equally problematic. Thus precision and recall, unlike accuracy, emphasize
true positives: ﬁnding the things that we are supposed to be looking for.
There are many ways to deﬁne a single metric that incorporates aspects of both
precision and recall. The simplest of these combinations is the F-measure (van F-measure
Rijsbergen, 1975) , deﬁned as:
Fβ=(β2+1)PR
β2P+R
Theβparameter differentially weights the importance of recall and precision,
based perhaps on the needs of an application. Values of β>1 favor recall, while
values of β<1 favor precision. When β=1, precision and recall are equally bal-
anced; this is the most frequently used metric, and is called F β=1or just F 1: F1
F1=2PR
P+R(4.41)
F-measure comes from a weighted harmonic mean of precision and recall. The
harmonic mean of a set of numbers is the reciprocal of the arithmetic mean of recip-

4.9 • E VALUATION : PRECISION , RECALL , F- MEASURE 83
rocals:
HarmonicMean (a1,a2,a3,a4,...,an) =n
1
a1+1
a2+1
a3+...+1
an(4.42)
and hence F-measure is
F=1
α1
P+(1−α)1
Ror(
withβ2=1−α
α)
F=(β2+1)PR
β2P+R(4.43)
Harmonic mean is used because the harmonic mean of two values is closer to the
minimum of the two values than the arithmetic mean is. Thus it weighs the lower of
the two numbers more heavily, which is more conservative in this situation.
4.9.1 Evaluating with more than two classes
Up to now we have been describing text classiﬁcation tasks with only two classes.
But lots of classiﬁcation tasks in language processing have more than two classes.
For sentiment analysis we generally have 3 classes (positive, negative, neutral) and
even more classes are common for tasks like part-of-speech tagging, word sense
disambiguation, semantic role labeling, emotion detection, and so on. Luckily the
naive Bayes algorithm is already a multi-class classiﬁcation algorithm.
851060urgentnormalgold labelssystemoutputrecallu = 88+5+3precisionu= 88+10+115030200spamurgentnormalspam3recalln = recalls = precisionn= 605+60+50precisions= 2003+30+2006010+60+302001+50+200
Figure 4.8 Confusion matrix for a three-class categorization task, showing for each pair of
classes (c1,c2), how many documents from c1were (in)correctly assigned to c2.
But we’ll need to slightly modify our deﬁnitions of precision and recall. Con-
sider the sample confusion matrix for a hypothetical 3-way one-of email catego-
rization decision (urgent, normal, spam) shown in Fig. 4.8. The matrix shows, for
example, that the system mistakenly labeled one spam document as urgent, and we
have shown how to compute a distinct precision and recall value for each class. In
order to derive a single metric that tells us how well the system is doing, we can com-
bine these values in two ways. In macroaveraging , we compute the performance macroaveraging
for each class, and then average over classes. In microaveraging , we collect the de- microaveraging
cisions for all classes into a single confusion matrix, and then compute precision and
recall from that table. Fig. 4.9 shows the confusion matrix for each class separately,
and shows the computation of microaveraged and macroaveraged precision.
As the ﬁgure shows, a microaverage is dominated by the more frequent class (in
this case spam), since the counts are pooled. The macroaverage better reﬂects the
statistics of the smaller classes, and so is more appropriate when performance on all
the classes is equally important.

84 CHAPTER 4 • L OGISTIC REGRESSION
8811340trueurgenttruenotsystemurgentsystemnot604055212truenormaltruenotsystemnormalsystemnot200513383truespamtruenotsystemspamsystemnot2689999635trueyestruenosystemyessystemnoprecision =8+118= .42precision =200+33200= .86precision =60+5560= .52microaverageprecision268+99268= .73=macroaverageprecision3.42+.52+.86= .60=PooledClass 3: SpamClass 2: NormalClass 1: Urgent
Figure 4.9 Separate confusion matrices for the 3 classes from the previous ﬁgure, showing the pooled confu-
sion matrix and the microaveraged and macroaveraged precision.
4.10 Test sets and Cross-validation
The training and testing procedure for text classiﬁcation follows what we saw with
language modeling (Section 3.2): we use the training set to train the model, then use
thedevelopment test set (also called a devset ) to perhaps tune some parameters,development
test set
devset and in general decide what the best model is. Once we come up with what we think
is the best model, we run it on the (hitherto unseen) test set to report its performance.
While the use of a devset avoids overﬁtting the test set, having a ﬁxed train-
ing set, devset, and test set creates another problem: in order to save lots of data
for training, the test set (or devset) might not be large enough to be representative.
Wouldn’t it be better if we could somehow use all our data for training and still use
all our data for test? We can do this by cross-validation . cross-validation
In cross-validation, we choose a number k, and partition our data into kdisjoint
subsets called folds . Now we choose one of those kfolds as a test set, train our folds
classiﬁer on the remaining k−1 folds, and then compute the error rate on the test
set. Then we repeat with another fold as the test set, again training on the other k−1
folds. We do this sampling process ktimes and average the test set error rate from
these kruns to get an average error rate. If we choose k=10, we would train 10
different models (each on 90% of our data), test the model 10 times, and average
these 10 values. This is called 10-fold cross-validation .10-fold
cross-validation
The only problem with cross-validation is that because all the data is used for
testing, we need the whole corpus to be blind; we can’t examine any of the data
to suggest possible features and in general see what’s going on, because we’d be
peeking at the test set, and such cheating would cause us to overestimate the perfor-
mance of our system. However, looking at the corpus to understand what’s going
on is important in designing NLP systems! What to do? For this reason, it is com-
mon to create a ﬁxed training set and test set, then do 10-fold cross-validation inside
the training set, but compute error rate the normal way in the test set, as shown in
Fig. 4.10.

4.11 • S TATISTICAL SIGNIFICANCE TESTING 85
Training Iterations13452678910DevDevDevDevDevDevDevDevDevDevTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTrainingTest SetTesting
Figure 4.10 10-fold cross-validation
4.11 Statistical Signiﬁcance Testing
In building systems we often need to compare the performance of two systems. How
can we know if the new system we just built is better than our old one? Or better
than some other system described in the literature? This is the domain of statistical
hypothesis testing, and in this section we introduce tests for statistical signiﬁcance
for NLP classiﬁers, drawing especially on the work of Dror et al. (2020) and Berg-
Kirkpatrick et al. (2012).
Suppose we’re comparing the performance of classiﬁers AandBon a metric M
such as F 1, or accuracy. Perhaps we want to know if our new sentiment classiﬁer
Agets a higher F 1score than our previous sentiment classiﬁer Bon a particular test
setx. Let’s call M(A,x)the score that system Agets on test set x, and δ(x)the
performance difference between AandBonx:
δ(x) =M(A,x)−M(B,x) (4.44)
We would like to know if δ(x)>0, meaning that our logistic regression classiﬁer
has a higher F 1than our naive Bayes classiﬁer on x.δ(x)is called the effect size ; a effect size
bigger δmeans that Aseems to be way better than B; a small δmeans Aseems to
be only a little better.
Why don’t we just check if δ(x)is positive? Suppose we do, and we ﬁnd that
the F 1score of Ais higher than B’s by .04. Can we be certain that Ais better? We
cannot! That’s because Amight just be accidentally better than Bon this particular x.
We need something more: we want to know if A’s superiority over Bis likely to hold
again if we checked another test set x′, or under some other set of circumstances.
In the paradigm of statistical hypothesis testing, we test this by formalizing two
hypotheses.
H0:δ(x)≤0
H1:δ(x)>0 (4.45)
The hypothesis H0, called the null hypothesis , supposes that δ(x)is actually nega- null hypothesis
tive or zero, meaning that Ais not better than B. We would like to know if we can
conﬁdently rule out this hypothesis, and instead support H1, that Ais better.
We do this by creating a random variable Xranging over all test sets. Now we
ask how likely is it, if the null hypothesis H0was correct, that among these test sets

86 CHAPTER 4 • L OGISTIC REGRESSION
we would encounter the value of δ(x)that we found, if we repeated the experiment
a great many times. We formalize this likelihood as the p-value : the probability, p-value
assuming the null hypothesis H0is true, of seeing the δ(x)that we saw or one even
greater
P(δ(X)≥δ(x)|H0is true ) (4.46)
So in our example, this p-value is the probability that we would see δ(x)assuming
Aisnotbetter than B. Ifδ(x)is huge (let’s say Ahas a very respectable F 1of .9
andBhas a terrible F 1of only .2 on x), we might be surprised, since that would be
extremely unlikely to occur if H0were in fact true, and so the p-value would be low
(unlikely to have such a large δifAis in fact not better than B). But if δ(x)is very
small, it might be less surprising to us even if H0were true and Ais not really better
than B, and so the p-value would be higher.
A very small p-value means that the difference we observed is very unlikely
under the null hypothesis, and we can reject the null hypothesis. What counts as very
small? It is common to use values like .05 or .01 as the thresholds. A value of .01
means that if the p-value (the probability of observing the δwe saw assuming H0is
true) is less than .01, we reject the null hypothesis and assume that Ais indeed better
than B. We say that a result (e.g., “ Ais better than B”) is statistically signiﬁcant ifstatistically
signiﬁcant
theδwe saw has a probability that is below the threshold and we therefore reject
this null hypothesis.
How do we compute this probability we need for the p-value? In NLP we gen-
erally don’t use simple parametric tests like t-tests or ANOV As that you might be
familiar with. Parametric tests make assumptions about the distributions of the test
statistic (such as normality) that don’t generally hold in our cases. So in NLP we
usually use non-parametric tests based on sampling: we artiﬁcially create many ver-
sions of the experimental setup. For example, if we had lots of different test sets x′
we could just measure all the δ(x′)for all the x′. That gives us a distribution. Now
we set a threshold (like .01) and if we see in this distribution that 99% or more of
those deltas are smaller than the delta we observed, i.e., that p-value( x)—the proba-
bility of seeing a δ(x)as big as the one we saw—is less than .01, then we can reject
the null hypothesis and agree that δ(x)was a sufﬁciently surprising difference and
Ais really a better algorithm than B.
There are two common non-parametric tests used in NLP: approximate ran-
domization (Noreen, 1989) and the bootstrap test . We will describe bootstrapapproximate
randomization
below, showing the paired version of the test, which again is most common in NLP.
Paired tests are those in which we compare two sets of observations that are aligned: paired
each observation in one set can be paired with an observation in another. This hap-
pens naturally when we are comparing the performance of two systems on the same
test set; we can pair the performance of system Aon an individual observation xi
with the performance of system Bon the same xi.
4.11.1 The Paired Bootstrap Test
Thebootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from pre- bootstrap test
cision, recall, or F1 to the BLEU metric used in machine translation. The word
bootstrapping refers to repeatedly drawing large numbers of samples with replace- bootstrapping
ment (called bootstrap samples ) from an original set. The intuition of the bootstrap
test is that we can create many virtual test sets from an observed test set by repeat-
edly sampling from it. The method only makes the assumption that the sample is
representative of the population.

4.11 • S TATISTICAL SIGNIFICANCE TESTING 87
Consider a tiny text classiﬁcation example with a test set xof 10 documents. The
ﬁrst row of Fig. 4.11 shows the results of two classiﬁers (A and B) on this test set.
Each document is labeled by one of the four possibilities (A and B both right, both
wrong, A right and B wrong, A wrong and B right). A slash through a letter ( B)
means that that classiﬁer got the answer wrong. On the ﬁrst document both A and
B get the correct class (AB), while on the second document A got it right but B got
it wrong (A B). If we assume for simplicity that our metric is accuracy, A has an
accuracy of .70 and B of .50, so δ(x)is .20.
Now we create a large number b(perhaps 105) of virtual test sets x(i), each of size
n=10. Fig. 4.11 shows a couple of examples. To create each virtual test set x(i), we
repeatedly ( n=10 times) select a cell from row xwith replacement. For example, to
create the ﬁrst cell of the ﬁrst virtual test set x(1), if we happened to randomly select
the second cell of the xrow, we would copy the value A B into our new cell, and
move on to create the second cell of x(1), each time sampling (randomly choosing)
from the original xwith replacement.
1 2 3 4 5 6 7 8 910A% B% δ()
x AB ABAB  AB AB  AB ABAB  ABAB.70 .50 .20
x(1)ABAB AB  AB  AB AB  AB AB  ABAB .60 .60 .00
x(2)ABAB  AB  AB  AB AB  AB ABAB AB .60 .70-.10
...
x(b)
Figure 4.11 The paired bootstrap test: Examples of bpseudo test sets x(i)being created
from an initial true test set x. Each pseudo test set is created by sampling n=10 times with
replacement; thus an individual sample is a single cell, a document with its gold label and
the correct or incorrect performance of classiﬁers A and B. Of course real test sets don’t have
only 10 examples, and bneeds to be large as well.
Now that we have the btest sets, providing a sampling distribution, we can do
statistics on how often Ahas an accidental advantage. There are various ways to
compute this advantage; here we follow the version laid out in Berg-Kirkpatrick
et al. (2012). Assuming H0(Aisn’t better than B), we would expect that δ(X),
estimated over many test sets, would be zero or negative; a much higher value would
be surprising, since H0speciﬁcally assumes Aisn’t better than B. To measure exactly
how surprising our observed δ(x)is, we would in other circumstances compute the
p-value by counting over many test sets how often δ(x(i))exceeds the expected zero
value by δ(x)or more:
p-value (x) =1
bb∑
i=11(
δ(x(i))−δ(x)≥0)
(We use the notation 1(x)to mean “1 if xis true, and 0 otherwise”.) However,
although it’s generally true that the expected value of δ(X)over many test sets,
(again assuming Aisn’t better than B) is 0, this isn’t true for the bootstrapped test
sets we created. That’s because we didn’t draw these samples from a distribution
with 0 mean; we happened to create them from the original test set x, which happens
to be biased (by .20) in favor of A. So to measure how surprising is our observed
δ(x), we actually compute the p-value by counting over many test sets how often

88 CHAPTER 4 • L OGISTIC REGRESSION
δ(x(i))exceeds the expected value of δ(x)byδ(x)or more:
p-value (x) =1
bb∑
i=11(
δ(x(i))−δ(x)≥δ(x))
=1
bb∑
i=11(
δ(x(i))≥2δ(x))
(4.47)
So if for example we have 10,000 test sets x(i)and a threshold of .01, and in only 47
of the test sets do we ﬁnd that A is accidentally better δ(x(i))≥2δ(x), the resulting
p-value of .0047 is smaller than .01, indicating that the delta we found, δ(x)is indeed
sufﬁciently surprising and unlikely to have happened by accident, and we can reject
the null hypothesis and conclude Ais better than B.
function BOOTSTRAP (test set x,num of samples b)returns p-value (x)
Calculate δ(x)# how much better does algorithm A do than B on x
s= 0
fori= 1tobdo
forj= 1tondo # Draw a bootstrap sample x(i)of size n
Select a member of xat random and add it to x(i)
Calculate δ(x(i))# how much better does algorithm A do than B on x(i)
s←s+ 1ifδ(x(i))≥2δ(x)
p-value( x)≈s
b# on what % of the b samples did algorithm A beat expectations?
return p-value( x) # if very few did, our observed δis probably not accidental
Figure 4.12 A version of the paired bootstrap algorithm after Berg-Kirkpatrick et al.
(2012).
The full algorithm for the bootstrap is shown in Fig. 4.12. It is given a test set
x, a number of samples b, and counts the percentage of the bbootstrap test sets in
which δ(x(i))>2δ(x). This percentage then acts as a one-sided empirical p-value.
4.12 Avoiding Harms in Classiﬁcation
It is important to avoid harms that may result from classiﬁers, harms that exist both
for naive Bayes classiﬁers and for the other classiﬁcation algorithms we introduce
in later chapters.
One class of harms is representational harms (Crawford 2017, Blodgett et al.representational
harms
2020), harms caused by a system that demeans a social group, for example by per-
petuating negative stereotypes about them. For example Kiritchenko and Moham-
mad (2018) examined the performance of 200 sentiment analysis systems on pairs of
sentences that were identical except for containing either a common African Amer-
ican ﬁrst name (like Shaniqua ) or a common European American ﬁrst name (like
Stephanie ), chosen from the Caliskan et al. (2017) study discussed in Chapter 5.
They found that most systems assigned lower sentiment and more negative emotion
to sentences with African American names, reﬂecting and perpetuating stereotypes
that associate African Americans with negative emotions (Popp et al., 2003).

4.13 • I NTERPRETING MODELS 89
In other tasks classiﬁers may lead to both representational harms and other
harms, such as silencing. For example the important text classiﬁcation task of tox-
icity detection is the task of detecting hate speech, abuse, harassment, or othertoxicity
detection
kinds of toxic language. While the goal of such classiﬁers is to help reduce soci-
etal harm, toxicity classiﬁers can themselves cause harms. For example, researchers
have shown that some widely used toxicity classiﬁers incorrectly ﬂag as being toxic
sentences that are non-toxic but simply mention identities like women (Park et al.,
2018), blind people (Hutchinson et al., 2020) or gay people (Dixon et al., 2018;
Dias Oliva et al., 2021), or simply use linguistic features characteristic of varieties
like African-American Vernacular English (Sap et al. 2019, Davidson et al. 2019).
Such false positive errors could lead to the silencing of discourse by or about these
groups.
These model problems can be caused by biases or other problems in the training
data; in general, machine learning systems replicate and even amplify the biases
in their training data. But these problems can also be caused by the labels (for
example due to biases in the human labelers), by the resources used (like lexicons,
or model components like pretrained embeddings), or even by model architecture
(like what the model is trained to optimize). While the mitigation of these biases
(for example by carefully considering the training data sources) is an important area
of research, we currently don’t have general solutions. For this reason it’s important,
when introducing any NLP model, to study these kinds of factors and make them
clear. One way to do this is by releasing a model card (Mitchell et al., 2019) for model card
each version of a model. A model card documents a machine learning model with
information like:
• training algorithms and parameters
• training data sources, motivation, and preprocessing
• evaluation data sources, motivation, and preprocessing
• intended use and users
• model performance across different demographic or other groups and envi-
ronmental situations
4.13 Interpreting models
Often we want to know more than just the correct classiﬁcation of an observation.
We want to know why the classiﬁer made the decision it did. That is, we want our
decision to be interpretable . Interpretability can be hard to deﬁne strictly, but the interpretable
core idea is that as humans we should know why our algorithms reach the conclu-
sions they do. Because the features to logistic regression are often human-designed,
one way to understand a classiﬁer’s decision is to understand the role each feature
plays in the decision. Logistic regression can be combined with statistical tests (the
likelihood ratio test, or the Wald test); investigating whether a particular feature is
signiﬁcant by one of these tests, or inspecting its magnitude (how large is the weight
wassociated with the feature?) can help us interpret why the classiﬁer made the
decision it makes. This is enormously important for building transparent models.
Furthermore, in addition to its use as a classiﬁer, logistic regression in NLP and
many other ﬁelds is widely used as an analytic tool for testing hypotheses about the
effect of various explanatory variables (features). In text classiﬁcation, perhaps we
want to know if logically negative words ( no, not, never ) are more likely to be asso-

90 CHAPTER 4 • L OGISTIC REGRESSION
ciated with negative sentiment, or if negative reviews of movies are more likely to
discuss the cinematography. However, in doing so it’s necessary to control for po-
tential confounds: other factors that might inﬂuence sentiment (the movie genre, the
year it was made, perhaps the length of the review in words). Or we might be study-
ing the relationship between NLP-extracted linguistic features and non-linguistic
outcomes (hospital readmissions, political outcomes, or product sales), but need to
control for confounds (the age of the patient, the county of voting, the brand of the
product). In such cases, logistic regression allows us to test whether some feature is
associated with some outcome above and beyond the effect of other features.
4.14 Advanced: Regularization
Numquam ponenda est pluralitas sine necessitate
‘Plurality should never be proposed unless needed’
William of Occam
There is a problem with learning weights that make the model perfectly match the
training data. If a feature is perfectly predictive of the outcome because it happens
to only occur in one class, it will be assigned a very high weight. The weights for
features will attempt to perfectly ﬁt details of the training set, in fact too perfectly,
modeling noisy factors that just accidentally correlate with the class. This problem is
called overﬁtting . A good model should be able to generalize well from the training overﬁtting
generalize data to the unseen test set, but a model that overﬁts will have poor generalization.
To avoid overﬁtting, a new regularization term R(θ)is added to the loss func- regularization
tion in Eq. 4.25, resulting in the following loss for a batch of mexamples (slightly
rewritten from Eq. 4.25 to be maximizing log probability rather than minimizing
loss, and removing the1
mterm which doesn’t affect the argmax):
ˆθ=argmax
θm∑
i=1logP(y(i)|x(i))−αR(θ) (4.48)
The new regularization term R(θ)is used to penalize large weights. Thus a setting of
the weights that matches the training data perfectly— but uses many weights with
high values to do so—will be penalized more than a setting that matches the data
a little less well, but does so using smaller weights. The higher the regularization
strength parameter α, the lower the model’s weights will be, reducing its reliance on
the training data.
There are two common ways to compute this regularization term R(θ).L2 reg-
ularization is a quadratic function of the weight values, named because it uses theL2
regularization
(square of the) L2 norm of the weight values. The L2 norm, ||θ||2, is the same as
theEuclidean distance of the vector θfrom the origin. If θconsists of nweights,
then:
R(θ) =||θ||2
2=n∑
j=1θ2
j (4.49)

4.14 • A DVANCED : REGULARIZATION 91
The L2 regularized loss function becomes:
ˆθ=argmax
θ[m∑
i=1logP(y(i)|x(i);θ)]
−αn∑
j=1θ2
j (4.50)
L1 regularization is a linear function of the weight values, named after the L1 normL1
regularization
||W||1, the sum of the absolute values of the weights, or Manhattan distance (the
Manhattan distance is the distance you’d have to walk between two points in a city
with a street grid like New York):
R(θ) =||θ||1=n∑
i=1|θi| (4.51)
The L1 regularized loss function becomes:
ˆθ=argmax
θ[m∑
i=1logP(y(i)|x(i);θ)]
−αn∑
j=1|θj| (4.52)
These kinds of regularization come from statistics, where L1 regularization is called
lasso regression (Tibshirani, 1996) and L2 regularization is called ridge regression , lasso
ridge and both are commonly used in language processing. L2 regularization is easier to
optimize because of its simple derivative (the derivative of θ2is just 2 θ), while
L1 regularization is more complex (the derivative of |θ|is non-continuous at zero).
But while L2 prefers weight vectors with many small weights, L1 prefers sparse
solutions with some larger weights but many more weights set to zero. Thus L1
regularization leads to much sparser weight vectors, that is, far fewer features.
Both L1 and L2 regularization have Bayesian interpretations as constraints on
the prior of how weights should look. L1 regularization can be viewed as a Laplace
prior on the weights. L2 regularization corresponds to assuming that weights are
distributed according to a Gaussian distribution with mean µ=0. In a Gaussian
or normal distribution, the further away a value is from the mean, the lower its
probability (scaled by the variance σ). By using a Gaussian prior on the weights, we
are saying that weights prefer to have the value 0. A Gaussian for a weight θjis
1√
2πσ2
jexp(
−(θj−µj)2
2σ2
j)
(4.53)
If we multiply each weight by a Gaussian prior on the weight, we are thus maximiz-
ing the following constraint:
ˆθ=argmax
θm∏
i=1P(y(i)|x(i))×n∏
j=11√
2πσ2
jexp(
−(θj−µj)2
2σ2
j)
(4.54)
which in log space, with µ=0, and assuming 2 σ2=1, corresponds to
ˆθ=argmax
θm∑
i=1logP(y(i)|x(i))−αn∑
j=1θ2
j (4.55)
which is in the same form as Eq. 4.50.

92 CHAPTER 4 • L OGISTIC REGRESSION
4.15 Advanced: Deriving the Gradient Equation
In this section we give the derivation of the gradient of the cross-entropy loss func-
tion LCEfor logistic regression. Let’s start with some quick calculus refreshers.
First, the derivative of ln (x):
d
dxln(x) =1
x(4.56)
Second, the (very elegant) derivative of the sigmoid:
dσ(z)
dz=σ(z)(1−σ(z)) (4.57)
Finally, the chain rule of derivatives. Suppose we are computing the derivative chain rule
of a composite function f(x) =u(v(x)). The derivative of f(x)is the derivative of
u(x)with respect to v(x)times the derivative of v(x)with respect to x:
d f
dx=du
dv·dv
dx(4.58)
First, we want to know the derivative of the loss function with respect to a single
weight wj(we’ll need to compute it for each weight, and for the bias):
∂LCE
∂wj=∂
∂wj−[ylogσ(w·x+b)+(1−y)log(1−σ(w·x+b))]
=−[∂
∂wjylogσ(w·x+b)+∂
∂wj(1−y)log[1−σ(w·x+b)]]
(4.59)
Next, using the chain rule, and relying on the derivative of log:
∂LCE
∂wj=−y
σ(w·x+b)∂
∂wjσ(w·x+b)−1−y
1−σ(w·x+b)∂
∂wj[1−σ(w·x+b)]
(4.60)
Rearranging terms:
∂LCE
∂wj=−[y
σ(w·x+b)−1−y
1−σ(w·x+b)]∂
∂wjσ(w·x+b)
And now plugging in the derivative of the sigmoid, and using the chain rule one
more time, we end up with Eq. 4.61:
∂LCE
∂wj=−[y−σ(w·x+b)
σ(w·x+b)[1−σ(w·x+b)]]
σ(w·x+b)[1−σ(w·x+b)]∂(w·x+b)
∂wj
=−[y−σ(w·x+b)
σ(w·x+b)[1−σ(w·x+b)]]
σ(w·x+b)[1−σ(w·x+b)]xj
=−[y−σ(w·x+b)]xj
= [σ(w·x+b)−y]xj (4.61)

4.16 • S UMMARY 93
4.16 Summary
This chapter introduced the logistic regression model of classiﬁcation .
• Logistic regression is a supervised machine learning classiﬁer that extracts
real-valued features from the input, multiplies each by a weight, sums them,
and passes the sum through a sigmoid function to generate a probability. A
threshold is used to make a decision.
• Logistic regression can be used with two classes (e.g., positive and negative
sentiment) or with multiple classes ( multinomial logistic regression , for ex-
ample for n-ary text classiﬁcation, part-of-speech labeling, etc.).
• Multinomial logistic regression uses the softmax function to compute proba-
bilities.
• The weights (vector wand bias b) are learned from a labeled training set via a
loss function, such as the cross-entropy loss , that must be minimized.
• Minimizing this loss function is a convex optimization problem, and iterative
algorithms like gradient descent are used to ﬁnd the optimal weights.
•Regularization is used to avoid overﬁtting.
• Logistic regression is also one of the most useful analytic tools, because of its
ability to transparently study the importance of individual features.
Historical Notes
Logistic regression was developed in the ﬁeld of statistics, where it was used for
the analysis of binary data by the 1960s, and was particularly common in medicine
(Cox, 1969). Starting in the late 1970s it became widely used in linguistics as one
of the formal foundations of the study of linguistic variation (Sankoff and Labov,
1979).
Nonetheless, logistic regression didn’t become common in natural language pro-
cessing until the 1990s, when it seems to have appeared simultaneously from two
directions. The ﬁrst source was the neighboring ﬁelds of information retrieval and
speech processing, both of which had made use of regression, and both of which
lent many other statistical techniques to NLP. Indeed a very early use of logistic
regression for document routing was one of the ﬁrst NLP applications to use (LSI)
embeddings as word representations (Sch ¨utze et al., 1995).
At the same time in the early 1990s logistic regression was developed and ap-
plied to NLP at IBM Research under the name maximum entropy modeling ormaximum
entropy
maxent (Berger et al., 1996), seemingly independent of the statistical literature. Un-
der that name it was applied to language modeling (Rosenfeld, 1996), part-of-speech
tagging (Ratnaparkhi, 1996), parsing (Ratnaparkhi, 1997), coreference resolution
(Kehler, 1997b), and text classiﬁcation (Nigam et al., 1999).
There are a variety of sources covering the many kinds of text classiﬁcation
tasks. For sentiment analysis see Pang and Lee (2008), and Liu and Zhang (2012).
Stamatatos (2009) surveys authorship attribute algorithms. On language identiﬁca-
tion see Jauhiainen et al. (2019); Jaech et al. (2016) is an important early neural
system. The task of newswire indexing was often used as a test case for text classi-
ﬁcation algorithms, based on the Reuters-21578 collection of newswire articles.
See Manning et al. (2008) and Aggarwal and Zhai (2012) on text classiﬁcation;
classiﬁcation in general is covered in machine learning textbooks (Hastie et al. 2001,

94 CHAPTER 4 • L OGISTIC REGRESSION
Witten and Frank 2005, Bishop 2006, Murphy 2012).
Non-parametric methods for computing statistical signiﬁcance were used ﬁrst in
NLP in the MUC competition (Chinchor et al., 1993), and even earlier in speech
recognition (Gillick and Cox 1989, Bisani and Ney 2004). Our description of the
bootstrap draws on the description in Berg-Kirkpatrick et al. (2012). Recent work
has focused on issues including multiple test sets and multiple metrics (Søgaard et al.
2014, Dror et al. 2017).
Feature selection is a method of removing features that are unlikely to generalize
well. Features are generally ranked by how informative they are about the classiﬁca-
tion decision. A very common metric, information gain , tells us how many bits ofinformation
gain
information the presence of the word gives us for guessing the class. Other feature
selection metrics include χ2, pointwise mutual information, and GINI index; see
Yang and Pedersen (1997) for a comparison and Guyon and Elisseeff (2003) for an
introduction to feature selection.
Exercises

