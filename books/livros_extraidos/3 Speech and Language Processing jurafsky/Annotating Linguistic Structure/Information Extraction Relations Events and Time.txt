CHAPTER
20Information Extraction:
Relations, Events, and Time
Time will explain.
Jane Austen, Persuasion
Imagine that you are an analyst with an investment ﬁrm that tracks airline stocks.
You’re given the task of determining the relationship (if any) between airline an-
nouncements of fare increases and the behavior of their stocks the next day. His-
torical data about stock prices is easy to come by, but what about the airline an-
nouncements? You will need to know at least the name of the airline, the nature of
the proposed fare hike, the dates of the announcement, and possibly the response of
other airlines. Fortunately, these can be all found in news articles like this one:
Citing high fuel prices, United Airlines said Friday it has increased fares
by $6 per round trip on ﬂights to some cities also served by lower-
cost carriers. American Airlines, a unit of AMR Corp., immediately
matched the move, spokesman Tim Wagner said. United, a unit of UAL
Corp., said the increase took effect Thursday and applies to most routes
where it competes against discount carriers, such as Chicago to Dallas
and Denver to San Francisco.
This chapter presents techniques for extracting limited kinds of semantic con-
tent from text. This process of information extraction (IE) turns the unstructuredinformation
extraction
information embedded in texts into structured data, for example for populating a
relational database to enable further processing.
We begin with the task of relation extraction : ﬁnding and classifying semanticrelation
extraction
relations among entities mentioned in a text, like child-of (X is the child-of Y), or
part-whole or geospatial relations. Relation extraction has close links to populat-
ing a relational database, and knowledge graphs , datasets of structured relationalknowledge
graphs
knowledge, are a useful way for search engines to present information to users.
Next, we discuss event extraction , the task of ﬁnding events in which these en-event
extraction
tities participate, like, in our sample text, the fare increases by United andAmerican
and the reporting events said andcite. Events are also situated in time , occurring at
a particular date or time, and events can be related temporally, happening before or
after or simultaneously with each other. We’ll need to recognize temporal expres-
sions like Friday ,Thursday ortwo days from now and times such as 3:30 P .M. , and
normalize them onto speciﬁc calendar dates or times. We’ll need to link Friday to
the time of United’s announcement, Thursday to the previous day’s fare increase,
and we’ll need to produce a timeline in which United’s announcement follows the
fare increase and American’s announcement follows both of those events.
The related task of template ﬁlling is to ﬁnd recurring stereotypical events or template ﬁlling
situations in documents and ﬁll in the template slots. These slot-ﬁllers may consist
of text segments extracted directly from the text, or concepts like times, amounts, or
ontology entities that have been inferred through additional processing. Our airline

452 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
ARTIFACTGENERALAFFILIATIONORGAFFILIATIONPART-WHOLEPERSON-SOCIALPHYSICALLocatedNearBusinessFamilyLasting PersonalCitizen-Resident-Ethnicity-ReligionOrg-Location-OriginFounderEmploymentMembershipOwnershipStudent-AlumInvestorUser-Owner-Inventor-ManufacturerGeographicalSubsidiary
Sports-Affiliation
Figure 20.1 The 17 relations used in the ACE relation extraction task.
text presents such a stereotypical situation since airlines often raise fares and then
wait to see if competitors follow along. Here we can identify United as a lead air-
line that initially raised its fares, $6 as the amount, Thursday as the increase date,
andAmerican as an airline that followed along, leading to a ﬁlled template like the
following:
FARE-RAISE ATTEMPT :
LEAD AIRLINE : U NITED AIRLINES
AMOUNT : $6
EFFECTIVE DATE: 2006-10-26
FOLLOWER : A MERICAN AIRLINES

20.1 Relation Extraction
Let’s assume that we have detected the named entities in our sample text (perhaps
using the techniques of Chapter 17), and would like to discern the relationships that
exist among the detected entities:
Citing high fuel prices, [ ORG United Airlines ] said [ TIME Friday ] it
has increased fares by [ MONEY $6] per round trip on ﬂights to some
cities also served by lower-cost carriers. [ ORG American Airlines ], a
unit of [ ORG AMR Corp.] , immediately matched the move, spokesman
[PER Tim Wagner ] said. [ ORG United] , a unit of [ ORG UAL Corp.] ,
said the increase took effect [ TIME Thursday] and applies to most
routes where it competes against discount carriers, such as [ LOC Chicago]
to [LOC Dallas] and [ LOC Denver] to [LOC San Francisco] .
The text tells us, for example, that Tim Wagner is a spokesman for American
Airlines , that United is a unit of UAL Corp. , and that American is a unit of AMR .
These binary relations are instances of more generic relations such as part-of or
employs that are fairly frequent in news-style texts. Figure 20.1 lists the 17 relations
used in the ACE relation extraction evaluations and Fig. 20.2 shows some sample
relations. We might also extract more domain-speciﬁc relations such as the notion of
an airline route. For example from this text we can conclude that United has routes
to Chicago, Dallas, Denver, and San Francisco.

20.1 • R ELATION EXTRACTION 453
Relations Types Examples
Physical-Located PER-GPE Hewas in Tennessee
Part-Whole-Subsidiary ORG-ORG XYZ , the parent company of ABC
Person-Social-Family PER-PER Yoko ’s husband John
Org-AFF-Founder PER-ORG Steve Jobs , co-founder of Apple ...
Figure 20.2 Semantic relations with examples and the named entity types they involve.
Sets of relations have been deﬁned for many other domains as well. For example
UMLS, the Uniﬁed Medical Language System from the US National Library of
Medicine has a network that deﬁnes 134 broad subject categories, entity types, and
54 relations between the entities, such as the following:
Entity Relation Entity
Injury disrupts Physiological Function
Bodily Location location-of Biologic Function
Anatomical Structure part-of Organism
Pharmacologic Substance causes Pathological Function
Pharmacologic Substance treats Pathologic Function
Given a medical sentence like this one:
(20.1) Doppler echocardiography can be used to diagnose left anterior descending
artery stenosis in patients with type 2 diabetes
We could thus extract the UMLS relation:
Echocardiography, Doppler Diagnoses Acquired stenosis
Wikipedia also offers a large supply of relations, drawn from infoboxes , struc- infoboxes
tured tables associated with certain Wikipedia articles. For example, the Wikipedia
infobox for Stanford includes structured facts like state = "California" or
president = "Marc Tessier-Lavigne" . These facts can be turned into rela-
tions like president-of orlocated-in . or into relations in a metalanguage called RDF RDF
(Resource Description Framework). An RDF triple is a tuple of entity-relation- RDF triple
entity, called a subject-predicate-object expression. Here’s a sample RDF triple:
subject predicate object
Golden Gate Park location San Francisco
For example the crowdsourced DBpedia (Bizer et al., 2009) is an ontology de-
rived from Wikipedia containing over 2 billion RDF triples. Another dataset from
Wikipedia infoboxes, Freebase (Bollacker et al., 2008), now part of Wikidata (Vrande ˇci´c Freebase
and Kr ¨otzsch, 2014), has relations between people and their nationality, or locations,
and other locations they are contained in.
WordNet or other ontologies offer useful ontological relations that express hier-
archical relations between words or concepts. For example WordNet has the is-aor is-a
hypernym relation between classes, hypernym
Giraffe is-a ruminant is-a ungulate is-a mammal is-a vertebrate ...
WordNet also has Instance-of relation between individuals and classes, so that for
example San Francisco is in the Instance-of relation with city. Extracting these
relations is an important step in extending or building ontologies.
Finally, there are large datasets that contain sentences hand-labeled with their
relations, designed for training and testing relation extractors. The TACRED dataset
(Zhang et al., 2017) contains 106,264 examples of relation triples about particular
people or organizations, labeled in sentences from news and web text drawn from the

454 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
annual TAC Knowledge Base Population (TAC KBP) challenges. TACRED contains
41 relation types (like per:city of birth, org:subsidiaries, org:member of, per:spouse),
plus a no relation tag; examples are shown in Fig. 20.3. About 80% of all examples
are annotated as no relation; having sufﬁcient negative data is important for training
supervised classiﬁers.
Example Entity Types & Label
Carey will succeed Cathleen P. Black, who held the position for 15
years and will take on a new role as chairwoman of Hearst Maga-
zines, the company said.PERSON /TITLE
Relation: per:title
Irene Morgan Kirkaldy, who was born and reared in Baltimore, lived
on Long Island and ran a child-care center in Queens with her second
husband, Stanley Kirkaldy.PERSON /CITY
Relation: per:city ofbirth
Baldwin declined further comment, and said JetBlue chief executive
Dave Barger was unavailable.Types: PERSON /TITLE
Relation: norelation
Figure 20.3 Example sentences and labels from the TACRED dataset (Zhang et al., 2017).
A standard dataset was also produced for the SemEval 2010 Task 8, detecting
relations between nominals (Hendrickx et al., 2009). The dataset has 10,717 exam-
ples, each with a pair of nominals (untyped) hand-labeled with one of 9 directed
relations like product-producer ( afactory manufactures suits) orcomponent-whole
(myapartment has a large kitchen ).
20.2 Relation Extraction Algorithms
There are ﬁve main classes of algorithms for relation extraction: handwritten pat-
terns ,supervised machine learning ,semi-supervised (viabootstrapping ordis-
tant supervision ), and unsupervised . We’ll introduce each of these in the next
sections.
20.2.1 Using Patterns to Extract Relations
The earliest and still common algorithm for relation extraction is lexico-syntactic
patterns, ﬁrst developed by Hearst (1992a), and therefore often called Hearst pat-
terns . Consider the following sentence: Hearst patterns
Agar is a substance prepared from a mixture of red algae, such as Ge-
lidium, for laboratory or industrial use.
Hearst points out that most human readers will not know what Gelidium is, but that
they can readily infer that it is a kind of (a hyponym of)red algae , whatever that is.
She suggests that the following lexico-syntactic pattern
NP0such as NP 1{,NP2...,(and|or)NPi},i≥1 (20.2)
implies the following semantics
∀NPi,i≥1,hyponym (NPi,NP0) (20.3)
allowing us to infer
hyponym (Gelidium,red algae ) (20.4)

20.2 • R ELATION EXTRACTION ALGORITHMS 455
NP{, NP}*{,}(and|or) other NP H temples, treasuries, and other important civic buildings
NPHsuch as{NP,}*{(or|and)}NP red algae such as Gelidium
such NP Has{NP,}*{(or|and)}NP such authors as Herrick, Goldsmith, and Shakespeare
NPH{,}including{NP,}*{(or|and)}NP common-law countries, including Canada and England
NPH{,}especially{NP}*{(or|and)}NP European countries, especially France, England, and Spain
Figure 20.4 Hand-built lexico-syntactic patterns for ﬁnding hypernyms, using {}to mark optionality (Hearst
1992a, Hearst 1998).
Figure 20.4 shows ﬁve patterns Hearst (1992a, 1998) suggested for inferring
the hyponym relation; we’ve shown NPHas the parent/hyponym. Modern versions
of the pattern-based approach extend it by adding named entity constraints. For
example if our goal is to answer questions about “Who holds what ofﬁce in which
organization?”, we can use patterns like the following:
PER, POSITION of ORG:
George Marshall, Secretary of State of the United States
PER (named|appointed|chose|etc.) PER Prep? POSITION
Truman appointed Marshall Secretary of State
PER [be]? (named|appointed|etc.) Prep? ORG POSITION
George Marshall was named US Secretary of State
Hand-built patterns have the advantage of high-precision and they can be tailored
to speciﬁc domains. On the other hand, they are often low-recall, and it’s a lot of
work to create them for all possible patterns.
20.2.2 Relation Extraction via Supervised Learning
Supervised machine learning approaches to relation extraction follow a scheme that
should be familiar by now. A ﬁxed set of relations and entities is chosen, a training
corpus is hand-annotated with the relations and entities, and the annotated texts are
then used to train classiﬁers to annotate an unseen test set.
The most straightforward approach, illustrated in Fig. 20.5 is: (1) Find pairs of
named entities (usually in the same sentence). (2): Apply a relation-classiﬁcation
on each pair. The classiﬁer can use any supervised technique (logistic regression,
RNN, Transformer, random forest, etc.).
An optional intermediate ﬁltering classiﬁer can be used to speed up the process-
ing by making a binary decision on whether a given pair of named entities are related
(by any relation). It’s trained on positive examples extracted directly from all rela-
tions in the annotated corpus, and negative examples generated from within-sentence
entity pairs that are not annotated with a relation.
Feature-based supervised relation classiﬁers. Let’s consider sample features for
a feature-based classiﬁer (like logistic regression or random forests), classifying the
relationship between American Airlines (Mention 1, or M1) and Tim Wagner (Men-
tion 2, M2) from this sentence:
(20.5) American Airlines , a unit of AMR, immediately matched the move,
spokesman Tim Wagner said
These include word features (as embeddings, or 1-hot, stemmed or not):
• The headwords of M1 and M2 and their concatenation
Airlines Wagner Airlines-Wagner

456 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
function FINDRELATIONS (words )returns relations
relations←nil
entities←FINDENTITIES (words )
forall entity pairs ⟨e1,e2⟩inentities do
ifRELATED ?(e1,e2)
relations←relations +CLASSIFY RELATION (e1,e2)
Figure 20.5 Finding and classifying the relations among entities in a text.
• Bag-of-words and bigrams in M1 and M2
American, Airlines, Tim, Wagner, American Airlines, Tim Wagner
• Words or bigrams in particular positions
M2: -1 spokesman
M2: +1 said
• Bag of words or bigrams between M1 and M2:
a, AMR, of, immediately, matched, move, spokesman, the, unit
Named entity features:
• Named-entity types and their concatenation
(M1: ORG, M2: PER, M1M2: ORG-PER)
• Entity Level of M1 and M2 (from the set NAME, NOMINAL, PRONOUN)
M1: NAME [it or he would be PRONOUN]
M2: NAME [the company would be NOMINAL]
• Number of entities between the arguments (in this case 1, for AMR)
Syntactic structure is a useful signal, often represented as the dependency or
constituency syntactic path traversed through the tree between the entities.
• Constituent paths between M1 and M2
NP↑NP↑S↑S↓NP
• Dependency-tree paths
Airlines←sub jmatched←comp said→sub jWagner
Neural supervised relation classiﬁers Neural models for relation extraction sim-
ilarly treat the task as supervised classiﬁcation. Let’s consider a typical system ap-
plied to the TACRED relation extraction dataset and task (Zhang et al., 2017). In
TACRED we are given a sentence and two spans within it: a subject, which is a
person or organization, and an object, which is any other entity. The task is to assign
a relation from the 42 TAC relations, including no relation.
A typical Transformer-encoder algorithm, shown in Fig. 20.6, simply takes a
pretrained encoder like BERT and adds a linear layer on top of the sentence repre-
sentation (for example the BERT [CLS] token), a linear layer that is ﬁnetuned as a
1-of-N classiﬁer to assign one of the 43 labels. The input to the BERT encoder is
partially de-lexiﬁed; the subject and object entities are replaced in the input by their
NER tags. This helps keep the system from overﬁtting to the individual lexical items
(Zhang et al., 2017). When using BERT-type Transformers for relation extraction, it
helps to use versions of BERT like RoBERTa (Liu et al., 2019) or spanBERT (Joshi
et al., 2020) that don’t have two sequences separated by a [SEP] token, but instead
form the input from a single long sequence of sentences.
In general, if the test set is similar enough to the training set, and if there is
enough hand-labeled data, supervised relation extraction systems can get high ac-

20.2 • R ELATION EXTRACTION ALGORITHMS 457
ENCODER[CLS][SUBJ_PERSON]wasbornin[OBJ_LOC],MichiganLinearClassiﬁerp(relation|SUBJ,OBJ)
Figure 20.6 Relation extraction as a linear layer on top of an encoder (in this case BERT),
with the subject and object entities replaced in the input by their NER tags (Zhang et al. 2017,
Joshi et al. 2020).
curacies. But labeling a large training set is extremely expensive and supervised
models are brittle: they don’t generalize well to different text genres. For this rea-
son, much research in relation extraction has focused on the semi-supervised and
unsupervised approaches we turn to next.
20.2.3 Semisupervised Relation Extraction via Bootstrapping
Supervised machine learning assumes that we have lots of labeled data. Unfortu-
nately, this is expensive. But suppose we just have a few high-precision seed pat-
terns , like those in Section 20.2.1, or perhaps a few seed tuples . That’s enough seed patterns
seed tuples to bootstrap a classiﬁer! Bootstrapping proceeds by taking the entities in the seed
bootstrapping pair, and then ﬁnding sentences (on the web, or whatever dataset we are using) that
contain both entities. From all such sentences, we extract and generalize the context
around the entities to learn new patterns. Fig. 20.7 sketches a basic algorithm.
function BOOTSTRAP (Relation R )returns new relation tuples
tuples←Gather a set of seed tuples that have relation R
iterate
sentences←ﬁnd sentences that contain entities in tuples
patterns←generalize the context between and around entities in sentences
newpairs←usepatterns to identify more tuples
newpairs←newpairs with high conﬁdence
tuples←tuples +newpairs
return tuples
Figure 20.7 Bootstrapping from seed entity pairs to learn relations.
Suppose, for example, that we need to create a list of airline/hub pairs, and we
know only that Ryanair has a hub at Charleroi. We can use this seed fact to discover
new patterns by ﬁnding other mentions of this relation in our corpus. We search
for the terms Ryanair ,Charleroi andhubin some proximity. Perhaps we ﬁnd the
following set of sentences:
(20.6) Budget airline Ryanair, which uses Charleroi as a hub, scrapped all
weekend ﬂights out of the airport.
(20.7) All ﬂights in and out of Ryanair’s hub at Charleroi airport were grounded on
Friday...
(20.8) A spokesman at Charleroi, a main hub for Ryanair, estimated that 8000
passengers had already been affected.

458 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
From these results, we can use the context of words between the entity mentions,
the words before mention one, the word after mention two, and the named entity
types of the two mentions, and perhaps other features, to extract general patterns
such as the following:
/ [ORG], which uses [LOC] as a hub /
/ [ORG]’s hub at [LOC] /
/ [LOC], a main hub for [ORG] /
These new patterns can then be used to search for additional tuples.
Bootstrapping systems also assign conﬁdence values to new tuples to avoid se-conﬁdence
values
mantic drift . In semantic drift, an erroneous pattern leads to the introduction of semantic drift
erroneous tuples, which, in turn, lead to the creation of problematic patterns and the
meaning of the extracted relations ‘drifts’. Consider the following example:
(20.9) Sydney has a ferry hub at Circular Quay.
If accepted as a positive example, this expression could lead to the incorrect in-
troduction of the tuple ⟨Sydney,CircularQuay⟩. Patterns based on this tuple could
propagate further errors into the database.
Conﬁdence values for patterns are based on balancing two factors: the pattern’s
performance with respect to the current set of tuples and the pattern’s productivity
in terms of the number of matches it produces in the document collection. More
formally, given a document collection D, a current set of tuples T, and a proposed
pattern p, we need to track two factors:
•hits(p): the set of tuples in Tthatpmatches while looking in D
•ﬁnds (p): The total set of tuples that pﬁnds in D
The following equation balances these considerations (Riloff and Jones, 1999).
ConfRlogF(p) =|hits(p)|
|ﬁnds(p)|log(|ﬁnds(p)|) (20.10)
This metric is generally normalized to produce a probability.
We can assess the conﬁdence in a proposed new tuple by combining the evidence
supporting it from all the patterns P′that match that tuple in D(Agichtein and Gra-
vano, 2000). One way to combine such evidence is the noisy-or technique. Assume noisy-or
that a given tuple is supported by a subset of the patterns in P, each with its own
conﬁdence assessed as above. In the noisy-or model, we make two basic assump-
tions. First, that for a proposed tuple to be false, allof its supporting patterns must
have been in error, and second, that the sources of their individual failures are all
independent. If we loosely treat our conﬁdence measures as probabilities, then the
probability of any individual pattern pfailing is 1−Conf(p); the probability of all
of the supporting patterns for a tuple being wrong is the product of their individual
failure probabilities, leaving us with the following equation for our conﬁdence in a
new tuple.
Conf(t) =1−∏
p∈P′(1−Conf(p)) (20.11)
Setting conservative conﬁdence thresholds for the acceptance of new patterns
and tuples during the bootstrapping process helps prevent the system from drifting
away from the targeted relation.

20.2 • R ELATION EXTRACTION ALGORITHMS 459
20.2.4 Distant Supervision for Relation Extraction
Although hand-labeling text with relation labels is expensive to produce, there are
ways to ﬁnd indirect sources of training data. The distant supervision methoddistant
supervision
(Mintz et al., 2009) combines the advantages of bootstrapping with supervised learn-
ing. Instead of just a handful of seeds, distant supervision uses a large database to
acquire a huge number of seed examples, creates lots of noisy pattern features from
all these examples and then combines them in a supervised classiﬁer.
For example suppose we are trying to learn the place-of-birth relationship be-
tween people and their birth cities. In the seed-based approach, we might have only
5 examples to start with. But Wikipedia-based databases like DBPedia or Freebase
have tens of thousands of examples of many relations; including over 100,000 ex-
amples of place-of-birth , (<Edwin Hubble, Marshfield> ,<Albert Einstein,
Ulm> , etc.,). The next step is to run named entity taggers on large amounts of text—
Mintz et al. (2009) used 800,000 articles from Wikipedia—and extract all sentences
that have two named entities that match the tuple, like the following:
...Hubble was born in Marshﬁeld...
...Einstein, born (1879), Ulm...
...Hubble’s birthplace in Marshﬁeld...
Training instances can now be extracted from this data, one training instance
for each identical tuple <relation, entity1, entity2> . Thus there will be one
training instance for each of:
<born-in, Edwin Hubble, Marshfield>
<born-in, Albert Einstein, Ulm>
<born-year, Albert Einstein, 1879>
and so on.
We can then apply feature-based or neural classiﬁcation. For feature-based
classiﬁcation, we can use standard supervised relation extraction features like the
named entity labels of the two mentions, the words and dependency paths in be-
tween the mentions, and neighboring words. Each tuple will have features col-
lected from many training instances; the feature vector for a single training instance
like (<born-in,Albert Einstein, Ulm> will have lexical and syntactic features
from many different sentences that mention Einstein and Ulm.
Because distant supervision has very large training sets, it is also able to use very
rich features that are conjunctions of these individual features. So we will extract
thousands of patterns that conjoin the entity types with the intervening words or
dependency paths like these:
PER was born in LOC
PER, born (XXXX), LOC
PER’s birthplace in LOC
To return to our running example, for this sentence:
(20.12) American Airlines , a unit of AMR, immediately matched the move,
spokesman Tim Wagner said
we would learn rich conjunction features like this one:
M1 = ORG & M2 = PER & nextword=“said”& path= NP↑NP↑S↑S↓NP
The result is a supervised classiﬁer that has a huge rich set of features to use
in detecting relations. Since not every test sentence will have one of the training

460 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
relations, the classiﬁer will also need to be able to label an example as no-relation .
This label is trained by randomly selecting entity pairs that do not appear in any
Freebase relation, extracting features for them, and building a feature vector for
each such tuple. The ﬁnal algorithm is sketched in Fig. 20.8.
function DISTANT SUPERVISION (Database D, Text T )returns relation classiﬁer C
foreach relation R
foreach tuple ( e1,e2 ) of entities with relation RinD
sentences←Sentences in Tthat contain e1ande2
f←Frequent features in sentences
observations←observations + new training tuple ( e1, e2, f, R )
C←Train supervised classiﬁer on observations
return C
Figure 20.8 The distant supervision algorithm for relation extraction. A neural classiﬁer
would skip the feature set f.
Distant supervision shares advantages with each of the methods we’ve exam-
ined. Like supervised classiﬁcation, distant supervision uses a classiﬁer with lots
of features, and supervised by detailed hand-created knowledge. Like pattern-based
classiﬁers, it can make use of high-precision evidence for the relation between en-
tities. Indeed, distance supervision systems learn patterns just like the hand-built
patterns of early relation extractors. For example the is-aorhypernym extraction
system of Snow et al. (2005) used hypernym/hyponym NP pairs from WordNet as
distant supervision, and then learned new patterns from large amounts of text. Their
system induced exactly the original 5 template patterns of Hearst (1992a), but also
70,000 additional patterns including these four:
NPHlike NP Many hormones like leptin...
NPHcalled NP ...using a markup language called XHTML
NP is a NP H Ruby is a programming language...
NP, a NP H IBM, a company with a long...
This ability to use a large number of features simultaneously means that, un-
like the iterative expansion of patterns in seed-based systems, there’s no semantic
drift. Like unsupervised classiﬁcation, it doesn’t use a labeled training corpus of
texts, so it isn’t sensitive to genre issues in the training corpus, and relies on very
large amounts of unlabeled data. Distant supervision also has the advantage that it
can create training tuples to be used with neural classiﬁers, where features are not
required.
The main problem with distant supervision is that it tends to produce low-precision
results, and so current research focuses on ways to improve precision. Furthermore,
distant supervision can only help in extracting relations for which a large enough
database already exists. To extract new relations without datasets, or relations for
new domains, purely unsupervised methods must be used.
20.2.5 Unsupervised Relation Extraction
The goal of unsupervised relation extraction is to extract relations from the web
when we have no labeled training data, and not even any list of relations. This task
is often called open information extraction orOpen IE . In Open IE, the relationsopen
information
extraction

20.2 • R ELATION EXTRACTION ALGORITHMS 461
are simply strings of words (usually beginning with a verb).
For example, the ReVerb system (Fader et al., 2011) extracts a relation from a
sentence sin 4 steps:
1. Run a part-of-speech tagger and entity chunker over s
2. For each verb in s, ﬁnd the longest sequence of words wthat start with a verb
and satisfy syntactic and lexical constraints, merging adjacent matches.
3. For each phrase w, ﬁnd the nearest noun phrase xto the left which is not a
relative pronoun, wh-word or existential “there”. Find the nearest noun phrase
yto the right.
4. Assign conﬁdence cto the relation r= (x,w,y)using a conﬁdence classiﬁer
and return it.
A relation is only accepted if it meets syntactic and lexical constraints. The
syntactic constraints ensure that it is a verb-initial sequence that might also include
nouns (relations that begin with light verbs like make ,have , ordooften express the
core of the relation with a noun, like have a hub in ):
V|VP|VW*P
V = verb particle? adv?
W = (noun|adj|adv|pron|det )
P = (prep|particle|inﬁnitive “to”)
The lexical constraints are based on a dictionary Dthat is used to prune very rare,
long relation strings. The intuition is to eliminate candidate relations that don’t oc-
cur with sufﬁcient number of distinct argument types and so are likely to be bad
examples. The system ﬁrst runs the above relation extraction algorithm ofﬂine on
500 million web sentences and extracts a list of all the relations that occur after nor-
malizing them (removing inﬂection, auxiliary verbs, adjectives, and adverbs). Each
relation ris added to the dictionary if it occurs with at least 20 different arguments.
Fader et al. (2011) used a dictionary of 1.7 million normalized relations.
Finally, a conﬁdence value is computed for each relation using a logistic re-
gression classiﬁer. The classiﬁer is trained by taking 1000 random web sentences,
running the extractor, and hand labeling each extracted relation as correct or incor-
rect. A conﬁdence classiﬁer is then trained on this hand-labeled data, using features
of the relation and the surrounding words. Fig. 20.9 shows some sample features
used in the classiﬁcation.
(x,r,y) covers all words in s
the last preposition in risfor
the last preposition in rison
len(s)≤10
there is a coordinating conjunction to the left of rins
rmatches a lone V in the syntactic constraints
there is preposition to the left of xins
there is an NP to the right of yins
Figure 20.9 Features for the classiﬁer that assigns conﬁdence to relations extracted by the
Open Information Extraction system REVERB (Fader et al., 2011).
For example the following sentence:
(20.13) United has a hub in Chicago, which is the headquarters of United
Continental Holdings.

462 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
has the relation phrases has a hub in andis the headquarters of (it also has hasand
is, but longer phrases are preferred). Step 3 ﬁnds United to the left and Chicago to
the right of has a hub in , and skips over which to ﬁnd Chicago to the left of is the
headquarters of . The ﬁnal output is:
r1: <United, has a hub in, Chicago>
r2: <Chicago, is the headquarters of, United Continental Holdings>
The great advantage of unsupervised relation extraction is its ability to handle
a huge number of relations without having to specify them in advance. The dis-
advantage is the need to map all the strings into some canonical form for adding
to databases or knowledge graphs. Current methods focus heavily on relations ex-
pressed with verbs, and so will miss many relations that are expressed nominally.
20.2.6 Evaluation of Relation Extraction
Supervised relation extraction systems are evaluated by using test sets with human-
annotated, gold-standard relations and computing precision, recall, and F-measure.
Labeled precision and recall require the system to classify the relation correctly,
whereas unlabeled methods simply measure a system’s ability to detect entities that
are related.
Semi-supervised andunsupervised methods are much more difﬁcult to evalu-
ate, since they extract totally new relations from the web or a large text. Because
these methods use very large amounts of text, it is generally not possible to run them
solely on a small labeled test set, and as a result it’s not possible to pre-annotate a
gold set of correct instances of relations.
For these methods it’s possible to approximate (only) precision by drawing a
random sample of relations from the output, and having a human check the accuracy
of each of these relations. Usually this approach focuses on the tuples to be extracted
from a body of text rather than on the relation mentions ; systems need not detect
every mention of a relation to be scored correctly. Instead, the evaluation is based
on the set of tuples occupying the database when the system is ﬁnished. That is,
we want to know if the system can discover that Ryanair has a hub at Charleroi; we
don’t really care how many times it discovers it. The estimated precision ˆPis then
ˆP=# of correctly extracted relation tuples in the sample
total # of extracted relation tuples in the sample.(20.14)
Another approach that gives us a little bit of information about recall is to com-
pute precision at different levels of recall. Assuming that our system is able to
rank the relations it produces (by probability, or conﬁdence) we can separately com-
pute precision for the top 1000 new relations, the top 10,000 new relations, the top
100,000, and so on. In each case we take a random sample of that set. This will
show us how the precision curve behaves as we extract more and more tuples. But
there is no way to directly evaluate recall.
20.3 Extracting Events
The task of event extraction is to identify mentions of events in texts. For theevent
extraction
purposes of this task, an event mention is any expression denoting an event or state
that can be assigned to a particular point, or interval, in time. The following markup
of the sample text on page 451 shows all the events in this text.

20.4 • R EPRESENTING TIME 463
[EVENT Citing] high fuel prices, United Airlines [ EVENT said] Fri-
day it has [ EVENT increased] fares by $6 per round trip on ﬂights to
some cities also served by lower-cost carriers. American Airlines, a unit
of AMR Corp., immediately [ EVENT matched] [ EVENT the move],
spokesman Tim Wagner [ EVENT said]. United, a unit of UAL Corp.,
[EVENT said] [ EVENT the increase] took effect Thursday and [ EVENT
applies] to most routes where it [ EVENT competes] against discount
carriers, such as Chicago to Dallas and Denver to San Francisco.
In English, most event mentions correspond to verbs, and most verbs introduce
events. However, as we can see from our example, this is not always the case. Events
can be introduced by noun phrases, as in the move andthe increase , and some verbs
fail to introduce events, as in the phrasal verb took effect , which refers to when the
event began rather than to the event itself. Similarly, light verbs such as make ,take, light verbs
andhave often fail to denote events. A light verb is a verb that has very little meaning
itself, and the associated event is instead expressed by its direct object noun. In light
verb examples like took a ﬂight , it’s the word ﬂight that deﬁnes the event; these light
verbs just provide a syntactic structure for the noun’s arguments.
Various versions of the event extraction task exist, depending on the goal. For
example in the TempEval shared tasks (Verhagen et al. 2009) the goal is to extract
events and aspects like their aspectual and temporal properties. Events are to be
classiﬁed as actions, states, reporting events (say, report, tell, explain ), perceptionreporting
events
events, and so on. The aspect, tense, and modality of each event also needs to be
extracted. Thus for example the various said events in the sample text would be
annotated as (class=REPORTING, tense=PAST, aspect=PERFECTIVE).
Event extraction is generally modeled via supervised learning, detecting events
via IOB sequence models and assigning event classes and attributes with multi-class
classiﬁers. The input can be neural models starting from encoders; or classic feature-
based models using features like those in Fig. 20.10.
Feature Explanation
Character afﬁxes Character-level preﬁxes and sufﬁxes of target word
Nominalization sufﬁx Character-level sufﬁxes for nominalizations (e.g., -tion )
Part of speech Part of speech of the target word
Light verb Binary feature indicating that the target is governed by a light verb
Subject syntactic category Syntactic category of the subject of the sentence
Morphological stem Stemmed version of the target word
Verb root Root form of the verb basis for a nominalization
WordNet hypernyms Hypernym set for the target
Figure 20.10 Features commonly used in classic feature-based approaches to event detection.
20.4 Representing Time
Let’s begin by introducing the basics of temporal logic and how human languages temporal logic
convey temporal information. The most straightforward theory of time holds that it
ﬂows inexorably forward and that events are associated with either points or inter-
vals in time, as on a timeline. We can order distinct events by situating them on the
timeline; one event precedes another if the ﬂow of time leads from the ﬁrst event

464 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
to the second. Accompanying these notions in most theories is the idea of the cur-
rent moment in time. Combining this notion with the idea of a temporal ordering
relationship yields the familiar notions of past, present, and future.
Various kinds of temporal representation systems can be used to talk about tem-
poral ordering relationship. One of the most commonly used in computational mod-
eling is the interval algebra of Allen (1984). Allen models all events and time interval algebra
expressions as intervals there is no representation for points (although intervals can
be very short). In order to deal with intervals without points, he identiﬁes 13 primi-
tive relations that can hold between these temporal intervals. Fig. 20.11 shows these
13Allen relations . Allen relations
BABA
BA
AAB
BAB
Time A  before BB after  AA overlaps BB overlaps' AA meets BB meets' AA equals B(B equals A)A starts BB starts' AA finishes BB finishes' A
BA during BB during' AA
Figure 20.11 The 13 temporal relations from Allen (1984).
20.4.1 Reichenbach’s reference point
The relation between simple verb tenses and points in time is by no means straight-
forward. The present tense can be used to refer to a future event, as in this example:
(20.15) Ok, we ﬂy from San Francisco to Boston at 10.
Or consider the following examples:
(20.16) Flight 1902 arrived late.
(20.17) Flight 1902 had arrived late.
Although both refer to events in the past, representing them in the same way seems
wrong. The second example seems to have another unnamed event lurking in the
background (e.g., Flight 1902 had already arrived late when something else hap-
pened).

20.4 • R EPRESENTING TIME 465
To account for this phenomena, Reichenbach (1947) introduced the notion of
areference point . In our simple temporal scheme, the current moment in time is reference point
equated with the time of the utterance and is used as a reference point for when
the event occurred (before, at, or after). In Reichenbach’s approach, the notion of
the reference point is separated from the utterance time and the event time. The
following examples illustrate the basics of this approach:
(20.18) When Mary’s ﬂight departed, I ate lunch.
(20.19) When Mary’s ﬂight departed, I had eaten lunch.
In both of these examples, the eating event has happened in the past, that is, prior
to the utterance. However, the verb tense in the ﬁrst example indicates that the eating
event began when the ﬂight departed, while the second example indicates that the
eating was accomplished prior to the ﬂight’s departure. Therefore, in Reichenbach’s
terms the departure event speciﬁes the reference point. These facts can be accom-
modated by additional constraints relating the eating anddeparture events. In the
ﬁrst example, the reference point precedes the eating event, and in the second exam-
ple, the eating precedes the reference point. Figure 20.12 illustrates Reichenbach’s
approach with the primary English tenses. Exercise 20.4 asks you to represent these
examples in FOL.
Past PerfectSimple PastPresent Perfect
Simple FutureFuture PerfectPresentEE
EER
RUR,EUR,U
U,R,EU,RU
Figure 20.12 Reichenbach’s approach applied to various English tenses. In these diagrams,
time ﬂows from left to right, Edenotes the time of the event, Rdenotes the reference time,
andUdenotes the time of the utterance.
Languages have many other ways to convey temporal information besides tense.
Most useful for our purposes will be temporal expressions like in the morning or
6:45 orafterwards .
(20.20) I’d like to go at 6:45 in the morning.
(20.21) Somewhere around noon, please.
(20.22) I want to take the train back afterwards.
Incidentally, temporal expressions display a fascinating metaphorical conceptual
organization. Temporal expressions in English are frequently expressed in spatial
terms, as is illustrated by the various uses of at,in,somewhere , and near in these
examples (Lakoff and Johnson 1980, Jackendoff 1983). Metaphorical organizations
such as these, in which one domain is systematically expressed in terms of another,
are very common in languages of the world.

466 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
20.5 Representing Aspect
A related notion to time is aspect , which is what we call the way events can be aspect
categorized by their internal temporal structure or temporal contour. By this we
mean questions like whether events are ongoing or have ended, or whether they are
conceptualized as happening at a point in time or over some interval. Such notions
of temporal contour have been used to divide event expressions into classes since
Aristotle, although the set of four classes we’ll introduce here is due to Vendler
(1967) (you may also see the German term aktionsart used to refer to these classes). aktionsart
The most basic aspectual distinction is between events (which involve change) events
andstates (which do not involve change). Stative expressions represent the notion states
stative of an event participant being in a state , or having a particular property, at a given
point in time. Stative expressions capture aspects of the world at a single point in
time, and conceptualize the participant as unchanging and continuous. Consider the
following ATIS examples.
(20.23) I like express trains.
(20.24) I need the cheapest fare.
(20.25) I want to go ﬁrst class.
In examples like these, the event participant denoted by the subject can be seen as
experiencing something at a speciﬁc point in time, and don’t involve any kind of
internal change over time (the liking or needing is conceptualized as continuous and
unchanging).
Non-states (which we’ll refer to as events ) are divided into subclasses; we’ll
introduce three here. Activity expressions describe events undertaken by a partic- activity
ipant that occur over a span of time (rather than being conceptualized as a single
point in time like stative expressions), and have no particular end point. Of course
in practice all things end, but the meaning of the expression doesn’t represent this
fact. Consider the following examples:
(20.26) She drove a Mazda.
(20.27) I live in Brooklyn.
These examples both specify that the subject is engaged in, or has engaged in, the
activity speciﬁed by the verb for some period of time, but doesn’t specify when the
driving or living might have stopped.
Two more classes of expressions, achievement expressions and accomplish-
ment expressions, describe events that take place over time, but also conceptualize
the event as having a particular kind of endpoint or goal. The Greek word telos
means ‘end’ or ’goal’ and so the events described by these kinds of expressions are
often called telic events. telic
Accomplishment expressions describe events that have a natural end point andaccomplishment
expressions
result in a particular state. Consider the following examples:
(20.28) He booked me a reservation.
(20.29) The 7:00 train got me to New York City.
In these examples, an event is seen as occurring over some period of time that ends
when the intended state is accomplished (i.e., the state of me having a reservation,
or me being in New York City).
The ﬁnal aspectual class, achievement expressions , is only subtly different thanachievement
expressions
accomplishments. Consider the following:

20.6 • T EMPORALLY ANNOTATED DATASETS : TIMEBANK 467
(20.30) She found her gate.
(20.31) I reached New York.
Like accomplishment expressions, achievement expressions result in a state. But
unlike accomplishments, achievement events are ‘punctual’: they are thought of as
happening in an instant and the verb doesn’t conceptualize the process or activ-
ity leading up the state. Thus the events in these examples may in fact have been
preceded by extended searching ortraveling events, but the verb doesn’t conceptu-
alize these preceding processes, but rather conceptualizes the events corresponding
toﬁnding andreaching as points, not intervals.
In summary, a standard way of categorizing event expressions by their temporal
contours is via these four general classes:
Stative: I know my departure gate.
Activity: John is ﬂying.
Accomplishment: Sally booked her ﬂight.
Achievement: She found her gate.
Before moving on, note that event expressions can easily be shifted from one
class to another. Consider the following examples:
(20.32) I ﬂew.
(20.33) I ﬂew to New York.
The ﬁrst example is a simple activity; it has no natural end point. The second ex-
ample is clearly an accomplishment event since it has an end point, and results in a
particular state. Clearly, the classiﬁcation of an event is not solely governed by the
verb, but by the semantics of the entire expression in context.
20.6 Temporally Annotated Datasets: TimeBank
TheTimeBank corpus consists of American English text annotated with temporal TimeBank
information (Pustejovsky et al., 2003). The annotations use TimeML (Saur ´ı et al.,
2006), a markup language for time based on Allen’s interval algebra discussed above
(Allen, 1984). There are three types of TimeML objects: an E VENT represent events
and states, a T IME represents time expressions like dates, and a L INK represents
various relationships between events and times (event-event, event-time, and time-
time). The links include temporal links (TL INK) for the 13 Allen relations, aspec-
tual links (AL INK) for aspectual relationships between events and subevents, and
SLINKS which mark factuality.
Consider the following sample sentence and its corresponding markup shown in
Fig. 20.13, selected from one of the TimeBank documents.
(20.34) Delta Air Lines earnings soared 33% to a record in the ﬁscal ﬁrst quarter,
bucking the industry trend toward declining proﬁts.
This text has three events and two temporal expressions (including the creation
time of the article, which serves as the document time), and four temporal links that
capture the using the Allen relations:
• Soaring e1isincluded in the ﬁscal ﬁrst quarter t58
• Soaring e1isbefore 1989-10-26 t57
• Soaring e1issimultaneous with the bucking e3

468 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
<TIMEX3 tid="t57" type="DATE" value="1989-10-26" functionInDocument="CREATION_TIME">
10/26/89 </TIMEX3>
Delta Air Lines earnings <EVENT eid="e1" class="OCCURRENCE"> soared </EVENT> 33% to a
record in <TIMEX3 tid="t58" type="DATE" value="1989-Q1" anchorTimeID="t57"> the
fiscal first quarter </TIMEX3>, <EVENT eid="e3" class="OCCURRENCE">bucking</EVENT>
the industry trend toward <EVENT eid="e4" class="OCCURRENCE">declining</EVENT>
profits.
Figure 20.13 Example from the TimeBank corpus.
• Declining e4includes soaring e1
We can also visualize the links as a graph. The TimeBank snippet in Eq. 20.35
would be represented with a graph like Fig. 20.14.
(20.35) [DCT:11/02/891] 1: Paciﬁc First Financial Corp. said 2shareholders
approved 3itsacquisition 4by Royal Trustco Ltd. of Toronto for $27 a share,
or $212 million. The thrift holding company said 5itexpects 6toobtain 7
regulatory approval 8andcomplete 9thetransaction 10byyear-end 11.
1234567811910BEFOREBEFOREAFTERSIMULTANEOUSENDSCULMINATESBEFOREEVIDENTIALMODALFACTIVEMODALEVIDENTIALMODAL
Figure 20.14 A graph of the text in Eq. 20.35, adapted from (Ocal et al., 2022). TL INKS
are shown in blue, AL INKS in red, and SL INKS in green.
20.7 Automatic Temporal Analysis
Here we introduce the three common steps used in analyzing time in text:
1. Extracting temporal expressions
2.Normalizing these expressions, by converting them to a standard format.
3.Linking events to times and extracting time graphs and timelines
20.7.1 Extracting Temporal Expressions
Temporal expressions are phrases that refer to absolute points in time, relative times,
durations, and sets of these. Absolute temporal expressions are those that can be absolute
mapped directly to calendar dates, times of day, or both. Relative temporal expres- relative
sions map to particular times through some other reference point (as in a week from
last Tuesday ). Finally, durations denote spans of time at varying levels of granular- duration
ity (seconds, minutes, days, weeks, centuries, etc.). Figure 20.15 lists some sample
temporal expressions in each of these categories.
Temporal expressions are grammatical constructions that often have temporal
lexical triggers as their heads, making them easy to ﬁnd. Lexical triggers might lexical triggers

20.7 • A UTOMATIC TEMPORAL ANALYSIS 469
Absolute Relative Durations
April 24, 1916 yesterday four hours
The summer of ’77 next semester three weeks
10:15 AM two weeks from yesterday six days
The 3rd quarter of 2006 last quarter the last three quarters
Figure 20.15 Examples of absolute, relational and durational temporal expressions.
be nouns, proper nouns, adjectives, and adverbs; full temporal expressions consist
of their phrasal projections: noun phrases, adjective phrases, and adverbial phrases
(Figure 20.16).
Category Examples
Noun morning ,noon ,night ,winter ,dusk,dawn
Proper Noun January, Monday, Ides, Easter, Rosh Hashana, Ramadan, Tet
Adjective recent, past, annual, former
Adverb hourly, daily, monthly, yearly
Figure 20.16 Examples of temporal lexical triggers.
The task is to detect temporal expressions in running text, like this examples,
shown with TIMEX3 tags (Pustejovsky et al. 2005, Ferro et al. 2005).
A fare increase initiated <TIMEX3>last week</TIMEX3>by UAL
Corp’s United Airlines was matched by competitors over <TIMEX3>the
weekend</TIMEX3>, marking the second successful fare increase in
<TIMEX3>two weeks</TIMEX3>.
Rule-based approaches use cascades of regular expressions to recognize larger
and larger chunks from previous stages, based on patterns containing parts of speech,
trigger words (e.g., February ) or classes (e.g., MONTH ) (Chang and Manning, 2012;
Str¨otgen and Gertz, 2013; Chambers, 2013). Here’s a rule from SUTime (Chang and
Manning, 2012) for detecting expressions like 3 years old :
/(\d+)[-\s]($TEUnits)(s)?([-\s]old)?/
Sequence-labeling approaches use the standard IOB scheme, marking words
that are either (I)nside, (O)utside or at the (B)eginning of a temporal expression:
A
Ofare
Oincrease
Oinitiated
Olast
Bweek
Iby
OUAL
OCorp’s...
O
A statistical sequence labeler is trained, using either embeddings or a ﬁne-tuned
encoder, or classic features extracted from the token and context including words,
lexical triggers, and POS.
Temporal expression recognizers are evaluated with the usual recall, precision,
andF-measures. A major difﬁculty for all of these very lexicalized approaches is
avoiding expressions that trigger false positives:
(20.36) 1984 tells the story of Winston Smith...
(20.37) ...U2’s classic Sunday Bloody Sunday
20.7.2 Temporal Normalization
Temporal normalization is the task of mapping a temporal expression to a pointtemporal
normalization
in time or to a duration. Points in time correspond to calendar dates, to times of
day, or both. Durations primarily consist of lengths of time. Normalized times

470 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
<TIMEX3 i d =” t1 ’ ’ t y p e =”DATE” v a l u e =” 2007 −07 −02 ” f u n c t i o n I n D o c u m e n t =”CREATION TIME” >
J u l y 2 , 2007 </TIMEX3 >A f a r e i n c r e a s e i n i t i a t e d <TIMEX3 i d =” t 2 ” t y p e =”DATE”
v a l u e =” 2007 −W26” anchorTimeID=” t 1 ” >l a s t week </TIMEX3 >by U n i t e d A i r l i n e s was
matched by c o m p e t i t o r s o ve r <TIMEX3 i d =” t 3 ” t y p e =”DURATION” v a l u e =”P1WE”
anchorTimeID=” t 1 ” >t h e weekend </TIMEX3 >, marking t h e second s u c c e s s f u l f a r e
i n c r e a s e i n <TIMEX3 i d =” t 4 ” t y p e =”DURATION” v a l u e =”P2W” anchorTimeID=” t 1 ” >two
weeks </TIMEX3 >.
Figure 20.17 TimeML markup including normalized values for temporal expressions.
are represented via the ISO 8601 standard for encoding temporal values (ISO8601,
2004). Fig. 20.17 reproduces our earlier example with these value attributes.
The dateline, or document date, for this text was July 2, 2007 . The ISO repre-
sentation for this kind of expression is YYYY-MM-DD, or in this case, 2007-07-02.
The encodings for the temporal expressions in our sample text all follow from this
date, and are shown here as values for the VALUE attribute.
The ﬁrst temporal expression in the text proper refers to a particular week of the
year. In the ISO standard, weeks are numbered from 01 to 53, with the ﬁrst week
of the year being the one that has the ﬁrst Thursday of the year. These weeks are
represented with the template YYYY-Wnn. The ISO week for our document date is
week 27; thus the value for last week is represented as “2007-W26”.
The next temporal expression is the weekend . ISO weeks begin on Monday;
thus, weekends occur at the end of a week and are fully contained within a single
week. Weekends are treated as durations, so the value of the VALUE attribute has
to be a length. Durations are represented according to the pattern P nx, where nis
an integer denoting the length and xrepresents the unit, as in P3Y for three years
or P2D for two days . In this example, one weekend is captured as P1WE. In this
case, there is also sufﬁcient information to anchor this particular weekend as part of
a particular week. Such information is encoded in the ANCHOR TIMEID attribute.
Finally, the phrase two weeks also denotes a duration captured as P2W. Figure 20.18
give some more examples, but there is a lot more to the various temporal annotation
standards; consult ISO8601 (2004), Ferro et al. (2005), and Pustejovsky et al. (2005)
for more details.
Unit Pattern Sample Value
Fully speciﬁed dates YYYY-MM-DD 1991-09-28
Weeks YYYY-Wnn 2007-W27
Weekends PnWE P1WE
24-hour clock times HH:MM:SS 11:13:45
Dates and times YYYY-MM-DDTHH:MM:SS 1991-09-28T11:00:00
Financial quarters Qn 1999-Q3
Figure 20.18 Sample ISO patterns for representing various times and durations.
Most current approaches to temporal normalization are rule-based (Chang and
Manning 2012, Str ¨otgen and Gertz 2013). Patterns that match temporal expressions
are associated with semantic analysis procedures. For example, the pattern above for
recognizing phrases like 3 years old can be associated with the predicate Duration
that takes two arguments, the length and the unit of time:
pattern: /(\d+)[-\s]($TEUnits)(s)?([-\s]old)?/
result: Duration($1, $2)
The task is difﬁcult because fully qualiﬁed temporal expressions are fairly rare
in real texts. Most temporal expressions in news articles are incomplete and are only
implicitly anchored, often with respect to the dateline of the article, which we refer

20.7 • A UTOMATIC TEMPORAL ANALYSIS 471
to as the document’s temporal anchor . The values of temporal expressions suchtemporal
anchor
astoday ,yesterday , ortomorrow can all be computed with respect to this temporal
anchor. The semantic procedure for today simply assigns the anchor, and the attach-
ments for tomorrow andyesterday add a day and subtract a day from the anchor,
respectively. Of course, given the cyclic nature of our representations for months,
weeks, days, and times of day, our temporal arithmetic procedures must use modulo
arithmetic appropriate to the time unit being used.
Unfortunately, even simple expressions such as the weekend orWednesday in-
troduce a fair amount of complexity. In our current example, the weekend clearly
refers to the weekend of the week that immediately precedes the document date. But
this won’t always be the case, as is illustrated in the following example.
(20.38) Random security checks that began yesterday at Sky Harbor will continue
at least through the weekend.
In this case, the expression the weekend refers to the weekend of the week that the
anchoring date is part of (i.e., the coming weekend). The information that signals
this meaning comes from the tense of continue , the verb governing the weekend .
Relative temporal expressions are handled with temporal arithmetic similar to
that used for today andyesterday . The document date indicates that our example
article is ISO week 27, so the expression last week normalizes to the current week
minus 1. To resolve ambiguous next andlastexpressions we consider the distance
from the anchoring date to the nearest unit. Next Friday can refer either to the
immediately next Friday or to the Friday following that, but the closer the document
date is to a Friday, the more likely it is that the phrase will skip the nearest one. Such
ambiguities are handled by encoding language and domain-speciﬁc heuristics into
the temporal attachments.
20.7.3 Temporal Ordering of Events
The goal of temporal analysis, is to link times to events and then ﬁt all these events
into a complete timeline. This ambitious task is the subject of considerable current
research but solving it with a high level of accuracy is beyond the capabilities of
current systems. A somewhat simpler, but still useful, task is to impose a partial or-
dering on the events and temporal expressions mentioned in a text. Such an ordering
can provide many of the same beneﬁts as a true timeline. An example of such a par-
tial ordering is the determination that the fare increase by American Airlines came
after the fare increase by United in our sample text. Determining such an ordering
can be viewed as a binary relation detection and classiﬁcation task.
Even this partial ordering task assumes that in addition to the detecting and nor-
malizing time expressions steps described above, we have already detected all the
events in the text. Indeed, many temporal expressions are anchored to events men-
tioned in a text and not directly to other temporal expressions. Consider the follow-
ing example:
(20.39) One week after the storm, JetBlue issued its customer bill of rights.
To determine when JetBlue issued its customer bill of rights we need to determine
the time of the storm event, and then we need to modify that time by the temporal
expression one week after .
Thus once the events and times have been detected, our goal next is to assert links
between all the times and events: i.e. creating event-event, event-time, time-time,
DCT-event, and DCT-time TimeML TL INKS . This can be done by training time
relation classiﬁers to predict the correct T: INKbetween each pair of times/events,

472 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
supervised by the gold labels in the TimeBank corpus with features like words/em-
beddings, parse paths, tense and aspect The sieve-based architecture using precision-
ranked sets of classiﬁers, which we’ll introduce in Chapter 23, is also commonly
used.
Systems that perform all 4 tasks (time extraction creation and normalization,
event extraction, and time/event linking) include TARSQI (Verhagen et al., 2005)
CLEAR TK (Bethard, 2013), CAEVO (Chambers et al., 2014), and CATENA (Mirza
and Tonelli, 2016).
20.8 Template Filling
Many texts contain reports of events, and possibly sequences of events, that often
correspond to fairly common, stereotypical situations in the world. These abstract
situations or stories, related to what have been called scripts (Schank and Abel- scripts
son, 1977), consist of prototypical sequences of sub-events, participants, and their
roles. The strong expectations provided by these scripts can facilitate the proper
classiﬁcation of entities, the assignment of entities into roles and relations, and most
critically, the drawing of inferences that ﬁll in things that have been left unsaid. In
their simplest form, such scripts can be represented as templates consisting of ﬁxed templates
sets of slots that take as values slot-ﬁllers belonging to particular classes. The task
oftemplate ﬁlling is to ﬁnd documents that invoke particular scripts and then ﬁll the template ﬁlling
slots in the associated templates with ﬁllers extracted from the text. These slot-ﬁllers
may consist of text segments extracted directly from the text, or they may consist of
concepts that have been inferred from text elements through some additional pro-
cessing.
A ﬁlled template from our original airline story might look like the following.
FARE-RAISE ATTEMPT :
LEAD AIRLINE : U NITED AIRLINES
AMOUNT : $6
EFFECTIVE DATE: 2006-10-26
FOLLOWER : A MERICAN AIRLINES

This template has four slots ( LEAD AIRLINE ,AMOUNT ,EFFECTIVE DATE ,FOL-
LOWER ). The next section describes a standard sequence-labeling approach to ﬁlling
slots. Section 20.8.2 then describes an older system based on the use of cascades of
ﬁnite-state transducers and designed to address a more complex template-ﬁlling task
that current learning-based systems don’t yet address.
20.8.1 Machine Learning Approaches to Template Filling
In the standard paradigm for template ﬁlling, we are given training documents with
text spans annotated with predeﬁned templates and their slot ﬁllers. Our goal is to
create one template for each event in the input, ﬁlling in the slots with text spans.
The task is generally modeled by training two separate supervised systems. The
ﬁrst system decides whether the template is present in a particular sentence. This
task is called template recognition or sometimes, in a perhaps confusing bit oftemplate
recognition
terminology, event recognition . Template recognition can be treated as a text classi-
ﬁcation task, with features extracted from every sequence of words that was labeled
in training documents as ﬁlling any slot from the template being detected. The usual

20.8 • T EMPLATE FILLING 473
set of features can be used: tokens, embeddings, word shapes, part-of-speech tags,
syntactic chunk tags, and named entity tags.
The second system has the job of role-ﬁller extraction . A separate classiﬁer isrole-ﬁller
extraction
trained to detect each role ( LEAD -AIRLINE ,AMOUNT , and so on). This can be a
binary classiﬁer that is run on every noun-phrase in the parsed input sentence, or a
sequence model run over sequences of words. Each role classiﬁer is trained on the
labeled data in the training set. Again, the usual set of features can be used, but now
trained only on an individual noun phrase or the ﬁllers of a single slot.
Multiple non-identical text segments might be labeled with the same slot la-
bel. For example in our sample text, the strings United orUnited Airlines might be
labeled as the L EAD AIRLINE . These are not incompatible choices and the corefer-
ence resolution techniques introduced in Chapter 23 can provide a path to a solution.
A variety of annotated collections have been used to evaluate this style of ap-
proach to template ﬁlling, including sets of job announcements, conference calls for
papers, restaurant guides, and biological texts. A key open question is extracting
templates in cases where there is no training data or even predeﬁned templates, by
inducing templates as sets of linked events (Chambers and Jurafsky, 2011).
20.8.2 Earlier Finite-State Template-Filling Systems
The templates above are relatively simple. But consider the task of producing a
template that contained all the information in a text like this one (Grishman and
Sundheim, 1995):
Bridgestone Sports Co. said Friday it has set up a joint venture in Taiwan
with a local concern and a Japanese trading house to produce golf clubs to be
shipped to Japan. The joint venture, Bridgestone Sports Taiwan Co., capital-
ized at 20 million new Taiwan dollars, will start production in January 1990
with production of 20,000 iron and “metal wood” clubs a month.
The MUC-5 ‘joint venture’ task (the Message Understanding Conferences were
a series of U.S. government-organized information-extraction evaluations) was to
produce hierarchically linked templates describing joint ventures. Figure 20.19
shows a structure produced by the FASTUS system (Hobbs et al., 1997). Note how
the ﬁller of the ACTIVITY slot of the TIE-UPtemplate is itself a template with slots.
Tie-up-1 Activity-1 :
RELATIONSHIP tie-up C OMPANY Bridgestone Sports Taiwan Co.
ENTITIES Bridgestone Sports Co. P RODUCT iron and “metal wood” clubs
a local concern S TART DATE DURING: January 1990
a Japanese trading house
JOINT VENTURE Bridgestone Sports Taiwan Co.
ACTIVITY Activity-1
AMOUNT NT$20000000
Figure 20.19 The templates produced by FASTUS given the input text on page 473.
Early systems for dealing with these complex templates were based on cascades
of transducers based on handwritten rules, as sketched in Fig. 20.20.
The ﬁrst four stages use handwritten regular expression and grammar rules to
do basic tokenization, chunking, and parsing. Stage 5 then recognizes entities and
events with a recognizer based on ﬁnite-state transducers (FSTs), and inserts the rec-
ognized objects into the appropriate slots in templates. This FST recognizer is based

474 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
No. Step Description
1 Tokens Tokenize input stream of characters
2 Complex Words Multiword phrases, numbers, and proper names.
3 Basic phrases Segment sentences into noun and verb groups
4 Complex phrases Identify complex noun groups and verb groups
5 Semantic Patterns Identify entities and events, insert into templates.
6 Merging Merge references to the same entity or event
Figure 20.20 Levels of processing in FASTUS (Hobbs et al., 1997). Each level extracts a
speciﬁc type of information which is then passed on to the next higher level.
on hand-built regular expressions like the following (NG indicates Noun-Group and
VG Verb-Group), which matches the ﬁrst sentence of the news story above.
NG(Company/ies) VG(Set-up) NG(Joint-Venture) with NG(Company/ies)
VG(Produce) NG(Product)
The result of processing these two sentences is the ﬁve draft templates (Fig. 20.21)
that must then be merged into the single hierarchical structure shown in Fig. 20.19.
The merging algorithm, after performing coreference resolution, merges two activi-
ties that are likely to be describing the same events.
#Template/Slot Value
1RELATIONSHIP : TIE-UP
ENTITIES : Bridgestone Co., a local concern, a Japanese trading house
2ACTIVITY : PRODUCTION
PRODUCT : “golf clubs”
3RELATIONSHIP : TIE-UP
JOINT VENTURE : “Bridgestone Sports Taiwan Co.”
AMOUNT : NT$20000000
4ACTIVITY : PRODUCTION
COMPANY : “Bridgestone Sports Taiwan Co.”
START DATE: DURING : January 1990
5ACTIVITY : PRODUCTION
PRODUCT : “iron and “metal wood” clubs”
Figure 20.21 The ﬁve partial templates produced by stage 5 of FASTUS . These templates
are merged in stage 6 to produce the ﬁnal template shown in Fig. 20.19 on page 473.
20.9 Summary
This chapter has explored techniques for extracting limited forms of semantic con-
tent from texts.
•Relations among entities can be extracted by pattern-based approaches, su-
pervised learning methods when annotated training data is available, lightly
supervised bootstrapping methods when small numbers of seed tuples or
seed patterns are available, distant supervision when a database of relations
is available, and unsupervised orOpen IE methods.
• Reasoning about time can be facilitated by detection and normalization of
temporal expressions .

HISTORICAL NOTES 475
•Events can be ordered in time using sequence models and classiﬁers trained
on temporally- and event-labeled data like the TimeBank corpus .
•Template-ﬁlling applications can recognize stereotypical situations in texts
and assign elements from the text to roles represented as ﬁxed sets of slots .
Historical Notes
The earliest work on information extraction addressed the template-ﬁlling task in the
context of the Frump system (DeJong, 1982). Later work was stimulated by the U.S.
government-sponsored MUC conferences (Sundheim 1991, Sundheim 1992, Sund-
heim 1993, Sundheim 1995). Early MUC systems like CIRCUS system (Lehnert
et al., 1991) and SCISOR (Jacobs and Rau, 1990) were quite inﬂuential and inspired
later systems like FASTUS (Hobbs et al., 1997). Chinchor et al. (1993) describe the
MUC evaluation techniques.
Due to the difﬁculty of porting systems from one domain to another, attention
shifted to machine learning approaches. Early supervised learning approaches to
IE (Cardie 1993, Cardie 1994, Riloff 1993, Soderland et al. 1995, Huffman 1996)
focused on automating the knowledge acquisition process, mainly for ﬁnite-state
rule-based systems. Their success, and the earlier success of HMM-based speech
recognition, led to the use of sequence labeling (HMMs: Bikel et al. 1997; MEMMs
McCallum et al. 2000; CRFs: Lafferty et al. 2001), and a wide exploration of fea-
tures (Zhou et al., 2005). Neural approaches followed from the pioneering results of
Collobert et al. (2011), who applied a CRF on top of a convolutional net.
Progress in this area continues to be stimulated by formal evaluations with shared
benchmark datasets, including the Automatic Content Extraction (ACE) evaluations
of 2000-2007 on named entity recognition, relation extraction, and temporal ex-
pressions1, the KBP (Knowledge Base Population ) evaluations (Ji et al. 2010, Sur- KBP
deanu 2013) of relation extraction tasks like slot ﬁlling (extracting attributes (‘slots’) slot ﬁlling
like age, birthplace, and spouse for a given entity) and a series of SemEval work-
shops (Hendrickx et al., 2009).
Semisupervised relation extraction was ﬁrst proposed by Hearst (1992b), and
extended by systems like AutoSlog-TS (Riloff, 1996), DIPRE (Brin, 1998), SNOW-
BALL (Agichtein and Gravano, 2000), and Jones et al. (1999). The distant super-
vision algorithm we describe was drawn from Mintz et al. (2009), who ﬁrst used
the term ‘distant supervision’ (which was suggested to them by Chris Manning)
but similar ideas had occurred in earlier systems like Craven and Kumlien (1999)
and Morgan et al. (2004) under the name weakly labeled data , as well as in Snow
et al. (2005) and Wu and Weld (2007). Among the many extensions are Wu and
Weld (2010), Riedel et al. (2010), and Ritter et al. (2013). Open IE systems include
KNOW ITALLEtzioni et al. (2005), TextRunner (Banko et al., 2007), and R EVERB
(Fader et al., 2011). See Riedel et al. (2013) for a universal schema that combines
the advantages of distant supervision and Open IE.
1www.nist.gov/speech/tests/ace/

476 CHAPTER 20 • I NFORMATION EXTRACTION : RELATIONS , EVENTS ,AND TIME
Exercises
20.1 Acronym expansion, the process of associating a phrase with an acronym, can
be accomplished by a simple form of relational analysis. Develop a system
based on the relation analysis approaches described in this chapter to populate
a database of acronym expansions. If you focus on English Three Letter
Acronyms (TLAs) you can evaluate your system’s performance by comparing
it to Wikipedia’s TLA page.
20.2 Acquire the CMU seminar corpus and develop a template-ﬁlling system by
using any of the techniques mentioned in Section 20.8. Analyze how well
your system performs as compared with state-of-the-art results on this corpus.
20.3 A useful functionality in newer email and calendar applications is the ability
to associate temporal expressions connected with events in email (doctor’s
appointments, meeting planning, party invitations, etc.) with speciﬁc calendar
entries. Collect a corpus of email containing temporal expressions related to
event planning. How do these expressions compare to the kinds of expressions
commonly found in news text that we’ve been discussing in this chapter?
20.4 For the following sentences, give FOL translations that capture the temporal
relationships between the events.
1. When Mary’s ﬂight departed, I ate lunch.
2. When Mary’s ﬂight departed, I had eaten lunch.

