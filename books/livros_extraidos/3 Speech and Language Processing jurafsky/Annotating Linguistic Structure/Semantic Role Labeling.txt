CHAPTER
21Semantic Role Labeling
“Who, What, Where, When, With what, Why, How”
The seven circumstances, associated with Hermagoras and Aristotle (Sloan, 2010)
Sometime between the 7th and 4th centuries BCE, the Indian grammarian P ¯an.ini1
wrote a famous treatise on Sanskrit grammar, the As .t.¯adhy ¯ay¯ı (‘8 books’), a treatise
that has been called “one of the greatest monuments of hu-
man intelligence” (Bloomﬁeld, 1933, 11). The work de-
scribes the linguistics of the Sanskrit language in the form
of 3959 sutras, each very efﬁciently (since it had to be
memorized!) expressing part of a formal rule system that
brilliantly preﬁgured modern mechanisms of formal lan-
guage theory (Penn and Kiparsky, 2012). One set of rules
describes the k¯arakas , semantic relationships between a
verb and noun arguments, roles like agent ,instrument , or
destination . P¯an.ini’s work was the earliest we know of
that modeled the linguistic realization of events and their
participants. This task of understanding how participants relate to events—being
able to answer the question “Who did what to whom” (and perhaps also “when and
where”)—is a central question of natural language processing.
Let’s move forward 2.5 millennia to the present and consider the very mundane
goal of understanding text about a purchase of stock by XYZ Corporation. This
purchasing event and its participants can be described by a wide variety of surface
forms. The event can be described by a verb ( sold, bought ) or a noun ( purchase ),
and XYZ Corp can be the syntactic subject (of bought ), the indirect object (of sold),
or in a genitive or noun compound relation (with the noun purchase ) despite having
notionally the same role in all of them:
• XYZ corporation bought the stock.
• They sold the stock to XYZ corporation.
• The stock was bought by XYZ corporation.
• The purchase of the stock by XYZ corporation...
• The stock purchase by XYZ corporation...
In this chapter we introduce a level of representation that captures the common-
ality between these sentences: there was a purchase event, the participants were
XYZ Corp and some stock, and XYZ Corp was the buyer. These shallow semantic
representations , semantic roles , express the role that arguments of a predicate take
in the event, codiﬁed in databases like PropBank and FrameNet. We’ll introduce
semantic role labeling , the task of assigning roles to spans in sentences, and selec-
tional restrictions , the preferences that predicates express about their arguments,
such as the fact that the theme of eatis generally something edible.
1Figure shows a birch bark manuscript from Kashmir of the Rupavatra, a grammatical textbook based
on the Sanskrit grammar of Panini. Image from the Wellcome Collection.

478 CHAPTER 21 • S EMANTIC ROLE LABELING
21.1 Semantic Roles
Consider the meanings of the arguments Sasha ,Pat,the window , and the door in
these two sentences.
(21.1) Sasha broke the window.
(21.2) Pat opened the door.
The subjects Sasha andPat, what we might call the breaker of the window-
breaking event and the opener of the door-opening event, have something in com-
mon. They are both volitional actors, often animate, and they have direct causal
responsibility for their events.
Thematic roles are a way to capture this semantic commonality between break- thematic roles
ersandopeners . We say that the subjects of both these verbs are agents . Thus, agents
AGENT is the thematic role that represents an abstract idea such as volitional causa-
tion. Similarly, the direct objects of both these verbs, the BrokenThing andOpenedThing ,
are both prototypically inanimate objects that are affected in some way by the action.
The semantic role for these participants is theme . theme
Thematic Role Deﬁnition
AGENT The volitional causer of an event
EXPERIENCER The experiencer of an event
FORCE The non-volitional causer of the event
THEME The participant most directly affected by an event
RESULT The end product of an event
CONTENT The proposition or content of a propositional event
INSTRUMENT An instrument used in an event
BENEFICIARY The beneﬁciary of an event
SOURCE The origin of the object of a transfer event
GOAL The destination of an object of a transfer event
Figure 21.1 Some commonly used thematic roles with their deﬁnitions.
Although thematic roles are one of the oldest linguistic models, as we saw above,
their modern formulation is due to Fillmore (1968) and Gruber (1965). Although
there is no universally agreed-upon set of roles, Figs. 21.1 and 21.2 list some the-
matic roles that have been used in various computational papers, together with rough
deﬁnitions and examples. Most thematic role sets have about a dozen roles, but we’ll
see sets with smaller numbers of roles with even more abstract meanings, and sets
with very large numbers of roles that are speciﬁc to situations. We’ll use the general
term semantic roles for all sets of roles, whether small or large. semantic roles
21.2 Diathesis Alternations
The main reason computational systems use semantic roles is to act as a shallow
meaning representation that can let us make simple inferences that aren’t possible
from the pure surface string of words, or even from the parse tree. To extend the
earlier examples, if a document says that Company A acquired Company B , we’d
like to know that this answers the query Was Company B acquired? despite the fact
that the two sentences have very different surface syntax. Similarly, this shallow
semantics might act as a useful intermediate language in machine translation.

21.2 • D IATHESIS ALTERNATIONS 479
Thematic Role Example
AGENT The waiter spilled the soup.
EXPERIENCER John has a headache.
FORCE The wind blows debris from the mall into our yards.
THEME Only after Benjamin Franklin broke the ice ...
RESULT The city built a regulation-size baseball diamond ...
CONTENT Mona asked “You met Mary Ann at a supermarket?”
INSTRUMENT He poached catﬁsh, stunning them with a shocking device ...
BENEFICIARY Whenever Ann Callahan makes hotel reservations for her boss ...
SOURCE I ﬂew in from Boston .
GOAL I drove to Portland .
Figure 21.2 Some prototypical examples of various thematic roles.
Semantic roles thus help generalize over different surface realizations of pred-
icate arguments. For example, while the AGENT is often realized as the subject of
the sentence, in other cases the THEME can be the subject. Consider these possible
realizations of the thematic arguments of the verb break :
(21.3) John
AGENTbroke the window.
THEME
(21.4) John
AGENTbroke the window
THEMEwith a rock.
INSTRUMENT
(21.5) The rock
INSTRUMENTbroke the window.
THEME
(21.6) The window
THEMEbroke.
(21.7) The window
THEMEwas broken by John.
AGENT
These examples suggest that break has (at least) the possible arguments AGENT ,
THEME , and INSTRUMENT . The set of thematic role arguments taken by a verb is
often called the thematic grid ,θ-grid, or case frame . We can see that there are thematic grid
case frame (among others) the following possibilities for the realization of these arguments of
break :
AGENT /Subject, THEME /Object
AGENT /Subject, THEME /Object, INSTRUMENT /PPwith
INSTRUMENT /Subject, THEME /Object
THEME /Subject
It turns out that many verbs allow their thematic roles to be realized in various
syntactic positions. For example, verbs like give can realize the THEME and GOAL
arguments in two different ways:
(21.8) a. Doris
AGENTgave the book
THEMEto Cary.
GOAL
b.Doris
AGENTgave Cary
GOALthe book.
THEME
These multiple argument structure realizations (the fact that break can take AGENT ,
INSTRUMENT , or THEME as subject, and give can realize its THEME and GOAL in
either order) are called verb alternations ordiathesis alternations . The alternationverb
alternation
we showed above for give, the dative alternation , seems to occur with particular se-dative
alternation
mantic classes of verbs, including “verbs of future having” ( advance ,allocate ,offer,

480 CHAPTER 21 • S EMANTIC ROLE LABELING
owe), “send verbs” ( forward ,hand ,mail), “verbs of throwing” ( kick,pass,throw ),
and so on. Levin (1993) lists for 3100 English verbs the semantic classes to which
they belong (47 high-level classes, divided into 193 more speciﬁc classes) and the
various alternations in which they participate. These lists of verb classes have been
incorporated into the online resource VerbNet (Kipper et al., 2000), which links each
verb to both WordNet and FrameNet entries.
21.3 Semantic Roles: Problems with Thematic Roles
Representing meaning at the thematic role level seems like it should be useful in
dealing with complications like diathesis alternations. Yet it has proved quite difﬁ-
cult to come up with a standard set of roles, and equally difﬁcult to produce a formal
deﬁnition of roles like AGENT ,THEME , or INSTRUMENT .
For example, researchers attempting to deﬁne role sets often ﬁnd they need to
fragment a role like AGENT orTHEME into many speciﬁc roles. Levin and Rappa-
port Hovav (2005) summarize a number of such cases, such as the fact there seem
to be at least two kinds of INSTRUMENTS ,intermediary instruments that can appear
as subjects and enabling instruments that cannot:
(21.9) a. Shelly cut the banana with a knife.
b. The knife cut the banana.
(21.10) a. Shelly ate the sliced banana with a fork.
b. *The fork ate the sliced banana.
In addition to the fragmentation problem, there are cases in which we’d like to
reason about and generalize across semantic roles, but the ﬁnite discrete lists of roles
don’t let us do this.
Finally, it has proved difﬁcult to formally deﬁne the thematic roles. Consider the
AGENT role; most cases of AGENTS are animate, volitional, sentient, causal, but any
individual noun phrase might not exhibit all of these properties.
These problems have led to alternative semantic role models that use either semantic role
many fewer or many more roles.
The ﬁrst of these options is to deﬁne generalized semantic roles that abstract
over the speciﬁc thematic roles. For example, PROTO -AGENT and PROTO -PATIENT proto-agent
proto-patient are generalized roles that express roughly agent-like and roughly patient-like mean-
ings. These roles are deﬁned, not by necessary and sufﬁcient conditions, but rather
by a set of heuristic features that accompany more agent-like or more patient-like
meanings. Thus, the more an argument displays agent-like properties (being voli-
tionally involved in the event, causing an event or a change of state in another par-
ticipant, being sentient or intentionally involved, moving) the greater the likelihood
that the argument can be labeled a PROTO -AGENT . The more patient-like the proper-
ties (undergoing change of state, causally affected by another participant, stationary
relative to other participants, etc.), the greater the likelihood that the argument can
be labeled a PROTO -PATIENT .
The second direction is instead to deﬁne semantic roles that are speciﬁc to a
particular verb or a particular group of semantically related verbs or nouns.
In the next two sections we describe two commonly used lexical resources that
make use of these alternative versions of semantic roles. PropBank uses both proto-
roles and verb-speciﬁc semantic roles. FrameNet uses semantic roles that are spe-
ciﬁc to a general semantic idea called a frame .

21.4 • T HEPROPOSITION BANK 481
21.4 The Proposition Bank
The Proposition Bank , generally referred to as PropBank , is a resource of sen- PropBank
tences annotated with semantic roles. The English PropBank labels all the sentences
in the Penn TreeBank; the Chinese PropBank labels sentences in the Penn Chinese
TreeBank. Because of the difﬁculty of deﬁning a universal set of thematic roles,
the semantic roles in PropBank are deﬁned with respect to an individual verb sense.
Each sense of each verb thus has a speciﬁc set of roles, which are given only numbers
rather than names: Arg0 ,Arg1 ,Arg2 , and so on. In general, Arg0 represents the
PROTO -AGENT , and Arg1 , the PROTO -PATIENT . The semantics of the other roles
are less consistent, often being deﬁned speciﬁcally for each verb. Nonetheless there
are some generalization; the Arg2 is often the benefactive, instrument, attribute, or
end state, the Arg3 the start point, benefactive, instrument, or attribute, and the Arg4
the end point.
Here are some slightly simpliﬁed PropBank entries for one sense each of the
verbs agree andfall. Such PropBank entries are called frame ﬁles ; note that the
deﬁnitions in the frame ﬁle for each role (“Other entity agreeing”, “Extent, amount
fallen”) are informal glosses intended to be read by humans, rather than being formal
deﬁnitions.
(21.11) agree.01
Arg0: Agreer
Arg1: Proposition
Arg2: Other entity agreeing
Ex1: [ Arg0 The group] agreed [Arg1 it wouldn’t make an offer].
Ex2: [ ArgM-TMP Usually] [ Arg0 John] agrees [Arg2 with Mary]
[Arg1 on everything].
(21.12) fall.01
Arg1: Logical subject, patient, thing falling
Arg2: Extent, amount fallen
Arg3: start point
Arg4: end point, end state of arg1
Ex1: [ Arg1 Sales] fell[Arg4 to $25 million] [ Arg3 from $27 million].
Ex2: [ Arg1 The average junk bond] fell[Arg2 by 4.2%].
Note that there is no Arg0 role for fall, because the normal subject of fallis a
PROTO -PATIENT .
The PropBank semantic roles can be useful in recovering shallow semantic in-
formation about verbal arguments. Consider the verb increase :
(21.13) increase.01 “go up incrementally”
Arg0: causer of increase
Arg1: thing increasing
Arg2: amount increased by, EXT, or MNR
Arg3: start point
Arg4: end point
A PropBank semantic role labeling would allow us to infer the commonality in
the event structures of the following three examples, that is, that in each case Big
Fruit Co. is the AGENT andthe price of bananas is the THEME , despite the differing
surface forms.

482 CHAPTER 21 • S EMANTIC ROLE LABELING
(21.14) [ Arg0 Big Fruit Co. ] increased [ Arg1 the price of bananas].
(21.15) [ Arg1 The price of bananas] was increased again [ Arg0 by Big Fruit Co. ]
(21.16) [ Arg1 The price of bananas] increased [ Arg2 5%].
PropBank also has a number of non-numbered arguments called ArgMs , (ArgM-
TMP, ArgM-LOC, etc.) which represent modiﬁcation or adjunct meanings. These
are relatively stable across predicates, so aren’t listed with each frame ﬁle. Data
labeled with these modiﬁers can be helpful in training systems to detect temporal,
location, or directional modiﬁcation across predicates. Some of the ArgMs include:
TMP when? yesterday evening, now
LOC where? at the museum, in San Francisco
DIR where to/from? down, to Bangkok
MNR how? clearly, with much enthusiasm
PRP/CAU why? because ... , in response to the ruling
REC themselves, each other
ADV miscellaneous
PRD secondary predication ...ate the meat raw
While PropBank focuses on verbs, a related project, NomBank (Meyers et al., NomBank
2004) adds annotations to noun predicates. For example the noun agreement in
Apple’s agreement with IBM would be labeled with Apple as the Arg0 and IBM as
the Arg2. This allows semantic role labelers to assign labels to arguments of both
verbal and nominal predicates.
21.5 FrameNet
While making inferences about the semantic commonalities across different sen-
tences with increase is useful, it would be even more useful if we could make such
inferences in many more situations, across different verbs, and also between verbs
and nouns. For example, we’d like to extract the similarity among these three sen-
tences:
(21.17) [ Arg1 The price of bananas] increased [ Arg2 5%].
(21.18) [ Arg1 The price of bananas] rose [ Arg2 5%].
(21.19) There has been a [ Arg2 5%] rise [ Arg1 in the price of bananas].
Note that the second example uses the different verb rise, and the third example
uses the noun rather than the verb rise. We’d like a system to recognize that the
price of bananas is what went up, and that 5%is the amount it went up, no matter
whether the 5%appears as the object of the verb increased or as a nominal modiﬁer
of the noun rise.
The FrameNet project is another semantic-role-labeling project that attempts FrameNet
to address just these kinds of problems (Baker et al. 1998, Fillmore et al. 2003,
Fillmore and Baker 2009, Ruppenhofer et al. 2016). Whereas roles in the PropBank
project are speciﬁc to an individual verb, roles in the FrameNet project are speciﬁc
to aframe .
What is a frame? Consider the following set of words:
reservation, ﬂight, travel, buy, price, cost, fare, rates, meal, plane
There are many individual lexical relations of hyponymy, synonymy, and so on
between many of the words in this list. The resulting set of relations does not,

21.5 • F RAME NET 483
however, add up to a complete account of how these words are related. They are
clearly all deﬁned with respect to a coherent chunk of common-sense background
information concerning air travel.
We call the holistic background knowledge that unites these words a frame (Fill- frame
more, 1985). The idea that groups of words are deﬁned with respect to some back-
ground information is widespread in artiﬁcial intelligence and cognitive science,
where besides frame we see related works like a model (Johnson-Laird, 1983), or model
even script (Schank and Abelson, 1977). script
A frame in FrameNet is a background knowledge structure that deﬁnes a set of
frame-speciﬁc semantic roles, called frame elements , and includes a set of predi- frame elements
cates that use these roles. Each word evokes a frame and proﬁles some aspect of the
frame and its elements. The FrameNet dataset includes a set of frames and frame
elements, the lexical units associated with each frame, and a set of labeled exam-
ple sentences. For example, the change position onascale frame is deﬁned as
follows:
This frame consists of words that indicate the change of an Item’s posi-
tion on a scale (the Attribute) from a starting point (Initial value) to an
end point (Final value).
Some of the semantic roles (frame elements) in the frame are deﬁned as in
Fig. 21.3. Note that these are separated into core roles , which are frame speciﬁc, and core roles
non-core roles , which are more like the Arg-M arguments in PropBank, expressing non-core roles
more general properties of time, location, and so on.
Core Roles
ATTRIBUTE The A TTRIBUTE is a scalar property that the I TEM possesses.
DIFFERENCE The distance by which an I TEM changes its position on the scale.
FINAL STATE A description that presents the I TEM’s state after the change in the A TTRIBUTE ’s
value as an independent predication.
FINAL VALUE The position on the scale where the I TEM ends up.
INITIAL STATE A description that presents the I TEM’s state before the change in the A T-
TRIBUTE ’s value as an independent predication.
INITIAL VALUE The initial position on the scale from which the I TEM moves away.
ITEM The entity that has a position on the scale.
VALUE RANGE A portion of the scale, typically identiﬁed by its end points, along which the
values of the A TTRIBUTE ﬂuctuate.
Some Non-Core Roles
DURATION The length of time over which the change takes place.
SPEED The rate of change of the V ALUE .
GROUP The G ROUP in which an I TEM changes the value of an
ATTRIBUTE in a speciﬁed way.
Figure 21.3 The frame elements in the change position onascale frame from the FrameNet Labelers
Guide (Ruppenhofer et al., 2016).
Here are some example sentences:
(21.20) [ ITEM Oil]rose [ATTRIBUTE in price] [ DIFFERENCE by 2%].
(21.21) [ ITEM It] has increased [FINAL STATE to having them 1 day a month].
(21.22) [ ITEM Microsoft shares] fell[FINAL VALUE to 7 5/8].
(21.23) [ ITEM Colon cancer incidence] fell[DIFFERENCE by 50%] [ GROUP among
men].

484 CHAPTER 21 • S EMANTIC ROLE LABELING
(21.24) a steady increase [INITIAL VALUE from 9.5] [ FINAL VALUE to 14.3] [ ITEM
in dividends]
(21.25) a [ DIFFERENCE 5%] [ ITEM dividend] increase ...
Note from these example sentences that the frame includes target words like rise,
fall, and increase . In fact, the complete frame consists of the following words:
VERBS: dwindle move soar escalation shift
advance edge mushroom swell explosion tumble
climb explode plummet swing fall
decline fall reach triple ﬂuctuation ADVERBS:
decrease ﬂuctuate rise tumble gain increasingly
diminish gain rocket growth
dip grow shift NOUNS: hike
double increase skyrocket decline increase
drop jump slide decrease rise
FrameNet also codes relationships between frames, allowing frames to inherit
from each other, or representing relations between frames like causation (and gen-
eralizations among frame elements in different frames can be represented by inheri-
tance as well). Thus, there is a Cause change ofposition onascale frame that is
linked to the Change ofposition onascale frame by the cause relation, but that
adds an A GENT role and is used for causative examples such as the following:
(21.26) [ AGENT They] raised [ITEM the price of their soda] [ DIFFERENCE by 2%].
Together, these two frames would allow an understanding system to extract the
common event semantics of all the verbal and nominal causative and non-causative
usages.
FrameNets have also been developed for many other languages including Span-
ish, German, Japanese, Portuguese, Italian, and Chinese.
21.6 Semantic Role Labeling
Semantic role labeling (sometimes shortened as SRL ) is the task of automaticallysemantic role
labeling
ﬁnding the semantic roles of each argument of each predicate in a sentence. Cur-
rent approaches to semantic role labeling are based on supervised machine learning,
often using the FrameNet and PropBank resources to specify what counts as a pred-
icate, deﬁne the set of roles used in the task, and provide training and test sets.
Recall that the difference between these two models of semantic roles is that
FrameNet (21.27) employs many frame-speciﬁc frame elements as roles, while Prop-
Bank (21.28) uses a smaller number of numbered argument labels that can be inter-
preted as verb-speciﬁc labels, along with the more general ARGM labels. Some
examples:
(21.27)[You] can’t [blame] [the program] [for being unable to identify it]
COGNIZER TARGET EVALUEE REASON
(21.28)[The San Francisco Examiner] issued [a special edition] [yesterday]
ARG0 TARGET ARG 1 ARGM -TMP
21.6.1 A Feature-based Algorithm for Semantic Role Labeling
A simpliﬁed feature-based semantic role labeling algorithm is sketched in Fig. 21.4.
Feature-based algorithms—from the very earliest systems like (Simmons, 1973)—
begin by parsing, using broad-coverage parsers to assign a parse to the input string.

21.6 • S EMANTIC ROLE LABELING 485
Figure 21.5 shows a parse of (21.28) above. The parse is then traversed to ﬁnd all
words that are predicates.
For each of these predicates, the algorithm examines each node in the parse
tree and uses supervised classiﬁcation to decide the semantic role (if any) it plays
for this predicate. Given a labeled training set such as PropBank or FrameNet, a
feature vector is extracted for each node, using feature templates described in the
next subsection. A 1-of-N classiﬁer is then trained to predict a semantic role for
each constituent given these features, where N is the number of potential semantic
roles plus an extra NONE role for non-role constituents. Any standard classiﬁcation
algorithms can be used. Finally, for each test sentence to be labeled, the classiﬁer is
run on each relevant constituent.
function SEMANTIC ROLELABEL (words )returns labeled tree
parse←PARSE (words )
for each predicate inparse do
for each node inparse do
featurevector←EXTRACT FEATURES (node ,predicate ,parse )
CLASSIFY NODE(node ,featurevector ,parse )
Figure 21.4 A generic semantic-role-labeling algorithm. C LASSIFY NODE is a 1-of- Nclas-
siﬁer that assigns a semantic role (or NONE for non-role constituents), trained on labeled data
such as FrameNet or PropBank.
S
NP-SBJ =A R G 0 VP
DT NNP NNP NNP
The San Francisco Examiner
VBD = TARGET NP=A R G 1 PP-TMP =A R G M - T M P
issued DT JJ NN IN NP
as p e c i a l e d i t i o n a r o u n d N N N P - T M P
noon yesterday
Figure 21.5 Parse tree for a PropBank sentence, showing the PropBank argument labels. The dotted line
shows the path feature NP↑S↓VP↓VBD for ARG0, the NP-SBJ constituent The San Francisco Examiner.
Instead of training a single-stage classiﬁer as in Fig. 21.5, the node-level classi-
ﬁcation task can be broken down into multiple steps:
1.Pruning: Since only a small number of the constituents in a sentence are
arguments of any given predicate, many systems use simple heuristics to prune
unlikely constituents.
2.Identiﬁcation: a binary classiﬁcation of each node as an argument to be la-
beled or a NONE .
3.Classiﬁcation: a 1-of- Nclassiﬁcation of all the constituents that were labeled
as arguments by the previous stage

486 CHAPTER 21 • S EMANTIC ROLE LABELING
The separation of identiﬁcation and classiﬁcation may lead to better use of fea-
tures (different features may be useful for the two tasks) or to computational efﬁ-
ciency.
Global Optimization
The classiﬁcation algorithm of Fig. 21.5 classiﬁes each argument separately (‘lo-
cally’), making the simplifying assumption that each argument of a predicate can be
labeled independently. This assumption is false; there are interactions between argu-
ments that require a more ‘global’ assignment of labels to constituents. For example,
constituents in FrameNet and PropBank are required to be non-overlapping. More
signiﬁcantly, the semantic roles of constituents are not independent. For example
PropBank does not allow multiple identical arguments; two constituents of the same
verb cannot both be labeled ARG0 .
Role labeling systems thus often add a fourth step to deal with global consistency
across the labels in a sentence. For example, the local classiﬁers can return a list of
possible labels associated with probabilities for each constituent, and a second-pass
Viterbi decoding or re-ranking approach can be used to choose the best consensus
label. Integer linear programming (ILP) is another common way to choose a solution
that conforms best to multiple constraints.
Features for Semantic Role Labeling
Most systems use some generalization of the core set of features introduced by
Gildea and Jurafsky (2000). Common basic features templates (demonstrated on
theNP-SBJ constituent The San Francisco Examiner in Fig. 21.5) include:
• The governing predicate , in this case the verb issued . The predicate is a cru-
cial feature since labels are deﬁned only with respect to a particular predicate.
• The phrase type of the constituent, in this case, NP(orNP-SBJ ). Some se-
mantic roles tend to appear as NPs, others as SorPP, and so on.
• The headword of the constituent, Examiner . The headword of a constituent
can be computed with standard head rules, such as those given in Appendix D
in Fig. 18.17. Certain headwords (e.g., pronouns) place strong constraints on
the possible semantic roles they are likely to ﬁll.
• The headword part of speech of the constituent, NNP .
• The path in the parse tree from the constituent to the predicate. This path is
marked by the dotted line in Fig. 21.5. Following Gildea and Jurafsky (2000),
we can use a simple linear representation of the path, NP ↑S↓VP↓VBD.↑and
↓represent upward and downward movement in the tree, respectively. The
path is very useful as a compact representation of many kinds of grammatical
function relationships between the constituent and the predicate.
• The voice of the clause in which the constituent appears, in this case, active
(as contrasted with passive ). Passive sentences tend to have strongly different
linkings of semantic roles to surface form than do active ones.
• The binary linear position of the constituent with respect to the predicate,
either before orafter .
• The subcategorization of the predicate, the set of expected arguments that
appear in the verb phrase. We can extract this information by using the phrase-
structure rule that expands the immediate parent of the predicate; VP →VBD
NP PP for the predicate in Fig. 21.5.
• The named entity type of the constituent.

21.6 • S EMANTIC ROLE LABELING 487
• The ﬁrst words and the last word of the constituent.
The following feature vector thus represents the ﬁrst NP in our example (recall
that most observations will have the value NONE rather than, for example, ARG0,
since most constituents in the parse tree will not bear a semantic role):
ARG0: [issued, NP, Examiner, NNP, NP ↑S↓VP↓VBD, active, before, VP →NP PP,
ORG, The, Examiner]
Other features are often used in addition, such as sets of n-grams inside the
constituent, or more complex versions of the path features (the upward or downward
halves, or whether particular nodes occur in the path).
It’s also possible to use dependency parses instead of constituency parses as the
basis of features, for example using dependency parse paths instead of constituency
paths.
21.6.2 A Neural Algorithm for Semantic Role Labeling
A simple neural approach to SRL is to treat it as a sequence labeling task like named-
entity recognition, using the BIO approach. Let’s assume that we are given the
predicate and the task is just detecting and labeling spans. Recall that with BIO
tagging, we have a begin and end tag for each possible role ( B-ARG0,I-ARG0;B-
ARG1,I-ARG1, and so on), plus an outside tag O.
ENCODER[CLS]thecatslovehats[SEP]love[SEP]FFNB-ARG0I-ARG0B-PREDconcatenate with predicateB-ARG1FFNSoftmaxFFNFFNFFN
Figure 21.6 A simple neural approach to semantic role labeling. The input sentence is
followed by [SEP] and an extra input for the predicate, in this case love. The encoder outputs
are concatenated to an indicator variable which is 1 for the predicate and 0 for all other words
After He et al. (2017) and Shi and Lin (2019).
As with all the taggers, the goal is to compute the highest probability tag se-
quence ˆ y, given the input sequence of words w:
ˆy=argmax
y∈TP(y|w)
Fig. 21.6 shows a sketch of a standard algorithm from He et al. (2017). Here each
input word is mapped to pretrained embeddings, and then each token is concatenated
with the predicate embedding and then passed through a feedforward network with
a softmax which outputs a distribution over each SRL label. For decoding, a CRF
layer can be used instead of the MLP layer on top of the biLSTM output to do global
inference, but in practice this doesn’t seem to provide much beneﬁt.

488 CHAPTER 21 • S EMANTIC ROLE LABELING
21.6.3 Evaluation of Semantic Role Labeling
The standard evaluation for semantic role labeling is to require that each argument
label must be assigned to the exactly correct word sequence or parse constituent, and
then compute precision, recall, and F-measure. Identiﬁcation and classiﬁcation can
also be evaluated separately. Two common datasets used for evaluation are CoNLL-
2005 (Carreras and M `arquez, 2005) and CoNLL-2012 (Pradhan et al., 2013).
21.7 Selectional Restrictions
We turn in this section to another way to represent facts about the relationship be-
tween predicates and arguments. A selectional restriction is a semantic type con-selectional
restriction
straint that a verb imposes on the kind of concepts that are allowed to ﬁll its argument
roles. Consider the two meanings associated with the following example:
(21.29) I want to eat someplace nearby.
There are two possible parses and semantic interpretations for this sentence. In
the sensible interpretation, eatis intransitive and the phrase someplace nearby is
an adjunct that gives the location of the eating event. In the nonsensical speaker-as-
Godzilla interpretation, eatis transitive and the phrase someplace nearby is the direct
object and the THEME of the eating, like the NP Malaysian food in the following
sentences:
(21.30) I want to eat Malaysian food.
How do we know that someplace nearby isn’t the direct object in this sentence?
One useful cue is the semantic fact that the THEME of E ATING events tends to be
something that is edible . This restriction placed by the verb eaton the ﬁller of its
THEME argument is a selectional restriction.
Selectional restrictions are associated with senses, not entire lexemes. We can
see this in the following examples of the lexeme serve :
(21.31) The restaurant serves green-lipped mussels.
(21.32) Which airlines serve Denver?
Example (21.31) illustrates the offering-food sense of serve , which ordinarily re-
stricts its THEME to be some kind of food Example (21.32) illustrates the provides a
commercial service to sense of serve , which constrains its THEME to be some type
of appropriate location.
Selectional restrictions vary widely in their speciﬁcity. The verb imagine , for
example, imposes strict requirements on its AGENT role (restricting it to humans
and other animate entities) but places very few semantic requirements on its THEME
role. A verb like diagonalize , on the other hand, places a very speciﬁc constraint
on the ﬁller of its THEME role: it has to be a matrix, while the arguments of the
adjective odorless are restricted to concepts that could possess an odor:
(21.33) In rehearsal, I often ask the musicians to imagine a tennis game.
(21.34) Radon is an odorless gas that can’t be detected by human senses.
(21.35) To diagonalize a matrix is to ﬁnd its eigenvalues.
These examples illustrate that the set of concepts we need to represent selectional
restrictions (being a matrix, being able to possess an odor, etc) is quite open ended.
This distinguishes selectional restrictions from other features for representing lexical
knowledge, like parts-of-speech, which are quite limited in number.

21.7 • S ELECTIONAL RESTRICTIONS 489
21.7.1 Representing Selectional Restrictions
One way to capture the semantics of selectional restrictions is to use and extend the
event representation of Appendix F. Recall that the neo-Davidsonian representation
of an event consists of a single variable that stands for the event, a predicate denoting
the kind of event, and variables and relations for the event roles. Ignoring the issue of
theλ-structures and using thematic roles rather than deep event roles, the semantic
contribution of a verb like eatmight look like the following:
∃e,x,y Eating (e)∧Agent (e,x)∧T heme (e,y)
With this representation, all we know about y, the ﬁller of the THEME role, is that
it is associated with an Eating event through the Theme relation. To stipulate the
selectional restriction that ymust be something edible, we simply add a new term to
that effect:
∃e,x,y Eating (e)∧Agent (e,x)∧T heme (e,y)∧EdibleT hing (y)
When a phrase like ate a hamburger is encountered, a semantic analyzer can form
the following kind of representation:
∃e,x,y Eating (e)∧Eater (e,x)∧T heme (e,y)∧EdibleT hing (y)∧Hamburger (y)
This representation is perfectly reasonable since the membership of yin the category
Hamburger is consistent with its membership in the category EdibleThing , assuming
a reasonable set of facts in the knowledge base. Correspondingly, the representation
for a phrase such as ate a takeoff would be ill-formed because membership in an
event-like category such as Takeoff would be inconsistent with membership in the
category EdibleThing .
While this approach adequately captures the semantics of selectional restrictions,
there are two problems with its direct use. First, using FOL to perform the simple
task of enforcing selectional restrictions is overkill. Other, far simpler, formalisms
can do the job with far less computational cost. The second problem is that this
approach presupposes a large, logical knowledge base of facts about the concepts
that make up selectional restrictions. Unfortunately, although such common-sense
knowledge bases are being developed, none currently have the kind of coverage
necessary to the task.
A more practical approach is to state selectional restrictions in terms of WordNet
synsets rather than as logical concepts. Each predicate simply speciﬁes a WordNet
synset as the selectional restriction on each of its arguments. A meaning representa-
tion is well-formed if the role ﬁller word is a hyponym (subordinate) of this synset.
For our ate a hamburger example, for instance, we could set the selectional
restriction on the THEME role of the verb eatto the synset {food, nutrient }, glossed
asany substance that can be metabolized by an animal to give energy and build
tissue . Luckily, the chain of hypernyms for hamburger shown in Fig. 21.7 reveals
that hamburgers are indeed food. Again, the ﬁller of a role need not match the
restriction synset exactly; it just needs to have the synset as one of its superordinates.
We can apply this approach to the THEME roles of the verbs imagine ,lift, and di-
agonalize , discussed earlier. Let us restrict imagine ’sTHEME to the synset{entity},
lift’sTHEME to{physical entity}, and diagonalize to{matrix}. This arrangement
correctly permits imagine a hamburger andlift a hamburger , while also correctly
ruling out diagonalize a hamburger .

490 CHAPTER 21 • S EMANTIC ROLE LABELING
Sense 1
hamburger, beefburger --
(a fried cake of minced beef served on a bun)
=> sandwich
=> snack food
=> dish
=> nutriment, nourishment, nutrition...
=> food, nutrient
=> substance
=> matter
=> physical entity
=> entity
Figure 21.7 Evidence from WordNet that hamburgers are edible.
21.7.2 Selectional Preferences
In the earliest implementations, selectional restrictions were considered strict con-
straints on the kind of arguments a predicate could take (Katz and Fodor 1963,
Hirst 1987). For example, the verb eatmight require that its THEME argument be
[+FOOD] . Early word sense disambiguation systems used this idea to rule out senses
that violated the selectional restrictions of their governing predicates.
Very quickly, however, it became clear that these selectional restrictions were
better represented as preferences rather than strict constraints (Wilks 1975b, Wilks
1975a). For example, selectional restriction violations (like inedible arguments of
eat) often occur in well-formed sentences, for example because they are negated
(21.36), or because selectional restrictions are overstated (21.37):
(21.36) But it fell apart in 1931, perhaps because people realized you can’t eat
gold for lunch if you’re hungry.
(21.37) In his two championship trials, Mr. Kulkarni ateglass on an empty
stomach, accompanied only by water and tea.
Modern systems for selectional preferences therefore specify the relation be-
tween a predicate and its possible arguments with soft constraints of some kind.
Selectional Association
One of the most inﬂuential has been the selectional association model of Resnik
(1993). Resnik deﬁnes the idea of selectional preference strength as the generalselectional
preference
strengthamount of information that a predicate tells us about the semantic class of its argu-
ments. For example, the verb eattells us a lot about the semantic class of its direct
objects, since they tend to be edible. The verb be, by contrast, tells us less about
its direct objects. The selectional preference strength can be deﬁned by the differ-
ence in information between two distributions: the distribution of expected semantic
classes P(c)(how likely is it that a direct object will fall into class c) and the dis-
tribution of expected semantic classes for the particular verb P(c|v)(how likely is
it that the direct object of the speciﬁc verb vwill fall into semantic class c). The
greater the difference between these distributions, the more information the verb
is giving us about possible objects. The difference between these two distributions
can be quantiﬁed by relative entropy , or the Kullback-Leibler divergence (Kullback relative entropy
and Leibler, 1951). The Kullback-Leibler or KL divergence D(P||Q)expresses the KL divergence

21.7 • S ELECTIONAL RESTRICTIONS 491
difference between two probability distributions PandQ
D(P||Q) =∑
xP(x)logP(x)
Q(x)(21.38)
The selectional preference SR(v)uses the KL divergence to express how much in-
formation, in bits, the verb vexpresses about the possible semantic class of its argu-
ment.
SR(v) = D(P(c|v)||P(c))
=∑
cP(c|v)logP(c|v)
P(c)(21.39)
Resnik then deﬁnes the selectional association of a particular class and verb as theselectional
association
relative contribution of that class to the general selectional preference of the verb:
AR(v,c) =1
SR(v)P(c|v)logP(c|v)
P(c)(21.40)
The selectional association is thus a probabilistic measure of the strength of asso-
ciation between a predicate and a class dominating the argument to the predicate.
Resnik estimates the probabilities for these associations by parsing a corpus, count-
ing all the times each predicate occurs with each argument word, and assuming
that each word is a partial observation of all the WordNet concepts containing the
word. The following table from Resnik (1996) shows some sample high and low
selectional associations for verbs and some WordNet semantic classes of their direct
objects.
Direct Object Direct Object
Verb Semantic Class Assoc Semantic Class Assoc
read WRITING 6.80 ACTIVITY -.20
write WRITING 7.26 COMMERCE 0
see ENTITY 5.79 METHOD -0.01
Selectional Preference via Conditional Probability
An alternative to using selectional association between a verb and the WordNet class
of its arguments is to use the conditional probability of an argument word given a
predicate verb, directly modeling the strength of association of one verb (predicate)
with one noun (argument).
The conditional probability model can be computed by parsing a very large cor-
pus (billions of words), and computing co-occurrence counts: how often a given
verb occurs with a given noun in a given relation. The conditional probability of an
argument noun given a verb for a particular relation P(n|v,r)can then be used as a
selectional preference metric for that pair of words (Brockmann and Lapata 2003,
Keller and Lapata 2003):
P(n|v,r) ={
C(n,v,r)
C(v,r)ifC(n,v,r)>0
0 otherwise
The inverse probability P(v|n,r)was found to have better performance in some cases
(Brockmann and Lapata, 2003):
P(v|n,r) ={
C(n,v,r)
C(n,r)ifC(n,v,r)>0
0 otherwise

492 CHAPTER 21 • S EMANTIC ROLE LABELING
An even simpler approach is to use the simple log co-occurrence frequency of
the predicate with the argument log count (v,n,r)instead of conditional probability;
this seems to do better for extracting preferences for syntactic subjects rather than
objects (Brockmann and Lapata, 2003).
Evaluating Selectional Preferences
One way to evaluate models of selectional preferences is to use pseudowords (Gale pseudowords
et al. 1992b, Sch ¨utze 1992a). A pseudoword is an artiﬁcial word created by concate-
nating a test word in some context (say banana ) with a confounder word (say door )
to create banana-door ). The task of the system is to identify which of the two words
is the original word. To evaluate a selectional preference model (for example on the
relationship between a verb and a direct object) we take a test corpus and select all
verb tokens. For each verb token (say drive ) we select the direct object (e.g., car),
concatenated with a confounder word that is its nearest neighbor , the noun with the
frequency closest to the original (say house ), to make car/house ). We then use the
selectional preference model to choose which of carandhouse are more preferred
objects of drive , and compute how often the model chooses the correct original ob-
ject (e.g., car) (Chambers and Jurafsky, 2010).
Another evaluation metric is to get human preferences for a test set of verb-
argument pairs, and have them rate their degree of plausibility. This is usually done
by using magnitude estimation, a technique from psychophysics, in which subjects
rate the plausibility of an argument proportional to a modulus item. A selectional
preference model can then be evaluated by its correlation with the human prefer-
ences (Keller and Lapata, 2003).
21.8 Primitive Decomposition of Predicates
One way of thinking about the semantic roles we have discussed through the chapter
is that they help us deﬁne the roles that arguments play in a decompositional way,
based on ﬁnite lists of thematic roles (agent, patient, instrument, proto-agent, proto-
patient, etc.). This idea of decomposing meaning into sets of primitive semantic
elements or features, called primitive decomposition orcomponential analysis ,componential
analysis
has been taken even further, and focused particularly on predicates.
Consider these examples of the verb kill:
(21.41) Jim killed his philodendron.
(21.42) Jim did something to cause his philodendron to become not alive.
There is a truth-conditional (‘propositional semantics’) perspective from which these
two sentences have the same meaning. Assuming this equivalence, we could repre-
sent the meaning of killas:
(21.43) KILL (x,y)⇔CAUSE (x,BECOME (NOT(ALIVE (y))))
thus using semantic primitives like do,cause ,become not , and alive .
Indeed, one such set of potential semantic primitives has been used to account
for some of the verbal alternations discussed in Section 21.2 (Lakoff 1965, Dowty
1979). Consider the following examples.
(21.44) John opened the door. ⇒CAUSE (John, BECOME (OPEN (door)))
(21.45) The door opened. ⇒BECOME (OPEN (door))

21.9 • S UMMARY 493
(21.46) The door is open. ⇒OPEN (door)
The decompositional approach asserts that a single state-like predicate associ-
ated with open underlies all of these examples. The differences among the meanings
of these examples arises from the combination of this single predicate with the prim-
itives CAUSE and BECOME .
While this approach to primitive decomposition can explain the similarity be-
tween states and actions or causative and non-causative predicates, it still relies on
having a large number of predicates like open . More radical approaches choose to
break down these predicates as well. One such approach to verbal predicate decom-
position that played a role in early natural language systems is conceptual depen-
dency (CD), a set of ten primitive predicates, shown in Fig. 21.8.conceptual
dependency
Primitive Deﬁnition
ATRANS The abstract transfer of possession or control from one entity to
another
PTRANS The physical transfer of an object from one location to another
MTRANS The transfer of mental concepts between entities or within an
entity
MBUILD The creation of new information within an entity
PROPEL The application of physical force to move an object
MOVE The integral movement of a body part by an animal
INGEST The taking in of a substance by an animal
EXPEL The expulsion of something from an animal
SPEAK The action of producing a sound
ATTEND The action of focusing a sense organ
Figure 21.8 A set of conceptual dependency primitives.
Below is an example sentence along with its CDrepresentation. The verb brought
is translated into the two primitives ATRANS and PTRANS to indicate that the waiter
both physically conveyed the check to Mary and passed control of it to her. Note
that CDalso associates a ﬁxed set of thematic roles with each primitive to represent
the various participants in the action.
(21.47) The waiter brought Mary the check.
∃x,y Atrans (x)∧Actor (x,Waiter )∧Ob ject (x,Check )∧To(x,Mary )
∧Ptrans (y)∧Actor (y,Waiter )∧Ob ject (y,Check )∧To(y,Mary )
21.9 Summary
•Semantic roles are abstract models of the role an argument plays in the event
described by the predicate.
•Thematic roles are a model of semantic roles based on a single ﬁnite list of
roles. Other semantic role models include per-verb semantic role lists and
proto-agent /proto-patient , both of which are implemented in PropBank ,
and per-frame role lists, implemented in FrameNet .

494 CHAPTER 21 • S EMANTIC ROLE LABELING
•Semantic role labeling is the task of assigning semantic role labels to the
constituents of a sentence. The task is generally treated as a supervised ma-
chine learning task, with models trained on PropBank or FrameNet. Algo-
rithms generally start by parsing a sentence and then automatically tag each
parse tree node with a semantic role. Neural models map straight from words
end-to-end.
• Semantic selectional restrictions allow words (particularly predicates) to post
constraints on the semantic properties of their argument words. Selectional
preference models (like selectional association or simple conditional proba-
bility) allow a weight or probability to be assigned to the association between
a predicate and an argument word or class.
Historical Notes
Although the idea of semantic roles dates back to P ¯an.ini, they were re-introduced
into modern linguistics by Gruber (1965), Fillmore (1966) and Fillmore (1968). Fill-
more had become interested in argument structure by studying Lucien Tesni `ere’s
groundbreaking ´El´ements de Syntaxe Structurale (Tesni `ere, 1959) in which the term
‘dependency’ was introduced and the foundations were laid for dependency gram-
mar. Following Tesni `ere’s terminology, Fillmore ﬁrst referred to argument roles as
actants (Fillmore, 1966) but quickly switched to the term case, (see Fillmore (2003))
and proposed a universal list of semantic roles or cases (Agent, Patient, Instrument,
etc.), that could be taken on by the arguments of predicates. Verbs would be listed in
the lexicon with their case frame , the list of obligatory (or optional) case arguments.
The idea that semantic roles could provide an intermediate level of semantic
representation that could help map from syntactic parse structures to deeper, more
fully-speciﬁed representations of meaning was quickly adopted in natural language
processing, and systems for extracting case frames were created for machine transla-
tion (Wilks, 1973), question-answering (Hendrix et al., 1973), spoken-language pro-
cessing (Nash-Webber, 1975), and dialogue systems (Bobrow et al., 1977). General-
purpose semantic role labelers were developed. The earliest ones (Simmons, 1973)
ﬁrst parsed a sentence by means of an ATN (Augmented Transition Network) parser.
Each verb then had a set of rules specifying how the parse should be mapped to se-
mantic roles. These rules mainly made reference to grammatical functions (subject,
object, complement of speciﬁc prepositions) but also checked constituent internal
features such as the animacy of head nouns. Later systems assigned roles from pre-
built parse trees, again by using dictionaries with verb-speciﬁc case frames (Levin
1977, Marcus 1980).
By 1977 case representation was widely used and taught in AI and NLP courses,
and was described as a standard of natural language processing in the ﬁrst edition of
Winston’s 1977 textbook Artiﬁcial Intelligence .
In the 1980s Fillmore proposed his model of frame semantics , later describing
the intuition as follows:
“The idea behind frame semantics is that speakers are aware of possi-
bly quite complex situation types, packages of connected expectations,
that go by various names—frames, schemas, scenarios, scripts, cultural
narratives, memes—and the words in our language are understood with
such frames as their presupposed background.” (Fillmore, 2012, p. 712)

HISTORICAL NOTES 495
The word frame seemed to be in the air for a suite of related notions proposed at
about the same time by Minsky (1974), Hymes (1974), and Goffman (1974), as
well as related notions with other names like scripts (Schank and Abelson, 1975)
andschemata (Bobrow and Norman, 1975) (see Tannen (1979) for a comparison).
Fillmore was also inﬂuenced by the semantic ﬁeld theorists and by a visit to the Yale
AI lab where he took notice of the lists of slots and ﬁllers used by early information
extraction systems like DeJong (1982) and Schank and Abelson (1977). In the 1990s
Fillmore drew on these insights to begin the FrameNet corpus annotation project.
At the same time, Beth Levin drew on her early case frame dictionaries (Levin,
1977) to develop her book which summarized sets of verb classes deﬁned by shared
argument realizations (Levin, 1993). The VerbNet project built on this work (Kipper
et al., 2000), leading soon afterwards to the PropBank semantic-role-labeled corpus
created by Martha Palmer and colleagues (Palmer et al., 2005).
The combination of rich linguistic annotation and corpus-based approach in-
stantiated in FrameNet and PropBank led to a revival of automatic approaches to
semantic role labeling, ﬁrst on FrameNet (Gildea and Jurafsky, 2000) and then on
PropBank data (Gildea and Palmer, 2002, inter alia). The problem ﬁrst addressed in
the 1970s by handwritten rules was thus now generally recast as one of supervised
machine learning enabled by large and consistent databases. Many popular features
used for role labeling are deﬁned in Gildea and Jurafsky (2002), Surdeanu et al.
(2003), Xue and Palmer (2004), Pradhan et al. (2005), Che et al. (2009), and Zhao
et al. (2009). The use of dependency rather than constituency parses was introduced
in the CoNLL-2008 shared task (Surdeanu et al., 2008). For surveys see Palmer
et al. (2010) and M `arquez et al. (2008).
The use of neural approaches to semantic role labeling was pioneered by Col-
lobert et al. (2011), who applied a CRF on top of a convolutional net. Early work
like Foland, Jr. and Martin (2015) focused on using dependency features. Later work
eschewed syntactic features altogether; Zhou and Xu (2015b) introduced the use of
a stacked (6-8 layer) biLSTM architecture, and (He et al., 2017) showed how to
augment the biLSTM architecture with highway networks and also replace the CRF
with A* decoding that make it possible to apply a wide variety of global constraints
in SRL decoding.
Most semantic role labeling schemes only work within a single sentence, fo-
cusing on the object of the verbal (or nominal, in the case of NomBank) predicate.
However, in many cases, a verbal or nominal predicate may have an implicit argu-
ment : one that appears only in a contextual sentence, or perhaps not at all and mustimplicit
argument
be inferred. In the two sentences This house has a new owner. The sale was ﬁnalized
10 days ago. thesale in the second sentence has no A RG1, but a reasonable reader
would infer that the Arg1 should be the house mentioned in the prior sentence. Find-
ing these arguments, implicit argument detection (sometimes shortened as iSRL ) iSRL
was introduced by Gerber and Chai (2010) and Ruppenhofer et al. (2010). See Do
et al. (2017) for more recent neural models.
To avoid the need for huge labeled training sets, unsupervised approaches for
semantic role labeling attempt to induce the set of semantic roles by clustering over
arguments. The task was pioneered by Riloff and Schmelzenbach (1998) and Swier
and Stevenson (2004); see Grenager and Manning (2006), Titov and Klementiev
(2012), Lang and Lapata (2014), Woodsend and Lapata (2015), and Titov and Khod-
dam (2014).
Recent innovations in frame labeling include connotation frames , which mark
richer information about the argument of predicates. Connotation frames mark the

496 CHAPTER 21 • S EMANTIC ROLE LABELING
sentiment of the writer or reader toward the arguments (for example using the verb
survive inhe survived a bombing expresses the writer’s sympathy toward the subject
heand negative sentiment toward the bombing. See Chapter 22 for more details.
Selectional preference has been widely studied beyond the selectional associa-
tion models of Resnik (1993) and Resnik (1996). Methods have included clustering
(Rooth et al., 1999), discriminative learning (Bergsma et al., 2008a), and topic mod-
els (S ´eaghdha 2010, Ritter et al. 2010), and constraints can be expressed at the level
of words or classes (Agirre and Martinez, 2001). Selectional preferences have also
been successfully integrated into semantic role labeling (Erk 2007, Zapirain et al.
2013, Do et al. 2017).
Exercises

