CHAPTER
23Coreference Resolution and
Entity Linking
and even Stigand, the patriotic archbishop of Canterbury, found it advisable–”’
‘Found WHAT?’ said the Duck.
‘Found IT, ’ the Mouse replied rather crossly: ‘of course you know what “it”means. ’
‘I know what “it”means well enough, when I ﬁnd a thing, ’ said the Duck: ‘it’s gener-
ally a frog or a worm. The question is, what did the archbishop ﬁnd?’
Lewis Carroll, Alice in Wonderland
An important component of language processing is knowing who is being talked
about in a text. Consider the following passage:
(23.1) Victoria Chen , CFO of Megabucks Banking, saw her pay jump to $2.3
million, as the 38-year-old became the company’s president. It is widely
known that she came to Megabucks from rival Lotsabucks.
Each of the underlined phrases in this passage is used by the writer to refer to
a person named Victoria Chen. We call linguistic expressions like herorVictoria
Chen mentions orreferring expressions , and the discourse entity that is referred mention
to (Victoria Chen) the referent . (To distinguish between referring expressions and referent
their referents, we italicize the former.)1Two or more referring expressions that are
used to refer to the same discourse entity are said to corefer ; thus, Victoria Chen corefer
andshecorefer in (23.1).
Coreference is an important component of natural language processing. A dia-
logue system that has just told the user “There is a 2pm ﬂight on United and a 4pm
one on Cathay Paciﬁc” must know which ﬂight the user means by “I’ll take the sec-
ond one” . A question answering system that uses Wikipedia to answer a question
about Marie Curie must know who shewas in the sentence “She was born in War-
saw” . And a machine translation system translating from a language like Spanish, in
which pronouns can be dropped, must use coreference from the previous sentence to
decide whether the Spanish sentence ‘ “Me encanta el conocimiento”, dice. ’ should
be translated as ‘ “I love knowledge”, he says ’, or ‘ “I love knowledge”, she says ’.
Indeed, this example comes from an actual news article in El Pa ´ısabout a female
professor and was mistranslated as “he” in machine translation because of inaccurate
coreference resolution (Schiebinger, 2013).
Natural language processing systems (and humans) interpret linguistic expres-
sions with respect to a discourse model (Karttunen, 1969). A discourse modeldiscourse
model
(Fig. 23.1) is a mental model that the understander builds incrementally when in-
terpreting a text, containing representations of the entities referred to in the text,
as well as properties of the entities and relations among them. When a referent is
ﬁrst mentioned in a discourse, we say that a representation for it is evoked into the evoked
model. Upon subsequent mention, this representation is accessed from the model. accessed
1As a convenient shorthand, we sometimes speak of a referring expression referring to a referent, e.g.,
saying that sherefers to Victoria Chen. However, the reader should keep in mind that what we really
mean is that the speaker is performing the act of referring to Victoria Chen by uttering she.

518 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
VDiscourse Model“Victoria”“she”coreferrefer (evoke)refer (access)$LotsabucksMegabuckspay
Figure 23.1 How mentions evoke and access discourse entities in a discourse model.
Reference in a text to an entity that has been previously introduced into the
discourse is called anaphora , and the referring expression used is said to be an anaphora
anaphor , or anaphoric.2In passage (23.1), the pronouns sheandherand the deﬁ- anaphor
nite NP the 38-year-old are therefore anaphoric. The anaphor corefers with a prior
mention (in this case Victoria Chen ) that is called the antecedent . Not every refer- antecedent
ring expression is an antecedent. An entity that has only a single mention in a text
(like Lotsabucks in (23.1)) is called a singleton . singleton
In this chapter we focus on the task of coreference resolution . Coreferencecoreference
resolution
resolution is the task of determining whether two mentions corefer , by which we
mean they refer to the same entity in the discourse model (the same discourse entity ).
The set of coreferring expressions is often called a coreference chain or acluster .coreference
chain
cluster For example, in processing (23.1), a coreference resolution algorithm would need
to ﬁnd at least four coreference chains, corresponding to the four entities in the
discourse model in Fig. 23.1.
1.{Victoria Chen ,her,the 38-year-old ,She}
2.{Megabucks Banking ,the company ,Megabucks}
3.{her pay}
4.{Lotsabucks}
Note that mentions can be nested; for example the mention heris syntactically
part of another mention, her pay , referring to a completely different discourse entity.
Coreference resolution thus comprises two tasks (although they are often per-
formed jointly): (1) identifying the mentions, and (2) clustering them into corefer-
ence chains/discourse entities.
We said that two mentions corefered if they are associated with the same dis-
course entity . But often we’d like to go further, deciding which real world entity is
associated with this discourse entity. For example, the mention Washington might
refer to the US state, or the capital city, or the person George Washington; the inter-
pretation of the sentence will of course be very different for each of these. The task
ofentity linking (Ji and Grishman, 2011) or entity resolution is the task of mapping entity linking
a discourse entity to some real-world individual.3We usually operationalize entity
2We will follow the common NLP usage of anaphor to mean any mention that has an antecedent, rather
than the more narrow usage to mean only mentions (like pronouns) whose interpretation depends on the
antecedent (under the narrower interpretation, repeated names are not anaphors).
3Computational linguistics/NLP thus differs in its use of the term reference from the ﬁeld of formal
semantics, which uses the words reference andcoreference to describe the relation between a mention
and a real-world entity. By contrast, we follow the functional linguistics tradition in which a mention
refers to adiscourse entity (Webber, 1978) and the relation between a discourse entity and the real world
individual requires an additional step of linking .

519
linking or resolution by mapping to an ontology : a list of entities in the world, like
a gazeteer (Appendix F). Perhaps the most common ontology used for this task is
Wikipedia; each Wikipedia page acts as the unique id for a particular entity. Thus
the entity linking task of wikiﬁcation (Mihalcea and Csomai, 2007) is the task of de-
ciding which Wikipedia page corresponding to an individual is being referred to by
a mention. But entity linking can be done with any ontology; for example if we have
an ontology of genes, we can link mentions of genes in text to the disambiguated
gene name in the ontology.
In the next sections we introduce the task of coreference resolution in more de-
tail, and survey a variety of architectures for resolution. We also introduce two
architectures for the task of entity linking.
Before turning to algorithms, however, we mention some important tasks we
will only touch on brieﬂy at the end of this chapter. First are the famous Winograd
Schema problems (so-called because they were ﬁrst pointed out by Terry Winograd
in his dissertation). These entity coreference resolution problems are designed to be
too difﬁcult to be solved by the resolution methods we describe in this chapter, and
the kind of real-world knowledge they require has made them a kind of challenge
task for natural language processing. For example, consider the task of determining
the correct antecedent of the pronoun they in the following example:
(23.2) The city council denied the demonstrators a permit because
a. they feared violence.
b. they advocated violence.
Determining the correct antecedent for the pronoun they requires understanding
that the second clause is intended as an explanation of the ﬁrst clause, and also
that city councils are perhaps more likely than demonstrators to fear violence and
that demonstrators might be more likely to advocate violence. Solving Winograd
Schema problems requires ﬁnding way to represent or discover the necessary real
world knowledge.
A problem we won’t discuss in this chapter is the related task of event corefer-
ence , deciding whether two event mentions (such as the buyand the acquisition inevent
coreference
these two sentences from the ECB+ corpus) refer to the same event:
(23.3) AMD agreed to [ buy] Markham, Ontario-based ATI for around $5.4 billion
in cash and stock, the companies announced Monday.
(23.4) The [ acquisition ] would turn AMD into one of the world’s largest providers
of graphics chips.
Event mentions are much harder to detect than entity mentions, since they can be ver-
bal as well as nominal. Once detected, the same mention-pair and mention-ranking
models used for entities are often applied to events.
An even more complex kind of coreference is discourse deixis (Webber, 1988), discourse deixis
in which an anaphor refers back to a discourse segment, which can be quite hard to
delimit or categorize, like the examples in (23.5) adapted from Webber (1991):
(23.5) According to Soleil, Beau just opened a restaurant
a. But thatturned out to be a lie.
b. But thatwas false.
c.That struck me as a funny way to describe the situation.
The referent of that is a speech act (see Chapter 25) in (23.5a), a proposition in
(23.5b), and a manner of description in (23.5c). We don’t give algorithms in this
chapter for these difﬁcult types of non-nominal antecedents , but see Kolhatkar
et al. (2018) for a survey.

520 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
23.1 Coreference Phenomena: Linguistic Background
We now offer some linguistic background on reference phenomena. We introduce
the four types of referring expressions (deﬁnite and indeﬁnite NPs, pronouns, and
names), describe how these are used to evoke and access entities in the discourse
model, and talk about linguistic features of the anaphor/antecedent relation (like
number/gender agreement, or properties of verb semantics).
23.1.1 Types of Referring Expressions
Indeﬁnite Noun Phrases: The most common form of indeﬁnite reference in En-
glish is marked with the determiner a(oran), but it can also be marked by a quan-
tiﬁer such as some or even the determiner this. Indeﬁnite reference generally intro-
duces into the discourse context entities that are new to the hearer.
(23.6) a. Mrs. Martin was so very kind as to send Mrs. Goddard a beautiful goose .
b. He had gone round one day to bring her some walnuts .
c. I saw this beautiful cauliﬂower today.
Deﬁnite Noun Phrases: Deﬁnite reference, such as via NPs that use the English
article the, refers to an entity that is identiﬁable to the hearer. An entity can be
identiﬁable to the hearer because it has been mentioned previously in the text and
thus is already represented in the discourse model:
(23.7) It concerns a white stallion which I have sold to an ofﬁcer. But the pedigree
ofthe white stallion was not fully established.
Alternatively, an entity can be identiﬁable because it is contained in the hearer’s
set of beliefs about the world, or the uniqueness of the object is implied by the
description itself, in which case it evokes a representation of the referent into the
discourse model, as in (23.9):
(23.8) I read about it in the New York Times .
(23.9) Have you seen the car keys?
These last uses are quite common; more than half of deﬁnite NPs in newswire
texts are non-anaphoric, often because they are the ﬁrst time an entity is mentioned
(Poesio and Vieira 1998, Bean and Riloff 1999).
Pronouns: Another form of deﬁnite reference is pronominalization, used for enti-
ties that are extremely salient in the discourse, (as we discuss below):
(23.10) Emma smiled and chatted as cheerfully as shecould,
Pronouns can also participate in cataphora , in which they are mentioned before cataphora
their referents are, as in (23.11).
(23.11) Even before shesawit, Dorothy had been thinking about the Emerald City
every day.
Here, the pronouns sheanditboth occur before their referents are introduced.
Pronouns also appear in quantiﬁed contexts in which they are considered to be
bound , as in (23.12). bound
(23.12) Every dancer brought herleft arm forward.
Under the relevant reading, herdoes not refer to some woman in context, but instead
behaves like a variable bound to the quantiﬁed expression every dancer . We are not
concerned with the bound interpretation of pronouns in this chapter.

23.1 • C OREFERENCE PHENOMENA : LINGUISTIC BACKGROUND 521
In some languages, pronouns can appear as clitics attached to a word, like lo
(‘it’) in this Spanish example from AnCora (Recasens and Mart ´ı, 2010):
(23.13) La intenci ´on es reconocer el gran prestigio que tiene la marat ´on y unir lo
con esta gran carrera.
‘The aim is to recognize the great prestige that the Marathon has and join |it
with this great race.”
Demonstrative Pronouns: Demonstrative pronouns thisandthat can appear ei-
ther alone or as determiners, for instance, this ingredient ,that spice :
(23.14) I just bought a copy of Thoreau’s Walden . I had bought one ﬁve years ago.
That one had been very tattered; this one was in much better condition.
Note that this NP is ambiguous; in colloquial spoken English, it can be indeﬁnite,
as in (23.6), or deﬁnite, as in (23.14).
Zero Anaphora: Instead of using a pronoun, in some languages (including Chi-
nese, Japanese, and Italian) it is possible to have an anaphor that has no lexical
realization at all, called a zero anaphor or zero pronoun, as in the following Italian zero anaphor
and Japanese examples from Poesio et al. (2016):
(23.15) EN [John] iwent to visit some friends. On the way [he] ibought some
wine.
IT [Giovanni] iand`o a far visita a degli amici. Per via φicompr `o del vino.
JA [John] i-wa yujin-o houmon-sita. Tochu-de φiwain-o ka-tta.
or this Chinese example:
(23.16) [ 我]前一会精神上太紧张。[0]现在比较平静了
[I] was too nervous a while ago. ... [0] am now calmer.
Zero anaphors complicate the task of mention detection in these languages.
Names: Names (such as of people, locations, or organizations) can be used to refer
to both new and old entities in the discourse:
(23.17) a. Miss Woodhouse certainly had not done him justice.
b.International Business Machines sought patent compensation
from Amazon; IBM had previously sued other companies.
23.1.2 Information Status
The way referring expressions are used to evoke new referents into the discourse
(introducing new information), or access old entities from the model (old informa-
tion), is called their information status orinformation structure . Entities can beinformation
status
discourse-new ordiscourse-old , and indeed it is common to distinguish at least discourse-new
discourse-old three kinds of entities informationally (Prince, 1981):
new NPs:
brand new NPs: these introduce entities that are discourse-new and hearer-
new like a fruit orsome walnuts .
unused NPs: these introduce entities that are discourse-new but hearer-old
(like Hong Kong ,Marie Curie , orthe New York Times .
old NPs: also called evoked NPs , these introduce entities that already in the dis-
course model, hence are both discourse-old and hearer-old, like itin “I went
to a new restaurant. It was... ”.

522 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
inferrables: these introduce entities that are neither hearer-old nor discourse-old,
but the hearer can infer their existence by reasoning based on other entities
that are in the discourse. Consider the following examples:
(23.18) I went to a superb restaurant yesterday. The chef had just opened it.
(23.19) Mix ﬂour, butter and water. Knead the dough until shiny.
Neither the chef northe dough were in the discourse model based on the ﬁrst
sentence of either example, but the reader can make a bridging inferencebridging
inference
that these entities should be added to the discourse model and associated with
the restaurant and the ingredients, based on world knowledge that restaurants
have chefs and dough is the result of mixing ﬂour and liquid (Haviland and
Clark 1974, Webber and Baldwin 1992, Nissim et al. 2004, Hou et al. 2018).
The form of an NP gives strong clues to its information status. We often talk
about an entity’s position on the given-new dimension, the extent to which the refer- given-new
ent is given (salient in the discourse, easier for the hearer to call to mind, predictable
by the hearer), versus new (non-salient in the discourse, unpredictable) (Chafe 1976,
Prince 1981, Gundel et al. 1993). A referent that is very accessible (Ariel, 2001) accessible
i.e., very salient in the hearer’s mind or easy to call to mind, can be referred to with
less linguistic material. For example pronouns are used only when the referent has
a high degree of activation or salience in the discourse model.4By contrast, less salience
salient entities, like a new referent being introduced to the discourse, will need to be
introduced with a longer and more explicit referring expression to help the hearer
recover the referent.
Thus when an entity is ﬁrst introduced into a discourse its mentions are likely
to have full names, titles or roles, or appositive or restrictive relative clauses, as in
the introduction of our protagonist in (23.1): Victoria Chen, CFO of Megabucks
Banking . As an entity is discussed over a discourse, it becomes more salient to the
hearer and its mentions on average typically becomes shorter and less informative,
for example with a shortened name (for example Ms. Chen ), a deﬁnite description
(the 38-year-old ), or a pronoun ( sheorher) (Hawkins 1978). However, this change
in length is not monotonic, and is sensitive to discourse structure (Grosz 1977b,
Reichman 1985, Fox 1993).
23.1.3 Complications: Non-Referring Expressions
Many noun phrases or other nominals are not referring expressions, although they
may bear a confusing superﬁcial resemblance. For example in some of the earliest
computational work on reference resolution, Karttunen (1969) pointed out that the
NPa car in the following example does not create a discourse referent:
(23.20) Janet doesn’t have a car .
and cannot be referred back to by anaphoric itorthe car :
(23.21) * Itis a Toyota.
(23.22) * The car is red.
We summarize here four common types of structures that are not counted as men-
tions in coreference tasks and hence complicate the task of mention-detection:
4Pronouns also usually (but not always) refer to entities that were introduced no further than one or two
sentences back in the ongoing discourse, whereas deﬁnite noun phrases can often refer further back.

23.1 • C OREFERENCE PHENOMENA : LINGUISTIC BACKGROUND 523
Appositives: An appositional structure is a noun phrase that appears next to a
head noun phrase, describing the head. In English they often appear in commas, like
“a unit of UAL” appearing in apposition to the NP United , orCFO of Megabucks
Banking in apposition to Victoria Chen .
(23.23) Victoria Chen, CFO of Megabucks Banking, saw ...
(23.24) United, a unit of UAL, matched the fares.
Appositional NPs are not referring expressions, instead functioning as a kind of
supplementary parenthetical description of the head NP. Nonetheless, sometimes it
is useful to link these phrases to an entity they describe, and so some datasets like
OntoNotes mark appositional relationships.
Predicative and Prenominal NPs: Predicative or attributive NPs describe prop-
erties of the head noun. In United is a unit of UAL , the NP a unit of UAL describes
a property of United, rather than referring to a distinct entity. Thus they are not
marked as mentions in coreference tasks; in our example the NPs $2.3 million and
the company’s president , are attributive, describing properties of her pay andthe
38-year-old ; Example (23.27) shows a Chinese example in which the predicate NP
(中国最大的城市; China’s biggest city ) is not a mention.
(23.25) her pay jumped to $2.3 million
(23.26) the 38-year-old became the company’s president
(23.27)上海是[中国最大的城市] [Shanghai is China’s biggest city ]
Expletives: Many uses of pronouns like itin English and corresponding pronouns
in other languages are not referential. Such expletive orpleonastic cases include expletive
it is raining , in idioms like hit it off , or in particular syntactic situations like clefts clefts
(23.28a) or extraposition (23.28b):
(23.28) a. Itwas Emma Goldman who founded Mother Earth
b.Itsurprised me that there was a herring hanging on her wall.
Generics: Another kind of expression that does not refer back to an entity explic-
itly evoked in the text is generic reference. Consider (23.29).
(23.29) I love mangos. They are very tasty.
Here, they refers, not to a particular mango or set of mangos, but instead to the class
of mangos in general. The pronoun youcan also be used generically:
(23.30) In July in San Francisco youhave to wear a jacket.
23.1.4 Linguistic Properties of the Coreference Relation
Now that we have seen the linguistic properties of individual referring expressions
we turn to properties of the antecedent/anaphor pair. Understanding these properties
is helpful both in designing novel features and performing error analyses.
Number Agreement: Referring expressions and their referents must generally
agree in number; English she/her/he/him/his/it are singular, we/us/they/them are plu-
ral, and youis unspeciﬁed for number. So a plural antecedent like the chefs cannot
generally corefer with a singular anaphor like she. However, algorithms cannot
enforce number agreement too strictly. First, semantically plural entities can be re-
ferred to by either itorthey:
(23.31) IBM announced a new machine translation product yesterday. They have
been working on it for 20 years.

524 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
Second, singular they has become much more common, in which they is used to singular they
describe singular individuals, often useful because they is gender neutral. Although
recently increasing, singular they is quite old, part of English for many centuries.5
Person Agreement: English distinguishes between ﬁrst, second, and third person,
and a pronoun’s antecedent must agree with the pronoun in person. Thus a third
person pronoun ( he, she, they, him, her, them, his, her, their ) must have a third person
antecedent (one of the above or any other noun phrase). However, phenomena like
quotation can cause exceptions; in this example I,my, and sheare coreferent:
(23.32) “I voted for Nader because he was most aligned with my values,” she said.
Gender or Noun Class Agreement: In many languages, all nouns have grammat-
ical gender or noun class6and pronouns generally agree with the grammatical gender
of their antecedent. In English this occurs only with third-person singular pronouns,
which distinguish between male (he, him, his ),female (she, her ), and nonpersonal
(it) grammatical genders. Non-binary pronouns like zeorhirmay also occur in more
recent texts. Knowing which gender to associate with a name in text can be complex,
and may require world knowledge about the individual. Some examples:
(23.33) Maryam has a theorem. She is exciting. (she=Maryam, not the theorem)
(23.34) Maryam has a theorem. It is exciting. (it=the theorem, not Maryam)
Binding Theory Constraints: The binding theory is a name for syntactic con-
straints on the relations between a mention and an antecedent in the same sentence
(Chomsky, 1981). Oversimplifying a bit, reﬂexive pronouns like himself andher- reﬂexive
selfcorefer with the subject of the most immediate clause that contains them (23.35),
whereas nonreﬂexives cannot corefer with this subject (23.36).
(23.35) Janet bought herself a bottle of ﬁsh sauce. [herself =Janet]
(23.36) Janet bought her a bottle of ﬁsh sauce. [her ̸=Janet]
Recency: Entities introduced in recent utterances tend to be more salient than
those introduced from utterances further back. Thus, in (23.37), the pronoun itis
more likely to refer to Jim’s map than the doctor’s map.
(23.37) The doctor found an old map in the captain’s chest. Jim found an even
older map hidden on the shelf. It described an island.
Grammatical Role: Entities mentioned in subject position are more salient than
those in object position, which are in turn more salient than those mentioned in
oblique positions. Thus although the ﬁrst sentence in (23.38) and (23.39) expresses
roughly the same propositional content, the preferred referent for the pronoun he
varies with the subject—Billy Bones in (23.38) and Jim Hawkins in (23.39).
(23.38) Billy Bones went to the bar with Jim Hawkins. He called for a glass of
rum. [ he = Billy ]
(23.39) Jim Hawkins went to the bar with Billy Bones. He called for a glass of
rum. [ he = Jim ]
5Here’s a bound pronoun example from Shakespeare’s Comedy of Errors :There’s not a man I meet but
doth salute me As if I were their well-acquainted friend
6The word “gender” is generally only used for languages with 2 or 3 noun classes, like most Indo-
European languages; many languages, like the Bantu languages or Chinese, have a much larger number
of noun classes.

23.2 • C OREFERENCE TASKS AND DATASETS 525
Verb Semantics: Some verbs semantically emphasize one of their arguments, bi-
asing the interpretation of subsequent pronouns. Compare (23.40) and (23.41).
(23.40) John telephoned Bill. He lost the laptop.
(23.41) John criticized Bill. He lost the laptop.
These examples differ only in the verb used in the ﬁrst sentence, yet “he” in (23.40)
is typically resolved to John, whereas “he” in (23.41) is resolved to Bill. This may
be partly due to the link between implicit causality and saliency: the implicit cause
of a “criticizing” event is its object, whereas the implicit cause of a “telephoning”
event is its subject. In such verbs, the entity which is the implicit cause may be more
salient.
Selectional Restrictions: Many other kinds of semantic knowledge can play a role
in referent preference. For example, the selectional restrictions that a verb places on
its arguments (Chapter 21) can help eliminate referents, as in (23.42).
(23.42) I ate the soup in my new bowl after cooking it for hours
There are two possible referents for it, the soup and the bowl. The verb eat, however,
requires that its direct object denote something edible, and this constraint can rule
outbowl as a possible referent.
23.2 Coreference Tasks and Datasets
We can formulate the task of coreference resolution as follows: Given a text T, ﬁnd
all entities and the coreference links between them. We evaluate our task by com-
paring the links our system creates with those in human-created gold coreference
annotations on T.
Let’s return to our coreference example, now using superscript numbers for each
coreference chain (cluster), and subscript letters for individual mentions in the clus-
ter:
(23.43) [Victoria Chen]1
a, CFO of [Megabucks Banking]2
a, saw [[her]1
bpay]3
ajump
to $2.3 million, as [the 38-year-old]1
calso became [[the company]2
b’s
president. It is widely known that [she]1
dcame to [Megabucks]2
cfrom rival
[Lotsabucks]4
a.
Assuming example (23.43) was the entirety of the article, the chains for her pay and
Lotsabucks are singleton mentions:
1.{Victoria Chen ,her,the 38-year-old ,She}
2.{Megabucks Banking ,the company ,Megabucks}
3.{her pay}
4.{Lotsabucks}
For most coreference evaluation campaigns, the input to the system is the raw
text of articles, and systems must detect mentions and then link them into clusters.
Solving this task requires dealing with pronominal anaphora (ﬁguring out that her
refers to Victoria Chen ), ﬁltering out non-referential pronouns like the pleonastic It
inIt has been ten years ), dealing with deﬁnite noun phrases to ﬁgure out that the
38-year-old is coreferent with Victoria Chen , and that the company is the same as
Megabucks . And we need to deal with names, to realize that Megabucks is the same
asMegabucks Banking .

526 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
Exactly what counts as a mention and what links are annotated differs from task
to task and dataset to dataset. For example some coreference datasets do not label
singletons, making the task much simpler. Resolvers can achieve much higher scores
on corpora without singletons, since singletons constitute the majority of mentions in
running text, and they are often hard to distinguish from non-referential NPs. Some
tasks use gold mention-detection (i.e. the system is given human-labeled mention
boundaries and the task is just to cluster these gold mentions), which eliminates the
need to detect and segment mentions from running text.
Coreference is usually evaluated by the CoNLL F1 score, which combines three
metrics: MUC, B3, and CEAF e; Section 23.8 gives the details.
Let’s mention a few characteristics of one popular coreference dataset, OntoNotes
(Pradhan et al. 2007c, Pradhan et al. 2007a), and the CoNLL 2012 Shared Task
based on it (Pradhan et al., 2012a). OntoNotes contains hand-annotated Chinese
and English coreference datasets of roughly one million words each, consisting of
newswire, magazine articles, broadcast news, broadcast conversations, web data and
conversational speech data, as well as about 300,000 words of annotated Arabic
newswire. The most important distinguishing characteristic of OntoNotes is that
it does not label singletons, simplifying the coreference task, since singletons rep-
resent 60%-70% of all entities. In other ways, it is similar to other coreference
datasets. Referring expression NPs that are coreferent are marked as mentions, but
generics and pleonastic pronouns are not marked. Appositive clauses are not marked
as separate mentions, but they are included in the mention. Thus in the NP, “Richard
Godown, president of the Industrial Biotechnology Association” the mention is the
entire phrase. Prenominal modiﬁers are annotated as separate entities only if they
are proper nouns. Thus wheat is not an entity in wheat ﬁelds , but UNis an entity in
UN policy (but not adjectives like American inAmerican policy ).
A number of corpora mark richer discourse phenomena. The ISNotes corpus
annotates a portion of OntoNotes for information status, include bridging examples
(Hou et al., 2018). The LitBank coreference corpus (Bamman et al., 2020) contains
coreference annotations for 210,532 tokens from 100 different literary novels, in-
cluding singletons and quantiﬁed and negated noun phrases. The AnCora-CO coref-
erence corpus (Recasens and Mart ´ı, 2010) contains 400,000 words each of Spanish
(AnCora-CO-Es) and Catalan (AnCora-CO-Ca) news data, and includes labels for
complex phenomena like discourse deixis in both languages. The ARRAU corpus
(Uryupina et al., 2020) contains 350,000 words of English marking all NPs, which
means singleton clusters are available. ARRAU includes diverse genres like dialog
(the TRAINS data) and ﬁction (the Pear Stories), and has labels for bridging refer-
ences, discourse deixis, generics, and ambiguous anaphoric relations.
23.3 Mention Detection
The ﬁrst stage of coreference is mention detection : ﬁnding the spans of text thatmention
detection
constitute each mention. Mention detection algorithms are usually very liberal in
proposing candidate mentions (i.e., emphasizing recall), and only ﬁltering later. For
example many systems run parsers and named entity taggers on the text and extract
every span that is either an NP, apossessive pronoun , or a named entity .
Doing so from our sample text repeated in (23.44):
(23.44) Victoria Chen, CFO of Megabucks Banking, saw her pay jump to $2.3

23.3 • M ENTION DETECTION 527
million, as the 38-year-old also became the company’s president. It is
widely known that she came to Megabucks from rival Lotsabucks.
might result in the following list of 13 potential mentions:
Victoria Chen $2.3 million she
CFO of Megabucks Banking the 38-year-old Megabucks
Megabucks Banking the company Lotsabucks
her the company’s president
her pay It
More recent mention detection systems are even more generous; the span-based
algorithm we will describe in Section 23.6 ﬁrst extracts literally all n-gram spans
of words up to N=10. Of course recall from Section 23.1.3 that many NPs—and
the overwhelming majority of random n-gram spans—are not referring expressions.
Therefore all such mention detection systems need to eventually ﬁlter out pleonas-
tic/expletive pronouns like Itabove, appositives like CFO of Megabucks Banking
Inc, or predicate nominals like the company’s president or$2.3 million .
Some of this ﬁltering can be done by rules. Early rule-based systems designed
regular expressions to deal with pleonastic it, like the following rules from Lappin
and Leass (1994) that use dictionaries of cognitive verbs (e.g., believe ,know ,antic-
ipate ) to capture pleonastic itin “It is thought that ketchup...”, or modal adjectives
(e.g., necessary ,possible ,certain ,important ), for, e.g., “It is likely that I...”. Such
rules are sometimes used as part of modern systems:
It is Modaladjective that S
It is Modaladjective (for NP) to VP
It is Cogv-ed that S
It seems/appears/means/follows (that) S
Mention-detection rules are sometimes designed speciﬁcally for particular eval-
uation campaigns. For OntoNotes, for example, mentions are not embedded within
larger mentions, and while numeric quantities are annotated, they are rarely coref-
erential. Thus for OntoNotes tasks like CoNLL 2012 (Pradhan et al., 2012a), a
common ﬁrst pass rule-based mention detection algorithm (Lee et al., 2013) is:
1.Take all NPs, possessive pronouns, and named entities.
2.Remove numeric quantities (100 dollars, 8%), mentions embedded in
larger mentions, adjectival forms of nations, and stop words (like there ).
3.Remove pleonastic itbased on regular expression patterns.
Rule-based systems, however, are generally insufﬁcient to deal with mention-
detection, and so modern systems incorporate some sort of learned mention detec-
tion component, such as a referentiality classiﬁer, an anaphoricity classiﬁer —
detecting whether an NP is an anaphor—or a discourse-new classiﬁer— detecting
whether a mention is discourse-new and a potential antecedent for a future anaphor.
Ananaphoricity detector , for example, can draw its positive training examplesanaphoricity
detector
from any span that is labeled as an anaphoric referring expression in hand-labeled
datasets like OntoNotes, ARRAU , or AnCora. Any other NP or named entity can be
marked as a negative training example. Anaphoricity classiﬁers use features of the
candidate mention such as its head word, surrounding words, deﬁniteness, animacy,
length, position in the sentence/discourse, many of which were ﬁrst proposed in
early work by Ng and Cardie (2002a); see Section 23.5 for more on features.

528 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
Referentiality or anaphoricity detectors can be run as ﬁlters, in which only men-
tions that are classiﬁed as anaphoric or referential are passed on to the coreference
system. The end result of such a ﬁltering mention detection system on our example
above might be the following ﬁltered set of 9 potential mentions:
Victoria Chen her pay she
Megabucks Bank the 38-year-old Megabucks
her the company Lotsabucks
It turns out, however, that hard ﬁltering of mentions based on an anaphoricity
or referentiality classiﬁer leads to poor performance. If the anaphoricity classiﬁer
threshold is set too high, too many mentions are ﬁltered out and recall suffers. If the
classiﬁer threshold is set too low, too many pleonastic or non-referential mentions
are included and precision suffers.
The modern approach is instead to perform mention detection, anaphoricity, and
coreference jointly in a single end-to-end model (Ng 2005b, Denis and Baldridge
2007, Rahman and Ng 2009). For example mention detection in the Lee et al.
(2017b),2018 system is based on a single end-to-end neural network that computes
a score for each mention being referential, a score for two mentions being corefer-
ence, and combines them to make a decision, training all these scores with a single
end-to-end loss. We’ll describe this method in detail in Section 23.6.7
Despite these advances, correctly detecting referential mentions seems to still be
an unsolved problem, since systems incorrectly marking pleonastic pronouns like
itand other non-referential NPs as coreferent is a large source of errors of modern
coreference resolution systems (Kummerfeld and Klein 2013, Martschat and Strube
2014, Martschat and Strube 2015, Wiseman et al. 2015, Lee et al. 2017a).
Mention, referentiality, or anaphoricity detection is thus an important open area
of investigation. Other sources of knowledge may turn out to be helpful, especially
in combination with unsupervised and semisupervised algorithms, which also mit-
igate the expense of labeled datasets. In early work, for example Bean and Riloff
(1999) learned patterns for characterizing anaphoric or non-anaphoric NPs; (by ex-
tracting and generalizing over the ﬁrst NPs in a text, which are guaranteed to be
non-anaphoric). Chang et al. (2012) look for head nouns that appear frequently in
the training data but never appear as gold mentions to help ﬁnd non-referential NPs.
Bergsma et al. (2008b) use web counts as a semisupervised way to augment standard
features for anaphoricity detection for English it, an important task because itis both
common and ambiguous; between a quarter and half itexamples are non-anaphoric.
Consider the following two examples:
(23.45) You can make [it] in advance. [anaphoric]
(23.46) You can make [it] in Hollywood. [non-anaphoric]
Theitinmake it is non-anaphoric, part of the idiom make it . Bergsma et al. (2008b)
turn the context around each example into patterns, like “make * in advance” from
(23.45), and “make * in Hollywood” from (23.46). They then use Google n-grams to
enumerate all the words that can replace itin the patterns. Non-anaphoric contexts
tend to only have itin the wildcard positions, while anaphoric contexts occur with
many other NPs (for example make them in advance is just as frequent in their data
7Some systems try to avoid mention detection or anaphoricity detection altogether. For datasets like
OntoNotes which don’t label singletons, an alternative to ﬁltering out non-referential mentions is to run
coreference resolution, and then simply delete any candidate mentions which were not corefered with
another mention. This likely doesn’t work as well as explicitly modeling referentiality, and cannot solve
the problem of detecting singletons, which is important for tasks like entity linking.

23.4 • A RCHITECTURES FOR COREFERENCE ALGORITHMS 529
asmake it in advance , but make them in Hollywood did not occur at all). These
n-gram contexts can be used as features in a supervised anaphoricity classiﬁer.
23.4 Architectures for Coreference Algorithms
Modern systems for coreference are based on supervised neural machine learning,
supervised from hand-labeled datasets like OntoNotes. In this section we overview
the various architecture of modern systems, using the categorization of Ng (2010),
which distinguishes algorithms based on whether they make each coreference deci-
sion in a way that is entity-based —representing each entity in the discourse model—
or only mention-based —considering each mention independently, and whether they
useranking models to directly compare potential antecedents. Afterwards, we go
into more detail on one state-of-the-art algorithm in Section 23.6.
23.4.1 The Mention-Pair Architecture
We begin with the mention-pair architecture, the simplest and most inﬂuential mention-pair
coreference architecture, which introduces many of the features of more complex
algorithms, even though other architectures perform better. The mention-pair ar- mention-pair
chitecture is based around a classiﬁer that— as its name suggests—is given a pair
of mentions, a candidate anaphor and a candidate antecedent, and makes a binary
classiﬁcation decision: coreferring or not.
Let’s consider the task of this classiﬁer for the pronoun shein our example, and
assume the slightly simpliﬁed set of potential antecedents in Fig. 23.2.
Victoria ChenMegabucks Bankingherher paythe 37-year-oldshep(coref|”Victoria Chen”,”she”)
p(coref|”Megabucks Banking”,”she”)
Figure 23.2 For each pair of a mention (like she), and a potential antecedent mention (like
Victoria Chen orher), the mention-pair classiﬁer assigns a probability of a coreference link.
For each prior mention ( Victoria Chen ,Megabucks Banking ,her, etc.), the binary
classiﬁer computes a probability: whether or not the mention is the antecedent of
she. We want this probability to be high for actual antecedents ( Victoria Chen ,her,
the 38-year-old ) and low for non-antecedents ( Megabucks Banking ,her pay ).
Early classiﬁers used hand-built features (Section 23.5); more recent classiﬁers
use neural representation learning (Section 23.6)
For training, we need a heuristic for selecting training samples; since most pairs
of mentions in a document are not coreferent, selecting every pair would lead to
a massive overabundance of negative samples. The most common heuristic, from
(Soon et al., 2001), is to choose the closest antecedent as a positive example, and all
pairs in between as the negative examples. More formally, for each anaphor mention
miwe create
• one positive instance ( mi,mj) where mjis the closest antecedent to mi, and

530 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
• a negative instance ( mi,mk) for each mkbetween mjandmi
Thus for the anaphor she, we would choose ( she,her) as the positive example
and no negative examples. Similarly, for the anaphor the company we would choose
(the company ,Megabucks ) as the positive example and ( the company ,she) (the com-
pany ,the 38-year-old ) (the company ,her pay ) and ( the company ,her) as negative
examples.
Once the classiﬁer is trained, it is applied to each test sentence in a clustering
step. For each mention iin a document, the classiﬁer considers each of the prior i−1
mentions. In closest-ﬁrst clustering (Soon et al., 2001), the classiﬁer is run right to
left (from mention i−1 down to mention 1) and the ﬁrst antecedent with probability
>.5 is linked to i. If no antecedent has probably >0.5, no antecedent is selected for
i. Inbest-ﬁrst clustering, the classiﬁer is run on all i−1 antecedents and the most
probable preceding mention is chosen as the antecedent for i. The transitive closure
of the pairwise relation is taken as the cluster.
While the mention-pair model has the advantage of simplicity, it has two main
problems. First, the classiﬁer doesn’t directly compare candidate antecedents to
each other, so it’s not trained to decide, between two likely antecedents, which one
is in fact better. Second, it ignores the discourse model, looking only at mentions,
not entities. Each classiﬁer decision is made completely locally to the pair, without
being able to take into account other mentions of the same entity. The next two
models each address one of these two ﬂaws.
23.4.2 The Mention-Rank Architecture
The mention ranking model directly compares candidate antecedents to each other,
choosing the highest-scoring antecedent for each anaphor.
In early formulations, for mention i, the classiﬁer decides which of the {1,...,i−
1}prior mentions is the antecedent (Denis and Baldridge, 2008). But suppose iis
in fact not anaphoric, and none of the antecedents should be chosen? Such a model
would need to run a separate anaphoricity classiﬁer on i. Instead, it turns out to be
better to jointly learn anaphoricity detection and coreference together with a single
loss (Rahman and Ng, 2009).
So in modern mention-ranking systems, for the ith mention (anaphor), we have
an associated random variable yiranging over the values Y(i) ={1,...,i−1,ϵ}. The
valueϵis a special dummy mention meaning that idoes not have an antecedent (i.e.,
is either discourse-new and starts a new coref chain, or is non-anaphoric).
Victoria ChenMegabucks Bankingherher paythe 37-year-oldshep(”Victoria Chen”|”she”)
p(ϵ|”she”)ϵOne or more of theseshould be highAll of theseshould be low}p(”her pay”|she”)p(”her”|she”)p(”the 37-year-old”|she”)
p(”Megabucks Banking”|she”)}
Figure 23.3 For each candidate anaphoric mention (like she), the mention-ranking system assigns a proba-
bility distribution over all previous mentions plus the special dummy mention ϵ.
At test time, for a given mention ithe model computes one softmax over all the
antecedents (plus ϵ) giving a probability for each candidate antecedent (or none).

23.5 • C LASSIFIERS USING HAND -BUILT FEATURES 531
Fig. 23.3 shows an example of the computation for the single candidate anaphor
she.
Once the antecedent is classiﬁed for each anaphor, transitive closure can be run
over the pairwise decisions to get a complete clustering.
Training is trickier in the mention-ranking model than the mention-pair model,
because for each anaphor we don’t know which of all the possible gold antecedents
to use for training. Instead, the best antecedent for each mention is latent ; that
is, for each mention we have a whole cluster of legal gold antecedents to choose
from. Early work used heuristics to choose an antecedent, for example choosing the
closest antecedent as the gold antecedent and all non-antecedents in a window of
two sentences as the negative examples (Denis and Baldridge, 2008). Various kinds
of ways to model latent antecedents exist (Fernandes et al. 2012, Chang et al. 2013,
Durrett and Klein 2013). The simplest way is to give credit to any legal antecedent
by summing over all of them, with a loss function that optimizes the likelihood of
all correct antecedents from the gold clustering (Lee et al., 2017b). We’ll see the
details in Section 23.6.
Mention-ranking models can be implemented with hand-build features or with
neural representation learning (which might also incorporate some hand-built fea-
tures). we’ll explore both directions in Section 23.5 and Section 23.6.
23.4.3 Entity-based Models
Both the mention-pair and mention-ranking models make their decisions about men-
tions . By contrast, entity-based models link each mention not to a previous mention
but to a previous discourse entity (cluster of mentions).
A mention-ranking model can be turned into an entity-ranking model simply
by having the classiﬁer make its decisions over clusters of mentions rather than
individual mentions (Rahman and Ng, 2009).
For traditional feature-based models, this can be done by extracting features over
clusters. The size of a cluster is a useful feature, as is its ‘shape’, which is the
list of types of the mentions in the cluster i.e., sequences of the tokens (P)roper,
(D)eﬁnite, (I)ndeﬁnite, (Pr)onoun, so that a cluster composed of {Victoria ,her,the
38-year-old}would have the shape P-Pr-D (Bj¨orkelund and Kuhn, 2014). An entity-
based model that includes a mention-pair classiﬁer can use as features aggregates of
mention-pair probabilities, for example computing the average probability of coref-
erence over all mention-pairs in the two clusters (Clark and Manning 2015).
Neural models can learn representations of clusters automatically, for example
by using an RNN over the sequence of cluster mentions to encode a state correspond-
ing to a cluster representation (Wiseman et al., 2016), or by learning distributed rep-
resentations for pairs of clusters by pooling over learned representations of mention
pairs (Clark and Manning, 2016b).
However, although entity-based models are more expressive, the use of cluster-
level information in practice has not led to large gains in performance, so mention-
ranking models are still more commonly used.
23.5 Classiﬁers using hand-built features
Feature-based classiﬁers, use hand-designed features in logistic regression, SVM,
or random forest classiﬁers for coreference resolution. These classiﬁers don’t per-

532 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
form as well as neural ones. Nonetheless, they are still sometimes useful to build
lightweight systems when compute or data are sparse, and the features themselves
are useful for error analysis even in neural systems.
Given an anaphor mention and a potential antecedent mention, feature based
classiﬁers make use of three types of features: (i) features of the anaphor, (ii) features
of the candidate antecedent, and (iii) features of the relationship between the pair.
Entity-based models can make additional use of two additional classes: (iv) feature
of all mentions from the antecedent’s entity cluster, and (v) features of the relation
between the anaphor and the mentions in the antecedent entity cluster.
Features of the Anaphor or Antecedent Mention
First (last) word Victoria/she First or last word (or embedding) of antecedent/anaphor
Head word Victoria/she Head word (or head embedding) of antecedent/anaphor
Attributes Sg-F-A-3-PER/
Sg-F-A-3-PERThe number, gender, animacy, person, named entity type
attributes of (antecedent/anaphor)
Length 2/1 length in words of (antecedent/anaphor)
Mention type P/Pr Type: (P)roper, (D)eﬁnite, (I)ndeﬁnite, (Pr)onoun) of an-
tecedent/anaphor
Features of the Antecedent Entity
Entity shape P-Pr-D The ‘shape’ or list of types of the mentions in the
antecedent entity (cluster), i.e., sequences of (P)roper,
(D)eﬁnite, (I)ndeﬁnite, (Pr)onoun.
Entity attributes Sg-F-A-3-PER The number, gender, animacy, person, named entity type
attributes of the antecedent entity
Ant. cluster size 3 Number of mentions in the antecedent cluster
Features of the Pair of Mentions
Sentence distance 1 The number of sentences between antecedent and anaphor
Mention distance 4 The number of mentions between antecedent and anaphor
i-within-i F Anaphor has i-within-i relation with antecedent
Cosine Cosine between antecedent and anaphor embeddings
Features of the Pair of Entities
Exact String Match F True if the strings of any two mentions from the antecedent
and anaphor clusters are identical.
Head Word Match F True if any mentions from antecedent cluster has same
headword as any mention in anaphor cluster
Word Inclusion F All words in anaphor cluster included in antecedent cluster
Figure 23.4 Feature-based coreference: sample feature values for anaphor “she” and potential antecedent
“Victoria Chen”.
Figure 23.4 shows a selection of commonly used features, and shows the value
that would be computed for the potential anaphor “she” and potential antecedent
“Victoria Chen” in our example sentence, repeated below:
(23.47) Victoria Chen , CFO of Megabucks Banking, saw her pay jump to $2.3
million, as the 38-year-old also became the company’s president. It is
widely known that shecame to Megabucks from rival Lotsabucks.
Features that prior work has found to be particularly useful are exact string
match, entity headword agreement, mention distance, as well as (for pronouns) exact
attribute match and i-within-i, and (for nominals and proper names) word inclusion
and cosine. For lexical features (like head words) it is common to only use words
that appear enough times ( >20 times).

23.6 • A NEURAL MENTION -RANKING ALGORITHM 533
It is crucial in feature-based systems to use conjunctions of features; one exper-
iment suggested that moving from individual features in a classiﬁer to conjunctions
of multiple features increased F1 by 4 points (Lee et al., 2017a). Speciﬁc conjunc-
tions can be designed by hand (Durrett and Klein, 2013), all pairs of features can be
conjoined (Bengtson and Roth, 2008), or feature conjunctions can be learned using
decision tree or random forest classiﬁers (Ng and Cardie 2002a, Lee et al. 2017a).
Features can also be used in neural models as well. Neural systems use contex-
tual word embeddings so don’t beneﬁt from shallow features like string match or or
mention types. However features like mention length, distance between mentions,
or genre can complement neural contextual embedding models.
23.6 A neural mention-ranking algorithm
In this section we describe the neural e2e-coref algorithms of Lee et al. (2017b)
(simpliﬁed and extended a bit, drawing on Joshi et al. (2019) and others). This is
amention-ranking algorithm that considers all possible spans of text in the docu-
ment, assigns a mention-score to each span, prunes the mentions based on this score,
then assigns coreference links to the remaining mentions.
More formally, given a document Dwith Twords, the model considers all of
theT(T+1)
2text spans in D(unigrams, bigrams, trigrams, 4-grams, etc; in practice
we only consider spans up a maximum length around 10). The task is to assign
to each span ian antecedent yi, a random variable ranging over the values Y(i) =
{1,...,i−1,ϵ}; each previous span and a special dummy token ϵ. Choosing the
dummy token means that idoes not have an antecedent, either because iis discourse-
new and starts a new coreference chain, or because iis non-anaphoric.
For each pair of spans iand j, the system assigns a score s(i,j)for the coref-
erence link between span iand span j. The system then learns a distribution P(yi)
over the antecedents for span i:
P(yi) =exp(s(i,yi))∑
y′∈Y(i)exp(s(i,y′))(23.48)
This score s(i,j)includes three factors that we’ll deﬁne below: m(i); whether span
iis a mention; m(j); whether span jis a mention; and c(i,j); whether jis the
antecedent of i:
s(i,j) =m(i)+m(j)+c(i,j) (23.49)
For the dummy antecedent ϵ, the score s(i,ϵ)is ﬁxed to 0. This way if any non-
dummy scores are positive, the model predicts the highest-scoring antecedent, but if
all the scores are negative it abstains.
23.6.1 Computing span representations
To compute the two functions m(i)andc(i,j)which score a span ior a pair of spans
(i,j), we’ll need a way to represent a span. The e2e-coref family of algorithms
represents each span by trying to capture 3 words/tokens: the ﬁrst word, the last
word, and the most important word. We ﬁrst run each paragraph or subdocument
through an encoder (like BERT) to generate embeddings hifor each token i. The
span iis then represented by a vector githat is a concatenation of the encoder output

534 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
embedding for the ﬁrst (start) token of the span, the encoder output for the last (end)
token of the span, and a third vector which is an attention-based representation:
gi= [hSTART (i),hEND (i),hATT(i)] (23.50)
The goal of the attention vector is to represent which word/token is the likely
syntactic head-word of the span; we saw in the prior section that head-words are
a useful feature; a matching head-word is a good indicator of coreference. The
attention representation is computed as usual; the system learns a weight vector wα,
and computes its dot product with the hidden state httransformed by a FFN:
αt=wα·FFNα(ht) (23.51)
The attention score is normalized into a distribution via a softmax:
ai,t=exp(αt)
∑END (i)
k=START (i)exp(αk)(23.52)
And then the attention distribution is used to create a vector hATT(i)which is an
attention-weighted sum of the embeddings etof each of the words in span i:
hATT(i)=END (i)∑
t=START (i)ai,t·et (23.53)
Fig. 23.5 shows the computation of the span representation and the mention
score.
Encodings (h)…Encoder GeneralElectricsaidthePostalServicecontactedthecompanySpan head (hATT) Span representation (g) Mention score (m)+++++General ElectricElectric said thethe Postal ServiceService contacted thethe company
Figure 23.5 Computation of the span representation g(and the mention score m) in a BERT version of the
e2e-coref model (Lee et al. 2017b, Joshi et al. 2019). The model considers all spans up to a maximum width of
say 10; the ﬁgure shows a small subset of the bigram and trigram spans.
23.6.2 Computing the mention and antecedent scores mandc
Now that we know how to compute the vector gifor representing span i, we can
see the details of the two scoring functions m(i)andc(i,j). Both are computed by
feedforward networks:
m(i) = wm·FFN m(gi) (23.54)
c(i,j) = wc·FFN c([gi,gj,gi◦gj,]) (23.55)
At inference time, this mention score mis used as a ﬁlter to keep only the best few
mentions.

23.6 • A NEURAL MENTION -RANKING ALGORITHM 535
We then compute the antecedent score for high-scoring mentions. The antecedent
score c(i,j)takes as input a representation of the spans iandj, but also the element-
wise similarity of the two spans to each other gi◦gj(here◦is element-wise mul-
tiplication). Fig. 23.6 shows the computation of the score sfor the three possible
antecedents of the company in the example sentence from Fig. 23.5.
Figure 23.6 The computation of the score sfor the three possible antecedents of the com-
pany in the example sentence from Fig. 23.5. Figure after Lee et al. (2017b).
Given the set of mentions, the joint distribution of antecedents for each docu-
ment is computed in a forward pass, and we can then do transitive closure on the
antecedents to create a ﬁnal clustering for the document.
Fig. 23.7 shows example predictions from the model, showing the attention
weights, which Lee et al. (2017b) ﬁnd correlate with traditional semantic heads.
Note that the model gets the second example wrong, presumably because attendants
andpilot likely have nearby word embeddings.
Figure 23.7 Sample predictions from the Lee et al. (2017b) model, with one cluster per
example, showing one correct example and one mistake. Bold, parenthesized spans are men-
tions in the predicted cluster. The amount of red color on a word indicates the head-ﬁnding
attention weight ai,tin Eq. 23.52. Figure adapted from Lee et al. (2017b).
23.6.3 Learning
For training, we don’t have a single gold antecedent for each mention; instead the
coreference labeling only gives us each entire cluster of coreferent mentions; so a
mention only has a latent antecedent. We therefore use a loss function that maxi-
mizes the sum of the coreference probability of any of the legal antecedents. For a
given mention iwith possible antecedents Y(i), let GOLD (i) be the set of mentions
in the gold cluster containing i. Since the set of mentions occurring before iisY(i),
the set of mentions in that gold cluster that also occur before iisY(i)∩GOLD (i). We

536 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
therefore want to maximize:
∑
ˆy∈Y(i)∩GOLD (i)P(ˆy) (23.56)
If a mention iis not in a gold cluster GOLD (i) =ϵ.
To turn this probability into a loss function, we’ll use the cross-entropy loss
function we deﬁned in Eq. 4.23 in Chapter 4, by taking the −log of the probability.
If we then sum over all mentions, we get the ﬁnal loss function for training:
L=N∑
i=2−log∑
ˆy∈Y(i)∩GOLD (i)P(ˆy) (23.57)
23.7 Entity Linking
Entity linking is the task of associating a mention in text with the representation of entity linking
some real-world entity in an ontology or knowledge base (Ji and Grishman, 2011). It
is the natural follow-on to coreference resolution; coreference resolution is the task
of associating textual mentions that corefer to the same entity. Entity linking takes
the further step of identifying who that entity is. It is especially important for any
NLP task that links to a knowledge base.
While there are all sorts of potential knowledge-bases, we’ll focus in this section
on Wikipedia, since it’s widely used as an ontology for NLP tasks. In this usage,
each unique Wikipedia page acts as the unique id for a particular entity. This task of
deciding which Wikipedia page corresponding to an individual is being referred to
by a text mention has its own name: wikiﬁcation (Mihalcea and Csomai, 2007). wikiﬁcation
Since the earliest systems (Mihalcea and Csomai 2007, Cucerzan 2007, Milne
and Witten 2008), entity linking is done in (roughly) two stages: mention detec-
tion andmention disambiguation . We’ll give two algorithms, one simple classic
baseline that uses anchor dictionaries and information from the Wikipedia graph
structure (Ferragina and Scaiella, 2011) and one modern neural algorithm (Li et al.,
2020). We’ll focus here mainly on the application of entity linking to questions,
since a lot of the literature has been in that context.
23.7.1 Linking based on Anchor Dictionaries and Web Graph
As a simple baseline we introduce the T AGME linker (Ferragina and Scaiella, 2011)
for Wikipedia, which itself draws on earlier algorithms (Mihalcea and Csomai 2007,
Cucerzan 2007, Milne and Witten 2008). Wikiﬁcation algorithms deﬁne the set of
entities as the set of Wikipedia pages, so we’ll refer to each Wikipedia page as a
unique entity e. T AGME ﬁrst creates a catalog of all entities (i.e. all Wikipedia
pages, removing some disambiguation and other meta-pages) and indexes them in a
standard IR engine like Lucene. For each page e, the algorithm computes an in-link
count in (e): the total number of in-links from other Wikipedia pages that point to e.
These counts can be derived from Wikipedia dumps.
Finally, the algorithm requires an anchor dictionary . An anchor dictionary
lists for each Wikipedia page, its anchor texts : the hyperlinked spans of text on anchor texts
other pages that point to it. For example, the web page for Stanford University,
http://www.stanford.edu , might be pointed to from another page using anchor
texts like Stanford orStanford University :

23.7 • E NTITY LINKING 537
<a href="http://www.stanford.edu">Stanford University</a>
We compute a Wikipedia anchor dictionary by including, for each Wikipedia
page e,e’s title as well as all the anchor texts from all Wikipedia pages that point to e.
For each anchor string awe’ll also compute its total frequency freq (a)in Wikipedia
(including non-anchor uses), the number of times aoccurs as a link (which we’ll call
link(a)), and its link probability linkprob (a) =link(a)/freq(a). Some cleanup of the
ﬁnal anchor dictionary is required, for example removing anchor strings composed
only of numbers or single characters, that are very rare, or that are very unlikely to
be useful entities because they have a very low linkprob.
Mention Detection Given a question (or other text we are trying to link), T AGME
detects mentions by querying the anchor dictionary for each token sequence up to
6 words. This large set of sequences is pruned with some simple heuristics (for
example pruning substrings if they have small linkprobs). The question:
When was Ada Lovelace born?
might give rise to the anchor Ada Lovelace and possibly Ada, but substrings spans
likeLovelace might be pruned as having too low a linkprob, and but spans like born
have such a low linkprob that they would not be in the anchor dictionary at all.
Mention Disambiguation If a mention span is unambiguous (points to only one
entity/Wikipedia page), we are done with entity linking! However, many spans are
ambiguous, matching anchors for multiple Wikipedia entities/pages. The T AGME
algorithm uses two factors for disambiguating ambiguous spans, which have been
referred to as prior probability andrelatedness/coherence . The ﬁrst factor is p(e|a),
the probability with which the span refers to a particular entity. For each page e∈
E(a), the probability p(e|a)that anchor apoints to e, is the ratio of the number of
links into ewith anchor text ato the total number of occurrences of aas an anchor:
prior(a→e) = p(e|a) =count (a→e)
link(a)(23.58)
Let’s see how that factor works in linking entities in the following question:
What Chinese Dynasty came before the Yuan?
The most common association for the span Yuan in the anchor dictionary is the name
of the Chinese currency, i.e., the probability p(Yuancurrency|yuan)is very high.
Rarer Wikipedia associations for Yuan include the common Chinese last name, a
language spoken in Thailand, and the correct entity in this case, the name of the
Chinese dynasty. So if we chose based only on p(e|a), we would make the wrong
disambiguation and miss the correct link, Yuandynasty .
To help in just this sort of case, T AGME uses a second factor, the relatedness of
this entity to other entities in the input question. In our example, the fact that the
question also contains the span Chinese Dynasty , which has a high probability link to
the pageDynasties inChinese history , ought to help match Yuandynasty .
Let’s see how this works. Given a question q, for each candidate anchors span
adetected in q, we assign a relatedness score to each possible entity e∈E(a)ofa.
The relatedness score of the link a→eis the weighted average relatedness between
eand all other entities in q. Two entities are considered related to the extent their
Wikipedia pages share many in-links. More formally, the relatedness between two
entities AandBis computed as
rel(A,B) =log(max(|in(A)|,|in(B)|))−log(|in(A)∩in(B)|)
log(|W|)−log(min(|in(A)|,|in(B)|))(23.59)

538 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
where in (x)is the set of Wikipedia pages pointing to xandWis the set of all Wiki-
pedia pages in the collection.
The vote given by anchor bto the candidate annotation a→Xis the average,
over all the possible entities of b, of their relatedness to X, weighted by their prior
probability:
vote(b,X) =1
|E(b)|∑
Y∈E(b)rel(X,Y)p(Y|b) (23.60)
The total relatedness score for a→Xis the sum of the votes of all the other anchors
detected in q:
relatedness (a→X) =∑
b∈Xq\avote(b,X) (23.61)
To score a→X, we combine relatedness and prior by choosing the entity X
that has the highest relatedness (a→X), ﬁnding other entities within a small ϵof
this value, and from this set, choosing the entity with the highest prior P(X|a). The
result of this step is a single entity assigned to each span in q.
The T AGME algorithm has one further step of pruning spurious anchor/entity
pairs, assigning a score averaging link probability with the coherence.
coherence (a→X) =1
|S|−1∑
B∈S\Xrel(B,X)
score(a→X) =coherence (a→X)+linkprob (a)
2(23.62)
Finally, pairs are pruned if score (a→X)<λ, where the threshold λis set on a
held-out set.
23.7.2 Neural Graph-based linking
More recent entity linking models are based on bi-encoders , encoding a candidate
mention span, encoding an entity, and computing the dot product between the en-
codings. This allows embeddings for all the entities in the knowledge base to be
precomputed and cached (Wu et al., 2020). Let’s sketch the ELQ linking algorithm
of Li et al. (2020), which is given a question qand a set of candidate entities from
Wikipedia with associated Wikipedia text, and outputs tuples (e,ms,me)of entity id,
mention start, and mention end. As Fig. 23.8 shows, it does this by encoding each
Wikipedia entity using text from Wikipedia, encoding each mention span using text
from the question, and computing their similarity, as we describe below.
Entity Mention Detection To get an h-dimensional embedding for each question
token, the algorithm runs the question through BERT in the normal way:
[q1···qn] =BERT ([CLS]q1···qn[SEP]) (23.63)
It then computes the likelihood of each span [i,j]inqbeing an entity mention, in
a way similar to the span-based algorithm we saw for the reader above. First we
compute the score for i/jbeing the start/end of a mention:
sstart(i) =wstart·qi,send(j) =wend·qj, (23.64)

23.7 • E NTITY LINKING 539
Figure 23.8 A sketch of the inference process in the ELQ algorithm for entity linking in
questions (Li et al., 2020). Each candidate question mention span and candidate entity are
separately encoded, and then scored by the entity/span dot product.
where wstartandwendare vectors learned during training. Next, another trainable
embedding, wmention is used to compute a score for each token being part of a men-
tion:
smention (t) =wmention·qt (23.65)
Mention probabilities are then computed by combining these three scores:
p([i,j]) =σ(
sstart(i)+send(j)+j∑
t=ismention (t))
(23.66)
Entity Linking To link mentions to entities, we next compute embeddings for
each entity in the set E=e1,···,ei,···,ewof all Wikipedia entities. For each en-
tityeiwe’ll get text from the entity’s Wikipedia page, the title t(ei)and the ﬁrst
128 tokens of the Wikipedia page which we’ll call the description d(ei). This is
again run through BERT, taking the output of the CLStoken BERT [CLS] as the entity
representation:
xei=BERT [CLS]([CLS]t(ei)[ENT]d(ei)[SEP]) (23.67)
Mention spans can be linked to entities by computing, for each entity eand span
[i,j], the dot product similarity between the span encoding (the average of the token
embeddings) and the entity encoding.
yi,j=1
(j−i+1)j∑
t=iqt
s(e,[i,j]) =x·
eyi,j (23.68)
Finally, we take a softmax to get a distribution over entities for each span:
p(e|[i,j]) =exp(s(e,[i,j]))∑
e′∈Eexp(s(e′,[i,j]))(23.69)
Training The ELQ mention detection and entity linking algorithm is fully super-
vised. This means, unlike the anchor dictionary algorithms from Section 23.7.1,

540 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
it requires datasets with entity boundaries marked and linked. Two such labeled
datasets are WebQuestionsSP (Yih et al., 2016), an extension of the WebQuestions
(Berant et al., 2013) dataset derived from Google search questions, and GraphQues-
tions (Su et al., 2016). Both have had entity spans in the questions marked and
linked (Sorokin and Gurevych 2018, Li et al. 2020) resulting in entity-labeled ver-
sions WebQSP ELand GraphQ EL(Li et al., 2020).
Given a training set, the ELQ mention detection and entity linking phases are
trained jointly, optimizing the sum of their losses. The mention detection loss is
a binary cross-entropy loss, with Lthe length of the passage and Nthe number of
candidates:
LMD=−1
N∑
1≤i≤j≤min(i+L−1,n)(
y[i,j]logp([i,j])+( 1−y[i,j])log(1−p([i,j])))
(23.70)
with y[i,j]=1 if[i,j]is a gold mention span, else 0. The entity linking loss is:
LED=−logp(eg|[i,j]) (23.71)
where egis the gold entity for mention [i,j].
23.8 Evaluation of Coreference Resolution
We evaluate coreference algorithms model-theoretically, comparing a set of hypoth-
esischains or clusters Hproduced by the system against a set of gold or reference
chains or clusters Rfrom a human labeling, and reporting precision and recall.
However, there are a wide variety of methods for doing this comparison. In fact,
there are 5 common metrics used to evaluate coreference algorithms: the link based
MUC (Vilain et al., 1995) and BLANC (Recasens and Hovy 2011, Luo et al. 2014)
metrics, the mention based B3metric (Bagga and Baldwin, 1998), the entity based
CEAF metric (Luo, 2005), and the link based entity aware LEA metric (Moosavi and
Strube, 2016).
Let’s just explore two of the metrics. The MUC F-measure (Vilain et al., 1995)MUC
F-measure
is based on the number of coreference links (pairs of mentions) common to Hand
R. Precision is the number of common links divided by the number of links in H.
Recall is the number of common links divided by the number of links in R; This
makes MUC biased toward systems that produce large chains (and fewer entities),
and it ignores singletons, since they don’t involve links.
B3is mention-based rather than link-based. For each mention in the reference B3
chain, we compute a precision and recall, and then we take a weighted sum over all
Nmentions in the document to compute a precision and recall for the entire task. For
a given mention i, letRbe the reference chain that includes i, and Hthe hypothesis
chain that has i. The set of correct mentions in HisH∩R. Precision for mention i
is thus|H∩R|
|H|, and recall for mention ithus|H∩R|
|R|. The total precision is the weighted
sum of the precision for mention i, weighted by a weight wi. The total recall is the
weighted sum of the recall for mention i, weighted by a weight wi. Equivalently:
Precision =N∑
i=1wi# of correct mentions in hypothesis chain containing entity i
# of mentions in hypothesis chain containing entity i
Recall =N∑
i=1wi# of correct mentions in hypothesis chain containing entity i
# of mentions in reference chain containing entity i

23.9 • W INOGRAD SCHEMA PROBLEMS 541
The weight wifor each entity can be set to different values to produce different
versions of the algorithm.
Following a proposal from Denis and Baldridge (2009), the CoNLL coreference
competitions were scored based on the average of MUC, CEAF-e, and B3(Pradhan
et al. 2011, Pradhan et al. 2012b), and so it is common in many evaluation campaigns
to report an average of these 3 metrics. See Luo and Pradhan (2016) for a detailed
description of the entire set of metrics; reference implementations of these should
be used rather than attempting to reimplement from scratch (Pradhan et al., 2014).
Alternative metrics have been proposed that deal with particular coreference do-
mains or tasks. For example, consider the task of resolving mentions to named
entities (persons, organizations, geopolitical entities), which might be useful for in-
formation extraction or knowledge base completion. A hypothesis chain that cor-
rectly contains all the pronouns referring to an entity, but has no version of the name
itself, or is linked with a wrong name, is not useful for this task. We might instead
want a metric that weights each mention by how informative it is (with names being
most informative) (Chen and Ng, 2013) or a metric that considers a hypothesis to
match a gold chain only if it contains at least one variant of a name (the NEC F1
metric of Agarwal et al. (2019)).
23.9 Winograd Schema problems
From early on in the ﬁeld, researchers have noted that some cases of coreference
are quite difﬁcult, seeming to require world knowledge or sophisticated reasoning
to solve. The problem was most famously pointed out by Winograd (1972) with the
following example:
(23.72) The city council denied the demonstrators a permit because
a. they feared violence.
b. they advocated violence.
Winograd noticed that the antecedent that most readers preferred for the pro-
noun they in continuation (a) was the city council , but in (b) was the demonstrators .
He suggested that this requires understanding that the second clause is intended
as an explanation of the ﬁrst clause, and also that our cultural frames suggest that
city councils are perhaps more likely than demonstrators to fear violence and that
demonstrators might be more likely to advocate violence.
In an attempt to get the ﬁeld of NLP to focus more on methods involving world
knowledge and common-sense reasoning, Levesque (2011) proposed a challenge
task called the Winograd Schema Challenge .8The problems in the challenge taskWinograd
schema
are coreference problems designed to be easily disambiguated by the human reader,
but hopefully not solvable by simple techniques such as selectional restrictions, or
other basic word association methods.
The problems are framed as a pair of statements that differ in a single word or
phrase, and a coreference question:
(23.73) The trophy didn’t ﬁt into the suitcase because it was too large .
Question: What was too large ? Answer: The trophy
8Levesque’s call was quickly followed up by Levesque et al. (2012) and Rahman and Ng (2012), a
competition at the IJCAI conference (Davis et al., 2017), and a natural language inference version of the
problem called WNLI (Wang et al., 2018a).

542 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
(23.74) The trophy didn’t ﬁt into the suitcase because it was too small .
Question: What was too small ? Answer: The suitcase
The problems have the following characteristics:
1. The problems each have two parties
2. A pronoun preferentially refers to one of the parties, but could grammatically
also refer to the other
3. A question asks which party the pronoun refers to
4. If one word in the question is changed, the human-preferred answer changes
to the other party
The kind of world knowledge that might be needed to solve the problems can
vary. In the trophy/suitcase example, it is knowledge about the physical world; that
a bigger object cannot ﬁt into a smaller object. In the original Winograd sentence,
it is stereotypes about social actors like politicians and protesters. In examples like
the following, it is knowledge about human actions like turn-taking or thanking.
(23.75) Bill passed the gameboy to John because his turn was [over/next]. Whose
turn was [over/next]? Answers: Bill/John
(23.76) Joan made sure to thank Susan for all the help she had [given/received].
Who had [given/received] help? Answers: Susan/Joan.
Although the Winograd Schema was designed to require common-sense rea-
soning, a large percentage of the original set of problems can be solved by pre-
trained language models, ﬁne-tuned on Winograd Schema sentences (Kocijan et al.,
2019). Large pretrained language models encode an enormous amount of world or
common-sense knowledge! The current trend is therefore to propose new datasets
with increasingly difﬁcult Winograd-like coreference resolution problems like K NOW REF
(Emami et al., 2019), with examples like:
(23.77) Marcus is undoubtedly faster than Jarrett right now but in [his] prime the
gap wasn’t all that big.
In the end, it seems likely that some combination of language modeling and knowl-
edge will prove fruitful; indeed, it seems that knowledge-based models overﬁt less
to lexical idiosyncracies in Winograd Schema training sets (Trichelair et al., 2018),
23.10 Gender Bias in Coreference
As with other aspects of language processing, coreference models exhibit gender and
other biases (Zhao et al. 2018a, Rudinger et al. 2018, Webster et al. 2018). For exam-
ple the WinoBias dataset (Zhao et al., 2018a) uses a variant of the Winograd Schema
paradigm to test the extent to which coreference algorithms are biased toward link-
ing gendered pronouns with antecedents consistent with cultural stereotypes. As we
summarized in Chapter 5, embeddings replicate societal biases in their training test,
such as associating men with historically sterotypical male occupations like doctors,
and women with stereotypical female occupations like secretaries (Caliskan et al.
2017, Garg et al. 2018).
A WinoBias sentence contain two mentions corresponding to stereotypically-
male and stereotypically-female occupations and a gendered pronoun that must be
linked to one of them. The sentence cannot be disambiguated by the gender of the
pronoun, but a biased model might be distracted by this cue. Here is an example
sentence:

23.11 • S UMMARY 543
(23.78) The secretary called the physician iand told him iabout a new patient
[pro-stereotypical]
(23.79) The secretary called the physician iand told her iabout a new patient
[anti-stereotypical]
Zhao et al. (2018a) consider a coreference system to be biased if it is more accu-
rate at linking pronouns consistent with gender stereotypical occupations (e.g., him
with physician in (23.78)) than linking pronouns inconsistent with gender-stereotypical
occupations (e.g., herwith physician in (23.79)). They show that coreference sys-
tems of all architectures (rule-based, feature-based machine learned, and end-to-
end-neural) all show signiﬁcant bias, performing on average 21 F 1points worse in
the anti-stereotypical cases.
One possible source of this bias is that female entities are signiﬁcantly un-
derrepresented in the OntoNotes dataset, used to train most coreference systems.
Zhao et al. (2018a) propose a way to overcome this bias: they generate a second
gender-swapped dataset in which all male entities in OntoNotes are replaced with
female ones and vice versa, and retrain coreference systems on the combined orig-
inal and swapped OntoNotes data, also using debiased GloVE embeddings (Boluk-
basi et al., 2016). The resulting coreference systems no longer exhibit bias on the
WinoBias dataset, without signiﬁcantly impacting OntoNotes coreference accuracy.
In a follow-up paper, Zhao et al. (2019) show that the same biases exist in ELMo
contextualized word vector representations and coref systems that use them. They
showed that retraining ELMo with data augmentation again reduces or removes bias
in coreference systems on WinoBias.
Webster et al. (2018) introduces another dataset, GAP, and the task of Gendered
Pronoun Resolution as a tool for developing improved coreference algorithms for
gendered pronouns. GAP is a gender-balanced labeled corpus of 4,454 sentences
with gendered ambiguous pronouns (by contrast, only 20% of the gendered pro-
nouns in the English OntoNotes training data are feminine). The examples were
created by drawing on naturally occurring sentences from Wikipedia pages to create
hard to resolve cases with two named entities of the same gender and an ambiguous
pronoun that may refer to either person (or neither), like the following:
(23.80) In May, Fujisawa joined Mari Motohashi’s rink as the team’s skip, moving
back from Karuizawa to Kitami where shehad spent her junior days.
Webster et al. (2018) show that modern coreference algorithms perform signif-
icantly worse on resolving feminine pronouns than masculine pronouns in GAP.
Kurita et al. (2019) shows that a system based on BERT contextualized word repre-
sentations shows similar bias.
23.11 Summary
This chapter introduced the task of coreference resolution .
• This is the task of linking together mentions in text which corefer , i.e. refer
to the same discourse entity in the discourse model , resulting in a set of
coreference chains (also called clusters orentities ).
• Mentions can be deﬁnite NPs orindeﬁnite NPs ,pronouns (including zero
pronouns ) ornames .

544 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
• The surface form of an entity mention is linked to its information status
(new,old, orinferrable ), and how accessible orsalient the entity is.
• Some NPs are not referring expressions, such as pleonastic itinIt is raining .
• Many corpora have human-labeled coreference annotations that can be used
for supervised learning, including OntoNotes for English, Chinese, and Ara-
bic, ARRAU for English, and AnCora for Spanish and Catalan.
• Mention detection can start with all nouns and named entities and then use
anaphoricity classiﬁers orreferentiality classiﬁers to ﬁlter out non-mentions.
• Three common architectures for coreference are mention-pair ,mention-rank ,
andentity-based , each of which can make use of feature-based or neural clas-
siﬁers.
• Modern coreference systems tend to be end-to-end, performing mention de-
tection and coreference in a single end-to-end architecture.
• Algorithms learn representations for text spans and heads, and learn to com-
pare anaphor spans with candidate antecedent spans.
• Entity linking is the task of associating a mention in text with the representa-
tion of some real-world entity in an ontology .
• Coreference systems are evaluated by comparing with gold entity labels using
precision/recall metrics like MUC ,B3,CEAF ,BLANC , orLEA .
• The Winograd Schema Challenge problems are difﬁcult coreference prob-
lems that seem to require world knowledge or sophisticated reasoning to solve.
• Coreference systems exhibit gender bias which can be evaluated using datasets
like Winobias and GAP.
Historical Notes
Coreference has been part of natural language processing since the 1970s (Woods
et al. 1972, Winograd 1972). The discourse model and the entity-centric foundation
of coreference was formulated by Karttunen (1969) (at the 3rd COLING confer-
ence), playing a role also in linguistic semantics (Heim 1982, Kamp 1981). But
it was Bonnie Webber’s 1978 dissertation and following work (Webber 1983) that
explored the model’s computational aspects, providing fundamental insights into
how entities are represented in the discourse model and the ways in which they can
license subsequent reference. Many of the examples she provided continue to chal-
lenge theories of reference to this day.
The Hobbs algorithm9is a tree-search algorithm that was the ﬁrst in a longHobbs
algorithm
series of syntax-based methods for identifying reference robustly in naturally occur-
ring text. The input to the Hobbs algorithm is a pronoun to be resolved, together
with a syntactic (constituency) parse of the sentences up to and including the cur-
rent sentence. The details of the algorithm depend on the grammar used, but can be
understood from a simpliﬁed version due to Kehler et al. (2004) that just searches
through the list of NPs in the current and prior sentences. This simpliﬁed Hobbs
algorithm searches NPs in the following order: “(i) in the current sentence from
right-to-left, starting with the ﬁrst NP to the left of the pronoun, (ii) in the previous
sentence from left-to-right, (iii) in two sentences prior from left-to-right, and (iv) in
9The simpler of two algorithms presented originally in Hobbs (1978).

HISTORICAL NOTES 545
the current sentence from left-to-right, starting with the ﬁrst noun group to the right
of the pronoun (for cataphora). The ﬁrst noun group that agrees with the pronoun
with respect to number, gender, and person is chosen as the antecedent” (Kehler
et al., 2004).
Lappin and Leass (1994) was an inﬂuential entity-based system that used weights
to combine syntactic and other features, extended soon after by Kennedy and Bogu-
raev (1996) whose system avoids the need for full syntactic parses.
Approximately contemporaneously centering (Grosz et al., 1995) was applied
to pronominal anaphora resolution by Brennan et al. (1987), and a wide variety of
work followed focused on centering’s use in coreference (Kameyama 1986, Di Eu-
genio 1990, Walker et al. 1994, Di Eugenio 1996, Strube and Hahn 1996, Kehler
1997a, Tetreault 2001, Iida et al. 2003). Kehler and Rohde (2013) show how center-
ing can be integrated with coherence-driven theories of pronoun interpretation. See
Chapter 24 for the use of centering in measuring discourse coherence.
Coreference competitions as part of the US DARPA-sponsored MUC confer-
ences provided early labeled coreference datasets (the 1995 MUC-6 and 1998 MUC-
7 corpora), and set the tone for much later work, choosing to focus exclusively
on the simplest cases of identity coreference (ignoring difﬁcult cases like bridging,
metonymy, and part-whole) and drawing the community toward supervised machine
learning and metrics like the MUC metric (Vilain et al., 1995). The later ACE eval-
uations produced labeled coreference corpora in English, Chinese, and Arabic that
were widely used for model training and evaluation.
This DARPA work inﬂuenced the community toward supervised learning begin-
ning in the mid-90s (Connolly et al. 1994, Aone and Bennett 1995, McCarthy and
Lehnert 1995). Soon et al. (2001) laid out a set of basic features, extended by Ng and
Cardie (2002b), and a series of machine learning models followed over the next 15
years. These often focused separately on pronominal anaphora resolution (Kehler
et al. 2004, Bergsma and Lin 2006), full NP coreference (Cardie and Wagstaff 1999,
Ng and Cardie 2002b, Ng 2005a) and deﬁnite NP reference (Poesio and Vieira 1998,
Vieira and Poesio 2000), as well as separate anaphoricity detection (Bean and Riloff
1999, Bean and Riloff 2004, Ng and Cardie 2002a, Ng 2004), or singleton detection
(de Marneffe et al., 2015).
The move from mention-pair to mention-ranking approaches was pioneered by
Yang et al. (2003) and Iida et al. (2003) who proposed pairwise ranking methods,
then extended by Denis and Baldridge (2008) who proposed to do ranking via a soft-
max over all prior mentions. The idea of doing mention detection, anaphoricity, and
coreference jointly in a single end-to-end model grew out of the early proposal of Ng
(2005b) to use a dummy antecedent for mention-ranking, allowing ‘non-referential’
to be a choice for coreference classiﬁers, Denis and Baldridge’s 2007 joint system
combining anaphoricity classiﬁer probabilities with coreference probabilities, the
Denis and Baldridge (2008) ranking model, and the Rahman and Ng (2009) pro-
posal to train the two models jointly with a single objective.
Simple rule-based systems for coreference returned to prominence in the 2010s,
partly because of their ability to encode entity-based features in a high-precision way
(Zhou et al. 2004b, Haghighi and Klein 2009, Raghunathan et al. 2010, Lee et al.
2011, Lee et al. 2013, Hajishirzi et al. 2013) but in the end they suffered from an
inability to deal with the semantics necessary to correctly handle cases of common
noun coreference.
A return to supervised learning led to a number of advances in mention-ranking
models which were also extended into neural architectures, for example using re-

546 CHAPTER 23 • C OREFERENCE RESOLUTION AND ENTITY LINKING
inforcement learning to directly optimize coreference evaluation models Clark and
Manning (2016a), doing end-to-end coreference all the way from span extraction
(Lee et al. 2017b, Zhang et al. 2018). Neural models also were designed to take
advantage of global entity-level information (Clark and Manning 2016b, Wiseman
et al. 2016, Lee et al. 2018).
Coreference is also related to the task of entity linking discussed in Chapter 11.
Coreference can help entity linking by giving more possible surface forms to help
link to the right Wikipedia page, and conversely entity linking can help improve
coreference resolution. Consider this example from Hajishirzi et al. (2013):
(23.81) [Michael Eisner] 1and [Donald Tsang] 2announced the grand opening of
[[Hong Kong] 3Disneyland] 4yesterday. [Eisner] 1thanked [the President] 2
and welcomed [fans] 5to [the park] 4.
Integrating entity linking into coreference can help draw encyclopedic knowl-
edge (like the fact that Donald Tsang is a president) to help disambiguate the men-
tionthe President . Ponzetto and Strube (2006) 2007 and Ratinov and Roth (2012)
showed that such attributes extracted from Wikipedia pages could be used to build
richer models of entity mentions in coreference. More recent research shows how to
do linking and coreference jointly (Hajishirzi et al. 2013, Zheng et al. 2013) or even
jointly with named entity tagging as well (Durrett and Klein 2014).
The coreference task as we introduced it involves a simplifying assumption that
the relationship between an anaphor and its antecedent is one of identity : the two
coreferring mentions refer to the identical discourse referent. In real texts, the rela-
tionship can be more complex, where different aspects of a discourse referent can
be neutralized or refocused. For example (23.82) (Recasens et al., 2011) shows an
example of metonymy , in which the capital city Washington is used metonymically metonymy
to refer to the US. (23.83-23.84) show other examples (Recasens et al., 2011):
(23.82) a strict interpretation of a policy requires The U.S. to notify foreign
dictators of certain coup plots ... Washington rejected the bid ...
(23.83) I once crossed that border into Ashgh-Abad on Nowruz, the Persian New
Year. In the South, everyone was celebrating New Year ; to the North, it
was a regular day.
(23.84) In France, the president is elected for a term of seven years, while in the
United States heis elected for a term of four years.
For further linguistic discussions of these complications of coreference see Puste-
jovsky (1991), van Deemter and Kibble (2000), Poesio et al. (2006), Fauconnier and
Turner (2008), Versley (2008), and Barker (2010).
Ng (2017) offers a useful compact history of machine learning models in coref-
erence resolution. There are three excellent book-length surveys of anaphora/coref-
erence resolution, covering different time periods: Hirst (1981) (early work until
about 1981), Mitkov (2002) (1986-2001), and Poesio et al. (2016) (2001-2015).
Andy Kehler wrote the Discourse chapter for the 2000 ﬁrst edition of this text-
book, which we used as the starting point for the second-edition chapter, and there
are some remnants of Andy’s lovely prose still in this third-edition coreference chap-
ter.
Exercises

