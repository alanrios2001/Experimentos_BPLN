Chapter 12 6093
Logical semantics 6094
The previous few chapters have focused on building systems that reconstruct the syntax 6095
of natural language — its structural organization — through tagging and parsing. But 6096
some of the most exciting and promising potential applications of language technology 6097
involve going beyond syntax to semantics — the underlying meaning of the text: 6098
•Answering questions, such as where is the nearest coffeeshop? orwhat is the middle name 6099
of the mother of the 44th President of the United States? . 6100
•Building a robot that can follow natural language instructions to execute tasks. 6101
•Translating a sentence from one language into another, while preserving the under- 6102
lying meaning. 6103
•Fact-checking an article by searching the web for contradictory evidence. 6104
•Logic-checking an argument by identifying contradictions, ambiguity, and unsup- 6105
ported assertions. 6106
Semantic analysis involves converting natural language into a meaning representa- 6107
tion. To be useful, a meaning representation must meet several criteria: 6108
•c1: it should be unambiguous: unlike natural language, there should be exactly one 6109
meaning per statement; 6110
•c2: it should provide a way to link language to external knowledge, observations, 6111
and actions; 6112
•c3: it should support computational inference , so that meanings can be combined 6113
to derive additional knowledge; 6114
•c4: it should be expressive enough to cover the full range of things that people talk 6115
about in natural language. 6116
287

288 CHAPTER 12. LOGICAL SEMANTICS
Much more than this can be said about the question of how best to represent knowledge 6117
for computation (e.g., Sowa, 2000), but this chapter will focus on these four criteria. 6118
12.1 Meaning and denotation 6119
The ﬁrst criterion for a meaning representation is that statements in the representation 6120
should be unambiguous — they should have only one possible interpretation. Natural 6121
language does not have this property: as we saw in chapter 10, sentences like cats scratch 6122
people with claws have multiple interpretations. 6123
But what does it mean for a statement to be unambiguous? Programming languages 6124
provide a useful example: the output of a program is completely speciﬁed by the rules of 6125
the language and the properties of the environment in which the program is run. For ex- 6126
ample, the python code 5 + 3 will have the output 8, as will the codes (4*4)-(3*3)+1 6127
and((8)) . This output is known as the denotation of the program, and can be written 6128
as, 6129
J5+3 K=J(4*4)-(3*3)+1 K=J((8)) K= 8. [12.1]
The denotations of these arithmetic expressions are determined by the meaning of the 6130
constants (e.g.,5,3) and the relations (e.g.,+,*,(,)). Now let’s consider another snippet 6131
of python code, double(4) . The denotation of this code could be, Jdouble(4) K=8, or 6132
it could be Jdouble(4) K=44— it depends on the meaning of double . This meaning 6133
is deﬁned in a world modelMas an inﬁnite set of pairs. We write the denotation with 6134
respect to model MasJ·KM, e.g., Jdouble KM={(0,0),(1,2),(2,4),...}. The world 6135
model would also deﬁne the (inﬁnite) list of constants, e.g., {0,1,2,...}. As long as the 6136
denotation of string φin modelMcan be computed unambiguously, the language can be 6137
said to be unambiguous. 6138
This approach to meaning is known as model-theoretic semantics , and it addresses 6139
not only criterion c1(no ambiguity), but also c2(connecting language to external knowl- 6140
edge, observations, and actions). For example, we can connect a representation of the 6141
meaning of a statement like the capital of Georgia with a world model that includes knowl- 6142
edge base of geographical facts, obtaining the denotation Atlanta . We might populate 6143
a world model by applying an image analysis algorithm to Figure 12.1, and then use this 6144
world model to evaluate propositions likea man is riding a moose . Another desirable prop- 6145
erty of model-theoretic semantics is that when the facts change, the denotations change 6146
too: the meaning representation of President of the USA would have a different denotation 6147
in the modelM2014as it would inM2022. 6148
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

12.2. LOGICAL REPRESENTATIONS OF MEANING 289
Figure 12.1: A (doctored) image, which could be the basis for a world model
12.2 Logical representations of meaning 6149
Criterionc3requires that the meaning representation support inference — for example, 6150
automatically deducing new facts from known premises. While many representations 6151
have been proposed that meet these criteria, the most mature is the language of ﬁrst-order 6152
logic.16153
12.2.1 Propositional logic 6154
The bare bones of logical meaning representation are Boolean operations on propositions: 6155
Propositional symbols. Greek symbols like φandψwill be used to represent proposi- 6156
tions , which are statements that are either true or false. For example, φmay corre- 6157
spond to the proposition, bagels are delicious . 6158
Boolean operators. We can build up more complex propositional formulas from Boolean 6159
operators. These include: 6160
•Negation¬φ, which is true if φis false. 6161
1Alternatives include the “variable-free” representation used in semantic parsing of geographical
queries (Zelle and Mooney, 1996) and robotic control (Ge and Mooney, 2005), and dependency-based com-
positional semantics (Liang et al., 2013).
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

290 CHAPTER 12. LOGICAL SEMANTICS
•Conjunction, φ∧ψ, which is true if both φandψare true. 6162
•Disjunction, φ∨ψ, which is true if at least one of φandψis true 6163
•Implication, φ⇒ψ, which is true unless φis true and ψis false. Implication 6164
has identical truth conditions to ¬φ∨ψ. 6165
•Equivalence, φ⇔ψ, which is true if φandψare both true or both false. Equiv- 6166
alence has identical truth conditions to (φ⇒ψ)∧(ψ⇒φ). 6167
It is not strictly necessary to have all ﬁve Boolean operators: readers familiar with 6168
Boolean logic will know that it is possible to construct all other operators from either the 6169
NAND (not-and) or NOR (not-or) operators. Nonetheless, it is clearest to use all ﬁve 6170
operators. From the truth conditions for these operators, it is possible to deﬁne a number 6171
of “laws” for these Boolean operators, such as, 6172
•Commutativity :φ∧ψ=ψ∧φ, φ∨ψ=ψ∨φ 6173
•Associativity :φ∧(ψ∧χ) = (φ∧ψ)∧χ, φ∨(ψ∨χ) = (φ∨ψ)∨χ 6174
•Complementation :φ∧¬φ=⊥, φ∨¬φ=⊤, where⊤indicates a true proposition 6175
and⊥indicates a false proposition. 6176
These laws can be combined to derive further equivalences, which can support logical
inferences. For example, suppose φ=The music is loud andψ=Max can’t sleep . Then if
we are given,
φ⇒ψIf the music is loud, Max can’t sleep.
φThe music is loud.
we can derive ψ(Max can’t sleep ) by application of modus ponens , which is one of a 6177
set of inference rules that can be derived from more basic laws and used to manipulate 6178
propositional formulas. Automated theorem provers are capable of applying inference 6179
rules to a set of premises to derive desired propositions (Loveland, 2016). 6180
12.2.2 First-order logic 6181
Propositional logic is so named because it treats propositions as its base units. However, 6182
the criterion c4states that our meaning representation should be sufﬁciently expressive. 6183
Now consider the sentence pair, 6184
(12.1) If anyone is making noise, then Max can’t sleep. 6185
Abigail is making noise. 6186
People are capable of making inferences from this sentence pair, but such inferences re- 6187
quire formal tools that are beyond propositional logic. To understand the relationship 6188
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

12.2. LOGICAL REPRESENTATIONS OF MEANING 291
between the statement anyone is making noise and the statement Abigail is making noise , our 6189
meaning representation requires the additional machinery of ﬁrst-order logic (FOL). 6190
In FOL, logical propositions can be constructed from relationships between entities. 6191
Speciﬁcally, FOL extends propositional logic with the following classes of terms: 6192
Constants. These are elements that name individual entities in the model, such as MAX 6193
and ABIGAIL . The denotation of each constant in a model Mis an element in the 6194
model, e.g., JMAX K=mand JABIGAIL K=a. 6195
Relations. Relations can be thought of as sets of entities, or sets of tuples. For example, 6196
the relation CAN -SLEEP is deﬁned as the set of entities who can sleep, and has the 6197
denotation JCAN -SLEEP K={a,m,...}. To test the truth value of the proposition 6198
CAN -SLEEP (MAX ), we ask whether JMAX K∈JCAN -SLEEP K. Logical relations that are 6199
deﬁned over sets of entities are sometimes called properties . 6200
Relations may also be ordered tuples of entities. For example BROTHER (MAX ,ABIGAIL ) 6201
expresses the proposition that MAX is the brother of ABIGAIL . The denotation of 6202
such relations is a set of tuples, JBROTHER K={(m,a),(x,y),...}. To test the 6203
truth value of the proposition BROTHER (MAX ,ABIGAIL ), we ask whether the tuple 6204
(JMAX K,JABIGAIL K)is in the denotation JBROTHER K. 6205
Using constants and relations, it is possible to express statements like Max can’t sleep
and Max is Abigail’s brother :
¬CAN -SLEEP (MAX )
BROTHER (MAX ,ABIGAIL ).
These statements can also be combined using Boolean operators, such as,
(BROTHER (MAX ,ABIGAIL )∨BROTHER (MAX ,STEVE ))⇒¬ CAN -SLEEP (MAX ).
This fragment of ﬁrst-order logic permits only statements about speciﬁc entities. To 6206
support inferences about statements like Ifanyone is making noise, then Max can’t sleep , 6207
two more elements must be added to the meaning representation: 6208
Variables. Variables are mechanisms for referring to entities that are not locally speciﬁed. 6209
We can then write CAN -SLEEP (x)orBROTHER (x,ABIGAIL ). In these cases, xis afree 6210
variable , meaning that we have not committed to any particular assignment. 6211
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

292 CHAPTER 12. LOGICAL SEMANTICS
Quantiﬁers. Variables are bound by quantiﬁers. There are two quantiﬁers in ﬁrst-order 6212
logic.26213
•The existential quantiﬁer ∃, which indicates that there must be at least one en- 6214
tity to which the variable can bind. For example, the statement ∃xMAKES -NOISE (X) 6215
indicates that there is at least one entity for which MAKES -NOISE is true. 6216
•The universal quantiﬁer ∀, which indicates that the variable must be able to 6217
bind to any entity in the model. For example, the statement, 6218
MAKES -NOISE (ABIGAIL )⇒(∀x¬CAN -SLEEP (x)) [12.3]
asserts that if Abigail makes noise, no one can sleep. 6219
The expressions∃xand∀xmakexinto a bound variable . A formula that contains 6220
no free variables is a sentence . 6221
Functions. Functions map from entities to entities, e.g., JCAPITAL -OF(GEORGIA )K=JATLANTA K. 6222
With functions, it is convenient to add an equality operator, supporting statements 6223
like, 6224
∀x∃yMOTHER -OF(x) = DAUGHTER -OF(y). [12.4]
Note that MOTHER -OFis a functional analogue of the relation MOTHER , so that 6225
MOTHER -OF(x) =yifMOTHER (x,y). Any logical formula that uses functions can be 6226
rewritten using only relations and quantiﬁcation. For example, 6227
MAKES -NOISE (MOTHER -OF(ABIGAIL )) [12.5]
can be rewritten as ∃xMAKES -NOISE (x)∧MOTHER (x,ABIGAIL ). 6228
An important property of quantiﬁers is that the order can matter. Unfortunately, natu-
ral language is rarely clear about this! The issue is demonstrated by examples like everyone
speaks a language , which has the following interpretations:
∀x∃ySPEAKS (x,y) [12.6]
∃y∀xSPEAKS (x,y). [12.7]
In the ﬁrst case, ymay refer to several different languages, while in the second case, there 6229
is a singleythat is spoken by everyone. 6230
2In ﬁrst-order logic, it is possible to quantify only over entities. In second-order logic , it is possible to
quantify over properties, supporting statements like Butch has every property that a good boxer has (example
from Blackburn and Bos, 2005),
∀P∀x((GOOD -BOXER (x)⇒P(x))⇒P(BUTCH )). [12.2]
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

12.2. LOGICAL REPRESENTATIONS OF MEANING 293
12.2.2.1 Truth-conditional semantics 6231
One way to look at the meaning of an FOL sentence φis as a set of truth conditions , 6232
or models under which φis satisﬁed. But how to determine whether a sentence is true 6233
or false in a given model? We will approach this inductively, starting with a predicate 6234
applied to a tuple of constants. The truth of such a sentence depends on whether the 6235
tuple of denotations of the constants is in the denotation of the predicate. For example, 6236
CAPITAL (GEORGIA ,ATLANTA ) is true in model Miff, 6237
(JGEORGIA KM,JATLANTA KM)∈JCAPITAL KM. [12.8]
The Boolean operators ∧,∨,...provide ways to construct more complicated sentences, 6238
and the truth of such statements can be assessed based on the truth tables associated with 6239
these operators. The statement ∃xφis true if there is some assignment of the variable x 6240
to an entity in the model such that φis true; the statement ∀xφis true ifφis true under 6241
all possible assignments of x. More formally, we would say that φissatisﬁed underM, 6242
written asM|=φ. 6243
Truth conditional semantics allows us to deﬁne several other properties of sentences 6244
and pairs of sentences. Suppose that in every Munder which φis satisﬁed, another 6245
formulaψis also satisﬁed; then φentailsψ, which is also written as φ|=ψ. For example, 6246
CAPITAL (GEORGIA ,ATLANTA )|=∃xCAPITAL (GEORGIA,x). [12.9]
A statement that is satisﬁed under any model, such as φ∨¬φ, isvalid , written|= (φ∨ 6247
¬φ). A statement that is not satisﬁed under any model, such as φ∧¬φ, isunsatisﬁable , 6248
orinconsistent . A model checker is a program that determines whether a sentence φ 6249
is satisﬁed inM. A model builder is a program that constructs a model in which φ 6250
is satisﬁed. The problems of checking for consistency and validity in ﬁrst-order logic 6251
areundecidable , meaning that there is no algorithm that can automatically determine 6252
whether an FOL formula is valid or inconsistent. 6253
12.2.2.2 Inference in ﬁrst-order logic 6254
Our original goal was to support inferences that combine general statements If anyone is 6255
making noise, then Max can’t sleep with speciﬁc statements like Abigail is making noise . We 6256
can now represent such statements in ﬁrst-order logic, but how are we to perform the 6257
inference that Max can’t sleep ? One approach is to use “generalized” versions of propo- 6258
sitional inference rules like modus ponens, which can be applied to FOL formulas. By 6259
repeatedly applying such inference rules to a knowledge base of facts, it is possible to 6260
produce proofs of desired propositions. To ﬁnd the right sequence of inferences to derive 6261
a desired theorem, classical artiﬁcial intelligence search algorithms like backward chain- 6262
ing can be applied. Such algorithms are implemented in interpreters for the prolog logic 6263
programming language (Pereira and Shieber, 2002). 6264
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

294 CHAPTER 12. LOGICAL SEMANTICS
S :likes (alex ,brit )
VP : ?
NP : brit
BritV : ?
likesNP : alex
Alex
Figure 12.2: The principle of compositionality requires that we identify meanings for the
constituents likes and likes Brit that will make it possible to compute the meaning for the
entire sentence.
12.3 Semantic parsing and the lambda calculus 6265
The previous section laid out a lot of formal machinery; the remainder of this chapter 6266
links these formalisms back to natural language. Given an English sentence like Alex likes 6267
Brit, how can we obtain the desired ﬁrst-order logical representation, LIKES (ALEX ,BRIT )? 6268
This is the task of semantic parsing . Just as a syntactic parser is a function from a natu- 6269
ral language sentence to a syntactic structure such as a phrase structure tree, a semantic 6270
parser is a function from natural language to logical formulas. 6271
As in syntactic analysis, semantic parsing is difﬁcult because the space of inputs and 6272
outputs is very large, and their interaction is complex. Our best hope is that, like syntactic 6273
parsing, semantic parsing can somehow be decomposed into simpler sub-problems. This 6274
idea, usually attributed to the German philosopher Gottlob Frege, is called the principle 6275
of compositionality : the meaning of a complex expression is a function of the meanings of 6276
that expression’s constituent parts. We will deﬁne these “constituent parts” as syntactic 6277
constituents: noun phrases and verb phrases. These constituents are combined using 6278
function application: if the syntactic parse contains the production x→y z, then the 6279
semantics of x, writtenx.sem, will be computed as a function of the semantics of the 6280
constituents, y.sem andz.sem.3 46281
3§9.3.2 brieﬂy discusses Combinatory Categorial Grammar (CCG) as an alternative to a phrase-structure
analysis of syntax. CCG is argued to be particularly well-suited to semantic parsing (Hockenmaier and
Steedman, 2007), and is used in much of the contemporary work on machine learning for semantic parsing,
summarized in§12.4.
4The approach of algorithmically building up meaning representations from a series of operations on the
syntactic structure of a sentence is generally attributed to the philosopher Richard Montague, who published
a series of inﬂuential papers on the topic in the early 1970s (e.g., Montague, 1973).
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

12.3. SEMANTIC PARSING AND THE LAMBDA CALCULUS 295
12.3.1 The lambda calculus 6282
Let’s see how this works for a simple sentence like Alex likes Brit , whose syntactic structure 6283
is shown in Figure 12.2. Our goal is the formula, LIKES (ALEX ,BRIT ), and it is clear that the 6284
meaning of the constituents Alex and Britshould be ALEX and BRIT . That leaves two more 6285
constituents: the verb likes, and the verb phrase likes Brit . The meanings of these units 6286
must be deﬁned in a way that makes it possible to recover the desired meaning for the 6287
entire sentence by function application. If the meanings of Alex and Brit are constants, 6288
then the meanings of likes and likes Brit must be functional expressions, which can be 6289
applied to their siblings to produce the desired analyses. 6290
Modeling these partial analyses requires extending the ﬁrst-order logic meaning rep- 6291
resentation. We do this by adding lambda expressions , which are descriptions of anony- 6292
mous functions,5e.g., 6293
λx.LIKES (x,BRIT). [12.10]
This functional expression is the meaning of the verb phrase likes Brit ; it takes a single 6294
argument, and returns the result of substituting that argument for xin the expression 6295
LIKES (x,BRIT). We write this substitution as, 6296
(λx.LIKES (x,BRIT))@ ALEX =LIKES (ALEX ,BRIT ), [12.11]
with the symbol “@” indicating function application. Function application in the lambda 6297
calculus is sometimes called β-reduction orβ-conversion . The expression φ@ψindicates 6298
a function application to be performed by β-reduction, and φ(ψ)indicates a function or 6299
predicate in the ﬁnal logical form. 6300
Equation 12.11 shows how to obtain the desired semantics for the sentence Alex likes 6301
Brit: by applying the lambda expression λx.LIKES (x,BRIT)to the logical constant ALEX . 6302
This rule of composition can be speciﬁed in a syntactic-semantic grammar , in which 6303
syntactic productions are paired with semantic operations. For the syntactic production 6304
S→NP VP, we have the semantic rule VP .sem@NP.sem. 6305
The meaning of the transitive verb phrase likes Brit can also be obtained by function
application on its syntactic constituents. For the syntactic production VP →V NP, we
apply the semantic rule,
VP.sem=(V.sem)@NP.sem [12.12]
=(λy.λx. LIKES (x,y))@( BRIT) [12.13]
=λx.LIKES (x,BRIT). [12.14]
5Formally, all ﬁrst-order logic formulas are lambda expressions; in addition, if φis a lambda expression,
thenλx.φ is also a lambda expression. Readers who are familiar with functional programming will recognize
lambda expressions from their use in programming languages such as Lisp and Python.
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

296 CHAPTER 12. LOGICAL SEMANTICS
S :likes (alex,brit )
VP :λx.likes (x,brit )
NP : brit
BritVt:λy.λx. likes (x,y)
likesNP : alex
Alex
Figure 12.3: Derivation of the semantic representation for Alex likes Brit in the grammar
G1.
S→NP VP VP .sem@NP.sem
VP→VtNP Vt.sem@NP.sem
VP→Vi Vi.sem
Vt→likesλy.λx. LIKES (x,y)
Vi→sleepsλx.SLEEPS (x)
NP→Alex ALEX
NP→Brit BRIT
Table 12.1: G1, a minimal syntactic-semantic context-free grammar
Thus, the meaning of the transitive verb likes is a lambda expression whose output is 6306
another lambda expression: it takes yas an argument to ﬁll in one of the slots in the LIKES 6307
relation, and returns a lambda expression that is ready to take an argument to ﬁll in the 6308
other slot.66309
Table 12.1 shows a minimal syntactic-semantic grammar fragment, G1. The complete 6310
derivation ofAlex likes Brit inG1is shown in Figure 12.3. In addition to the transitive 6311
verb likes, the grammar also includes the intransitive verb sleeps ; it should be clear how 6312
to derive the meaning of sentences like Alex sleeps . For verbs that can be either transitive 6313
or intransitive, such as eats, we would have two terminal productions, one for each sense 6314
(terminal productions are also called the lexical entries ). Indeed, most of the grammar is 6315
in the lexicon (the terminal productions), since these productions select the basic units of 6316
the semantic interpretation. 6317
6This can be written in a few different ways. The notation λy,x. LIKES (x,y)is a somewhat informal way to
indicate a lambda expression that takes two arguments; this would be acceptable in functional programming.
Logicians (e.g., Carpenter, 1997) often prefer the more formal notation λy.λx. LIKES (x)(y), indicating that each
lambda expression takes exactly one argument.
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

12.3. SEMANTIC PARSING AND THE LAMBDA CALCULUS 297
S :∃xdog(x)∧sleeps (x)
VP :λx.sleeps (x)
Vi:λx.sleeps (x)
sleepsNP :λP.∃xP(x)∧dog(x)
NN : dog
dogDT :λQ.λP. ∃x.P(x)∧Q(x)
A
Figure 12.4: Derivation of the semantic representation for A dog sleeps , in grammar G2
12.3.2 Quantiﬁcation 6318
Things get more complicated when we move from sentences about named entities to sen- 6319
tences that involve more general noun phrases. Let’s consider the example, A dog sleeps , 6320
which has the meaning ∃xDOG(x)∧SLEEPS (x). Clearly, the DOG relation will be intro- 6321
duced by the word dog, and the SLEEP relation will be introduced by the word sleeps . 6322
The existential quantiﬁer ∃must be introduced by the lexical entry for the determiner a.76323
However, this seems problematic for the compositional approach taken in the grammar 6324
G1: if the semantics of the noun phrase a dog is an existentially quantiﬁed expression, how 6325
can it be the argument to the semantics of the verb sleeps , which expects an entity? And 6326
where does the logical conjunction come from? 6327
There are a few different approaches to handling these issues.8We will begin by re- 6328
versing the semantic relationship between subject NPs and VPs, so that the production 6329
S→NP VP has the semantics NP .sem@VP.sem: the meaning of the sentence is now the 6330
semantics of the noun phrase applied to the verb phrase. The implications of this change 6331
are best illustrated by exploring the derivation of the example, shown in Figure 12.4. Let’s 6332
start with the indeﬁnite article a, to which we assign the rather intimidating semantics, 6333
λP.λQ.∃xP(x)∧Q(x). [12.15]
This is a lambda expression that takes two relations as arguments, PandQ. The relation
Pis scoped to the outer lambda expression, so it will be provided by the immediately
7Conversely, the sentence Every dog sleeps would involve a universal quantiﬁer, ∀xDOG(x)⇒SLEEPS (x).
The deﬁnite article therequires more consideration, since the dog must refer to some dog which is uniquely
identiﬁable, perhaps from contextual information external to the sentence. Carpenter (1997, pp. 96-100)
summarizes recent approaches to handling deﬁnite descriptions.
8Carpenter (1997) offers an alternative treatment based on combinatory categorial grammar.
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

298 CHAPTER 12. LOGICAL SEMANTICS
adjacent noun, which in this case is DOG . Thus, the noun phrase a dog has the semantics,
NP.sem=DET.sem@NN.sem [12.16]
=(λP.λQ.∃xP(x)∧Q(x))@( DOG) [12.17]
=λQ.∃xDOG(x)∧Q(x). [12.18]
This is a lambda expression that is expecting another relation, Q, which will be provided 6334
by the verb phrase, SLEEPS . This gives the desired analysis, ∃xDOG(x)∧SLEEPS (x).96335
If noun phrases like a dog are interpreted as lambda expressions, then proper nouns 6336
like Alex must be treated in the same way. This is achieved by type-raising from con- 6337
stants to lambda expressions, x⇒λP.P (x). After type-raising, the semantics of Alex is 6338
λP.P (ALEX )— a lambda expression that expects a relation to tell us something about 6339
ALEX .10Again, make sure you see how the analysis in Figure 12.4 can be applied to the 6340
sentence Alex sleeps . 6341
Direct objects are handled by applying the same type-raising operation to transitive 6342
verbs: the meaning of verbs such as likes is raised to, 6343
λP.λx.P (λy.LIKES (x,y)) [12.19]
As a result, we can keep the verb phrase production VP .sem=V.sem@NP.sem, knowing
that the direct object will provide the function Pin Equation 12.19. To see how this works,
let’s analyze the verb phrase likes a dog . After uniquely relabeling each lambda variable,
we have,
VP.sem=V.sem@NP.sem
=(λP.λx.P (λy.LIKES (x,y)))@(λQ.∃zDOG(z)∧Q(z))
=λx.(λQ.∃zDOG(z)∧Q(z))@(λy.LIKES (x,y))
=λx.∃zDOG(z)∧(λy.LIKES (x,y))@z
=λx.∃zDOG(z)∧LIKES (x,z).
These changes are summarized in the revised grammar G2, shown in Table 12.2. Fig- 6344
ure 12.5 shows a derivation that involves a transitive verb, an indeﬁnite noun phrase, and 6345
a proper noun. 6346
9When applying β-reduction to arguments that are themselves lambda expressions, be sure to use unique
variable names to avoid confusion. For example, it is important to distinguish the xin the semantics for a
from thexin the semantics for likes. Variable names are abstractions, and can always be changed — this is
known asα-conversion . For example, λx.P (x)can be converted to λy.P (y), etc.
10Compositional semantic analysis is often supported by type systems , which make it possible to check
whether a given function application is valid. The base types are entities eand truth values t. A property,
such as DOG , is a function from entities to truth values, so its type is written ⟨e,t⟩. A transitive verb has type
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

12.4. LEARNING SEMANTIC PARSERS 299
S :∃xdog(x)∧likes (x,alex )
VP :λx.likes (x,alex )
NP :λP.P (alex )
NNP : alex
AlexVt:λP.λx.P (λy.likes (x,y))
likesNP :λQ.∃xdog(x)∧Q(x)
NN : dog
dogDT :λP.λQ. ∃xP(x)∧Q(x)
A
Figure 12.5: Derivation of the semantic representation for A dog likes Alex .
S→NP VP NP .sem@VP.sem
VP→VtNP V t.sem@NP.sem
VP→Vi Vi.sem
NP→DETNNDET.sem@NN.sem
NP→NNPλP.P (NNP.sem)
DET→aλP.λQ.∃xP(x)∧Q(x)
DET→everyλP.λQ.∀x(P(x)⇒Q(x))
Vt→likesλP.λx.P (λy.LIKES (x,y))
Vi→sleepsλx.SLEEPS (x)
NN→dog DOG
NNP→Alex ALEX
NNP→Brit BRIT
Table 12.2: G2, a syntactic-semantic context-free grammar fragment, which supports
quantiﬁed noun phrases
12.4 Learning semantic parsers 6347
As with syntactic parsing, any syntactic-semantic grammar with sufﬁcient coverage risks 6348
producing many possible analyses for any given sentence. Machine learning is the dom- 6349
inant approach to selecting a single analysis. We will focus on algorithms that learn to 6350
score logical forms by attaching weights to features of their derivations (Zettlemoyer 6351
and Collins, 2005). Alternative approaches include transition-based parsing (Zelle and 6352
Mooney, 1996; Misra and Artzi, 2016) and methods inspired by machine translation (Wong 6353
and Mooney, 2006). Methods also differ in the form of supervision used for learning, 6354
⟨e,⟨e,t⟩⟩: after receiving the ﬁrst entity (the direct object), it returns a function from entities to truth values,
which will be applied to the subject of the sentence. The type-raising operation x⇒λP.P (x)corresponds
to a change in type from eto⟨⟨e,t⟩,t⟩: it expects a function from entities to truth values, and returns a truth
value.
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

300 CHAPTER 12. LOGICAL SEMANTICS
which can range from complete derivations to much more limited training signals. We 6355
will begin with the case of complete supervision, and then consider how learning is still 6356
possible even when seemingly key information is missing. 6357
Datasets Early work on semantic parsing focused on natural language expressions of 6358
geographical database queries, such as What states border Texas . The GeoQuery dataset 6359
of Zelle and Mooney (1996) was originally coded in prolog, but has subsequently been 6360
expanded and converted into the SQL database query language by Popescu et al. (2003) 6361
and into ﬁrst-order logic with lambda calculus by Zettlemoyer and Collins (2005), pro- 6362
viding logical forms like λx.STATE (x)∧BORDERS (x,TEXAS ). Another early dataset con- 6363
sists of instructions for RoboCup robot soccer teams (Kate et al., 2005). More recent work 6364
has focused on broader domains, such as the Freebase database (Bollacker et al., 2008), 6365
for which queries have been annotated by Krishnamurthy and Mitchell (2012) and Cai 6366
and Yates (2013). Other recent datasets include child-directed speech (Kwiatkowski et al., 6367
2012) and elementary school science exams (Krishnamurthy, 2016). 6368
12.4.1 Learning from derivations 6369
Letw(i)indicate a sequence of text, and let y(i)indicate the desired logical form. For
example:
w(i)=Alex eats shoots and leaves
y(i)=EATS (ALEX ,SHOOTS )∧EATS (ALEX ,LEAVES )
In the standard supervised learning paradigm that was introduced in §2.2, we ﬁrst de- 6370
ﬁne a feature function, f(w,y), and then learn weights on these features, so that y(i)= 6371
argmaxyθ·f(w,y). The weight vector θis learned by comparing the features of the true 6372
labelf(w(i),y(i))against either the features of the predicted label f(w(i),ˆy)(perceptron, 6373
support vector machine) or the expected feature vector Ey|w[f(w(i),y)](logistic regres- 6374
sion). 6375
While this basic framework seems similar to discriminative syntactic parsing, there is 6376
a crucial difference. In (context-free) syntactic parsing, the annotation y(i)contains all of 6377
the syntactic productions; indeed, the task of identifying the correct set of productions 6378
is identical to the task of identifying the syntactic structure. In semantic parsing, this is 6379
not the case: the logical form EATS (ALEX ,SHOOTS )∧EATS (ALEX ,LEAVES ) does not reveal 6380
the syntactic-semantic productions that were used to obtain it. Indeed, there may be spu- 6381
rious ambiguity , so that a single logical form can be reached by multiple derivations. 6382
(We previously encountered spurious ambiguity in transition-based dependency parsing, 6383
§11.3.2.) 6384
These ideas can be formalized by introducing an additional variable z, representing 6385
thederivation of the logical form yfrom the text w. Assume that the feature function de- 6386
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

12.4. LEARNING SEMANTIC PARSERS 301
S :eats (alex,shoots )∧eats (alex,leaves n)
VP :λx.eats (x,shoots )∧eats (x,leaves n)
NP :λP.P (shoots )∧P(leaves n)
NP :λP.P (leaves n)
leavesCC :λP.λQ.λx.P (x)∧Q(x)
andNP :λP.P (shoots )
shootsVt:λP.λx.P (λy.eats (x,y))
eatsNP :λP.P (alex )
Alex
Figure 12.6: Derivation for gold semantic analysis of Alex eats shoots and leaves
S :eats (alex,shoots )∧leaves v(alex )
VP :λx.eats (x,shoots )∧leaves v(x)
VP :λx.leaves v(x)
Vi:λx.leaves v(x)
leavesCC :λP.λQ.λx.P (x)∧Q(x)
andVP :λx.eats (x,shoots )
NP :λP.P (shoots )
shootsVt:λP.λx.P (λy.eats (x,y))
eatsNP :λP.P (alex )
Alex
Figure 12.7: Derivation for incorrect semantic analysis of Alex eats shoots and leaves
composes across the productions in the derivation, f(w,z,y) =∑T
t=1f(w,zt,y), where 6387
ztindicates a single syntactic-semantic production. For example, we might have a feature 6388
for the production S →NP VP :NP.sem@VP.sem, as well as for terminal productions 6389
like N NP→Alex :ALEX . Under this decomposition, it is possible to compute scores 6390
for each semantically-annotated subtree in the analysis of w, so that bottom-up parsing 6391
algorithms like CKY ( §10.1) can be applied to ﬁnd the best-scoring semantic analysis. 6392
Figure 12.6 shows a derivation of the correct semantic analysis of the sentence Alex 6393
eats shoots and leaves , in a simpliﬁed grammar in which the plural noun phrases shoots 6394
and leaves are interpreted as logical constants SHOOTS and LEAVESn. Figure 12.7 shows a 6395
derivation of an incorrect analysis. Assuming one feature per production, the perceptron 6396
update is shown in Table 12.3. From this update, the parser would learn to prefer the 6397
noun interpretation of leaves over the verb interpretation. It would also learn to prefer 6398
noun phrase coordination over verb phrase coordination. 6399
While the update is explained in terms of the perceptron, it would be easy to replace 6400
the perceptron with a conditional random ﬁeld. In this case, the online updates would be 6401
based on feature expectations, which can be computed using the inside-outside algorithm 6402
(§10.6). 6403
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

302 CHAPTER 12. LOGICAL SEMANTICS
NP1→NP2CCNP3(CC.sem@(NP2.sem))@( NP3.sem)+1
VP1→VP2CCVP3(CC.sem@(VP2.sem))@( VP3.sem)-1
NP→leaves LEAVESn +1
VP→Vi Vi.sem -1
Vi→leaves λx.LEAVESv -1
Table 12.3: Perceptron update for analysis in Figure 12.6 (gold) and Figure 12.7 (predicted)
12.4.2 Learning from logical forms 6404
Complete derivations are expensive to annotate, and are rarely available.11One solution
is to focus on learning from logical forms directly, while treating the derivations as la-
tent variables (Zettlemoyer and Collins, 2005). In a conditional probabilistic model over
logical forms yand derivations z, we have,
p(y,z|w) =exp(θ·f(w,z,y))∑
y′,z′exp(θ·f(w,z′,y′)), [12.20]
which is the standard log-linear model, applied to the logical form yand the derivation 6405
z. 6406
Since the derivation zunambiguously determines the logical form y, it may seem silly
to model the joint probability over yandz. However, since zis unknown, it can be
marginalized out,
p(y|w) =∑
zp(y,z|w). [12.21]
The semantic parser can then select the logical form with the maximum log marginal
probability,
log∑
zp(y,z|w) = log∑
zexp(θ·f(w,z,y))∑y′,z′exp(θ·f(w,z′,y′))[12.22]
∝log∑
zexp(θ·f(w,z′,y′)) [12.23]
≥max
zθ·f(w,z,y). [12.24]
It is impossible to push the logterm inside the sum over z, so our usual linear scoring 6407
function does not apply. We can recover this scoring function only in approximation, by 6408
taking the max (rather than the sum) over derivations z, which provides a lower bound. 6409
11An exception is the work of Ge and Mooney (2005), who annotate the meaning of each syntactic con-
stituents for several hundred sentences.
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

12.4. LEARNING SEMANTIC PARSERS 303
Learning can be performed by maximizing the log marginal likelihood,
ℓ(θ) =N∑
i=1logp(y(i)|w(i);θ) [12.25]
=N∑
i=1log∑
zp(y(i),z(i)|w(i);θ). [12.26]
This log-likelihood is not convex inθ, unlike the log-likelihood of a fully-observed condi- 6410
tional random ﬁeld. This means that learning can give different results depending on the 6411
initialization. 6412
The derivative of Equation 12.26 is,
∂ℓi
∂θ=∑
zp(z|y,w;θ)f(w,z,y)−∑
y′,z′p(y′,z′|w;θ)f(w,z′,y′) [12.27]
=Ez|y,wf(w,z,y)−Ey,z|wf(w,z,y) [12.28]
Both expectations can be computed via bottom-up algorithms like inside-outside. Al- 6413
ternatively, we can again maximize rather than marginalize over derivations for an ap- 6414
proximate solution. In either case, the ﬁrst term of the gradient requires us to identify 6415
derivationszthat are compatible with the logical form y. This can be done in a bottom- 6416
up dynamic programming algorithm, by having each cell in the table t[i,j,X ]include the 6417
set of all possible logical forms for X⇝wi+1:j. The resulting table may therefore be much 6418
larger than in syntactic parsing. This can be controlled by using pruning to eliminate in- 6419
termediate analyses that are incompatible with the ﬁnal logical form y(Zettlemoyer and 6420
Collins, 2005), or by using beam search and restricting the size of each cell to some ﬁxed 6421
constant (Liang et al., 2013). 6422
If we replace each expectation in Equation 12.28 with argmax and then apply stochastic 6423
gradient descent to learn the weights, we obtain the latent variable perceptron , a simple 6424
and general algorithm for learning with missing data. The algorithm is shown in its most 6425
basic form in Algorithm 16, but the usual tricks such as averaging and margin loss can 6426
be applied (Yu and Joachims, 2009). Aside from semantic parsing, the latent variable 6427
perceptron has been used in tasks such as machine translation (Liang et al., 2006) and 6428
named entity recognition (Sun et al., 2009). In latent conditional random ﬁelds , we use 6429
the full expectations rather than maximizing over the hidden variable. This model has 6430
also been employed in a range of problems beyond semantic parsing, including parse 6431
reranking (Koo and Collins, 2005) and gesture recognition (Quattoni et al., 2007). 6432
12.4.3 Learning from denotations 6433
Logical forms are easier to obtain than complete derivations, but the annotation of logical
forms still requires considerable expertise. However, it is relatively easy to obtain deno-
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

304 CHAPTER 12. LOGICAL SEMANTICS
Algorithm 16 Latent variable perceptron
1:procedure LATENT VARIABLE PERCEPTRON (w(1:N),y(1:N))
2:θ←0
3: repeat
4: Select an instance i
5:z(i)←argmaxzθ·f(w(i),z,y(i))
6: ˆy,ˆz←argmaxy′,z′θ·f(w(i),z′,y′)
7:θ←θ+f(w(i),z(i),y(i))−f(w(i),ˆz,ˆy)
8: until tired
9: returnθ
tations for many natural language sentences. For example, in the geography domain, the
denotation of a question would be its answer (Clarke et al., 2010; Liang et al., 2013):
Text :What states border Georgia?
Logical form :λx.STATE (x)∧BORDER (x,GEORGIA )
Denotation :{Alabama, Florida, North Carolina,
South Carolina, Tennessee }
Similarly, in a robotic control setting, the denotation of a command would be an action or 6434
sequence of actions (Artzi and Zettlemoyer, 2013). In both cases, the idea is to reward the 6435
semantic parser for choosing an analysis whose denotation is correct: the right answer to 6436
the question, or the right action. 6437
Learning from logical forms was made possible by summing or maxing over deriva-
tions. This idea can be carried one step further, summing or maxing over all logical forms
with the correct denotation. Let vi(y)∈{0,1}be a validation function , which assigns a
binary score indicating whether the denotation JyKfor the textw(i)is correct. We can then
learn by maximizing a conditional-likelihood objective,
ℓ(i)(θ) = log∑
yvi(y)×p(y|w;θ) [12.29]
= log∑
yvi(y)×∑
zp(y,z|w;θ), [12.30]
which sums over all derivations zof all valid logical forms, {y:vi(y) = 1}. This cor- 6438
responds to the log-probability that the semantic parser produces a logical form with a 6439
valid denotation. 6440
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

12.4. LEARNING SEMANTIC PARSERS 305
Differentiating with respect to θ, we obtain,
∂ℓ(i)
∂θ=∑
y,z:vi(y)=1p(y,z|w)f(w,z,y)−∑
y′,z′p(y′,z′|w)f(w,z′,y′), [12.31]
which is the usual difference in feature expectations. The positive term computes the 6441
expected feature expectations conditioned on the denotation being valid, while the second 6442
term computes the expected feature expectations according to the current model, without 6443
regard to the ground truth. Large-margin learning formulations are also possible for this 6444
problem. For example, Artzi and Zettlemoyer (2013) generate a set of valid and invalid 6445
derivations, and then impose a constraint that all valid derivations should score higher 6446
than all invalid derivations. This constraint drives a perceptron-like learning rule. 6447
Additional resources 6448
A key issue not considered here is how to handle semantic underspeciﬁcation : cases in 6449
which there are multiple semantic interpretations for a single syntactic structure. Quanti- 6450
ﬁer scope ambiguity is a classic example. Blackburn and Bos (2005) enumerate a number 6451
of approaches to this issue, and also provide links between natural language semantics 6452
and computational inference techniques. Much of the contemporary research on semantic 6453
parsing uses the framework of combinatory categorial grammar (CCG). Carpenter (1997) 6454
provides a comprehensive treatment of how CCG can support compositional semantic 6455
analysis. Another recent area of research is the semantics of multi-sentence texts. This can 6456
be handled with models of dynamic semantics , such as dynamic predicate logic (Groe- 6457
nendijk and Stokhof, 1991). 6458
Alternative readings on formal semantics include an “informal” reading from Levy 6459
and Manning (2009), and a more involved introduction from Briscoe (2011). To learn more 6460
about ongoing research on data-driven semantic parsing, readers may consult the survey 6461
article by Liang and Potts (2015), tutorial slides and videos by Artzi and Zettlemoyer 6462
(2013),12and the source code by Yoav Artzi13and Percy Liang.146463
Exercises 6464
1. Derive the modus ponens inference rule, which states that if we know φ⇒ψand 6465
φ, thenψmust be true. The derivation can be performed using the deﬁnition of the 6466
⇒operator and some of the laws provided in §12.2.1, plus one additional identity: 6467
⊥∨φ=φ. 6468
12Videos are currently available at http://yoavartzi.com/tutorial/
13http://yoavartzi.com/spf
14https://github.com/percyliang/sempre
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

306 CHAPTER 12. LOGICAL SEMANTICS
2. Convert the following examples into ﬁrst-order logic, using the relations CAN -SLEEP , 6469
MAKES -NOISE , and BROTHER . 6470
•If Abigail makes noise, no one can sleep. 6471
•If Abigail makes noise, someone cannot sleep. 6472
•None of Abigail’s brothers can sleep. 6473
•If one of Abigail’s brothers makes noise, Abigail cannot sleep. 6474
3. Extend the grammar fragment G1to include the ditransitive verb teaches and the 6475
proper noun Swahili . Show how to derive the interpretation for the sentence Alex 6476
teaches Brit Swahili , which should be TEACHES (ALEX ,BRIT ,SWAHILI ). The grammar 6477
need not be in Chomsky Normal Form. For the ditransitive verb, use NP 1and NP 2 6478
to indicate the two direct objects. 6479
4. Derive the semantic interpretation for the sentence Alex likes every dog , using gram- 6480
mar fragment G2. 6481
5. Extend the grammar fragment G2to handle adjectives, so that the meaning of an 6482
angry dog isλP.∃xDOG(x)∧ANGRY (x)∧P(x). Speciﬁcally, you should supply the 6483
lexical entry for the adjective angry , and you should specify the syntactic-semantic 6484
productions NP→DETNOM, NOM→JJNOM, and NOM →NN. 6485
6. Extend your answer to the previous question to cover copula constructions with 6486
predicative adjectives, such as Alex is angry . The interpretation should be ANGRY (ALEX ). 6487
You should add a verb phrase production VP →VcopJJ, and a terminal production 6488
Vcop→is. Show why your grammar extensions result in the correct interpretation. 6489
7. In Figure 12.6 and Figure 12.7, we treat the plurals shoots and leaves as entities. Revise 6490
G2so that the interpretation of Alex eats leaves is∀x.(LEAF (x)⇒EATS (ALEX,x)), and 6491
show the resulting perceptron update. 6492
8. Statements like every student eats a pizza have two possible interpretations, depend-
ing on quantiﬁer scope:
∀x∃yPIZZA (y)∧(STUDENT (x)⇒EATS (x,y)) [12.32]
∃y∀xPIZZA (y)∧(STUDENT (x)⇒EATS (x,y)) [12.33]
Explain why these interpretations really are different, and modify the grammar G2 6493
so that it can produce both interpretations. 6494
9. Derive Equation 12.27. 6495
10. In the GeoQuery domain, give a natural language query that has multiple plausible 6496
semantic interpretations with the same denotation. List both interpretaions and the 6497
denotation. 6498
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

12.4. LEARNING SEMANTIC PARSERS 307
Hint: There are many ways to do this, but one approach involves using toponyms 6499
(place names) that could plausibly map to several different entities in the model. 6500
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.



