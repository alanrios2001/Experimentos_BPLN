Chapter 10 4982
Context-free parsing 4983
Parsing is the task of determining whether a string can be derived from a given context- 4984
free grammar, and if so, how. The parse structure can answer basic questions of who-did- 4985
what-to-whom, and is useful for various downstream tasks, such as semantic analysis 4986
(chapter 12 and 13) and information extraction (chapter 17). 4987
For a given input and grammar, how many parse trees are there? Consider a minimal
context-free grammar with only one non-terminal, X, and the following productions:
X→X X
X→aardvark|abacus|...|zyther
The second line indicates unary productions to every nonterminal in Σ. In this gram-
mar, the number of possible derivations for a string wis equal to the number of binary
bracketings, e.g.,
((((w1w2)w3)w4)w5),(((w1(w2w3))w4)w5),((w1(w2(w3w4)))w5), ....
The number of such bracketings is a Catalan number , which grows super-exponentially 4988
in the length of the sentence, Cn=(2n)!
(n+1)!n!. As with sequence labeling, it is only possible to 4989
exhaustively search the space of parses by resorting to locality assumptions, which make it 4990
possible to search efﬁciently by reusing shared substructures with dynamic programming. 4991
This chapter focuses on a bottom-up dynamic programming algorithm, which enables 4992
exhaustive search of the space of possible parses, but imposes strict limitations on the 4993
form of scoring function. These limitations can be relaxed by abandoning exhaustive 4994
search. Non-exact search methods will be brieﬂy discussed at the end of this chapter, and 4995
one of them — transition-based parsing — will be the focus of chapter 11. 4996
233

234 CHAPTER 10. CONTEXT-FREE PARSING
S→NP VP
NP→NP PP|we|sushi|chopsticks
PP→INNP
IN→with
VP→V NP|VP PP
V→eat
Table 10.1: A toy example context-free grammar
10.1 Deterministic bottom-up parsing 4997
The CKY algorithm1is a bottom-up approach to parsing in a context-free grammar. It 4998
efﬁciently tests whether a string is in a language, without enumerating all possible parses. 4999
The algorithm ﬁrst forms small constituents, and then tries to merge them into larger 5000
constituents. 5001
To understand the algorithm, consider the input, We eat sushi with chopsticks . Accord- 5002
ing to the toy grammar in Table 10.1, each terminal symbol can be generated by exactly 5003
one unary production, resulting in the sequence NP V NP I NNP. Next, we try to apply 5004
binary productions to merge adjacent symbols into larger constituents: for example, V 5005
NP can be merged into a verb phrase (VP), and I NNP can be merged into a prepositional 5006
phrase (PP). Bottom-up parsing tries to ﬁnd some series of mergers that ultimately results 5007
in the start symbol S covering the entire input. 5008
The CKY algorithm systematizes this approach, incrementally constructing a table tin 5009
which each cell t[i,j]contains the set of nonterminals that can derive the span wi+1:j. The 5010
algorithm ﬁlls in the upper right triangle of the table; it begins with the diagonal, which 5011
corresponds to substrings of length 1, and then computes derivations for progressively 5012
larger substrings, until reaching the upper right corner t[0,M], which corresponds to the 5013
entire input, w1:M. If the start symbol S is in t[0,M], then the string wis in the language 5014
deﬁned by the grammar. This process is detailed in Algorithm 13, and the resulting data 5015
structure is shown in Figure 10.1. Informally, here’s how it works: 5016
•Begin by ﬁlling in the diagonal: the cells t[m−1,m]for allm∈{1,2,...,M}. These 5017
cells are ﬁlled with terminal productions that yield the individual tokens; for the 5018
wordw2=sushi , we ﬁll int[1,2] ={NP}, and so on. 5019
•Then ﬁll in the next diagonal, in which each cell corresponds to a subsequence of 5020
length two: t[0,2],t[1,3],...,t [M−2,M]. These cells are ﬁlled in by looking for 5021
binary productions capable of producing at least one entry in each of the cells corre- 5022
1The name is for Cocke-Kasami-Younger, the inventors of the algorithm. It is a special case chart parsing ,
because its stores reusable computations in a chart-like data structure.
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

10.1. DETERMINISTIC BOTTOM-UP PARSING 235
Algorithm 13 The CKY algorithm for parsing a sequence w∈Σ∗in a context-free gram-
marG= (N,Σ,R,S ), with non-terminals N, production rules R, and start symbol
S. The grammar is assumed to be in Chomsky normal form ( §9.2.1.2). The function
PICKFROM(b[i,j,X ])selects an element of the set b[i,j,X ]arbitrarily. All values of tand
bare initialized to ∅.
1:procedure CKY(w,G= (N,Σ,R,S ))
2: form∈{1...M}do
3:t[m−1,m]←{X: (X→wm)∈R}
4: forℓ∈{2,3,...,M}do ⊿Iterate over constituent lengths
5: form∈{0,1,...M−ℓ}do ⊿Iterate over left endpoints
6: fork∈{m+ 1,m+ 2,...,m +ℓ−1}do ⊿Iterate over split points
7: for(X→Y Z)∈Rdo ⊿Iterate over rules
8: ifY∈t[m,k]∧Z∈t[k,m +ℓ]then
9: t[m,m +ℓ]←t[m,m +ℓ]∪X ⊿ Add non-terminal to table
10: b[m,m +ℓ,X]←b[m,m +ℓ,X]∪(Y,Z,k ) ⊿Add back-pointers
11: ifS∈t[0,M]then
12: return TRACE BACK(S,0,M,b )
13: else
14: return ∅
15:procedure TRACE BACK (X,i,j,b )
16: ifj=i+ 1then
17: returnX
18: else
19: (Y,Z,k )←PICKFROM(b[i,j,X ])
20: returnX→(TRACE BACK(Y,i,k,b ),TRACE BACK(Z,k,j,b ))
sponding to left and right children. For example, the cell t[1,3]includes VP because 5023
the grammar includes the production VP →V NP, and the chart contains V ∈t[1,2] 5024
and NP∈t[2,3]. 5025
•At the next diagonal, the entries correspond to spans of length three. At this level, 5026
there is an additional decision at each cell: where to split the left and right children. 5027
The cellt[i,j]corresponds to the subsequence wi+1:j, and we must choose some 5028
split pointi<k <j , so thatwi+1:kis the left child and wk+1:jis the right child. We 5029
consider all possible k, looking for productions that generate elements in t[i,k]and 5030
t[k,j]; the left-hand side of all such productions can be added to t[i,j]. When it is 5031
time to compute t[i,j], the cellst[i,k]andt[k,j]are guaranteed to be complete, since 5032
these cells correspond to shorter sub-strings of the input. 5033
•The process continues until we reach t[0,M]. 5034
Figure 10.1 shows the chart that arises from parsing the sentence We eat sushi with chop- 5035
sticks using the grammar deﬁned above. 5036
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

236 CHAPTER 10. CONTEXT-FREE PARSING
We eat sushi with chopsticks
We NP∅S∅ S
eat VVP ∅ VP
sushi NP ∅ NP
with P PP
chopsticks NP
Figure 10.1: An example completed CKY chart. The solid and dashed lines show the back
pointers resulting from the two different derivations of VP in position t[1,5].
10.1.1 Recovering the parse tree 5037
As with the Viterbi algorithm, it is possible to identify a successful parse by storing and 5038
traversing an additional table of back-pointers. If we add an entry Xto cellt[i,j]by using 5039
the production X→YZand the split point k, then we store the back-pointer b[i,j,X ] = 5040
(Y,Z,k ). Once the table is complete, we can recover a parse by tracing this pointers, 5041
starting atb[0,M, S], and stopping when they ground out at terminal productions. 5042
For ambiguous sentences, there will be multiple paths to reach S ∈t[0,M]. For exam- 5043
ple, in Figure 10.1, the goal state S ∈t[0,M]is reached through the state VP ∈t[1,5], and 5044
there are two different ways to generate this constituent: one with ( eat sushi ) and ( with 5045
chopsticks ) as children, and another with ( eat) and ( sushi with chopsticks ) as children. The 5046
presence of multiple paths indicates that the input can be generated by the grammar in 5047
more than one way. In Algorithm 13, one of these derivations is selected arbitrarily. As 5048
discussed in§10.3, weighted context-free grammars can select a single parse that maxi- 5049
mizes a scoring function. 5050
10.1.2 Non-binary productions 5051
The CKY algorithm assumes that all productions with non-terminals on the right-hand 5052
side (RHS) are binary. But in real grammars, such as the one considered in chapter 9, 5053
there will be productions with more than two elements on the right-hand side, and other 5054
productions with only a single element. 5055
•Productions with more than two elements on the right-hand side can be binarized 5056
by creating additional non-terminals, as described in §9.2.1.2. For example, given 5057
the production VP →V NP NP (for ditransitive verbs), we can convert to VP → 5058
VPditrans /NP NP, and then add the production VP ditrans/NP→V NP. 5059
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

10.2. AMBIGUITY 237
•What about unary productions like VP →V? In practice, this is handled by mak- 5060
ing a second pass on each diagonal, in which each cell t[i,j]is augmented with all 5061
possible unary productions capable of generating each item already in the cell — 5062
formally,t[i,j]is extended to its unary closure . Suppose the example grammar in 5063
Table 10.1 were extended to include the production VP →V, enabling sentences 5064
with intransitive verb phrases, like we eat . Then the cell t[1,2]— corresponding to 5065
the word eat— would ﬁrst include the set {V}, and would be augmented to the set 5066
{V,VP}during this second pass. 5067
10.1.3 Complexity 5068
For an input of length Mand a grammar with Rproductions and Nnon-terminals, the 5069
space complexity of the CKY algorithm is O(M2N): the number of cells in the chart is 5070
O(M2), and each cell must hold O(N)elements. The time complexity is O(M3R): each 5071
cell is computed by searching over O(M)split points, with Rpossible productions for 5072
each split point. Both the time and space complexity are considerably worse than the 5073
Viterbi algorithm, which is linear in the length of the input. 5074
10.2 Ambiguity 5075
Syntactic ambiguity is endemic to natural language. Here are a few broad categories: 5076
•Attachment ambiguity : e.g., We eat sushi with chopsticks ,I shot an elephant in my 5077
pajamas. In these examples, the prepositions ( with,in) can attach to either the verb 5078
or the direct object. 5079
•Modiﬁer scope : e.g., southern food store ,plastic cup holder . In these examples, the ﬁrst 5080
word could be modifying the subsequent adjective, or the ﬁnal noun. 5081
•Particle versus preposition : e.g., The puppy tore up the staircase. Phrasal verbs like 5082
tore up often include particles which could also act as prepositions. This has struc- 5083
tural implications: if upis a preposition, then up the staircase is a prepositional 5084
phrase; if upis a particle, then the staircase is the direct object to the verb. 5085
•Complement structure : e.g., The students complained to the professor that they didn’t 5086
understand. This is another form of attachment ambiguity, where the complement 5087
that they didn’t understand could attach to the main verb ( complained ), or to the indi- 5088
rect object ( the professor ). 5089
•Coordination scope : e.g., “I see,” said the blind man, as he picked up the hammer and 5090
saw. In this example, the lexical ambiguity for sawenables it to be coordinated either 5091
with the noun hammer or the verb picked up . 5092
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

238 CHAPTER 10. CONTEXT-FREE PARSING
These forms of ambiguity can combine, so that seemingly simple headlines like Fed 5093
raises interest rates have dozens of possible analyses even in a minimal grammar. In a 5094
broad coverage grammar, typical sentences can have millions of parses. While careful 5095
grammar design can chip away at this ambiguity, a better strategy is combine broad cov- 5096
erage parsers with data driven strategies for identifying the correct analysis. 5097
10.2.1 Parser evaluation 5098
Before continuing to parsing algorithms that are able to handle ambiguity, we stop to con- 5099
sider how to measure parsing performance. Suppose we have a set of reference parses — 5100
the ground truth — and a set of system parses that we would like to score. A simple solu- 5101
tion would be per-sentence accuracy: the parser is scored by the proportion of sentences 5102
on which the system and reference parses exactly match.2But as any good student knows, 5103
it is better to get partial credit , which we can assign to analyses that correctly match parts 5104
of the reference parse. The PARSEval metrics (Grishman et al., 1992) score each system 5105
parse via: 5106
Precision: the fraction of constituents in the system parse that match a constituent in the 5107
reference parse. 5108
Recall: the fraction of constituents in the reference parse that match a constituent in the 5109
system parse. 5110
Inlabeled precision and recall , the system must also match the phrase type for each 5111
constituent; in unlabeled precision and recall , it is only required to match the constituent 5112
structure. As in chapter 4, the precision and recall can be combined into an F-MEASURE , 5113
F=2×P×R
P+R. 5114
In Figure 10.2, suppose that the left tree is the system parse and the right tree is the 5115
reference parse. We have the following spans: 5116
•S→w1:5istrue positive , because it appears in both trees. 5117
•VP→w2:5istrue positive as well. 5118
•NP→w3:5isfalse positive , because it appears only in the system output. 5119
•PP→w4:5istrue positive , because it appears in both trees. 5120
•VP→w2:3isfalse negative , because it appears only in the reference. 5121
2Most parsing papers do not report results on this metric, but Finkel et al. (2008) ﬁnd that a strong parser
ﬁnds the exact correct parse on 35% of sentences of length ≤40, and on 62% of parses of length ≤15in the
Penn Treebank.
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

10.2. AMBIGUITY 239
S
VP
NP
PP
NP
chopsticksIN
withNP
sushiV
eatNP
We
(a) system output
S
VP
PP
NP
chopsticksIN
withVP
NP
sushiV
eatNP
We (b) reference
Figure 10.2: Two possible analyses from the grammar in Table 10.1
The labeled and unlabeled precision of this parse is3
4= 0.75, and the recall is3
4= 0.75, for 5122
an F-measure of 0.75. For an example in which precision and recall are not equal, suppose 5123
the reference parse instead included the production VP →V NP PP. In this parse, the 5124
reference does not contain the constituent w2:3, so the recall would be 1.35125
10.2.2 Local solutions 5126
Some ambiguity can be resolved locally. Consider the following examples, 5127
(10.1) We met the President on Monday. 5128
(10.2) We met the President of Mexico. 5129
Each case ends with a preposition, which can be attached to the verb metor the noun
phrase the president . This ambiguity can be resolved by using a labeled corpus to compare
the likelihood of the observing the preposition alongside each candidate attachment point,
p(on|met)≷p(on|President ) [10.1]
p(of|met)≷p(of|President ). [10.2]
A comparison of these probabilities would successfully resolve this case (Hindle and 5130
Rooth, 1993). Other cases, such as the example . . . eat sushi with chopsticks , require consider- 5131
ing the object of the preposition — consider the alternative . . . eat sushi with soy sauce . With 5132
sufﬁcient labeled data, the problem of prepositional phrase attachment can be treated as 5133
a classiﬁcation task (Ratnaparkhi et al., 1994). 5134
3While the grammar must be binarized before applying the CKY algorithm, evaluation is performed on
the original parses. It is therefore necessary to “unbinarize” the output of a CKY-based parser, converting it
back to the original grammar.
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

240 CHAPTER 10. CONTEXT-FREE PARSING
However, there are inherent limitations to local solutions. While toy examples may 5135
have just a few ambiguities to resolve, realistic sentences have thousands or millions of 5136
possible parses. Furthermore, attachment decisions are interdependent, as shown in the 5137
garden path example: 5138
(10.3) Cats scratch people with claws with knives. 5139
We may want to attach with claws toscratch , as would be correct in the shorter sentence 5140
incats scratch people with claws . But this leaves nowhere to attach with knives . The cor- 5141
rect interpretation can be identiﬁed only be considering the attachment decisions jointly. 5142
The huge number of potential parses may seem to make exhaustive search impossible. 5143
But as with sequence labeling, locality assumptions make it possible to search this space 5144
efﬁciently. 5145
10.3 Weighted Context-Free Grammars 5146
Let us deﬁne a derivation τas a set of anchored productions , 5147
τ={X→α,(i,j,k )}, [10.3]
withXcorresponding to the left-hand side non-terminal and αcorresponding to the right- 5148
hand side. For grammars in Chomsky normal formal, αis either a pair of non-terminals or 5149
a terminal symbol. The indices i,j,k anchor the production in the input, with Xderiving 5150
the spanwi+1:j. For binary productions, wi+1:kindicates the span of the left child, and 5151
wk+1:jindicates the span of the right child; for unary productions, kis ignored. For an 5152
inputw, the optimal parse is then, 5153
ˆτ= argmax
τ∈T(w)Ψ(τ), [10.4]
whereT(w)is the set of derivations that yield the input w. 5154
The scoring function Ψdecomposes across anchored productions, 5155
Ψ(τ) =∑
(X→α,(i,j,k))∈τψ(X→α,(i,j,k )). [10.5]
This is a locality assumption, akin to the assumption in Viterbi sequence labeling. In this 5156
case, the assumption states that the overall score is a sum over scores of productions, 5157
which are computed independently. In a weighted context-free grammar (WCFG), the 5158
score of each anchored production X→(α,i,j,k )is simplyψ(X→α), ignoring the 5159
anchors (i,j,k ). In other parsing models, the anchors can be used to access features of the 5160
input, while still permitting efﬁcient bottom-up parsing. 5161
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

10.3. WEIGHTED CONTEXT-FREE GRAMMARS 241
ψ(·) expψ(·)
S→NP VP 0 1
NP→NP PP−11
2
→we−21
4
→sushi−31
8
→chopsticks−31
8
PP→INNP 0 1
IN→with 0 1
VP→V NP−11
2
→VP PP−21
4
→MDV−21
4
V→eat 0 1
Table 10.2: An example weighted context-free grammar (WCFG). The weights are chosen
so that expψ(·)sums to one over right-hand sides for each non-terminal; this is required
by probabilistic context-free grammars, but not by WCFGs in general.
Example Consider the weighted grammar shown in Table 10.2, and the analysis in Fig-
ure 10.2b.
Ψ(τ) =ψ(S→NP VP ) +ψ(VP→VP PP ) +ψ(VP→V NP ) +ψ(PP→INNP)
+ψ(NP→We) +ψ(V→eat) +ψ(NP→sushi ) +ψ(IN→with) +ψ(NP→chopsticks )
[10.6]
=0−2−1 + 0−2 + 0−3 + 0−3 =−11. [10.7]
In the alternative parse in Figure 10.2a, the production VP →VP PP (with score −2) is 5162
replaced with the production NP →NP PP (with score −1); all other productions are the 5163
same. As a result, the score for this parse is −10. 5164
This example hints at a big problem with WCFG parsing on non-terminals such as 5165
NP, VP, and PP: a WCFG will always prefer either VP or NP attachment, without regard 5166
to what is being attached! This problem is addressed in §10.5. 5167
10.3.1 Parsing with weighted context-free grammars 5168
The optimization problem in Equation 10.4 can be solved by modifying the CKY algo- 5169
rithm. In the deterministic CKY algorithm, each cell t[i,j]stored a set of non-terminals 5170
capable of deriving the span wi+1:j. We now augment the table so that the cell t[i,j,X ] 5171
is the score of the best derivation ofwi+1:jfrom non-terminal X. This score is computed 5172
recursively: for the anchored binary production (X→Y Z, (i,j,k )), we compute: 5173
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

242 CHAPTER 10. CONTEXT-FREE PARSING
Algorithm 14 CKY algorithm for parsing a string w∈Σ∗in a weighted context-free
grammar (N,Σ,R,S ), whereNis the set of non-terminals and Ris the set of weighted
productions. The grammar is assumed to be in Chomsky normal form ( §9.2.1.2). The
function T RACE BACK is deﬁned in Algorithm 13.
procedure WCKY(w,G= (N,Σ,R,S ))
for alli,j,X do ⊿Initialization
t[i,j,X ]←0
b[i,j,X ]←∅
form∈{1,2,...,M}do
for allX∈Ndo
t[m,m + 1,X]←ψ(X→wm,(m,m + 1,m))
forℓ∈{2,3,...M}do
form∈{0,1,...,M−ℓ}do
fork∈{m+ 1,m+ 2,...,m +ℓ−1}do
t[m,m +ℓ,X]←max
k,Y,Zψ(X→Y Z, (m,m +ℓ,k)) +t[m,k,Y ] +t[k,m +ℓ,Z]
b[m,m +ℓ,X]←argmax
k,Y,Zψ(X→Y Z, (m+ℓ,k)) +t[m,k,Y ] +t[k,m +ℓ,Z]
return TRACE BACK(S,0,M,b )
•the score of the anchored production, ψ(X→Y Z, (i,j,k )); 5174
•the score of the best derivation of the left child, t[i,k,Y ]; 5175
•the score of the best derivation of the right child, t[k,j,Z ]. 5176
These scores are combined by addition. As in the unscored CKY algorithm, the table 5177
is constructed by considering spans of increasing length, so the scores for spans t[i,k,Y ] 5178
andt[k,j,Z ]are guaranteed to be available at the time we compute the score t[i,j,X ]. The 5179
valuet[0,M, S]is the score of the best derivation of wfrom the grammar. Algorithm 14 5180
formalizes this procedure. 5181
As in unweighted CKY, the parse is recovered from the table of back pointers b, where 5182
eachb[i,j,X ]stores the argmax split point kand production X→Y Z in the derivation of 5183
wi+1:jfromX. The best parse can be obtained by tracing these pointers backwards from 5184
b[0,M, S], all the way to the terminal symbols. This is analogous to the computation of the 5185
best sequence of labels in the Viterbi algorithm by tracing pointers backwards from the 5186
end of the trellis. Note that we need only store back-pointers for the bestpath tot[i,j,X ]; 5187
this follows from the locality assumption that the global score for a parse is a combination 5188
of the local scores of each production in the parse. 5189
Example Let’s revisit the parsing table in Figure 10.1. In a weighted CFG, each cell
would include a score for each non-terminal; non-terminals that cannot be generated are
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

10.3. WEIGHTED CONTEXT-FREE GRAMMARS 243
Algorithm 15 Generative model for derivations from probabilistic context-free grammars
in Chomsky Normal Form (CNF).
procedure DRAW SUBTREE (X)
sample (X→α)∼p(α|X)
ifα= (Y Z)then
return DRAW SUBTREE (Y)∪DRAW SUBTREE (Z)
else
return (X→α)⊿In CNF, all unary productions yield terminal symbols
assumed to have a score of −∞. The ﬁrst diagonal contains the scores of unary produc-
tions:t[0,1,NP] =−2, t[1,2,V] = 0,and so on. At the next diagonal, we compute the
scores for spans of length 2: t[1,3,VP] =−1+0−3 =−4, t[3,5,PP] = 0+0−3 =−3,and
so on. Things get interesting when we reach the cell t[1,5,VP], which contains the score
for the derivation of the span w2:5from the non-terminal VP. This score is computed as a
max over two alternatives,
t[1,5,VP] = max(ψ(VP→VP PP,(1,3,5)) +t[1,3,VP] +t[3,5,PP],
ψ(VP→V NP,(1,2,5)) +t[1,2,V] +t[2,5,NP]) [10.8]
= max(−2−4−3,−1 + 0−7) =−8. [10.9]
Since the second case is the argmax, we set the back-pointer b[1,5,VP] = ( V,NP,2), en- 5190
abling the optimal derivation to be recovered. 5191
10.3.2 Probabilistic context-free grammars 5192
Probabilistic context-free grammars (PCFGs) are a special case of weighted context- 5193
free grammars that arises when the weights correspond to probabilities. Speciﬁcally, the 5194
weightψ(X→α,(i,j,k )) = log p(α|X), where the probability of the right-hand side 5195
αis conditioned on the non-terminal X. These probabilities must be normalized over all 5196
possible right-hand sides, so that∑
αp(α|X) = 1 , for allX. For a given parse τ, the prod- 5197
uct of the probabilities of the productions is equal to p (τ), under the generative model 5198
τ∼DRAW SUBTREE (S), where the function D RAW SUBTREE is deﬁned in Algorithm 15. 5199
The conditional probability of a parse given a string is, 5200
p(τ|w) =p(τ)∑
τ′∈T(w)p(τ′)=exp Ψ(τ)∑
τ′∈T(w)exp Ψ(τ′), [10.10]
where Ψ(τ) =∑
X→α,(i,j,k)∈τψ(X→α); the anchor is ignored. Because the probability 5201
is monotonic in the score Ψ(τ), the maximum likelihood parse can be identiﬁed by the 5202
CKY algorithm without modiﬁcation. If a normalized probability p (τ|w)is required, 5203
the denominator of Equation 10.10 can be computed by the inside recurrence , described 5204
below. 5205
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

244 CHAPTER 10. CONTEXT-FREE PARSING
Example The WCFG in Table 10.2 is designed so that the weights are log-probabilities,
satisfying the constraint∑
αexpψ(X→α) = 1 . As noted earlier, there are two parses in
T(we eat sushi with chopsticks ), with scores Ψ(τ1) = log p(τ1) =−10andΨ(τ2) = log p(τ2) =
−11. Therefore, the conditional probability p (τ1|w)is equal to,
p(τ1|w) =p(τ1)
p(τ1) +p(τ2)=exp Ψ(τ1)
exp Ψ(τ1) + exp Ψ(τ2)=2−10
2−10+ 2−11=2
3. [10.11]
The inside recurrence The denominator of Equation 10.10 can be viewed as a language 5206
model, summing over all valid derivations of the string w, 5207
p(w) =∑
τ′:yield (τ′)=wp(τ′). [10.12]
Just as the CKY algorithm makes it possible to maximize over all such analyses, with
a few modiﬁcations it can also compute their sum. Each cell t[i,j,X ]must store the log
probability of deriving wi+1:jfrom non-terminal X. To compute this, we replace the max-
imization over split points kand productions X→Y Z with a “log-sum-exp” operation,
which exponentiates the log probabilities of the production and the children, sums them
in probability space, and then converts back to the log domain:
t[i,j,X ] = log∑
k,Y,Zexp (ψ(X→Y Z) +t[i,k,Y ] +t[k,j,Z ]) [10.13]
= log∑
k,Y,Zexp (log p(Y Z|X) + log p(Y→wi+1:k) + log p(Z→wk+1:j))
[10.14]
= log∑
k,Y,Zp(Y Z|X)×p(Y→wi+1:k)×p(Z→wk+1:j) [10.15]
= log∑
k,Y,Zp(Y Z,wi+1:k,wk+1:j|X) [10.16]
= log p(X→wi+1:j). [10.17]
This is called the inside recurrence , because it computes the probability of each subtree 5208
as a combination of the probabilities of the smaller subtrees that are inside of it. The 5209
name implies a corresponding outside recurrence , which computes the probability of 5210
a non-terminal Xspanningwi+1:j, joint with the outside context (w1:i,wj+1:M). This 5211
recurrence is described in §10.4.3. The inside and outside recurrences are analogous to the 5212
forward and backward recurrences in probabilistic sequence labeling (see §7.5.3.3). They 5213
can be used to compute the marginal probabilities of individual anchored productions, 5214
p(X→α,(i,j,k )|w), summing over all possible derivations of w. 5215
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

10.4. LEARNING WEIGHTED CONTEXT-FREE GRAMMARS 245
10.3.3 *Semiring weighted context-free grammars 5216
The weighted and unweighted CKY algorithms can be uniﬁed with the inside recurrence
using the same semiring notation described in §7.7.3. The generalized recurrence is:
t[i,j,X ] =⨁
k,Y,Zψ(X→Y Z, (i,j,k ))⊗t[i,k,Y ]⊗t[k,j,Z ]. [10.18]
This recurrence subsumes all of the algorithms that we have encountered in this chapter. 5217
Unweighted CKY. Whenψ(X→α,(i,j,k ))is aBoolean truth value {⊤,⊥},⊗is logical 5218
conjunction, and⨁is logical disjunction, then we derive the CKY recurrence for 5219
unweighted context-free grammars, discussed in §10.1 and Algorithm 13. 5220
Weighted CKY. Whenψ(X→α,(i,j,k ))is a scalar score,⊗is addition, and⨁is maxi- 5221
mization, then we derive the CKY recurrence for weighted context-free grammars, 5222
discussed in§10.3 and Algorithm 14. When ψ(X→α,(i,j,k )) = log p(α|X), 5223
this same setting derives the CKY recurrence for ﬁnding the maximum likelihood 5224
derivation in a probabilistic context-free grammar. 5225
Inside recurrence. Whenψ(X→α,(i,j,k ))is a log probability, ⊗is addition, and⨁= 5226
log∑exp, then we derive the inside recurrence for probabilistic context-free gram- 5227
mars, discussed in §10.3.2. It is also possible to set ψ(X→α,(i,j,k ))directly equal 5228
to the probability p (α|X). In this case,⊗is multiplication, and⨁is addition. 5229
While this may seem more intuitive than working with logprobabilities, there is the 5230
risk of underﬂow on long inputs. 5231
Regardless of how the scores are combined, the key point is the locality assumption: 5232
the score for a derivation is the combination of the independent scores for each anchored 5233
production, and these scores do not depend on any other part of the derivation. For exam- 5234
ple, if two non-terminals are siblings, the scores of productions from these non-terminals 5235
are computed independently. This locality assumption is analogous to the ﬁrst-order 5236
Markov assumption in sequence labeling, where the score for transitions between tags 5237
depends only on the previous tag and current tag, and not on the history. As with se- 5238
quence labeling, this assumption makes it possible to ﬁnd the optimal parse efﬁciently; its 5239
linguistic limitations are discussed in §10.5. 5240
10.4 Learning weighted context-free grammars 5241
Like sequence labeling, context-free parsing is a form of structure prediction. As a result, 5242
WCFGs can be learned using the same set of algorithms: generative probabilistic models, 5243
structured perceptron, maximum conditional likelihood, and maximum margin learning. 5244
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

246 CHAPTER 10. CONTEXT-FREE PARSING
In all cases, learning requires a treebank , which is a dataset of sentences labeled with 5245
context-free parses. Parsing research was catalyzed by the Penn Treebank (Marcus et al., 5246
1993), the ﬁrst large-scale dataset of this type (see §9.2.2). Phrase structure treebanks exist 5247
for roughly two dozen other languages, with coverage mainly restricted to European and 5248
East Asian languages, plus Arabic and Urdu. 5249
10.4.1 Probabilistic context-free grammars 5250
Probabilistic context-free grammars are similar to hidden Markov models, in that they are
generative models of text. In this case, the parameters of interest correspond to probabil-
ities of productions, conditional on the left-hand side. As with hidden Markov models,
these parameters can be estimated by relative frequency:
ψ(X→α) = log p(X→α) [10.19]
ˆp(X→α) =count (X→α)
count (X). [10.20]
For example, the probability of the production NP →DETNNis the corpus count of 5251
this production, divided by the count of the non-terminal NP. This estimator applies 5252
to terminal productions as well: the probability of N N→whale is the count of how often 5253
whale appears in the corpus as generated from an N Ntag, divided by the total count of the 5254
NNtag. Even with the largest treebanks — currently on the order of one million tokens 5255
— it is difﬁcult to accurately compute probabilities of even moderately rare events, such 5256
as N N→whale . Therefore, smoothing is critical for making PCFGs effective. 5257
10.4.2 Feature-based parsing 5258
The scores for each production can be computed as an inner product of weights and fea- 5259
tures, 5260
ψ(X→α) =θ·f(X,α, (i,j,k ),w), [10.21]
where the feature vector f(X,α)is a function of the left-hand side X, the right-hand side 5261
α, the anchor indices (i,j,k ), and the input w. 5262
The basic feature f(X,α, (i,j,k )) ={(X,α)}encodes only the identity of the pro- 5263
duction itself, which is a discriminatively-trained model with the same expressiveness as 5264
a PCFG. Features on anchored productions can include the words that border the span 5265
wi,wj+1, the word at the split point wk+1, the presence of a verb or noun in the left child 5266
spanwi+1:k, and so on (Durrett and Klein, 2015). Scores on anchored productions can be 5267
incorporated into CKY parsing without any modiﬁcation to the algorithm, because it is 5268
still possible to compute each element of the table t[i,j,X ]recursively from its immediate 5269
children. 5270
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

10.4. LEARNING WEIGHTED CONTEXT-FREE GRAMMARS 247
Other features can be obtained by grouping elements on either the left-hand or right- 5271
hand side: for example it can be particularly beneﬁcial to compute additional features 5272
by clustering terminal symbols, with features corresponding to groups of words with 5273
similar syntactic properties. The clustering can be obtained from unlabeled datasets that 5274
are much larger than any treebank, improving coverage. Such methods are described in 5275
chapter 14. 5276
Feature-based parsing models can be estimated using the usual array of discrimina-
tive learning techniques. For example, a structure perceptron update can be computed
as (Carreras et al., 2008),
f(τ,w(i)) =∑
(X→α,(i,j,k))∈τf(X,α, (i,j,k ),w(i)) [10.22]
ˆτ= argmax
τ∈T(w)θ·f(τ,w(i)) [10.23]
θ←f(τ(i),w(i))−f(ˆτ,w(i)). [10.24]
A margin-based objective can be optimized by selecting ˆτthrough cost-augmented decod- 5277
ing (§2.3.2), enforcing a margin of ∆(ˆτ,τ)between the hypothesis and the reference parse, 5278
where ∆is a non-negative cost function, such as the Hamming loss (Stern et al., 2017). It 5279
is also possible to train feature-based parsing models by conditional log-likelihood, as 5280
described in the next section. 5281
10.4.3 *Conditional random ﬁeld parsing 5282
The score of a derivation Ψ(τ)can be converted into a probability by normalizing over all 5283
possible derivations, 5284
p(τ|w) =exp Ψ(τ)∑
τ′∈T(w)exp Ψ(τ′). [10.25]
Using this probability, a WCFG can be trained by maximizing the conditional log-likelihood 5285
of a labeled corpus. 5286
Just as in logistic regression and the conditional random ﬁeld over sequences, the 5287
gradient of the conditional log-likelihood is the difference between the observed and ex- 5288
pected counts of each feature. The expectation Eτ|w[f(τ,w(i));θ]requires summing over 5289
all possible parses, and computing the marginal probabilities of anchored productions, 5290
p(X→α,(i,j,k )|w). In CRF sequence labeling, marginal probabilities over tag bigrams 5291
are computed by the two-pass forward-backward algorithm (§7.5.3.3). The analogue for 5292
context-free grammars is the inside-outside algorithm , in which marginal probabilities 5293
are computed from terms generated by an upward and downward pass over the parsing 5294
chart: 5295
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

248 CHAPTER 10. CONTEXT-FREE PARSING
Y
Z
wj+1. . . w kX
wi+1. . . w jY
X
wi+1. . . w jZ
wk+1. . . w i
Figure 10.3: The two cases faced by the outside recurrence in the computation of β(i,j,X )
•The upward pass is performed by the inside recurrence , which is described in
§10.3.2. Each inside variable α(i,j,X )is the score of deriving wi+1:jfrom the non-
terminalX. In a PCFG, this corresponds to the log-probability logp(wi+1:j|X).
This is computed by the recurrence,
α(i,j,X )≜log∑
(X→Y Z)j∑
k=i+1exp (ψ(X→Y Z, (i,j,k )) +α(i,k,Y ) +α(k,j,Z )).
[10.26]
The initial condition of this recurrence is α(m−1,m,X ) =ψ(X→wm). The de- 5296
nominator∑
τ∈T(w)exp Ψ(τ)is equal to expα(0,M, S). 5297
•The downward pass is performed by the outside recurrence , which recursively pop-
ulates the same table structure, starting at the root of the tree. Each outside variable
β(i,j,X )is the score of having a phrase of type Xcovering the span (i+ 1 :j), joint
with the exterior context w1:iandwj+1:M. In a PCFG, this corresponds to the log
probability logp((X,i+ 1,j),w1:i,wj+1:M). Each outside variable is computed by
the recurrence,
expβ(i,j,X )≜∑
(Y→X Z)M∑
k=j+1exp [ψ(Y→X Z, (i,k,j )) +α(j,k,Z ) +β(i,k,Y )]
[10.27]
+∑
(Y→Z X)i−1∑
k=0exp [ψ(Y→ZX, (k,i,j )) +α(k,i,Z ) +β(k,j,Y )].
[10.28]
The ﬁrst line of Equation 10.28 is the score under the condition that Xis a left child 5298
of its parent, which spans wi+1:k, withk > j ; the second line is the score under the 5299
condition that Xis a right child of its parent Y, which spans wk+1:j, withk < i . 5300
The two cases are shown in Figure 10.3. In each case, we sum over all possible 5301
productions with Xon the right-hand side. The parent Yis bounded on one side 5302
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

10.4. LEARNING WEIGHTED CONTEXT-FREE GRAMMARS 249
by eitheriorj, depending on whether Xis a left or right child of Y; we must sum 5303
over all possible values for the other boundary. The initial conditions for the outside 5304
recurrence are β(0,M, S) = 0 andβ(0,M,X̸=S) =−∞. 5305
The marginal probability of a non-terminal Xover spanwi+1:jis written p (X⇝
wi+1:j|w), and can be computed from the inside and outside scores,
p(X⇝wi+1:j|w) =p(X⇝wi+1:j,w)
p(w)[10.29]
=p(wi+1:j|X)×p(X,w1:i,xj+1:M)
p(w)[10.30]
=exp (α(i,j,X ) +β(i,j,X ))
expα(0,M,S ). [10.31]
Marginal probabilities of individual productions can be computed similarly (see exercise 5306
2). These marginal probabilities can be used for training a conditional random ﬁeld parser, 5307
and also for the task of unsupervised grammar induction , in which a PCFG is estimated 5308
from a dataset of unlabeled text (Lari and Young, 1990; Pereira and Schabes, 1992). 5309
10.4.4 Neural context-free grammars 5310
Recent work has applied neural representations to parsing, representing each span with 5311
a dense numerical vector (Socher et al., 2013; Durrett and Klein, 2015; Cross and Huang, 5312
2016).4For example, the anchor (i,j,k )and sentence wcan be associated with a ﬁxed- 5313
length column vector, 5314
v(i,j,k)= [uwi−1;uwi;uwj−1;uwj;uwk−1;uwk], [10.32]
whereuwiis a word embedding associated with the word wi. The vectorvi,j,kcan then be
passed through a feedforward neural network, and used to compute the score of the an-
chored production. For example, this score can be computed as a bilinear product (Durrett
and Klein, 2015),
˜v(i,j,k)=FeedForward (v(i,j,k)) [10.33]
ψ(X→α,(i,j,k )) =˜v⊤
(i,j,k)Θf(X→α), [10.34]
wheref(X→α)is a vector of discrete features of the production, and Θis a parameter 5315
matrix. The matrix Θand the parameters of the feedforward network can be learned by 5316
backpropagating from an objective such as the margin loss or the negative conditional 5317
log-likelihood. 5318
4Earlier work on neural constituent parsing used transition-based parsing algorithms ( §10.6.2) rather
than CKY-style chart parsing (Henderson, 2004; Titov and Henderson, 2007).
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

250 CHAPTER 10. CONTEXT-FREE PARSING
S
VP
NP
PP
NP
NP
NNP
ItalyCC
andNP
NNP
FranceP
fromNN
wineV
likesNP
PRP
sheS
VP
NP
NP
NNP
ItalyCC
andNP
PP
NP
NNP
FranceP
fromNN
wineV
likesNP
PRP
she
Figure 10.4: The left parse is preferable because of the conjunction of phrases headed by
France and Italy , but these parses cannot be distinguished by a WCFG.
10.5 Grammar reﬁnement 5319
The locality assumptions underlying CFG parsing depend on the granularity of the non- 5320
terminals. For the Penn Treebank non-terminals, there are several reasons to believe that 5321
these assumptions are too strong to enable accurate parsing (Johnson, 1998): 5322
•The context-free assumption is too strict: for example, the probability of the produc- 5323
tion NP→NP PP is much higher (in the PTB) if the parent of the noun phrase is a 5324
verb phrase (indicating that the NP is a direct object) than if the parent is a sentence 5325
(indicating that the NP is the subject of the sentence). 5326
•The Penn Treebank non-terminals are too coarse: there are many kinds of noun 5327
phrases and verb phrases, and accurate parsing sometimes requires knowing the 5328
difference. As we have already seen, when faced with prepositional phrase at- 5329
tachment ambiguity, a weighted CFG will either always choose NP attachment (if 5330
ψ(NP→NP PP )> ψ(VP→VP PP )), or it will always choose VP attachment. To 5331
get more nuanced behavior, more ﬁne-grained non-terminals are needed. 5332
•More generally, accurate parsing requires some amount of semantics — understand- 5333
ing the meaning of the text to be parsed. Consider the example cats scratch people with 5334
claws : knowledge of about cats,claws , and scratching is necessary to correctly resolve 5335
the attachment ambiguity. 5336
An extreme example is shown in Figure 10.4. The analysis on the left is preferred 5337
because of the conjunction of similar entities France and Italy . But given the non-terminals 5338
shown in the analyses, there is no way to differentiate these two parses, since they include 5339
exactly the same productions. What is needed seems to be more precise non-terminals. 5340
One possibility would be to rethink the linguistics behind the Penn Treebank, and ask 5341
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

10.5. GRAMMAR REFINEMENT 251
S
VP
NP
NN
bearDT
theV
heardNP
sheS
VP-S
NP-VP
NN-NP
bearDT-NP
theVP-VP
heardNP-S
she
Figure 10.5: Parent annotation in a CFG derivation
the annotators to try again. But the original annotation effort took ﬁve years, and there 5342
is a little appetite for another annotation effort of this scope. Researchers have therefore 5343
turned to automated techniques. 5344
10.5.1 Parent annotations and other tree transformations 5345
The key assumption underlying context-free parsing is that productions depend only on
the identity of the non-terminal on the left-hand side, and not on its ancestors or neigh-
bors. The validity of this assumption is an empirical question, and it depends on the
non-terminals themselves: ideally, every noun phrase (and verb phrase, etc) would be
distributionally identical, so the assumption would hold. But in the Penn Treebank, the
observed probability of productions often depends on the parent of the left-hand side.
For example, noun phrases are more likely to be modiﬁed by prepositional phrases when
they are in the object position (e.g., they amused the students from Georgia ) than in the subject
position (e.g., the students from Georgia amused them ). This means that the NP →NP PP
production is more likely if the entire constituent is the child of a VP than if it is the child
of S. The observed statistics are (Johnson, 1998):
Pr(NP→NP PP ) =11% [10.35]
Pr(NP under S→NP PP ) =9% [10.36]
Pr(NP under VP→NP PP ) =23%. [10.37]
This phenomenon can be captured by parent annotation (Johnson, 1998), in which each 5346
non-terminal is augmented with the identity of its parent, as shown in Figure 10.5). This is 5347
sometimes called vertical Markovization , since a Markov dependency is introduced be- 5348
tween each node and its parent (Klein and Manning, 2003). It is analogous to moving from 5349
a bigram to a trigram context in a hidden Markov model. In principle, parent annotation 5350
squares the size of the set of non-terminals, which could make parsing considerably less 5351
efﬁcient. But in practice, the increase in the number of non-terminals that actually appear 5352
in the data is relatively modest (Johnson, 1998). 5353
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

252 CHAPTER 10. CONTEXT-FREE PARSING
Parent annotation weakens the WCFG locality assumptions. This improves accuracy 5354
by enabling the parser to make more ﬁne-grained distinctions, which better capture real 5355
linguistic phenomena. However, each production is more rare, and so careful smoothing 5356
or regularization is required to control the variance over production scores. 5357
10.5.2 Lexicalized context-free grammars 5358
The examples in §10.2.2 demonstrate the importance of individual words in resolving 5359
parsing ambiguity: the preposition onis more likely to attach to met, while the preposition 5360
ofis more likely to attachment to President . But of all word pairs, which are relevant to 5361
attachment decisions? Consider the following variants on the original examples: 5362
(10.4) We met the President of Mexico. 5363
(10.5) We met the ﬁrst female President of Mexico. 5364
(10.6) They had supposedly met the President on Monday. 5365
The underlined words are the head words of their respective phrases: metheads the verb 5366
phrase, and President heads the direct object noun phrase. These heads provide useful 5367
semantic information. But they break the context-free assumption, which states that the 5368
score for a production depends only on the parent and its immediate children, and not 5369
the substructure under each child. 5370
The incorporation of head words into context-free parsing is known as lexicalization ,
and is implemented in rules of the form,
NP(President )→NP(President )PP(of) [10.38]
NP(President )→NP(President )PP(on). [10.39]
Lexicalization was a major step towards accurate PCFG parsing. It requires solving three 5371
problems: identifying the heads of all constituents in a treebank; parsing efﬁciently while 5372
keeping track of the heads; and estimating the scores for lexicalized productions. 5373
10.5.2.1 Identifying head words 5374
The head of a constituent is the word that is the most useful for determining how that 5375
constituent is integrated into the rest of the sentence.5The head word of a constituent is 5376
determined recursively: for any non-terminal production, the head of the left-hand side 5377
must be the head of one of the children. The head is typically selected according to a set of 5378
deterministic rules, sometimes called head percolation rules . In many cases, these rules 5379
are straightforward: the head of a noun phrase in a NP →DETNNproduction is the head 5380
5This is a pragmatic deﬁnition, beﬁtting our goal of using head words to improve parsing; for a more
formal deﬁnition, see (Bender, 2013, chapter 7).
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

10.5. GRAMMAR REFINEMENT 253
VP(meet)
PP(on)
NP
NN
MondayP
onNP(President)
NN
PresidentDT
theVB
meetVP(meet)
NP(President)
PP(of)
NP
NN
MexicoP
ofNP(President)
NN
PresidentDT
theVB
meet
(a) Lexicalization and attachment ambiguity
NP(Italy)
NP(Italy)
NNS
ItalyCC
andNP(wine)
PP(from)
NP(France)
NNP
FranceIN
fromNP(wine)
NN
wineNP(wine)
PP(from)
NP(Italy)
NP(Italy)
NNS
ItalyCC
andNP(France)
NNP
FranceIN
fromNP(wine)
NN
wine
(b) Lexicalization and coordination scope ambiguity
Figure 10.6: Examples of lexicalization
of the noun; the head of a sentence in a S →NP VP production is the head of the verb 5381
phrase. 5382
Table 10.3 shows a fragment of the head percolation rules used in many English pars- 5383
ing systems. The meaning of the ﬁrst rule is that to ﬁnd the head of an S constituent, ﬁrst 5384
look for the rightmost VP child; if you don’t ﬁnd one, then look for the rightmost SBAR 5385
child, and so on down the list. Verb phrases are headed by left verbs (the head of can plan 5386
on walking isplanned , since the modal verb canis tagged M D); noun phrases are headed by 5387
the rightmost noun-like non-terminal (so the head of the red cat iscat),6and prepositional 5388
phrases are headed by the preposition (the head of at Georgia Tech isat). Some of these 5389
rules are somewhat arbitrary — there’s no particular reason why the head of cats and dogs 5390
should be dogs — but the point here is just to get some lexical information that can support 5391
parsing, not to make deep claims about syntax. Figure 10.6 shows the application of these 5392
rules to two of the running examples. 5393
6The noun phrase non-terminal is sometimes treated as a special case. Collins (1997) uses a heuristic that
looks for the rightmost child which is a noun-like part-of-speech (e.g., N N, NNP), a possessive marker, or a
superlative adjective (e.g., the greatest ). If no such child is found, the heuristic then looks for the leftmost NP.
If there is no child with tag NP, the heuristic then applies another priority list, this time from right to left.
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

254 CHAPTER 10. CONTEXT-FREE PARSING
Non-terminal Direction Priority
S right VP SBAR ADJP UCP NP
VP left VBD VBN MD VBZ TO VB VP VBG VBP ADJP NP
NP right N* EX $ CD QP PRP . . .
PP left IN TO FW
Table 10.3: A fragment of head percolation rules for English, from http://www.cs.
columbia.edu/ ˜mcollins/papers/heads
10.5.2.2 Parsing lexicalized context-free grammars 5394
A na ¨ıve application of lexicalization would simply increase the set of non-terminals by 5395
taking the cross-product with the set of terminal symbols, so that the non-terminals now 5396
include symbols like NP( President ) and VP( meet ). Under this approach, the CKY parsing 5397
algorithm could be applied directly to the lexicalized production rules. However, the 5398
complexity would be cubic in the size of the vocabulary of terminal symbols, which would 5399
clearly be intractable. 5400
Another approach is to augment the CKY table with an additional index, keeping track
of the head of each constituent. The cell t[i,j,h,X ]stores the score of the best derivation in
which non-terminal Xspanswi+1:jwith head word h, wherei<h≤j. To compute such
a table recursively, we must consider the possibility that each phrase gets its head from
either its left or right child. The scores of the best derivations in which the head comes
from the left and right child are denoted tℓandtrrespectively, leading to the following
recurrence:
tℓ[i,j,h,X ] = max
(X→YZ)max
k>hmax
k<h′≤jt[i,k,h,Y ] +t[k,j,h′,Z] +ψ(X(h)→Y(h)Z(h′))
[10.40]
tr[i,j,h,X ] = max
(X→YZ)max
k<hmax
i<h′≤kt[i,k,h′,Y] +t[k,j,h,Z ] + (ψ(X(h)→Y(h′)Z(h)))
[10.41]
t[i,j,h,X ] = max (tℓ[i,j,h,X ],tr[i,j,h,X ]). [10.42]
To compute tℓ, we maximize over all split points k>h , since the head word must be in 5401
the left child. We then maximize again over possible head words h′for the right child. An 5402
analogous computation is performed for tr. The size of the table is now O(M3N), where 5403
Mis the length of the input and Nis the number of non-terminals. Furthermore, each 5404
cell is computed by performing O(M2)operations, since we maximize over both the split 5405
pointkand the head h′. The time complexity of the algorithm is therefore O(RM5N), 5406
whereRis the number of rules in the grammar. Fortunately, more efﬁcient solutions are 5407
possible. In general, the complexity of parsing can be reduced to O(M4)in the length of 5408
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

10.5. GRAMMAR REFINEMENT 255
the input; for a broad class of lexicalized CFGs, the complexity can be made cubic in the 5409
length of the input, just as in unlexicalized CFGs (Eisner, 2000). 5410
10.5.2.3 Estimating lexicalized context-free grammars 5411
The ﬁnal problem for lexicalized parsing is how to estimate weights for lexicalized pro- 5412
ductionsX(i)→Y(j)Z(k). These productions are said to be bilexical , because they 5413
involve scores over pairs of words: in the example meet the President of Mexico , we hope 5414
to choose the correct attachment point by modeling the bilexical afﬁnities of (meet,of)and 5415
(President,of). The number of such word pairs is quadratic in the size of the vocabulary, 5416
making it difﬁcult to estimate the weights of lexicalized production rules directly from 5417
data. This is especially true for probabilistic context-free grammars, in which the weights 5418
are obtained from smoothed relative frequency. In a treebank with a million tokens, a 5419
vanishingly small fraction of the possible lexicalized productions will be observed more 5420
than once.7The Charniak (1997) and Collins (1997) parsers therefore focus on approxi- 5421
mating the probabilities of lexicalized productions, using various smoothing techniques 5422
and independence assumptions. 5423
In discriminatively-trained weighted context-free grammars, the scores for each pro-
duction can be computed from a set of features, which can be made progressively more
ﬁne-grained (Finkel et al., 2008). For example, the score of the lexicalized production
NP(President )→NP(President )PP(of)can be computed from the following features:
f(NP(President )→NP(President )PP(of)) ={NP(*)→NP(*)PP(*),
NP(President )→NP(President )PP(*),
NP(*)→NP(*)PP(of),
NP(President )→NP(President )PP(of)}
The ﬁrst feature scores the unlexicalized production NP →NP PP; the next two features 5424
lexicalize only one element of the production, thereby scoring the appropriateness of NP 5425
attachment for the individual words President and of; the ﬁnal feature scores the speciﬁc 5426
bilexical afﬁnity of President and of. For bilexical pairs that are encountered frequently in 5427
the treebank, this bilexical feature can play an important role in parsing; for pairs that are 5428
absent or rare, regularization will drive its weight to zero, forcing the parser to rely on the 5429
more coarse-grained features. 5430
In chapter 14, we will encounter techniques for clustering words based on their distri- 5431
butional properties — the contexts in which they appear. Such a clustering would group 5432
rare and common words, such as whale, shark, beluga, Leviathan . Word clusters can be used 5433
7The real situation is even more difﬁcult, because non-binary context-free grammars can involve trilexical
or higher-order dependencies, between the head of the constituent and multiple of its children (Carreras et al.,
2008).
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

256 CHAPTER 10. CONTEXT-FREE PARSING
as features in discriminative lexicalized parsing, striking a middle ground between full 5434
lexicalization and non-terminals (Finkel et al., 2008). In this way, labeled examples con- 5435
taining relatively common words like whale can help to improve parsing for rare words 5436
likebeluga , as long as those two words are clustered together. 5437
10.5.3 *Reﬁnement grammars 5438
Lexicalization improves on context-free parsing by adding detailed information in the 5439
form of lexical heads. However, estimating the scores of lexicalized productions is dif- 5440
ﬁcult. Klein and Manning (2003) argue that the right level of linguistic detail is some- 5441
where between treebank categories and individual words. Some parts-of-speech and non- 5442
terminals are truly substitutable: for example, cat/N and dog/N. But others are not: for 5443
example, the preposition ofexclusively attaches to nouns, while the preposition asis more 5444
likely to modify verb phrases. Klein and Manning (2003) obtained a 2% improvement in 5445
F-MEASURE on a parent-annotated PCFG parser by making a single change: splitting the 5446
preposition category into six subtypes. They propose a series of linguistically-motivated 5447
reﬁnements to the Penn Treebank annotations, which in total yielded a 40% error reduc- 5448
tion. 5449
Non-terminal reﬁnement process can be automated by treating the reﬁned categories 5450
as latent variables. For example, we might split the noun phrase non-terminal into NP1, NP2, NP3, . . . , 5451
without deﬁning in advance what each reﬁned non-terminal corresponds to. This can 5452
be treated as partially supervised learning , similar to the multi-component document 5453
classiﬁcation model described in §5.2.3. A latent variable PCFG can be estimated by 5454
expectation-maximization (Matsuzaki et al., 2005): 5455
•In the E-step, estimate a marginal distribution qover the reﬁnement type of each 5456
non-terminal in each derivation. These marginals are constrained by the original 5457
annotation: an NP can be reannotated as NP4, but not as VP3. Marginal probabil- 5458
ities over reﬁned productions can be computed from the inside-outside algorithm , 5459
as described in§10.4.3, where the E-step enforces the constraints imposed by the 5460
original annotations. 5461
•In the M-step, recompute the parameters of the grammar, by summing over the 5462
probabilities of anchored productions that were computed in the E-step: 5463
E[count (X→Y Z)] =M∑
i=0M∑
j=ij∑
k=ip(X→Y Z, (i,j,k )|w). [10.43]
As usual, this process can be iterated to convergence. To determine the number of re- 5464
ﬁnement types for each tag, Petrov et al. (2006) apply a split-merge heuristic; Liang et al. 5465
(2007) and Finkel et al. (2007) apply Bayesian nonparametrics (Cohen, 2016). 5466
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

10.6. BEYOND CONTEXT-FREE PARSING 257
Proper nouns
NNP-14 Oct. Nov. Sept.
NNP-12 John Robert James
NNP-2 J. E. L.
NNP-1 Bush Noriega Peters
NNP-15 New San Wall
NNP-3 York Francisco Street
Personal Pronouns
PRP-0 It He I
PRP-1 it he they
PRP-2 it them him
Table 10.4: Examples of automatically reﬁned non-terminals and some of the words that
they generate (Petrov et al., 2006).
Some examples of reﬁned non-terminals are shown in Table 10.4. The proper nouns 5467
differentiate months, ﬁrst names, middle initials, last names, ﬁrst names of places, and 5468
second names of places; each of these will tend to appear in different parts of grammatical 5469
productions. The personal pronouns differentiate grammatical role, with PRP-0 appear- 5470
ing in subject position at the beginning of the sentence (note the capitalization), PRP-1 5471
appearing in subject position but not at the beginning of the sentence, and PRP-2 appear- 5472
ing in object position. 5473
10.6 Beyond context-free parsing 5474
In the context-free setting, the score for a parse is a combination of the scores of individual 5475
productions. As we have seen, these models can be improved by using ﬁner-grained non- 5476
terminals, via parent-annotation, lexicalization, and automated reﬁnement. However, the 5477
inherent limitations to the expressiveness of context-free parsing motivate the consider- 5478
ation of other search strategies. These strategies abandon the optimality guaranteed by 5479
bottom-up parsing, in exchange for the freedom to consider arbitrary properties of the 5480
proposed parses. 5481
10.6.1 Reranking 5482
A simple way to relax the restrictions of context-free parsing is to perform a two-stage pro- 5483
cess, in which a context-free parser generates a k-best list of candidates, and a reranker 5484
then selects the best parse from this list (Charniak and Johnson, 2005; Collins and Koo, 5485
2005). The reranker can be trained from an objective that is similar to multi-class classi- 5486
ﬁcation: the goal is to learn weights that assign a high score to the reference parse, or to 5487
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

258 CHAPTER 10. CONTEXT-FREE PARSING
the parse on the k-best list that has the lowest error. In either case, the reranker need only 5488
evaluate the Kbest parses, and so no context-free assumptions are necessary. This opens 5489
the door to more expressive scoring functions: 5490
•It is possible to incorporate arbitrary non-local features, such as the structural par- 5491
allelism and right-branching orientation of the parse (Charniak and Johnson, 2005). 5492
•Reranking enables the use of recursive neural networks , in which each constituent 5493
spanwi+1:jreceives a vector ui,jwhich is computed from the vector representa- 5494
tions of its children, using a composition function that is linked to the production 5495
rule (Socher et al., 2013), e.g., 5496
ui,j=f(
ΘX→Y Z[ui,k
uk,j])
[10.44]
The overall score of the parse can then be computed from the ﬁnal vector, Ψ(τ) = 5497
θu0,M. 5498
Reranking can yield substantial improvements in accuracy. The main limitation is that it 5499
can only ﬁnd the best parse among the K-best offered by the generator, so it is inherently 5500
limited by the ability of the bottom-up parser to ﬁnd high-quality candidates. 5501
10.6.2 Transition-based parsing 5502
Structure prediction can be viewed as a form of search. An alternative to bottom-up pars- 5503
ing is to read the input from left-to-right, gradually building up a parse structure through 5504
a series of transitions . Transition-based parsing is described in more detail in the next 5505
chapter, in the context of dependency parsing. However, it can also be applied to CFG 5506
parsing, as brieﬂy described here. 5507
For any context-free grammar, there is an equivalent pushdown automaton , a model 5508
of computation that accepts exactly those strings that can be derived from the grammar. 5509
This computational model consumes the input from left to right, while pushing and pop- 5510
ping elements on a stack. This architecture provides a natural transition-based parsing 5511
framework for context-free grammars, known as shift-reduce parsing . 5512
Shift-reduce parsing is a type of transition-based parsing, in which the parser can take 5513
the following actions: 5514
•shift the next terminal symbol onto the stack; 5515
•unary-reduce the top item on the stack, using a unary production rule in the gram- 5516
mar; 5517
•binary-reduce the top two items onto the stack, using a binary production rule in the 5518
grammar. 5519
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

10.6. BEYOND CONTEXT-FREE PARSING 259
The set of available actions is constrained by the situation: the parser can only shift if 5520
there are remaining terminal symbols in the input, and it can only reduce if an applicable 5521
production rule exists in the grammar. If the parser arrives at a state where the input 5522
has been completely consumed, and the stack contains only the element S, then the input 5523
is accepted. If the parser arrives at a non-accepting state where there are no possible 5524
actions, the input is rejected. A parse error occurs if there is some action sequence that 5525
would accept an input, but the parser does not ﬁnd it. 5526
Example Consider the input we eat sushi and the grammar in Table 10.1. The input can 5527
be parsed through the following sequence of actions: 5528
1.Shift the ﬁrst token weonto the stack. 5529
2.Reduce the top item on the stack to NP, using the production NP →we. 5530
3.Shift the next token eatonto the stack, and reduce it to V with the production V → 5531
eat. 5532
4.Shift the ﬁnal token sushi onto the stack, and reduce it to NP. The input has been 5533
completely consumed, and the stack contains [NP,V,NP]. 5534
5.Reduce the top two items using the production VP →V NP. The stack now con- 5535
tains [VP,NP]. 5536
6.Reduce the top two items using the production S →NP VP. The stack now contains 5537
[S]. Since the input is empty, this is an accepting state. 5538
One thing to notice from this example is that the number of shift actions is equal to the 5539
length of the input. The number of reduce actions is equal to the number of non-terminals 5540
in the analysis, which grows linearly in the length of the input. Thus, the overall time 5541
complexity of shift-reduce parsing is linear in the length of the input (assuming the com- 5542
plexity of each individual classiﬁcation decision is constant in the length of the input). 5543
This is far better than the cubic time complexity required by CKY parsing. 5544
Transition-based parsing as inference In general, it is not possible to guarantee that 5545
a transition-based parser will ﬁnd the optimal parse, argmaxτΨ(τ;w), even under the 5546
usual CFG independence assumptions. We could assign a score to each anchored parsing 5547
action in each context, with ψ(a,c)indicating the score of performing action ain contextc. 5548
One might imagine that transition-based parsing could efﬁciently ﬁnd the derivation that 5549
maximizes the sum of such scores. But this too would require backtracking and searching 5550
over an exponentially large number of possible action sequences: if a bad decision is 5551
made at the beginning of the derivation, then it may be impossible to recover the optimal 5552
action sequence without backtracking to that early mistake. This is known as a search 5553
error . Transition-based parsers can incorporate arbitrary features, without the restrictive 5554
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

260 CHAPTER 10. CONTEXT-FREE PARSING
independence assumptions required by chart parsing; search errors are the price that must 5555
be paid for this ﬂexibility. 5556
Learning transition-based parsing Transition-based parsing can be combined with ma- 5557
chine learning by training a classiﬁer to select the correct action in each situation. This 5558
classiﬁer is free to choose any feature of the input, the state of the parser, and the parse 5559
history. However, there is no optimality guarantee: the parser may choose a suboptimal 5560
parse, due to a mistake at the beginning of the analysis. Nonetheless, some of the strongest 5561
CFG parsers are based on the shift-reduce architecture, rather than CKY. A recent gener- 5562
ation of models links shift-reduce parsing with recurrent neural networks, updating a 5563
hidden state vector while consuming the input (e.g., Cross and Huang, 2016; Dyer et al., 5564
2016). Learning algorithms for transition-based parsing are discussed in more detail in 5565
§11.3. 5566
Exercises 5567
1. Consider the following PCFG:
p(X→X X) =1
2[10.45]
p(X→Y) =1
2[10.46]
p(Y→σ) =1
|Σ|,∀σ∈Σ [10.47]
a) Compute the probability p (ˆτ)of the maximum probability parse for a string 5568
w∈ΣM. 5569
b) Compute the marginal probability p (w) =∑
τ:yield (τ)=wp(τ). 5570
c) Compute the conditional probability p (ˆτ|w). 5571
2. Use the inside and outside scores to compute the marginal probability p (Xi:j→Yi:k−1Zk:j|w), 5572
indicating that Yspanswi:k−1,Zspanswk:j, andXis the parent of YandZ, span- 5573
ningwi:j. 5574
3. Suppose that the potentials Ψ(X→α)are log-probabilities, so that∑
αexp Ψ(X→α) = 1 5575
for allX. Verify that the semiring inside recurrence from Equation 10.26 generates 5576
the log-probability logp(w) = log∑
τ:yield (τ)=wp(τ). 5577
4. more exercises tk 5578
(c) Jacob Eisenstein 2018. Draft of June 1, 2018.

